[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v3",
                "updated": "2025-07-15T12:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    59,
                    47,
                    1,
                    196,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11273v1",
                "updated": "2025-07-15T12:52:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T12:52:12Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    52,
                    12,
                    1,
                    196,
                    0
                ],
                "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware\n  Rotary Positional Embedding"
                },
                "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v3",
                "updated": "2025-07-15T11:31:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    11,
                    31,
                    14,
                    1,
                    196,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index. This paper introduces VSAG, an\nopen-source framework that aims to enhance the in production performance of\ngraph-based ANNS algorithms. VSAG has been deployed at scale in the services of\nAnt Group, and it incorporates three key optimizations: (i) efficient memory\naccess: it reduces L3 cache misses with pre-fetching and cache-friendly vector\norganization; (ii) automated parameter tuning: it automatically selects\nperformance-optimal parameters without requiring index rebuilding; (iii)\nefficient distance computation: it leverages modern hardware, scalar\nquantization, and smartly switches to low-precision representation to\ndramatically reduce the distance computation costs. We evaluate VSAG on\nreal-world datasets. The experimental results show that VSAG achieves the\nstate-of-the-art performance and provides up to 4x speedup over HNSWlib (an\nindustry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag) accepted by VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11121v1",
                "updated": "2025-07-15T09:15:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T09:15:18Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    15,
                    18,
                    1,
                    196,
                    0
                ],
                "title": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional single-crystal photonic scintillator for enhanced X-ray\n  imaging"
                },
                "summary": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of X-ray detection technology has significantly enhanced\nsensitivity and spatial resolution in non-destructive imaging of internal\nstructure. However, the problem of low luminescence and transparency of\nscintillator materials restricts imaging with lower radiation doses and thicker\nmaterials. Here, we propose a two-dimensional photonic scintillator for single\ncrystal and demonstrate that the optical guiding effect emerging from the\nstructure reduces luminescence leakage and increases the signal intensity by\naround a factor of 2 from 200 to 450 kV. This approach has the potential to\nenhance the output rate by an order of magnitude. The photonic structure\nfeatures a fine array pitch and large-scale detection area with fast\nfabrication time. Our scheme paves the way for high sensitivity X-ray imaging."
                },
                "authors": [
                    {
                        "name": "Tatsunori Shibuya"
                    },
                    {
                        "name": "Eichi Terasawa"
                    },
                    {
                        "name": "Hiromi Kimura"
                    },
                    {
                        "name": "Takeshi Fujiwara"
                    }
                ],
                "author_detail": {
                    "name": "Takeshi Fujiwara"
                },
                "author": "Takeshi Fujiwara",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11067v1",
                "updated": "2025-07-15T08:00:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T08:00:11Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    8,
                    0,
                    11,
                    1,
                    196,
                    0
                ],
                "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix\n  Unit"
                },
                "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version."
                },
                "authors": [
                    {
                        "name": "Yinuo Wang"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Lin Gan"
                    },
                    {
                        "name": "Wubing Wan"
                    },
                    {
                        "name": "Zeyu Song"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Lanke He"
                    },
                    {
                        "name": "Wenqiang Wang"
                    },
                    {
                        "name": "Zekun Yin"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Guangwen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guangwen Yang"
                },
                "author": "Guangwen Yang",
                "arxiv_comment": "Yinuo Wang and Tianqi Mao contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v1",
                "updated": "2025-07-14T20:38:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v2",
                "updated": "2025-07-14T19:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    51,
                    9,
                    0,
                    195,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, an limitation that has kept FUSE distributed file systems\nout of write-intensive cloud workloads.\n  To this end, We present DistFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DistFUSE achieves\nthis by offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DistFUSE\nachieves up to 68.0% higher throughput and 40.4% lower latency than the\nexisting write-through design of FUSE-based distributed file system."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10757v1",
                "updated": "2025-07-14T19:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T19:31:06Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    19,
                    31,
                    6,
                    0,
                    195,
                    0
                ],
                "title": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAFO: Over 1 million TPS on a single node running EVM while still\n  Merkleizing every block"
                },
                "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo."
                },
                "authors": [
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v4",
                "updated": "2025-07-14T18:22:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    18,
                    22,
                    53,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compound AI systems, such as agentic systems, are an emerging trend in\nlarge-scale enterprise settings, with multiple LLMs specialized for different\nusers, tasks, and/or roles working together. In these scenarios, different\nmodels often process inputs that share the same context prefix. Although much\nwork was done in the past to enable the reuse of prefix KV caches across inputs\nfor a single model, how to enable one model to reuse the prefix KV caches of a\ndifferent model remains an open question.\n  We introduce DroidSpeak, the first distributed LLM inference system that\nenables KV cache reuse across distributed nodes running inference of different\nLLMs, so long as the LLMs have the same architecture. We present the first\nstudy that aims at understanding the impact of sharing KV caches across\ndifferent LLMs, and if/when such sharing affects quality. Inspired by the\nfindings, we present DroidSpeak, which selectively recomputes a few layers of\nthe KV cache produced by another LLM and reuses the remaining layers, with\nnegligible quality loss. Moreover, carefully pipelining the layer-wise\nre-computation and the loading of reused KV cache further improves the\ninference performance. Experiments on diverse datasets and model pairs\ndemonstrate that DroidSpeak achieves up to 4x throughput improvement and about\n3.1x faster prefill (time to first token), with negligible loss of quality in\nF1 scores, Rouge-L or code similarity score, compared to the baseline which\ndoes not allow any sharing across models."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v1",
                "updated": "2025-07-14T17:49:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v3",
                "updated": "2025-07-14T16:14:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    14,
                    49,
                    0,
                    195,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at https://github.com/THU-MIG/PrefixKV."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jiaxin Li"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v1",
                "updated": "2025-07-14T15:09:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v2",
                "updated": "2025-07-14T09:45:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    9,
                    45,
                    34,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning the Tree: Rethinking RPKI Architecture From The Ground Up"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nwhich introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70\\% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "arxiv_comment": "Accepted for publication at NDSS2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v1",
                "updated": "2025-07-14T08:53:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v3",
                "updated": "2025-07-14T07:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    5,
                    28,
                    0,
                    195,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02814v2",
                "updated": "2025-07-14T07:03:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    7,
                    3,
                    30,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-05T05:22:14Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    22,
                    14,
                    1,
                    310,
                    0
                ],
                "title": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hitchhiker's Guide to Programming and Optimizing Cache Coherent\n  Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric"
                },
                "summary": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a thorough analysis of the use of modern heterogeneous systems\ninterconnected by various cachecoherent links, including CXL, NVLink-C2C, and\nInfinity Fabric. We studied a wide range of server systems that combined CPUs\nfrom different vendors and various types of coherent memory devices, including\nCXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a\nHBM. For this study, we developed a heterogeneous memory benchmark suite,\nHeimdall, to profile the performance of such heterogeneous systems and present\na detailed performance comparison across systems. By leveraging H E I M DA L L\n, we unveiled the detailed architecture design in these systems, drew\nobservations on optimizing performance for workloads, and pointed out\ndirections for future development of cache coherent heterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Luyi Li"
                    },
                    {
                        "name": "Jangseon Park"
                    },
                    {
                        "name": "Jinpyo Kim"
                    },
                    {
                        "name": "Theodore Michailidis"
                    },
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Dean Tullsen"
                    },
                    {
                        "name": "Steven Swanson"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v2",
                "updated": "2025-07-14T02:22:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    14,
                    2,
                    22,
                    43,
                    0,
                    195,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The revolutionary capabilities of Large Language Models (LLMs) are attracting\nrapidly growing popularity and leading to soaring user requests to inference\nserving systems. Caching techniques, which leverage data reuse to reduce\ncomputation, offer opportunities to optimize the performance of LLM inference\nengines. On the one hand, the low-level key-value (KV) cache working at the\ntoken level is widely adopted, albeit it incurs significant overhead as request\nvolume grows. On the other hand, instruction-level caching, which stores full\ninstruction-response pairs, is expected to play an increasingly crucial role.\nHowever, the high variability in the content and length of instructions make it\nrare for identical instructions to recur within a short time window, presenting\nchallenges for effective caching instruction-response pairs. To address this\nchallenge, we propose InstCache, a predictive caching mechanism for LLM serving\nsystems. Leveraging the capability of LLMs, we can effectively reorder the\nrepresentation space of instruction texts and develop a sufficient level of\nspatial locality. Such spatial locality enables us to predict potential\ninstructions located in a compact region in the space, resulting in an\neffective caching system at runtime. Experimental results demonstrate that\nInstCache achieves a 2.3x higher hit rate compared to the upper bound of\ntraditional caching mechanisms on WildChat dataset and reduces the time per\noutput token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Jiamu Kang"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v1",
                "updated": "2025-07-13T05:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v2",
                "updated": "2025-07-13T04:42:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    13,
                    4,
                    42,
                    28,
                    6,
                    194,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v4",
                "updated": "2025-07-11T22:14:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    22,
                    14,
                    1,
                    4,
                    192,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by TPAMI 2025. arXiv admin note: substantial text overlap\n  with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v1",
                "updated": "2025-07-11T21:19:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v3",
                "updated": "2025-07-11T19:57:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    19,
                    57,
                    51,
                    4,
                    192,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HotSwap: Enabling Live Dependency Sharing in Serverless Computing"
                },
                "summary": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents HotSwap, a novel provider-side cold-start optimization for\nserverless computing. This optimization reduces cold-start time when booting\nand loading dependencies at runtime inside a function container. Previous\nresearch has extensively focused on reducing cold-start latency for specific\nfunctions. However, little attention has been given to skewed production\nworkloads. In such cases, cross-function optimization becomes essential.\nWithout cross-function optimization, a cloud provider is left with two equally\npoor options: (i) Either the cloud provider gives up optimization for each\nfunction in the long tail (which is slow); or (ii) the cloud provider applies\nfunction-specific optimizations (e.g., cache function images) to every function\nin the long tail (which violates the vendor's cache constraints). HotSwap\ndemonstrates cross-function optimization using a novel pre-warming strategy. In\nthis strategy, a pre-initialized live dependency image is migrated to the new\nfunction instance. At the same time, HotSwap respects the provider's cache\nconstraints, because a single pre-warmed dependency image in the cache can be\nshared among all serverless functions that require that image. HotSwap has been\ntested on seven representative functions from FunctionBench. In those tests,\nHotSwap accelerates dependency loading for those serverless functions with\nlarge dependency requirements by a factor ranging from 2.2 to 3.2. Simulation\nexperiments using Azure traces indicate that HotSwap can save 88\\% of space,\ncompared with a previous function-specific method, PreBaking, when sharing a\ndependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "10 pages, 7 figures. This work was accepted at the IEEE International\n  Conference on Cloud Computing 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v1",
                "updated": "2025-07-11T17:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Inducing Reasoning in Small Language Models"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08717v1",
                "updated": "2025-07-11T16:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T16:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    16,
                    17,
                    46,
                    4,
                    192,
                    0
                ],
                "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System\n  Design"
                },
                "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system."
                },
                "authors": [
                    {
                        "name": "Akshay Jain"
                    },
                    {
                        "name": "Sylvaine Kerboeuf"
                    },
                    {
                        "name": "Sokratis Barmpounakis"
                    },
                    {
                        "name": "Cristbal Vinagre Z."
                    },
                    {
                        "name": "Stefan Wendt"
                    },
                    {
                        "name": "Dinh Thai Bui"
                    },
                    {
                        "name": "Pol Alemany"
                    },
                    {
                        "name": "Riccardo Nicolicchia"
                    },
                    {
                        "name": "Jos Mara Jorquera Valero"
                    },
                    {
                        "name": "Dani Korpi"
                    },
                    {
                        "name": "Mohammad Hossein Moghaddam"
                    },
                    {
                        "name": "Mikko A. Uusitalo"
                    },
                    {
                        "name": "Patrik Rugeland"
                    },
                    {
                        "name": "Abdelkader Outtagarts"
                    },
                    {
                        "name": "Karthik Upadhya"
                    },
                    {
                        "name": "Panagiotis Demestichas"
                    },
                    {
                        "name": "Raul Muoz"
                    },
                    {
                        "name": "Manuel Gil Prez"
                    },
                    {
                        "name": "Daniel Adanza"
                    },
                    {
                        "name": "Ricard Vilalta"
                    }
                ],
                "author_detail": {
                    "name": "Ricard Vilalta"
                },
                "author": "Ricard Vilalta",
                "arxiv_comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v9",
                "updated": "2025-07-11T14:27:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    27,
                    25,
                    4,
                    192,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08607v1",
                "updated": "2025-07-11T14:02:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T14:02:54Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    14,
                    2,
                    54,
                    4,
                    192,
                    0
                ],
                "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language\n  Models via Gaussian Discriminant Analysis"
                },
                "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}."
                },
                "authors": [
                    {
                        "name": "Shuang Cui"
                    },
                    {
                        "name": "Jinglin Xu"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Xiongxin Tang"
                    },
                    {
                        "name": "Jiangmeng Li"
                    },
                    {
                        "name": "Jiahuan Zhou"
                    },
                    {
                        "name": "Fanjiang Xu"
                    },
                    {
                        "name": "Fuchun Sun"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v1",
                "updated": "2025-07-11T12:21:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10579v1",
                "updated": "2025-07-11T10:57:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T10:57:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    57,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment\n  of AI-powered Tutors"
                },
                "summary": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This shared task has aimed to assess pedagogical abilities of AI tutors\npowered by large language models (LLMs), focusing on evaluating the quality of\ntutor responses aimed at student's mistake remediation within educational\ndialogues. The task consisted of five tracks designed to automatically evaluate\nthe AI tutor's performance across key dimensions of mistake identification,\nprecise location of the mistake, providing guidance, and feedback\nactionability, grounded in learning science principles that define good and\neffective tutor responses, as well as the track focusing on detection of the\ntutor identity. The task attracted over 50 international teams across all\ntracks. The submitted models were evaluated against gold-standard human\nannotations, and the results, while promising, show that there is still\nsignificant room for improvement in this domain: the best results for the four\npedagogical ability assessment tracks range between macro F1 scores of 58.34\n(for providing guidance) and 71.81 (for mistake identification) on three-class\nproblems, with the best F1 score in the tutor identification track reaching\n96.98 on a 9-class task. In this paper, we overview the main findings of the\nshared task, discuss the approaches taken by the teams, and analyze their\nperformance. All resources associated with this task are made publicly\navailable to support future research in this critical domain."
                },
                "authors": [
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Anas Tack"
                    },
                    {
                        "name": "Justin Vasselli"
                    }
                ],
                "author_detail": {
                    "name": "Justin Vasselli"
                },
                "author": "Justin Vasselli",
                "arxiv_comment": "Proceedings of the 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08432v1",
                "updated": "2025-07-11T09:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:18:41Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    18,
                    41,
                    4,
                    192,
                    0
                ],
                "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented\n  Generation and Large Language Models"
                },
                "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency."
                },
                "authors": [
                    {
                        "name": "Gustavo Correa Publio"
                    },
                    {
                        "name": "Jos Emilio Labra Gayo"
                    }
                ],
                "author_detail": {
                    "name": "Jos Emilio Labra Gayo"
                },
                "author": "Jos Emilio Labra Gayo",
                "arxiv_comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v1",
                "updated": "2025-07-11T09:07:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08278v1",
                "updated": "2025-07-11T02:57:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T02:57:44Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    2,
                    57,
                    44,
                    4,
                    192,
                    0
                ],
                "title": "Observation of the electric Breit-Rabi Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of the electric Breit-Rabi Effect"
                },
                "summary": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The response of an atom to external electric and magnetic fields can reveal\nfundamental atomic properties. It has long been verified that, in a static\nmagnetic field, those atomic energy levels with hyperfine interactions shift\naccording to the Breit-Rabi formula, which introduces nonlinear dependence on\nthe magnetic field. On the other hand, the corresponding Breit-Rabi dependence\non a static electric field has not been observed before due to a combination of\nexperimental challenges. Here we precisely measure the Stark shift of the\n$6s^2\\ ^1S_0\\ \\leftrightarrow\\ 6s6p\\ ^1P_1$ transition of $^{171}$Yb ($I$ =\n1/2) with cold atoms held by an optical dipole trap in a static electric field\nup to 120 kV/cm. We observe the electric Breit-Rabi effect displaying\nhigh-order ($E^4$ and $E^6$) DC Stark shifts. These effects arise from the\ninfluence of the strong electric field on hyperfine interactions."
                },
                "authors": [
                    {
                        "name": "S. -Z. Wang"
                    },
                    {
                        "name": "S. -B. Wang"
                    },
                    {
                        "name": "Z. -J. Tao"
                    },
                    {
                        "name": "T. Xia"
                    },
                    {
                        "name": "Z. -T. Lu"
                    }
                ],
                "author_detail": {
                    "name": "Z. -T. Lu"
                },
                "author": "Z. -T. Lu",
                "arxiv_doi": "10.1073/pnas.2423902122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2423902122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_journal_ref": "122 (26)e2423902122 June 27 2025",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08232v1",
                "updated": "2025-07-11T00:36:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "published": "2025-07-11T00:36:57Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    0,
                    36,
                    57,
                    4,
                    192,
                    0
                ],
                "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and\n  Reading Comprehension?"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08143v1",
                "updated": "2025-07-10T20:03:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T20:03:35Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    20,
                    3,
                    35,
                    3,
                    191,
                    0
                ],
                "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compactor: Calibrated Query-Agnostic KV Cache Compression with\n  Approximate Leverage Scores"
                },
                "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families."
                },
                "authors": [
                    {
                        "name": "Vivek Chari"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07990v1",
                "updated": "2025-07-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    59,
                    2,
                    3,
                    191,
                    0
                ],
                "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs"
                },
                "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm."
                },
                "authors": [
                    {
                        "name": "Jeongseok Hyun"
                    },
                    {
                        "name": "Sukjun Hwang"
                    },
                    {
                        "name": "Su Ho Han"
                    },
                    {
                        "name": "Taeoh Kim"
                    },
                    {
                        "name": "Inwoong Lee"
                    },
                    {
                        "name": "Dongyoon Wee"
                    },
                    {
                        "name": "Joon-Young Lee"
                    },
                    {
                        "name": "Seon Joo Kim"
                    },
                    {
                        "name": "Minho Shim"
                    }
                ],
                "author_detail": {
                    "name": "Minho Shim"
                },
                "author": "Minho Shim",
                "arxiv_comment": "Accepted at ICCV2025; Project page:\n  https://www.jshyun.me/projects/sttm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v1",
                "updated": "2025-07-10T17:47:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code and models are available at https://github.com/NVlabs/Long-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v3",
                "updated": "2025-07-10T17:10:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    10,
                    49,
                    3,
                    191,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07400v1",
                "updated": "2025-07-10T03:39:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T03:39:23Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    39,
                    23,
                    3,
                    191,
                    0
                ],
                "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent\n  Workflows"
                },
                "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows."
                },
                "authors": [
                    {
                        "name": "Zaifeng Pan"
                    },
                    {
                        "name": "Ajjkumar Patel"
                    },
                    {
                        "name": "Zhengding Hu"
                    },
                    {
                        "name": "Yipeng Shen"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Wan-Lu Li"
                    },
                    {
                        "name": "Lianhui Qin"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Yufei Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ding"
                },
                "author": "Yufei Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v1",
                "updated": "2025-07-10T01:51:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07290v1",
                "updated": "2025-07-09T21:18:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T21:18:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    21,
                    18,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stabilization of the first-order phase transition character and\n  Enhancement of the Electrocaloric Effect by NBT substitution in BaTiO$_3$\n  ceramics"
                },
                "summary": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electrocaloric properties of BaTiO$_3$-based lead-free ferroelectric\nmaterials have been widely investigated. One approach to achieving a large\nelectrocaloric response is to exploit the substantial polarization change\nassociated with the first-order phase transition at the Curie temperature.\nFollowing this strategy, we investigated the electrocaloric response of\n(1$-x$)BaTiO$_3$-$x$Na$_{0.5}$Bi$_{0.5}$TiO$_3$ (BT-NBT) ceramics for x = 0.05,\n0.10, 0.20, and 0.30. In this BT-rich region of the solid solution, it is\nestablished that increasing the NBT content enhances the tetragonality of\nBaTiO$_3$. We show that this increase in tetragonality helps maintain the\nfirst-order nature of the phase transition and enables a correspondingly large\nelectrocaloric response, despite the simultaneous enhancement of relaxor\nferroelectric character with NBT substitution. A significantly large effective\nelectrocaloric temperature change ($\\Delta T_{\\mathrm{eff}}$) of ~1.65 K was\nobtained for the x = 0.20 composition under an applied field of 40 kV/cm using\ndirect electrocaloric measurements, in reasonable agreement with the indirect\nresults."
                },
                "authors": [
                    {
                        "name": "M. Karakaya"
                    },
                    {
                        "name": "I. Gurbuz"
                    },
                    {
                        "name": "L. Fulanovic"
                    },
                    {
                        "name": "U. Adem"
                    }
                ],
                "author_detail": {
                    "name": "U. Adem"
                },
                "author": "U. Adem",
                "arxiv_doi": "10.1039/D4TC01735H",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1039/D4TC01735H",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted version of the article published in J. Mater. Chem. C. 10\n  Pages, 7 Figures. Plus SI file as a single pdf",
                "arxiv_journal_ref": "J. Mater. Chem. C, 2024,12, 19612-19619",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06739v1",
                "updated": "2025-07-09T10:53:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:53:05Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold"
                },
                "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."
                },
                "authors": [
                    {
                        "name": "Zishen Huang"
                    },
                    {
                        "name": "Chunyu Yang"
                    },
                    {
                        "name": "Mengyuan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Ren"
                },
                "author": "Mengyuan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v2",
                "updated": "2025-07-09T07:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Safety Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Safety Inference Scaling"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for\n  LLM Safety Assurance\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06567v1",
                "updated": "2025-07-09T05:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v3",
                "updated": "2025-07-09T04:43:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    4,
                    43,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06517v1",
                "updated": "2025-07-09T03:33:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T03:33:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance."
                },
                "authors": [
                    {
                        "name": "Zicong Tang"
                    },
                    {
                        "name": "Shi Luohe"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "arxiv_comment": "Accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v2",
                "updated": "2025-07-09T02:35:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    35,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted By ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v2",
                "updated": "2025-07-08T21:23:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    21,
                    23,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "21 pages, 10 figures. Supplement 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06349v1",
                "updated": "2025-07-08T19:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T19:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design"
                },
                "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization."
                },
                "authors": [
                    {
                        "name": "Erin Ransom"
                    },
                    {
                        "name": "Andrew Lim"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v3",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07061v1",
                "updated": "2025-07-08T09:20:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:20:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems"
                },
                "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems."
                },
                "authors": [
                    {
                        "name": "Shervin Ghaffari"
                    },
                    {
                        "name": "Zohre Bahranifard"
                    },
                    {
                        "name": "Mohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Akbari"
                },
                "author": "Mohammad Akbari",
                "arxiv_comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v4",
                "updated": "2025-07-08T07:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03622v3",
                "updated": "2025-07-08T02:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    15,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2023-06-06T12:19:05Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    12,
                    19,
                    5,
                    1,
                    157,
                    0
                ],
                "title": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference"
                },
                "summary": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Xiaonan Luo"
                    },
                    {
                        "name": "Zhuohao Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Yu Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yu Ding"
                },
                "author": "Yu Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v2",
                "updated": "2025-07-08T00:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    51,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07120v1",
                "updated": "2025-07-07T19:47:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T19:47:24Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    19,
                    47,
                    24,
                    0,
                    188,
                    0
                ],
                "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helix Parallelism: Rethinking Sharding Strategies for Interactive\n  Multi-Million-Token LLM Decoding"
                },
                "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical."
                },
                "authors": [
                    {
                        "name": "Nidhi Bhatia"
                    },
                    {
                        "name": "Ankit More"
                    },
                    {
                        "name": "Ritika Borkar"
                    },
                    {
                        "name": "Tiyasa Mitra"
                    },
                    {
                        "name": "Ramon Matas"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Maximilian Golub"
                    },
                    {
                        "name": "Dheevatsa Mudigere"
                    },
                    {
                        "name": "Brian Pharris"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    }
                ],
                "author_detail": {
                    "name": "Bita Darvish Rouhani"
                },
                "author": "Bita Darvish Rouhani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v2",
                "updated": "2025-07-07T09:25:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    25,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v1",
                "updated": "2025-07-06T15:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v2",
                "updated": "2025-07-05T15:51:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    51,
                    57,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas Khler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v2",
                "updated": "2025-07-05T15:40:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    40,
                    51,
                    5,
                    186,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v2",
                "updated": "2025-07-05T13:37:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    37,
                    48,
                    5,
                    186,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Gabriel Franco"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03980v1",
                "updated": "2025-07-05T10:11:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T10:11:37Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "title": "Combination generators with optimal cache utilization and communication\n  free parallel execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combination generators with optimal cache utilization and communication\n  free parallel execution"
                },
                "summary": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators."
                },
                "authors": [
                    {
                        "name": "Xi He"
                    },
                    {
                        "name": "Max. A. Little"
                    }
                ],
                "author_detail": {
                    "name": "Max. A. Little"
                },
                "author": "Max. A. Little",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03919v1",
                "updated": "2025-07-05T06:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T06:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "title": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery"
                },
                "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design"
                },
                "authors": [
                    {
                        "name": "Duy Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy Le"
                },
                "author": "Duy Le",
                "arxiv_comment": "6 pages, 3 figures, 3 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v3",
                "updated": "2025-07-05T01:08:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    1,
                    8,
                    40,
                    5,
                    186,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Sean Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "36 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v1",
                "updated": "2025-07-04T21:09:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. Khn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Khn"
                },
                "author": "Martin J. Khn",
                "arxiv_comment": "29 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03445v1",
                "updated": "2025-07-04T10:01:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "summary": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results."
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03396v1",
                "updated": "2025-07-04T09:03:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:03:18Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "title": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge"
                },
                "summary": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains."
                },
                "authors": [
                    {
                        "name": "Fariborz Momtazzadeh"
                    },
                    {
                        "name": "Farshad Sohbatzadeh"
                    },
                    {
                        "name": "Hamed Soltani Ahmadi"
                    },
                    {
                        "name": "Ramin Mehrabifard"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Mehrabifard"
                },
                "author": "Ramin Mehrabifard",
                "arxiv_doi": "10.1007/S40042-025-01392-9.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/S40042-025-01392-9.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.03396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v3",
                "updated": "2025-07-04T06:49:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    31,
                    4,
                    185,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v3",
                "updated": "2025-07-04T06:36:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    36,
                    38,
                    4,
                    185,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03231v1",
                "updated": "2025-07-04T00:16:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T00:16:15Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "title": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching"
                },
                "summary": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Brian Plancher"
                    }
                ],
                "author_detail": {
                    "name": "Brian Plancher"
                },
                "author": "Brian Plancher",
                "arxiv_comment": "Accepted to IROS 2025, 7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03153v1",
                "updated": "2025-07-03T20:20:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T20:20:33Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "title": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference"
                },
                "summary": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware."
                },
                "authors": [
                    {
                        "name": "Weishu Deng"
                    },
                    {
                        "name": "Yujie Yang"
                    },
                    {
                        "name": "Peiran Du"
                    },
                    {
                        "name": "Lingfeng Xiang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Jia Rao"
                    }
                ],
                "author_detail": {
                    "name": "Jia Rao"
                },
                "author": "Jia Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04789v2",
                "updated": "2025-07-03T17:11:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    11,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2023-12-08T02:03:55Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    2,
                    3,
                    55,
                    4,
                    342,
                    0
                ],
                "title": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System"
                },
                "summary": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jishen Zhao"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_doi": "10.1145/3676642.3736119",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736119",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appears in the Proceedings of the 30th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems,\n  Volume 3 (ASPLOS 25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v3",
                "updated": "2025-07-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    22,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02397v1",
                "updated": "2025-07-03T07:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics"
                },
                "summary": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li",
                "arxiv_comment": "16 pages, 5 figures, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02227v1",
                "updated": "2025-07-03T01:22:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations"
                },
                "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications."
                },
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02006v1",
                "updated": "2025-07-02T00:35:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T00:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design"
                },
                "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Shakya Jayakody"
                    },
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v1",
                "updated": "2025-07-01T22:27:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01154v1",
                "updated": "2025-07-01T19:28:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T19:28:37Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDP: Private Training Large Language Models with Efficient DP-SGD"
                },
                "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."
                },
                "authors": [
                    {
                        "name": "Liangyu Wang"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zihang Xiang"
                    },
                    {
                        "name": "David E. Keyes"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00797v1",
                "updated": "2025-07-01T14:30:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T14:30:31Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator"
                },
                "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."
                },
                "authors": [
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Haroon Waris"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Jianfei Jiang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Guanghui He"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui He"
                },
                "author": "Guanghui He",
                "arxiv_comment": "DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00727v1",
                "updated": "2025-07-01T13:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T13:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "title": "On Hierarchical Coded Caching with Offline Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Hierarchical Coded Caching with Offline Users"
                },
                "summary": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A short version of this is accepted for presentation in 2025 IEEE\n  Information Theory Workshop; 8 pages, one figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00716v1",
                "updated": "2025-07-01T12:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:51:09Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "title": "Accelerating Loading WebGraphs in ParaGrapher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Loading WebGraphs in ParaGrapher"
                },
                "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Koohi Esfahani"
                },
                "author": "Mohsen Koohi Esfahani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00715v1",
                "updated": "2025-07-01T12:42:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:42:06Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "title": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens"
                },
                "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Xianjing Han"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00614v1",
                "updated": "2025-07-01T09:47:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T09:47:38Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "title": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications"
                },
                "summary": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart."
                },
                "authors": [
                    {
                        "name": "B. Ramachandran"
                    },
                    {
                        "name": "N. Sudarshan"
                    },
                    {
                        "name": "G. Mangamma"
                    },
                    {
                        "name": "M. S. Ramachandra Rao"
                    }
                ],
                "author_detail": {
                    "name": "M. S. Ramachandra Rao"
                },
                "author": "M. S. Ramachandra Rao",
                "arxiv_doi": "10.1007/s10832-025-00423-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10832-025-00423-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 7 figures, 1 Table and Accepted for publication in Journal\n  of Electroceramics",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00462v1",
                "updated": "2025-07-01T06:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T06:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation"
                },
                "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training."
                },
                "authors": [
                    {
                        "name": "Jizhou Han"
                    },
                    {
                        "name": "Chenhao Ding"
                    },
                    {
                        "name": "SongLin Dong"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Xinyuan Gao"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24060v1",
                "updated": "2025-06-30T17:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints"
                },
                "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.12464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12464v1",
                "updated": "2025-07-16T17:59:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    59,
                    32,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:59:32Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    59,
                    32,
                    2,
                    197,
                    0
                ],
                "title": "CytoSAE: Interpretable Cell Embeddings for Hematology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CytoSAE: Interpretable Cell Embeddings for Hematology"
                },
                "summary": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic\ninterpretability of transformer-based foundation models. Very recently, SAEs\nwere also adopted for the visual domain, enabling the discovery of visual\nconcepts and their patch-wise attribution to tokens in the transformer model.\nWhile a growing number of foundation models emerged for medical imaging, tools\nfor explaining their inferences are still lacking. In this work, we show the\napplicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder\nwhich is trained on over 40,000 peripheral blood single-cell images. CytoSAE\ngeneralizes to diverse and out-of-domain datasets, including bone marrow\ncytology, where it identifies morphologically relevant concepts which we\nvalidated with medical experts. Furthermore, we demonstrate scenarios in which\nCytoSAE can generate patient-specific and disease-specific concepts, enabling\nthe detection of pathognomonic cells and localized cellular abnormalities at\nthe patch level. We quantified the effect of concepts on a patient-level AML\nsubtype classification task and show that CytoSAE concepts reach performance\ncomparable to the state-of-the-art, while offering explainability on the\nsub-cellular level. Source code and model weights are available at\nhttps://github.com/dynamical-inference/cytosae."
                },
                "authors": [
                    {
                        "name": "Muhammed Furkan Dasdelen"
                    },
                    {
                        "name": "Hyesu Lim"
                    },
                    {
                        "name": "Michele Buck"
                    },
                    {
                        "name": "Katharina S. Gtze"
                    },
                    {
                        "name": "Carsten Marr"
                    },
                    {
                        "name": "Steffen Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Schneider"
                },
                "author": "Steffen Schneider",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07615v2",
                "updated": "2025-07-16T17:59:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    59,
                    28,
                    2,
                    197,
                    0
                ],
                "published": "2025-05-12T14:36:47Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    14,
                    36,
                    47,
                    0,
                    132,
                    0
                ],
                "title": "Diffused Responsibility: Analyzing the Energy Consumption of Generative\n  Text-to-Audio Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffused Responsibility: Analyzing the Energy Consumption of Generative\n  Text-to-Audio Diffusion Models"
                },
                "summary": "Text-to-audio models have recently emerged as a powerful technology for\ngenerating sound from textual descriptions. However, their high computational\ndemands raise concerns about energy consumption and environmental impact. In\nthis paper, we conduct an analysis of the energy usage of 7 state-of-the-art\ntext-to-audio diffusion-based generative models, evaluating to what extent\nvariations in generation parameters affect energy consumption at inference\ntime. We also aim to identify an optimal balance between audio quality and\nenergy consumption by considering Pareto-optimal solutions across all selected\nmodels. Our findings provide insights into the trade-offs between performance\nand environmental impact, contributing to the development of more efficient\ngenerative audio models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-audio models have recently emerged as a powerful technology for\ngenerating sound from textual descriptions. However, their high computational\ndemands raise concerns about energy consumption and environmental impact. In\nthis paper, we conduct an analysis of the energy usage of 7 state-of-the-art\ntext-to-audio diffusion-based generative models, evaluating to what extent\nvariations in generation parameters affect energy consumption at inference\ntime. We also aim to identify an optimal balance between audio quality and\nenergy consumption by considering Pareto-optimal solutions across all selected\nmodels. Our findings provide insights into the trade-offs between performance\nand environmental impact, contributing to the development of more efficient\ngenerative audio models."
                },
                "authors": [
                    {
                        "name": "Riccardo Passoni"
                    },
                    {
                        "name": "Francesca Ronchini"
                    },
                    {
                        "name": "Luca Comanducci"
                    },
                    {
                        "name": "Romain Serizel"
                    },
                    {
                        "name": "Fabio Antonacci"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Antonacci"
                },
                "author": "Fabio Antonacci",
                "arxiv_comment": "Accepted at WASPAA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12457v2",
                "updated": "2025-07-17T06:38:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    6,
                    38,
                    8,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T17:56:44Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    56,
                    44,
                    2,
                    197,
                    0
                ],
                "title": "Does $K$-fold CV based penalty perform variable selection or does it\n  lead to $n^{1/2}$-consistency in Lasso?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does $K$-fold CV based penalty perform variable selection or does it\n  lead to $n^{1/2}$-consistency in Lasso?"
                },
                "summary": "Least absolute shrinkage and selection operator or Lasso, introduced by\nTibshirani (1996), is one of the widely used regularization methods in\nregression. It is observed that the properties of Lasso vary wildly depending\non the choice of the penalty parameter. The recent results of Lahiri (2021)\nsuggest that, depending on the nature of the penalty parameter, Lasso can\neither be variable selection consistent or be $n^{1/2}-$consistent. However,\npractitioners generally implement Lasso by choosing the penalty parameter in a\ndata-dependent way, the most popular being the $K$-fold cross-validation. In\nthis paper, we explore the variable selection consistency and\n$n^{1/2}-$consistency of Lasso when the penalty is chosen based on $K$-fold\ncross-validation with $K$ being fixed. We consider the fixed-dimensional\nheteroscedastic linear regression model and show that Lasso with $K$-fold\ncross-validation based penalty is $n^{1/2}-$consistent, but not variable\nselection consistent. We also establish the $n^{1/2}-$consistency of the\n$K$-fold cross-validation based penalty as an intermediate result.\nAdditionally, as a consequence of $n^{1/2}-$consistency, we establish the\nvalidity of Bootstrap to approximate the distribution of the Lasso estimator\nbased on $K-$fold cross-validation. We validate the Bootstrap approximation in\nfinite samples based on a moderate simulation study. Thus, our results\nessentially justify the use of $K$-fold cross-validation in practice to draw\ninferences based on $n^{1/2}-$scaled pivotal quantities in Lasso regression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Least absolute shrinkage and selection operator or Lasso, introduced by\nTibshirani (1996), is one of the widely used regularization methods in\nregression. It is observed that the properties of Lasso vary wildly depending\non the choice of the penalty parameter. The recent results of Lahiri (2021)\nsuggest that, depending on the nature of the penalty parameter, Lasso can\neither be variable selection consistent or be $n^{1/2}-$consistent. However,\npractitioners generally implement Lasso by choosing the penalty parameter in a\ndata-dependent way, the most popular being the $K$-fold cross-validation. In\nthis paper, we explore the variable selection consistency and\n$n^{1/2}-$consistency of Lasso when the penalty is chosen based on $K$-fold\ncross-validation with $K$ being fixed. We consider the fixed-dimensional\nheteroscedastic linear regression model and show that Lasso with $K$-fold\ncross-validation based penalty is $n^{1/2}-$consistent, but not variable\nselection consistent. We also establish the $n^{1/2}-$consistency of the\n$K$-fold cross-validation based penalty as an intermediate result.\nAdditionally, as a consequence of $n^{1/2}-$consistency, we establish the\nvalidity of Bootstrap to approximate the distribution of the Lasso estimator\nbased on $K-$fold cross-validation. We validate the Bootstrap approximation in\nfinite samples based on a moderate simulation study. Thus, our results\nessentially justify the use of $K$-fold cross-validation in practice to draw\ninferences based on $n^{1/2}-$scaled pivotal quantities in Lasso regression."
                },
                "authors": [
                    {
                        "name": "Mayukh Choudhury"
                    },
                    {
                        "name": "Debraj Das"
                    }
                ],
                "author_detail": {
                    "name": "Debraj Das"
                },
                "author": "Debraj Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19982v2",
                "updated": "2025-07-16T17:52:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    52,
                    44,
                    2,
                    197,
                    0
                ],
                "published": "2025-04-28T16:57:17Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    57,
                    17,
                    0,
                    118,
                    0
                ],
                "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons"
                },
                "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research."
                },
                "authors": [
                    {
                        "name": "Emre Can Acikgoz"
                    },
                    {
                        "name": "Carl Guo"
                    },
                    {
                        "name": "Suvodip Dey"
                    },
                    {
                        "name": "Akul Datta"
                    },
                    {
                        "name": "Takyoung Kim"
                    },
                    {
                        "name": "Gokhan Tur"
                    },
                    {
                        "name": "Dilek Hakkani-Tr"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani-Tr"
                },
                "author": "Dilek Hakkani-Tr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05470v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05470v4",
                "updated": "2025-07-16T17:37:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    37,
                    2,
                    2,
                    197,
                    0
                ],
                "published": "2025-05-08T17:58:45Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    58,
                    45,
                    3,
                    128,
                    0
                ],
                "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-GRPO: Training Flow Matching Models via Online RL"
                },
                "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation. Flow-GRPO\nalso achieves substantial gains in human preference alignment. Notably, very\nlittle reward hacking occurred, meaning rewards did not increase at the cost of\nappreciable image quality or diversity degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from 63% to 95%. In visual text rendering, its accuracy\nimproves from 59% to 92%, significantly enhancing text generation. Flow-GRPO\nalso achieves substantial gains in human preference alignment. Notably, very\nlittle reward hacking occurred, meaning rewards did not increase at the cost of\nappreciable image quality or diversity degradation."
                },
                "authors": [
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Gongye Liu"
                    },
                    {
                        "name": "Jiajun Liang"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "arxiv_comment": "Code: https://github.com/yifan123/flow_grpo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05470v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05470v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12443v1",
                "updated": "2025-07-16T17:29:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    29,
                    15,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:29:15Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    29,
                    15,
                    2,
                    197,
                    0
                ],
                "title": "LLM-Based Config Synthesis requires Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Config Synthesis requires Disambiguation"
                },
                "summary": "Beyond hallucinations, another problem in program synthesis using LLMs is\nambiguity in user intent. We illustrate the ambiguity problem in a networking\ncontext for LLM-based incremental configuration synthesis of route-maps and\nACLs. These structures frequently overlap in header space, making the relative\npriority of actions impossible for the LLM to infer without user interaction.\nMeasurements in a large cloud identify complex ACLs with 100's of overlaps,\nshowing ambiguity is a real problem. We propose a prototype system, Clarify,\nwhich uses an LLM augmented with a new module called a Disambiguator that helps\nelicit user intent. On a small synthetic workload, Clarify incrementally\nsynthesizes routing policies after disambiguation and then verifies them. Our\ntreatment of ambiguities is useful more generally when the intent of updates\ncan be correctly synthesized by LLMs, but their integration is ambiguous and\ncan lead to different global behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond hallucinations, another problem in program synthesis using LLMs is\nambiguity in user intent. We illustrate the ambiguity problem in a networking\ncontext for LLM-based incremental configuration synthesis of route-maps and\nACLs. These structures frequently overlap in header space, making the relative\npriority of actions impossible for the LLM to infer without user interaction.\nMeasurements in a large cloud identify complex ACLs with 100's of overlaps,\nshowing ambiguity is a real problem. We propose a prototype system, Clarify,\nwhich uses an LLM augmented with a new module called a Disambiguator that helps\nelicit user intent. On a small synthetic workload, Clarify incrementally\nsynthesizes routing policies after disambiguation and then verifies them. Our\ntreatment of ambiguities is useful more generally when the intent of updates\ncan be correctly synthesized by LLMs, but their integration is ambiguous and\ncan lead to different global behaviors."
                },
                "authors": [
                    {
                        "name": "Rajdeep Mondal"
                    },
                    {
                        "name": "Nikolaj Bjorner"
                    },
                    {
                        "name": "Todd Millstein"
                    },
                    {
                        "name": "Alan Tang"
                    },
                    {
                        "name": "George Varghese"
                    }
                ],
                "author_detail": {
                    "name": "George Varghese"
                },
                "author": "George Varghese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12442v1",
                "updated": "2025-07-16T17:28:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    28,
                    40,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:28:40Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    28,
                    40,
                    2,
                    197,
                    0
                ],
                "title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid\n  Language Model Performance with Long Context Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid\n  Language Model Performance with Long Context Length"
                },
                "summary": "The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework."
                },
                "authors": [
                    {
                        "name": "Saptarshi Mitra"
                    },
                    {
                        "name": "Rachid Karami"
                    },
                    {
                        "name": "Haocheng Xu"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Hyoukjun Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Hyoukjun Kwon"
                },
                "author": "Hyoukjun Kwon",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01798v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01798v2",
                "updated": "2025-07-16T17:27:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    27,
                    18,
                    2,
                    197,
                    0
                ],
                "published": "2025-06-02T15:39:36Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    15,
                    39,
                    36,
                    0,
                    153,
                    0
                ],
                "title": "Shape and ionization of equatorial matter near compact objects from\n  X-ray polarization reflection signatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shape and ionization of equatorial matter near compact objects from\n  X-ray polarization reflection signatures"
                },
                "summary": "Motivated by the success of the IXPE mission, we elucidate what can be\ninferred about 3D matter structures forming about the equatorial plane of\naccreting compact objects from 0.1-100 keV linear polarization induced by\nnon-relativistic large-scale reflection. We construct a model of an optically\nthick elevated axially symmetric reflecting medium with arbitrary ionization\nprofile, representing the known diverse scattering environments: from thick\nwinds and super-Eddington funnel structures formed around black holes and\nneutron stars, to Compton-thick dusty tori of active galactic nuclei and their\nbroad line regions. We assume a central X-ray power-law source with an\nisotropic, cosine, and slab-corona emission distribution, including intrinsic\npolarization. The reprocessing is based on constant-density local reflection\ntables produced with a Monte Carlo method combined with detailed non-LTE\nradiative transfer, although we also show examples with classical\n(semi-)analytical reflection prescriptions. We conclude that varying ionization\nhas a similarly strong impact on observed polarization as the observer's\ninclination and the skew and opening angle of the reflector's inner walls,\naltogether producing up to tens of % of reflected polarization both parallelly\nor perpendicularly to the projected axis, depending on the parameter values\ncombination. After testing 3 different ad-hoc shapes of the reflector: a cone,\nan elliptical torus, and a bowl, we conclude that while in some configurations,\ntheir altered curvature produces more than 30% absolute difference in observed\ntotal polarization, in others, the adopted shape has a marginal impact. Lastly,\nwe discuss the change of the observed polarization due to relaxing the\noptically thick assumption on equatorial winds and accreted matter, providing a\ncontinuous range of energy-dependent examples between the optically thick and\nthin scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the success of the IXPE mission, we elucidate what can be\ninferred about 3D matter structures forming about the equatorial plane of\naccreting compact objects from 0.1-100 keV linear polarization induced by\nnon-relativistic large-scale reflection. We construct a model of an optically\nthick elevated axially symmetric reflecting medium with arbitrary ionization\nprofile, representing the known diverse scattering environments: from thick\nwinds and super-Eddington funnel structures formed around black holes and\nneutron stars, to Compton-thick dusty tori of active galactic nuclei and their\nbroad line regions. We assume a central X-ray power-law source with an\nisotropic, cosine, and slab-corona emission distribution, including intrinsic\npolarization. The reprocessing is based on constant-density local reflection\ntables produced with a Monte Carlo method combined with detailed non-LTE\nradiative transfer, although we also show examples with classical\n(semi-)analytical reflection prescriptions. We conclude that varying ionization\nhas a similarly strong impact on observed polarization as the observer's\ninclination and the skew and opening angle of the reflector's inner walls,\naltogether producing up to tens of % of reflected polarization both parallelly\nor perpendicularly to the projected axis, depending on the parameter values\ncombination. After testing 3 different ad-hoc shapes of the reflector: a cone,\nan elliptical torus, and a bowl, we conclude that while in some configurations,\ntheir altered curvature produces more than 30% absolute difference in observed\ntotal polarization, in others, the adopted shape has a marginal impact. Lastly,\nwe discuss the change of the observed polarization due to relaxing the\noptically thick assumption on equatorial winds and accreted matter, providing a\ncontinuous range of energy-dependent examples between the optically thick and\nthin scenarios."
                },
                "authors": [
                    {
                        "name": "J. Podgorn"
                    }
                ],
                "author_detail": {
                    "name": "J. Podgorn"
                },
                "author": "J. Podgorn",
                "arxiv_comment": "19 pages, 10 figures, submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01798v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01798v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12435v1",
                "updated": "2025-07-16T17:24:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    24,
                    6,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:24:06Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    24,
                    6,
                    2,
                    197,
                    0
                ],
                "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal\n  Inference in Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal\n  Inference in Neural Networks"
                },
                "summary": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets."
                },
                "authors": [
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "David Mccoy"
                    },
                    {
                        "name": "Nolan Gunter"
                    },
                    {
                        "name": "Kaitlyn Lee"
                    },
                    {
                        "name": "Alejandro Schuler"
                    },
                    {
                        "name": "Mark van der Laan"
                    }
                ],
                "author_detail": {
                    "name": "Mark van der Laan"
                },
                "author": "Mark van der Laan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09850v3",
                "updated": "2025-07-16T17:16:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    16,
                    18,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-14T01:14:50Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    1,
                    14,
                    50,
                    0,
                    195,
                    0
                ],
                "title": "The Challenge of Teaching Reasoning to LLMs Without RL or Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Challenge of Teaching Reasoning to LLMs Without RL or Distillation"
                },
                "summary": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective."
                },
                "authors": [
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Branislav Kisacanin"
                    },
                    {
                        "name": "George Armstrong"
                    },
                    {
                        "name": "Shubham Toshniwal"
                    },
                    {
                        "name": "Ivan Moshkov"
                    },
                    {
                        "name": "Alexan Ayrapetyan"
                    },
                    {
                        "name": "Sadegh Mahdavi"
                    },
                    {
                        "name": "Dan Zhao"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Dragan Masulovic"
                    },
                    {
                        "name": "Marius Stanean"
                    },
                    {
                        "name": "Advaith Avadhanam"
                    },
                    {
                        "name": "Max Wang"
                    },
                    {
                        "name": "Ashmit Dutta"
                    },
                    {
                        "name": "Shitij Govil"
                    },
                    {
                        "name": "Sri Yanamandara"
                    },
                    {
                        "name": "Mihir Tandon"
                    },
                    {
                        "name": "Sriram Ananthakrishnan"
                    },
                    {
                        "name": "Vedant Rathi"
                    },
                    {
                        "name": "David Zhang"
                    },
                    {
                        "name": "Joonseok Kang"
                    },
                    {
                        "name": "Leon Luo"
                    },
                    {
                        "name": "Titu Andreescu"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Igor Gitman"
                    }
                ],
                "author_detail": {
                    "name": "Igor Gitman"
                },
                "author": "Igor Gitman",
                "arxiv_comment": "Accepted at the Second AI for Math Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12425v1",
                "updated": "2025-07-16T17:13:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    13,
                    6,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:13:06Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    13,
                    6,
                    2,
                    197,
                    0
                ],
                "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data"
                },
                "summary": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot"
                },
                "authors": [
                    {
                        "name": "Chandana Cheerla"
                    }
                ],
                "author_detail": {
                    "name": "Chandana Cheerla"
                },
                "author": "Chandana Cheerla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11662v2",
                "updated": "2025-07-16T17:12:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    12,
                    9,
                    2,
                    197,
                    0
                ],
                "published": "2024-02-18T17:42:19Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    17,
                    42,
                    19,
                    6,
                    49,
                    0
                ],
                "title": "TDE-3: An improved prior for optical flow computation in spiking neural\n  networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDE-3: An improved prior for optical flow computation in spiking neural\n  networks"
                },
                "summary": "Motion detection is a primary task required for robotic systems to perceive\nand navigate in their environment. Proposed in the literature bioinspired\nneuromorphic Time-Difference Encoder (TDE-2) combines event-based sensors and\nprocessors with spiking neural networks to provide real-time and\nenergy-efficient motion detection through extracting temporal correlations\nbetween two points in space. However, on the algorithmic level, this design\nleads to loss of direction-selectivity of individual TDEs in textured\nenvironments. Here we propose an augmented 3-point TDE (TDE-3) with additional\ninhibitory input that makes TDE-3 direction-selectivity robust in textured\nenvironments. We developed a procedure to train the new TDE-3 using\nbackpropagation through time and surrogate gradients to linearly map input\nvelocities into an output spike count or an Inter-Spike Interval (ISI). Our\nwork is the first instance of training a spiking neuron to have a specific ISI.\nUsing synthetic data we compared training and inference with spike count and\nISI with respect to changes in stimuli dynamic range, spatial frequency, and\nlevel of noise. ISI turns out to be more robust towards variation in spatial\nfrequency, whereas the spike count is a more reliable training signal in the\npresence of noise. We performed the first in-depth quantitative investigation\nof optical flow coding with TDE and compared TDE-2 vs TDE-3 in terms of\nenergy-efficiency and coding precision. Results show that on the network level\nboth detectors show similar precision (20 degree angular error, 88% correlation\nwith ground truth). Yet, due to the more robust direction-selectivity of\nindividual TDEs, TDE-3 based network spike less and hence is more\nenergy-efficient. Reported precision is on par with model-based methods but the\nspike-based processing of the TDEs provides allows more energy-efficient\ninference with neuromorphic hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion detection is a primary task required for robotic systems to perceive\nand navigate in their environment. Proposed in the literature bioinspired\nneuromorphic Time-Difference Encoder (TDE-2) combines event-based sensors and\nprocessors with spiking neural networks to provide real-time and\nenergy-efficient motion detection through extracting temporal correlations\nbetween two points in space. However, on the algorithmic level, this design\nleads to loss of direction-selectivity of individual TDEs in textured\nenvironments. Here we propose an augmented 3-point TDE (TDE-3) with additional\ninhibitory input that makes TDE-3 direction-selectivity robust in textured\nenvironments. We developed a procedure to train the new TDE-3 using\nbackpropagation through time and surrogate gradients to linearly map input\nvelocities into an output spike count or an Inter-Spike Interval (ISI). Our\nwork is the first instance of training a spiking neuron to have a specific ISI.\nUsing synthetic data we compared training and inference with spike count and\nISI with respect to changes in stimuli dynamic range, spatial frequency, and\nlevel of noise. ISI turns out to be more robust towards variation in spatial\nfrequency, whereas the spike count is a more reliable training signal in the\npresence of noise. We performed the first in-depth quantitative investigation\nof optical flow coding with TDE and compared TDE-2 vs TDE-3 in terms of\nenergy-efficiency and coding precision. Results show that on the network level\nboth detectors show similar precision (20 degree angular error, 88% correlation\nwith ground truth). Yet, due to the more robust direction-selectivity of\nindividual TDEs, TDE-3 based network spike less and hence is more\nenergy-efficient. Reported precision is on par with model-based methods but the\nspike-based processing of the TDEs provides allows more energy-efficient\ninference with neuromorphic hardware."
                },
                "authors": [
                    {
                        "name": "Matthew Yedutenko"
                    },
                    {
                        "name": "Federico Paredes-Valles"
                    },
                    {
                        "name": "Lyes Khacef"
                    },
                    {
                        "name": "Guido C. H. E. De Croon"
                    }
                ],
                "author_detail": {
                    "name": "Guido C. H. E. De Croon"
                },
                "author": "Guido C. H. E. De Croon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12424v1",
                "updated": "2025-07-16T17:11:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    11,
                    48,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:11:48Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    11,
                    48,
                    2,
                    197,
                    0
                ],
                "title": "Hierarchical Temporal Point Process Modeling of Aggressive Behavior\n  Onset in Psychiatric Inpatient Youth with Autism for Branching Factor\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Temporal Point Process Modeling of Aggressive Behavior\n  Onset in Psychiatric Inpatient Youth with Autism for Branching Factor\n  Estimation"
                },
                "summary": "Aggressive behavior in autistic inpatient youth often arises in temporally\nclustered bursts complicating efforts to distinguish external triggers from\ninternal escalation. The sample population branching factor-the expected number\nof new onsets triggered by a given event-is a key summary of self-excitation in\nbehavior dynamics. Prior pooled models overestimate this quantity by ignoring\npatient-specific variability. We addressed this using a hierarchical Hawkes\nprocess with an exponential kernel and edge-effect correction allowing partial\npooling across patients. This approach reduces bias from high-frequency\nindividuals and stabilizes estimates for those with sparse data. Bayesian\ninference was performed using the No U-Turn Sampler with model evaluation via\nconvergence diagnostics, power-scaling sensitivity analysis, and multiple\nGoodness-of-Fit (GOF) metrics: PSIS-LOO the Lewis test with Durbin's\nmodification and residual analysis based on the Random Time Change Theorem\n(RTCT). The hierarchical model yielded a significantly lower and more precise\nbranching factor estimate mean (0.742 +- 0.026) than the pooled model (0.899 +-\n0.015) and narrower intervals than the unpooled model (0.717 +- 0.139). This\nled to a threefold smaller cascade of events per onset under the hierarchical\nmodel. Sensitivity analyses confirmed robustness to prior and likelihood\nperturbations while the unpooled model showed instability for sparse\nindividuals. GOF measures consistently favored or on par to the hierarchical\nmodel. Hierarchical Hawkes modeling with edge-effect correction provides robust\nestimation of branching dynamics by capturing both within- and between-patient\nvariability. This enables clearer separation of endogenous from exogenous\nevents supports linkage to physiological signals and enhances early warning\nsystems individualized treatment and resource allocation in inpatient care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aggressive behavior in autistic inpatient youth often arises in temporally\nclustered bursts complicating efforts to distinguish external triggers from\ninternal escalation. The sample population branching factor-the expected number\nof new onsets triggered by a given event-is a key summary of self-excitation in\nbehavior dynamics. Prior pooled models overestimate this quantity by ignoring\npatient-specific variability. We addressed this using a hierarchical Hawkes\nprocess with an exponential kernel and edge-effect correction allowing partial\npooling across patients. This approach reduces bias from high-frequency\nindividuals and stabilizes estimates for those with sparse data. Bayesian\ninference was performed using the No U-Turn Sampler with model evaluation via\nconvergence diagnostics, power-scaling sensitivity analysis, and multiple\nGoodness-of-Fit (GOF) metrics: PSIS-LOO the Lewis test with Durbin's\nmodification and residual analysis based on the Random Time Change Theorem\n(RTCT). The hierarchical model yielded a significantly lower and more precise\nbranching factor estimate mean (0.742 +- 0.026) than the pooled model (0.899 +-\n0.015) and narrower intervals than the unpooled model (0.717 +- 0.139). This\nled to a threefold smaller cascade of events per onset under the hierarchical\nmodel. Sensitivity analyses confirmed robustness to prior and likelihood\nperturbations while the unpooled model showed instability for sparse\nindividuals. GOF measures consistently favored or on par to the hierarchical\nmodel. Hierarchical Hawkes modeling with edge-effect correction provides robust\nestimation of branching dynamics by capturing both within- and between-patient\nvariability. This enables clearer separation of endogenous from exogenous\nevents supports linkage to physiological signals and enhances early warning\nsystems individualized treatment and resource allocation in inpatient care."
                },
                "authors": [
                    {
                        "name": "Michael Potter"
                    },
                    {
                        "name": "Michael Everett"
                    },
                    {
                        "name": "Deniz Erdogmus"
                    },
                    {
                        "name": "Yuna Watanabe"
                    },
                    {
                        "name": "Tales Imbiriba"
                    },
                    {
                        "name": "Matthew S. Goodwin"
                    }
                ],
                "author_detail": {
                    "name": "Matthew S. Goodwin"
                },
                "author": "Matthew S. Goodwin",
                "arxiv_comment": "Submitted to BMC Bioinformatics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12415v1",
                "updated": "2025-07-16T17:05:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    5,
                    17,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:05:17Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    5,
                    17,
                    2,
                    197,
                    0
                ],
                "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?"
                },
                "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field."
                },
                "authors": [
                    {
                        "name": "Xinyi He"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Mingzhe Du"
                    },
                    {
                        "name": "Lin Yan"
                    },
                    {
                        "name": "Zhijie Fan"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zejian Yuan"
                    },
                    {
                        "name": "Zejun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zejun Ma"
                },
                "author": "Zejun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12412v1",
                "updated": "2025-07-16T17:00:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    0,
                    41,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:00:41Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    0,
                    41,
                    2,
                    197,
                    0
                ],
                "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal\n  Data"
                },
                "summary": "In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines."
                },
                "authors": [
                    {
                        "name": "Dzung Dinh"
                    },
                    {
                        "name": "Boqi Chen"
                    },
                    {
                        "name": "Marc Niethammer"
                    },
                    {
                        "name": "Junier Oliva"
                    }
                ],
                "author_detail": {
                    "name": "Junier Oliva"
                },
                "author": "Junier Oliva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10786v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10786v2",
                "updated": "2025-07-16T16:58:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    58,
                    46,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-14T20:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    27,
                    40,
                    0,
                    195,
                    0
                ],
                "title": "\"Is it always watching? Is it always listening?\" Exploring Contextual\n  Privacy and Security Concerns Toward Domestic Social Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Is it always watching? Is it always listening?\" Exploring Contextual\n  Privacy and Security Concerns Toward Domestic Social Robots"
                },
                "summary": "Equipped with artificial intelligence (AI) and advanced sensing capabilities,\nsocial robots are gaining interest among consumers in the United States. These\nrobots seem like a natural evolution of traditional smart home devices.\nHowever, their extensive data collection capabilities, anthropomorphic\nfeatures, and capacity to interact with their environment make social robots a\nmore significant security and privacy threat. Increased risks include data\nlinkage, unauthorized data sharing, and the physical safety of users and their\nhomes. It is critical to investigate U.S. users' security and privacy needs and\nconcerns to guide the design of social robots while these devices are still in\nthe early stages of commercialization in the U.S. market. Through 19\nsemi-structured interviews, we identified significant security and privacy\nconcerns, highlighting the need for transparency, usability, and robust privacy\ncontrols to support adoption. For educational applications, participants\nworried most about misinformation, and in medical use cases, they worried about\nthe reliability of these devices. Participants were also concerned with the\ndata inference that social robots could enable. We found that participants\nexpect tangible privacy controls, indicators of data collection, and\ncontext-appropriate functionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equipped with artificial intelligence (AI) and advanced sensing capabilities,\nsocial robots are gaining interest among consumers in the United States. These\nrobots seem like a natural evolution of traditional smart home devices.\nHowever, their extensive data collection capabilities, anthropomorphic\nfeatures, and capacity to interact with their environment make social robots a\nmore significant security and privacy threat. Increased risks include data\nlinkage, unauthorized data sharing, and the physical safety of users and their\nhomes. It is critical to investigate U.S. users' security and privacy needs and\nconcerns to guide the design of social robots while these devices are still in\nthe early stages of commercialization in the U.S. market. Through 19\nsemi-structured interviews, we identified significant security and privacy\nconcerns, highlighting the need for transparency, usability, and robust privacy\ncontrols to support adoption. For educational applications, participants\nworried most about misinformation, and in medical use cases, they worried about\nthe reliability of these devices. Participants were also concerned with the\ndata inference that social robots could enable. We found that participants\nexpect tangible privacy controls, indicators of data collection, and\ncontext-appropriate functionality."
                },
                "authors": [
                    {
                        "name": "Henry Bell"
                    },
                    {
                        "name": "Jabari Kwesi"
                    },
                    {
                        "name": "Hiba Laabadli"
                    },
                    {
                        "name": "Pardis Emami-Naeini"
                    }
                ],
                "author_detail": {
                    "name": "Pardis Emami-Naeini"
                },
                "author": "Pardis Emami-Naeini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10786v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10786v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08218v2",
                "updated": "2025-07-16T16:57:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    57,
                    48,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-10T23:47:05Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    23,
                    47,
                    5,
                    3,
                    191,
                    0
                ],
                "title": "Simple Mechanistic Explanations for Out-Of-Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Mechanistic Explanations for Out-Of-Context Reasoning"
                },
                "summary": "Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment."
                },
                "authors": [
                    {
                        "name": "Atticus Wang"
                    },
                    {
                        "name": "Joshua Engels"
                    },
                    {
                        "name": "Oliver Clive-Griffin"
                    },
                    {
                        "name": "Senthooran Rajamanoharan"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "ICML 2025 Workshop R2-FM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23175v2",
                "updated": "2025-07-16T16:49:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    49,
                    21,
                    2,
                    197,
                    0
                ],
                "published": "2025-03-29T18:09:36Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    18,
                    9,
                    36,
                    5,
                    88,
                    0
                ],
                "title": "Large Language Models are Unreliable for Cyber Threat Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Unreliable for Cyber Threat Intelligence"
                },
                "summary": "Several recent works have argued that Large Language Models (LLMs) can be\nused to tame the data deluge in the cybersecurity field, by improving the\nautomation of Cyber Threat Intelligence (CTI) tasks. This work presents an\nevaluation methodology that other than allowing to test LLMs on CTI tasks when\nusing zero-shot learning, few-shot learning and fine-tuning, also allows to\nquantify their consistency and their confidence level. We run experiments with\nthree state-of-the-art LLMs and a dataset of 350 threat intelligence reports\nand present new evidence of potential security risks in relying on LLMs for\nCTI. We show how LLMs cannot guarantee sufficient performance on real-size\nreports while also being inconsistent and overconfident. Few-shot learning and\nfine-tuning only partially improve the results, thus posing doubts about the\npossibility of using LLMs for CTI scenarios, where labelled datasets are\nlacking and where confidence is a fundamental factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several recent works have argued that Large Language Models (LLMs) can be\nused to tame the data deluge in the cybersecurity field, by improving the\nautomation of Cyber Threat Intelligence (CTI) tasks. This work presents an\nevaluation methodology that other than allowing to test LLMs on CTI tasks when\nusing zero-shot learning, few-shot learning and fine-tuning, also allows to\nquantify their consistency and their confidence level. We run experiments with\nthree state-of-the-art LLMs and a dataset of 350 threat intelligence reports\nand present new evidence of potential security risks in relying on LLMs for\nCTI. We show how LLMs cannot guarantee sufficient performance on real-size\nreports while also being inconsistent and overconfident. Few-shot learning and\nfine-tuning only partially improve the results, thus posing doubts about the\npossibility of using LLMs for CTI scenarios, where labelled datasets are\nlacking and where confidence is a fundamental factor."
                },
                "authors": [
                    {
                        "name": "Emanuele Mezzi"
                    },
                    {
                        "name": "Fabio Massacci"
                    },
                    {
                        "name": "Katja Tuma"
                    }
                ],
                "author_detail": {
                    "name": "Katja Tuma"
                },
                "author": "Katja Tuma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12399v1",
                "updated": "2025-07-16T16:44:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    44,
                    29,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:44:29Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    44,
                    29,
                    2,
                    197,
                    0
                ],
                "title": "ROC-n-reroll: How verifier imperfection affects test-time scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROC-n-reroll: How verifier imperfection affects test-time scaling"
                },
                "summary": "Test-time scaling aims to improve language model performance by leveraging\nadditional compute during inference. While many works have empirically studied\ntechniques like Best-of-N (BoN) and rejection sampling that make use of a\nverifier to enable test-time scaling, there is little theoretical understanding\nof how verifier imperfection affects performance. In this work, we address this\ngap. Specifically, we prove how instance-level accuracy of these methods is\nprecisely characterized by the geometry of the verifier's ROC curve.\nInterestingly, while scaling is determined by the local geometry of the ROC\ncurve for rejection sampling, it depends on global properties of the ROC curve\nfor BoN. As a consequence when the ROC curve is unknown, it is impossible to\nextrapolate the performance of rejection sampling based on the low-compute\nregime. Furthermore, while rejection sampling outperforms BoN for fixed\ncompute, in the infinite-compute limit both methods converge to the same level\nof accuracy, determined by the slope of the ROC curve near the origin. Our\ntheoretical results are confirmed by experiments on GSM8K using different\nversions of Llama and Qwen to generate and verify solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling aims to improve language model performance by leveraging\nadditional compute during inference. While many works have empirically studied\ntechniques like Best-of-N (BoN) and rejection sampling that make use of a\nverifier to enable test-time scaling, there is little theoretical understanding\nof how verifier imperfection affects performance. In this work, we address this\ngap. Specifically, we prove how instance-level accuracy of these methods is\nprecisely characterized by the geometry of the verifier's ROC curve.\nInterestingly, while scaling is determined by the local geometry of the ROC\ncurve for rejection sampling, it depends on global properties of the ROC curve\nfor BoN. As a consequence when the ROC curve is unknown, it is impossible to\nextrapolate the performance of rejection sampling based on the low-compute\nregime. Furthermore, while rejection sampling outperforms BoN for fixed\ncompute, in the infinite-compute limit both methods converge to the same level\nof accuracy, determined by the slope of the ROC curve near the origin. Our\ntheoretical results are confirmed by experiments on GSM8K using different\nversions of Llama and Qwen to generate and verify solutions."
                },
                "authors": [
                    {
                        "name": "Florian E. Dorner"
                    },
                    {
                        "name": "Yatong Chen"
                    },
                    {
                        "name": "Andr F. Cruz"
                    },
                    {
                        "name": "Fanny Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Yang"
                },
                "author": "Fanny Yang",
                "arxiv_comment": "35 pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12395v1",
                "updated": "2025-07-16T16:41:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    41,
                    17,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:41:17Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    41,
                    17,
                    2,
                    197,
                    0
                ],
                "title": "Invariant non-equilibrium dynamics of transcriptional regulation\n  optimize information flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariant non-equilibrium dynamics of transcriptional regulation\n  optimize information flow"
                },
                "summary": "Eukaryotic gene regulation is based on stochastic yet controlled promoter\nswitching, during which genes transition between transcriptionally active and\ninactive states. Despite the molecular complexity of this process, recent\nstudies reveal a surprising invariance of the \"switching correlation time\"\n($T_C$), which characterizes promoter activity fluctuations, across gene\nexpression levels in diverse genes and organisms. A biophysically plausible\nexplanation for this invariance remains missing. Here, we show that this\ninvariance imposes stringent constraints on minimal yet plausible models of\ntranscriptional regulation, requiring at least four system states and\nnon-equilibrium dynamics that break detailed balance. Using Bayesian inference\non Drosophila gap gene expression data, we demonstrate that such models (i)\naccurately reproduce the observed $T_C$-invariance; (ii) remain robust to\nparameter perturbations; and (iii) maximize information transmission from\ntranscription factor concentration to gene expression. These findings suggest\nthat eukaryotic gene regulation has evolved to balance precision with reaction\nrate and energy dissipation constraints, favoring non-equilibrium architectures\nfor optimal information transmission.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eukaryotic gene regulation is based on stochastic yet controlled promoter\nswitching, during which genes transition between transcriptionally active and\ninactive states. Despite the molecular complexity of this process, recent\nstudies reveal a surprising invariance of the \"switching correlation time\"\n($T_C$), which characterizes promoter activity fluctuations, across gene\nexpression levels in diverse genes and organisms. A biophysically plausible\nexplanation for this invariance remains missing. Here, we show that this\ninvariance imposes stringent constraints on minimal yet plausible models of\ntranscriptional regulation, requiring at least four system states and\nnon-equilibrium dynamics that break detailed balance. Using Bayesian inference\non Drosophila gap gene expression data, we demonstrate that such models (i)\naccurately reproduce the observed $T_C$-invariance; (ii) remain robust to\nparameter perturbations; and (iii) maximize information transmission from\ntranscription factor concentration to gene expression. These findings suggest\nthat eukaryotic gene regulation has evolved to balance precision with reaction\nrate and energy dissipation constraints, favoring non-equilibrium architectures\nfor optimal information transmission."
                },
                "authors": [
                    {
                        "name": "Benjamin Zoller"
                    },
                    {
                        "name": "Alexis Bnichou"
                    },
                    {
                        "name": "Thomas Gregor"
                    },
                    {
                        "name": "Gaper Tkaik"
                    }
                ],
                "author_detail": {
                    "name": "Gaper Tkaik"
                },
                "author": "Gaper Tkaik",
                "arxiv_comment": "13 pages, 4 figures, 10 SI figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12391v1",
                "updated": "2025-07-16T16:37:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    37,
                    13,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:37:13Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    37,
                    13,
                    2,
                    197,
                    0
                ],
                "title": "Assessing the Value of Visual Input: A Benchmark of Multimodal Large\n  Language Models for Robotic Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Value of Visual Input: A Benchmark of Multimodal Large\n  Language Models for Robotic Path Planning"
                },
                "summary": "Large Language Models (LLMs) show potential for enhancing robotic path\nplanning. This paper assesses visual input's utility for multimodal LLMs in\nsuch tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on\ngenerating valid and optimal paths in 2D grid environments, simulating\nsimplified robotic planning, comparing text-only versus text-plus-visual inputs\nacross varying model sizes and grid complexities. Our results indicate moderate\nsuccess rates on simpler small grids, where visual input or few-shot text\nprompting offered some benefits. However, performance significantly degraded on\nlarger grids, highlighting a scalability challenge. While larger models\ngenerally achieved higher average success, the visual modality was not\nuniversally dominant over well-structured text for these multimodal systems,\nand successful paths on simpler grids were generally of high quality. These\nresults indicate current limitations in robust spatial reasoning, constraint\nadherence, and scalable multimodal integration, identifying areas for future\nLLM development in robotic path planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show potential for enhancing robotic path\nplanning. This paper assesses visual input's utility for multimodal LLMs in\nsuch tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on\ngenerating valid and optimal paths in 2D grid environments, simulating\nsimplified robotic planning, comparing text-only versus text-plus-visual inputs\nacross varying model sizes and grid complexities. Our results indicate moderate\nsuccess rates on simpler small grids, where visual input or few-shot text\nprompting offered some benefits. However, performance significantly degraded on\nlarger grids, highlighting a scalability challenge. While larger models\ngenerally achieved higher average success, the visual modality was not\nuniversally dominant over well-structured text for these multimodal systems,\nand successful paths on simpler grids were generally of high quality. These\nresults indicate current limitations in robust spatial reasoning, constraint\nadherence, and scalable multimodal integration, identifying areas for future\nLLM development in robotic path planning."
                },
                "authors": [
                    {
                        "name": "Jacinto Colan"
                    },
                    {
                        "name": "Ana Davila"
                    },
                    {
                        "name": "Yasuhisa Hasegawa"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhisa Hasegawa"
                },
                "author": "Yasuhisa Hasegawa",
                "arxiv_comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)",
                "arxiv_journal_ref": "2025 SICE Festival with Annual Conference (SICE FES)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12385v1",
                "updated": "2025-07-16T16:31:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    31,
                    40,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:31:40Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    31,
                    40,
                    2,
                    197,
                    0
                ],
                "title": "Convergence of drift-diffusion PDEs arising as Wasserstein gradient\n  flows of convex functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convergence of drift-diffusion PDEs arising as Wasserstein gradient\n  flows of convex functions"
                },
                "summary": "We study the quantitative convergence of drift-diffusion PDEs that arise as\nWasserstein gradient flows of linearly convex functions over the space of\nprobability measures on ${\\mathbb R}^d$. In this setting, the objective is in\ngeneral not displacement convex, so it is not clear a priori whether global\nconvergence even holds. Still, our analysis reveals that diffusion {allows} a\nfavorable interaction between Wasserstein geometry and linear convexity,\nleading to a general quantitative convergence theory, analogous to that of\ngradient flows in convex settings in the Euclidean space.\n  Specifically, we prove that if the objective is convex and suitably coercive,\nthe suboptimality gap decreases at a rate $O(1/t)$. This improves to a rate\nfaster than any polynomial -- or even exponential in compact settings -- when\nthe objective is strongly convex relative to the entropy.\n  Our results extend the range of mean-field Langevin dynamics that enjoy\nquantitative convergence guarantees, and enable new applications to\noptimization over the space of probability measures. To illustrate this, we\nshow quantitative convergence results for the minimization of\nentropy-regularized nonconvex problems, we propose and study an\n\\emph{approximate Fisher Information} regularization covered by our setting,\nand we apply our results to an estimator for trajectory inference which\ninvolves the minimization of the relative entropy with respect to the Wiener\nmeasure in path space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the quantitative convergence of drift-diffusion PDEs that arise as\nWasserstein gradient flows of linearly convex functions over the space of\nprobability measures on ${\\mathbb R}^d$. In this setting, the objective is in\ngeneral not displacement convex, so it is not clear a priori whether global\nconvergence even holds. Still, our analysis reveals that diffusion {allows} a\nfavorable interaction between Wasserstein geometry and linear convexity,\nleading to a general quantitative convergence theory, analogous to that of\ngradient flows in convex settings in the Euclidean space.\n  Specifically, we prove that if the objective is convex and suitably coercive,\nthe suboptimality gap decreases at a rate $O(1/t)$. This improves to a rate\nfaster than any polynomial -- or even exponential in compact settings -- when\nthe objective is strongly convex relative to the entropy.\n  Our results extend the range of mean-field Langevin dynamics that enjoy\nquantitative convergence guarantees, and enable new applications to\noptimization over the space of probability measures. To illustrate this, we\nshow quantitative convergence results for the minimization of\nentropy-regularized nonconvex problems, we propose and study an\n\\emph{approximate Fisher Information} regularization covered by our setting,\nand we apply our results to an estimator for trajectory inference which\ninvolves the minimization of the relative entropy with respect to the Wiener\nmeasure in path space."
                },
                "authors": [
                    {
                        "name": "Lnac Chizat"
                    },
                    {
                        "name": "Maria Colombo"
                    },
                    {
                        "name": "Xavier Fernndez-Real"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Fernndez-Real"
                },
                "author": "Xavier Fernndez-Real",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49Q22, 35Q84, 35B40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12384v1",
                "updated": "2025-07-16T16:31:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    31,
                    20,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:31:20Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    31,
                    20,
                    2,
                    197,
                    0
                ],
                "title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog\n  CAM with Inherent Soft Boundaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog\n  CAM with Inherent Soft Boundaries"
                },
                "summary": "The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency."
                },
                "authors": [
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Guoyun Gao"
                    },
                    {
                        "name": "Zhicheng Xu"
                    },
                    {
                        "name": "Ruibin Mao"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "X. Sharon Hu"
                    },
                    {
                        "name": "Xunzhao Yin"
                    },
                    {
                        "name": "Can Li"
                    }
                ],
                "author_detail": {
                    "name": "Can Li"
                },
                "author": "Can Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12378v1",
                "updated": "2025-07-16T16:27:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    27,
                    5,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:27:05Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    27,
                    5,
                    2,
                    197,
                    0
                ],
                "title": "Developing Visual Augmented Q&A System using Scalable Vision Embedding\n  Retrieval & Late Interaction Re-ranker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Visual Augmented Q&A System using Scalable Vision Embedding\n  Retrieval & Late Interaction Re-ranker"
                },
                "summary": "Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises."
                },
                "authors": [
                    {
                        "name": "Rachna Saxena"
                    },
                    {
                        "name": "Abhijeet Kumar"
                    },
                    {
                        "name": "Suresh Shanmugam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Shanmugam"
                },
                "author": "Suresh Shanmugam",
                "arxiv_comment": "Presented at NLP@IR workshop at SIGIR conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12372v1",
                "updated": "2025-07-16T16:21:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    21,
                    1,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:21:01Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    21,
                    1,
                    2,
                    197,
                    0
                ],
                "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User\n  Demographics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User\n  Demographics"
                },
                "summary": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes."
                },
                "authors": [
                    {
                        "name": "Meysam Alizadeh"
                    },
                    {
                        "name": "Fabrizio Gilardi"
                    },
                    {
                        "name": "Zeynab Samei"
                    },
                    {
                        "name": "Mohsen Mosleh"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Mosleh"
                },
                "author": "Mohsen Mosleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02179v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02179v3",
                "updated": "2025-07-16T16:15:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    15,
                    30,
                    2,
                    197,
                    0
                ],
                "published": "2024-11-04T15:37:18Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    15,
                    37,
                    18,
                    0,
                    309,
                    0
                ],
                "title": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile\n  Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile\n  Augmented Reality"
                },
                "summary": "High-quality environment lighting is essential for creating immersive mobile\naugmented reality (AR) experiences. However, achieving visually coherent\nestimation for mobile AR is challenging due to several key limitations in AR\ndevice sensing capabilities, including low camera FoV and limited pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address two key limitations of\ncontent quality and slow inference. In this work, we design and implement a\ngenerative lighting estimation system called CleAR that can produce\nhigh-quality, diverse environment maps in the format of 360{\\deg} HDR images.\nSpecifically, we design a two-step generation pipeline guided by AR environment\ncontext data to ensure the output aligns with the physical environment's visual\ncontext and color appearance. To improve the estimation robustness under\ndifferent lighting conditions, we design a real-time refinement component to\nadjust lighting estimation results on AR devices. Through a combination of\nquantitative and qualitative evaluations, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy,\nlatency, and robustness, and is rated by 31 participants as producing better\nrenderings for most virtual objects. For example, CleAR achieves 51% to 56%\naccuracy improvement on virtual object renderings across objects of three\ndistinctive types of materials and reflective properties. CleAR produces\nlighting estimates of comparable or better quality in just 3.2 seconds -- over\n110X faster than state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality environment lighting is essential for creating immersive mobile\naugmented reality (AR) experiences. However, achieving visually coherent\nestimation for mobile AR is challenging due to several key limitations in AR\ndevice sensing capabilities, including low camera FoV and limited pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address two key limitations of\ncontent quality and slow inference. In this work, we design and implement a\ngenerative lighting estimation system called CleAR that can produce\nhigh-quality, diverse environment maps in the format of 360{\\deg} HDR images.\nSpecifically, we design a two-step generation pipeline guided by AR environment\ncontext data to ensure the output aligns with the physical environment's visual\ncontext and color appearance. To improve the estimation robustness under\ndifferent lighting conditions, we design a real-time refinement component to\nadjust lighting estimation results on AR devices. Through a combination of\nquantitative and qualitative evaluations, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy,\nlatency, and robustness, and is rated by 31 participants as producing better\nrenderings for most virtual objects. For example, CleAR achieves 51% to 56%\naccuracy improvement on virtual object renderings across objects of three\ndistinctive types of materials and reflective properties. CleAR produces\nlighting estimates of comparable or better quality in just 3.2 seconds -- over\n110X faster than state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yiqin Zhao"
                    },
                    {
                        "name": "Mallesham Dasari"
                    },
                    {
                        "name": "Tian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Tian Guo"
                },
                "author": "Tian Guo",
                "arxiv_doi": "10.1145/3749535",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3749535",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02179v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02179v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12370v1",
                "updated": "2025-07-16T16:15:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    15,
                    25,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:15:25Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    15,
                    25,
                    2,
                    197,
                    0
                ],
                "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests\n  through Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests\n  through Debate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems."
                },
                "authors": [
                    {
                        "name": "Ana Davila"
                    },
                    {
                        "name": "Jacinto Colan"
                    },
                    {
                        "name": "Yasuhisa Hasegawa"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhisa Hasegawa"
                },
                "author": "Yasuhisa Hasegawa",
                "arxiv_comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)",
                "arxiv_journal_ref": "2025 SICE Festival with Annual Conference (SICE FES)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12367v1",
                "updated": "2025-07-16T16:10:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    10,
                    42,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:10:42Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    10,
                    42,
                    2,
                    197,
                    0
                ],
                "title": "GitChameleon: Evaluating AI Code Generation Against Python Library\n  Version Incompatibilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GitChameleon: Evaluating AI Code Generation Against Python Library\n  Version Incompatibilities"
                },
                "summary": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark."
                },
                "authors": [
                    {
                        "name": "Diganta Misra"
                    },
                    {
                        "name": "Nizar Islah"
                    },
                    {
                        "name": "Victor May"
                    },
                    {
                        "name": "Brice Rauby"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Justine Gehring"
                    },
                    {
                        "name": "Antonio Orvieto"
                    },
                    {
                        "name": "Muawiz Chaudhary"
                    },
                    {
                        "name": "Eilif B. Muller"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Samira Ebrahimi Kahou"
                    },
                    {
                        "name": "Massimo Caccia"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Caccia"
                },
                "author": "Massimo Caccia",
                "arxiv_comment": "Version 2 of the dataset from: arXiv:2411.05830",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12364v1",
                "updated": "2025-07-16T16:08:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    8,
                    24,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:08:24Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    8,
                    24,
                    2,
                    197,
                    0
                ],
                "title": "Rethinking the confidential cloud through a unified low-level\n  abstraction for composable isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the confidential cloud through a unified low-level\n  abstraction for composable isolation"
                },
                "summary": "Securing sensitive cloud workloads requires composing confidential virtual\nmachines (CVMs) with nested enclaves or sandboxes. Unfortunately, each new\nisolation boundary adds ad-hoc access control mechanisms, hardware extensions,\nand trusted software. This escalating complexity bloats the TCB, complicates\nend-to-end attestation, and leads to fragmentation across platforms and cloud\nservice providers (CSPs).\n  We introduce a unified isolation model that delegates enforceable,\ncomposable, and attestable isolation to a single trusted security monitor:\nTyche. Tyche provides an API for partitioning, sharing, attesting, and\nreclaiming resources through its core abstraction, trust domains (TDs). To\nprovide fine-grain isolation, TDs can recursively create and manage sub-TDs.\nTyche captures these relationships in attestations, allowing cloud tenants to\nreason about end-to-end security. TDs serve as the building blocks for\nconstructing composable enclaves, sandboxes, and CVMs.\n  Tyche runs on commodity x86_64 without hardware security extensions and can\nmaintain backward compatibility with existing software. We provide an SDK to\nrun and compose unmodified workloads as sandboxes, enclaves, and CVMs with\nminimal overhead compared to native Linux execution. Tyche supports complex\ncloud scenarios, such as confidential inference with mutually distrustful\nusers, model owners, and CSPs. An additional RISC-V prototype demonstrates\nTyche's portability across platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing sensitive cloud workloads requires composing confidential virtual\nmachines (CVMs) with nested enclaves or sandboxes. Unfortunately, each new\nisolation boundary adds ad-hoc access control mechanisms, hardware extensions,\nand trusted software. This escalating complexity bloats the TCB, complicates\nend-to-end attestation, and leads to fragmentation across platforms and cloud\nservice providers (CSPs).\n  We introduce a unified isolation model that delegates enforceable,\ncomposable, and attestable isolation to a single trusted security monitor:\nTyche. Tyche provides an API for partitioning, sharing, attesting, and\nreclaiming resources through its core abstraction, trust domains (TDs). To\nprovide fine-grain isolation, TDs can recursively create and manage sub-TDs.\nTyche captures these relationships in attestations, allowing cloud tenants to\nreason about end-to-end security. TDs serve as the building blocks for\nconstructing composable enclaves, sandboxes, and CVMs.\n  Tyche runs on commodity x86_64 without hardware security extensions and can\nmaintain backward compatibility with existing software. We provide an SDK to\nrun and compose unmodified workloads as sandboxes, enclaves, and CVMs with\nminimal overhead compared to native Linux execution. Tyche supports complex\ncloud scenarios, such as confidential inference with mutually distrustful\nusers, model owners, and CSPs. An additional RISC-V prototype demonstrates\nTyche's portability across platforms."
                },
                "authors": [
                    {
                        "name": "Adrien Ghosn"
                    },
                    {
                        "name": "Charly Castes"
                    },
                    {
                        "name": "Neelu S. Kalani"
                    },
                    {
                        "name": "Yuchen Qian"
                    },
                    {
                        "name": "Marios Kogias"
                    },
                    {
                        "name": "Edouard Bugnion"
                    }
                ],
                "author_detail": {
                    "name": "Edouard Bugnion"
                },
                "author": "Edouard Bugnion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10530v2",
                "updated": "2025-07-16T15:55:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    55,
                    28,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-14T17:54:47Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    54,
                    47,
                    0,
                    195,
                    0
                ],
                "title": "Accurate generation of chemical reaction transition states by\n  conditional flow matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate generation of chemical reaction transition states by\n  conditional flow matching"
                },
                "summary": "Transition state (TS) structures define the critical geometries and energy\nbarriers underlying chemical reactivity, yet their fleeting nature renders them\nexperimentally elusive and drives the reliance on costly, high-throughput\ndensity functional theory (DFT) calculations. Here, we introduce TS-GEN, a\nconditional flow-matching generative model that maps samples from a simple\nGaussian prior directly to transition-state saddle-point geometries in a\nsingle, deterministic pass. By embedding both reactant and product\nconformations as conditioning information, TS-GEN learns to transport latent\nnoise to true TS structures via an optimal-transport path, effectively\nreplacing the iterative optimization common in nudged-elastic band or\nstring-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a\nroot-mean-square deviation of $0.004\\ \\rm{\\mathring{A}}$ (vs. $0.103\\\n\\rm{\\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error\nof $1.019\\ {\\rm kcal/mol}$ (vs. $2.864\\ {\\rm kcal/mol}$), while requiring only\n$0.06\\ {\\rm s}$ GPU time per inference. Over 87% of generated TSs meet\nchemical-accuracy criteria ($<1.58\\ {\\rm kcal/mol}$ error), substantially\noutpacing existing methods. TS-GEN also exhibits strong transferability to\nout-of-distribution reactions from a larger database. By uniting sub-angstrom\nprecision, sub-second speed, and broad applicability, TS-GEN will be highly\nuseful for high-throughput exploration of complex reaction networks, paving the\nway to the exploration of novel chemical reaction mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transition state (TS) structures define the critical geometries and energy\nbarriers underlying chemical reactivity, yet their fleeting nature renders them\nexperimentally elusive and drives the reliance on costly, high-throughput\ndensity functional theory (DFT) calculations. Here, we introduce TS-GEN, a\nconditional flow-matching generative model that maps samples from a simple\nGaussian prior directly to transition-state saddle-point geometries in a\nsingle, deterministic pass. By embedding both reactant and product\nconformations as conditioning information, TS-GEN learns to transport latent\nnoise to true TS structures via an optimal-transport path, effectively\nreplacing the iterative optimization common in nudged-elastic band or\nstring-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a\nroot-mean-square deviation of $0.004\\ \\rm{\\mathring{A}}$ (vs. $0.103\\\n\\rm{\\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error\nof $1.019\\ {\\rm kcal/mol}$ (vs. $2.864\\ {\\rm kcal/mol}$), while requiring only\n$0.06\\ {\\rm s}$ GPU time per inference. Over 87% of generated TSs meet\nchemical-accuracy criteria ($<1.58\\ {\\rm kcal/mol}$ error), substantially\noutpacing existing methods. TS-GEN also exhibits strong transferability to\nout-of-distribution reactions from a larger database. By uniting sub-angstrom\nprecision, sub-second speed, and broad applicability, TS-GEN will be highly\nuseful for high-throughput exploration of complex reaction networks, paving the\nway to the exploration of novel chemical reaction mechanisms."
                },
                "authors": [
                    {
                        "name": "Ping Tuo"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Ju Li"
                    }
                ],
                "author_detail": {
                    "name": "Ju Li"
                },
                "author": "Ju Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23603v2",
                "updated": "2025-07-16T15:51:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    51,
                    30,
                    2,
                    197,
                    0
                ],
                "published": "2025-06-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    8,
                    15,
                    0,
                    181,
                    0
                ],
                "title": "SoK: Semantic Privacy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Semantic Privacy in Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs."
                },
                "authors": [
                    {
                        "name": "Baihe Ma"
                    },
                    {
                        "name": "Yanna Jiang"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Guangsheng Yu"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Caijun Sun"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Xuelei Qi"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Ren Ping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ren Ping Liu"
                },
                "author": "Ren Ping Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09477v2",
                "updated": "2025-07-16T15:44:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    44,
                    18,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-13T03:29:41Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    3,
                    29,
                    41,
                    6,
                    194,
                    0
                ],
                "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs"
                },
                "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning."
                },
                "authors": [
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yuyao Yang"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Yaozu Wu"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Zhongfen Deng"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Dongyuan Li"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "submitted to ARR May",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03103v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03103v3",
                "updated": "2025-07-16T15:44:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    44,
                    4,
                    2,
                    197,
                    0
                ],
                "published": "2024-10-04T02:53:52Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    53,
                    52,
                    4,
                    278,
                    0
                ],
                "title": "Planning-Aware Code Infilling via Horizon-Length Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning-Aware Code Infilling via Horizon-Length Prediction"
                },
                "summary": "Fill-in-the-Middle (FIM), or infilling, has become integral to code language\nmodels, enabling generation of missing code given both left and right contexts.\nHowever, the current FIM training paradigm which performs next-token prediction\n(NTP) over reordered sequence often leads to models struggling to generate\ncontent that aligns well with the surrounding context. We hypothesize that NTP\nalone is insufficient for models to learn effective planning conditioned on the\ndistant right context, a critical factor for successful code infilling. To\novercome this, we propose Horizon-Length Prediction (HLP), a novel training\nobjective that teaches models to predict the number of remaining middle tokens\nat each step. HLP advances FIM with lookahead planning, enabling models to\ninherently learn infilling boundaries for arbitrary left and right contexts\nwithout relying on dataset-specific post-processing. Our evaluation across\ndifferent model families and sizes shows that HLP significantly improves FIM\nperformance by up to 24% relatively on diverse benchmarks, across file-level\nand repository-level. Furthermore, the enhanced planning capability gained\nthrough HLP boosts model performance on code reasoning. Importantly, HLP incurs\nnegligible training overhead and no additional inference cost, ensuring its\npracticality for real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fill-in-the-Middle (FIM), or infilling, has become integral to code language\nmodels, enabling generation of missing code given both left and right contexts.\nHowever, the current FIM training paradigm which performs next-token prediction\n(NTP) over reordered sequence often leads to models struggling to generate\ncontent that aligns well with the surrounding context. We hypothesize that NTP\nalone is insufficient for models to learn effective planning conditioned on the\ndistant right context, a critical factor for successful code infilling. To\novercome this, we propose Horizon-Length Prediction (HLP), a novel training\nobjective that teaches models to predict the number of remaining middle tokens\nat each step. HLP advances FIM with lookahead planning, enabling models to\ninherently learn infilling boundaries for arbitrary left and right contexts\nwithout relying on dataset-specific post-processing. Our evaluation across\ndifferent model families and sizes shows that HLP significantly improves FIM\nperformance by up to 24% relatively on diverse benchmarks, across file-level\nand repository-level. Furthermore, the enhanced planning capability gained\nthrough HLP boosts model performance on code reasoning. Importantly, HLP incurs\nnegligible training overhead and no additional inference cost, ensuring its\npracticality for real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Yifeng Ding"
                    },
                    {
                        "name": "Hantian Ding"
                    },
                    {
                        "name": "Shiqi Wang"
                    },
                    {
                        "name": "Qing Sun"
                    },
                    {
                        "name": "Varun Kumar"
                    },
                    {
                        "name": "Zijian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zijian Wang"
                },
                "author": "Zijian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03103v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03103v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12341v1",
                "updated": "2025-07-16T15:36:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    36,
                    15,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T15:36:15Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    36,
                    15,
                    2,
                    197,
                    0
                ],
                "title": "Nonlinear Concept Erasure: a Density Matching Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear Concept Erasure: a Density Matching Approach"
                },
                "summary": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness."
                },
                "authors": [
                    {
                        "name": "Antoine Saillenfest"
                    },
                    {
                        "name": "Pirmin Lemberger"
                    }
                ],
                "author_detail": {
                    "name": "Pirmin Lemberger"
                },
                "author": "Pirmin Lemberger",
                "arxiv_comment": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10644v2",
                "updated": "2025-07-16T15:30:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    30,
                    42,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-14T16:47:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    47,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web\n  of Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web\n  of Agents"
                },
                "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA."
                },
                "authors": [
                    {
                        "name": "Tatiana Petrova"
                    },
                    {
                        "name": "Boris Bliznioukov"
                    },
                    {
                        "name": "Aleksandr Puzikov"
                    },
                    {
                        "name": "Radu State"
                    }
                ],
                "author_detail": {
                    "name": "Radu State"
                },
                "arxiv_affiliation": "SEDAN SnT, University of Luxembourg, Luxembourg, Luxembourg",
                "author": "Radu State",
                "arxiv_comment": "33 pages, 9 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08526v2",
                "updated": "2025-07-16T15:30:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    30,
                    36,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-11T12:25:06Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    25,
                    6,
                    4,
                    192,
                    0
                ],
                "title": "Anisotropic Diffusion of $e^\\pm$ in Pulsar Halos over Multiple Coherence\n  of Magnetic Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anisotropic Diffusion of $e^\\pm$ in Pulsar Halos over Multiple Coherence\n  of Magnetic Fields"
                },
                "summary": "The slow particle diffusion in pulsar halos, inferred from TeV gamma-ray\nsurface brightness profiles, is attributed to cross-field diffusion under the\nanisotropic diffusion model. This model assumes sub-Alfv\\'enic interstellar\nturbulence in the surrounding medium of the pulsar and a rough alignment of the\nline-of-sight of observers towards the pulsars with the local mean magnetic\nfield direction in the halo. In this model, the expected morphology of a pulsar\nhalo is highly dependent on the properties of the interstellar magnetic field.\nWe investigate the anisotropic diffusion of electron-positron pairs across\nmultiple coherence of magnetic fields in pulsar halos in this work. We focus\nparticularly on their influences on the predicted gamma-ray surface brightness\nprofile and the asymmetry of the halo's morphology, as well as the\nobservational expectations by the Large High Altitude Air Shower Observatory\n(LHAASO). Our results indicate that the requirement of a specific magnetic\nfield geometry can be alleviated when accounting for a limited (and realistic)\ncoherence length of the magnetic field in the model. Also, the halo's\nmorphology may appear less asymmetric, especially after being smoothed by the\npoint spread function of instruments. It largely relaxes the tension between\nthe asymmetric morphology of halos predicted by the model and lack of apparent\nasymmetric halos detected so far. Our findings demonstrate the important\ninfluence of the coherence length of interstellar magnetic field on the\ndistribution of particles around their accelerators, and the consequence on the\nmeasured source morphology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The slow particle diffusion in pulsar halos, inferred from TeV gamma-ray\nsurface brightness profiles, is attributed to cross-field diffusion under the\nanisotropic diffusion model. This model assumes sub-Alfv\\'enic interstellar\nturbulence in the surrounding medium of the pulsar and a rough alignment of the\nline-of-sight of observers towards the pulsars with the local mean magnetic\nfield direction in the halo. In this model, the expected morphology of a pulsar\nhalo is highly dependent on the properties of the interstellar magnetic field.\nWe investigate the anisotropic diffusion of electron-positron pairs across\nmultiple coherence of magnetic fields in pulsar halos in this work. We focus\nparticularly on their influences on the predicted gamma-ray surface brightness\nprofile and the asymmetry of the halo's morphology, as well as the\nobservational expectations by the Large High Altitude Air Shower Observatory\n(LHAASO). Our results indicate that the requirement of a specific magnetic\nfield geometry can be alleviated when accounting for a limited (and realistic)\ncoherence length of the magnetic field in the model. Also, the halo's\nmorphology may appear less asymmetric, especially after being smoothed by the\npoint spread function of instruments. It largely relaxes the tension between\nthe asymmetric morphology of halos predicted by the model and lack of apparent\nasymmetric halos detected so far. Our findings demonstrate the important\ninfluence of the coherence length of interstellar magnetic field on the\ndistribution of particles around their accelerators, and the consequence on the\nmeasured source morphology."
                },
                "authors": [
                    {
                        "name": "Kai Yan"
                    },
                    {
                        "name": "Sha Wu"
                    },
                    {
                        "name": "Ruo-Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ruo-Yu Liu"
                },
                "author": "Ruo-Yu Liu",
                "arxiv_doi": "10.3847/1538-4357/add6a4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/add6a4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 8 figures, 1 table; Published by The Astrophysical Journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10628v2",
                "updated": "2025-07-16T15:30:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    30,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-14T08:10:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    10,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement\n  Learning"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for facilitating the self-improvement of large language\nmodels (LLMs), particularly in the domain of complex reasoning tasks. However,\nprevailing on-policy RL methods often contend with significant training\ninstability and inefficiency. This is primarily due to a capacity-difficulty\nmismatch, where the complexity of training data frequently outpaces the model's\ncurrent capabilities, leading to critically sparse reward signals and stalled\nlearning progress. This challenge is particularly acute for smaller, more\nresource-efficient LLMs. To overcome this, we introduce the Guided Hybrid\nPolicy Optimization (GHPO), a novel difficulty-aware reinforcement learning\nframework. GHPO dynamically calibrates task difficulty by employing adaptive\nprompt refinement to provide targeted guidance. This unique approach adaptively\nbalances direct imitation learning for problems currently beyond the model's\nreach with exploration-based reinforcement learning for more manageable tasks,\neffectively creating a smooth and optimized learning curriculum. Extensive\nexperiments demonstrate that GHPO achieves an average performance gain of\napproximately 5% across six challenging mathematics benchmarks, consistently\noutperforming strong on-policy reinforcement learning and curriculum learning\nbaselines. Further analysis confirms that our framework significantly enhances\nboth training stability and final reasoning performance, thus offering a\nscalable and efficient solution for developing powerful and robust reasoning\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for facilitating the self-improvement of large language\nmodels (LLMs), particularly in the domain of complex reasoning tasks. However,\nprevailing on-policy RL methods often contend with significant training\ninstability and inefficiency. This is primarily due to a capacity-difficulty\nmismatch, where the complexity of training data frequently outpaces the model's\ncurrent capabilities, leading to critically sparse reward signals and stalled\nlearning progress. This challenge is particularly acute for smaller, more\nresource-efficient LLMs. To overcome this, we introduce the Guided Hybrid\nPolicy Optimization (GHPO), a novel difficulty-aware reinforcement learning\nframework. GHPO dynamically calibrates task difficulty by employing adaptive\nprompt refinement to provide targeted guidance. This unique approach adaptively\nbalances direct imitation learning for problems currently beyond the model's\nreach with exploration-based reinforcement learning for more manageable tasks,\neffectively creating a smooth and optimized learning curriculum. Extensive\nexperiments demonstrate that GHPO achieves an average performance gain of\napproximately 5% across six challenging mathematics benchmarks, consistently\noutperforming strong on-policy reinforcement learning and curriculum learning\nbaselines. Further analysis confirms that our framework significantly enhances\nboth training stability and final reasoning performance, thus offering a\nscalable and efficient solution for developing powerful and robust reasoning\nmodels."
                },
                "authors": [
                    {
                        "name": "Ziru Liu"
                    },
                    {
                        "name": "Cheng Gong"
                    },
                    {
                        "name": "Xinyu Fu"
                    },
                    {
                        "name": "Yaofang Liu"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Shoubo Hu"
                    },
                    {
                        "name": "Suiyun Zhang"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Dandan Tu"
                    }
                ],
                "author_detail": {
                    "name": "Dandan Tu"
                },
                "author": "Dandan Tu",
                "arxiv_comment": "Code avaiable at https://github.com/hkgc-1/GHPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12314v1",
                "updated": "2025-07-16T15:09:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    9,
                    13,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T15:09:13Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    9,
                    13,
                    2,
                    197,
                    0
                ],
                "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack"
                },
                "summary": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures."
                },
                "authors": [
                    {
                        "name": "Zihao Xue"
                    },
                    {
                        "name": "Zhen Bi"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Zhenlin Hu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zhenfang Liu"
                    },
                    {
                        "name": "Qing Sheng"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Jungang Lou"
                    }
                ],
                "author_detail": {
                    "name": "Jungang Lou"
                },
                "author": "Jungang Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12308v1",
                "updated": "2025-07-16T15:05:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    5,
                    30,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T15:05:30Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    5,
                    30,
                    2,
                    197,
                    0
                ],
                "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and\n  Summarization"
                },
                "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL."
                },
                "authors": [
                    {
                        "name": "Prashanth Vijayaraghavan"
                    },
                    {
                        "name": "Apoorva Nitsure"
                    },
                    {
                        "name": "Charles Mackin"
                    },
                    {
                        "name": "Luyao Shi"
                    },
                    {
                        "name": "Stefano Ambrogio"
                    },
                    {
                        "name": "Arvind Haran"
                    },
                    {
                        "name": "Viresh Paruthi"
                    },
                    {
                        "name": "Ali Elzein"
                    },
                    {
                        "name": "Dan Coops"
                    },
                    {
                        "name": "David Beymer"
                    },
                    {
                        "name": "Tyler Baldwin"
                    },
                    {
                        "name": "Ehsan Degan"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Degan"
                },
                "author": "Ehsan Degan",
                "arxiv_doi": "10.1145/3670474.3685966",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3670474.3685966",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.12308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12305v1",
                "updated": "2025-07-16T15:04:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    4,
                    46,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T15:04:46Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    4,
                    46,
                    2,
                    197,
                    0
                ],
                "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt\n  Online Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt\n  Online Learning"
                },
                "summary": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL."
                },
                "authors": [
                    {
                        "name": "M. Anwar Ma'sum"
                    },
                    {
                        "name": "Mahardhika Pratama"
                    },
                    {
                        "name": "Savitha Ramasamy"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Habibullah Habibullah"
                    },
                    {
                        "name": "Ryszard Kowalczyk"
                    }
                ],
                "author_detail": {
                    "name": "Ryszard Kowalczyk"
                },
                "author": "Ryszard Kowalczyk",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12296v1",
                "updated": "2025-07-16T14:49:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    49,
                    45,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:49:45Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    49,
                    45,
                    2,
                    197,
                    0
                ],
                "title": "Humans are more gullible than LLMs in believing common psychological\n  myths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans are more gullible than LLMs in believing common psychological\n  myths"
                },
                "summary": "Despite widespread debunking, many psychological myths remain deeply\nentrenched. This paper investigates whether Large Language Models (LLMs) mimic\nhuman behaviour of myth belief and explores methods to mitigate such\ntendencies. Using 50 popular psychological myths, we evaluate myth belief\nacross multiple LLMs under different prompting strategies, including\nretrieval-augmented generation and swaying prompts. Results show that LLMs\nexhibit significantly lower myth belief rates than humans, though user\nprompting can influence responses. RAG proves effective in reducing myth belief\nand reveals latent debiasing potential within LLMs. Our findings contribute to\nthe emerging field of Machine Psychology and highlight how cognitive science\nmethods can inform the evaluation and development of LLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite widespread debunking, many psychological myths remain deeply\nentrenched. This paper investigates whether Large Language Models (LLMs) mimic\nhuman behaviour of myth belief and explores methods to mitigate such\ntendencies. Using 50 popular psychological myths, we evaluate myth belief\nacross multiple LLMs under different prompting strategies, including\nretrieval-augmented generation and swaying prompts. Results show that LLMs\nexhibit significantly lower myth belief rates than humans, though user\nprompting can influence responses. RAG proves effective in reducing myth belief\nand reveals latent debiasing potential within LLMs. Our findings contribute to\nthe emerging field of Machine Psychology and highlight how cognitive science\nmethods can inform the evaluation and development of LLM-based systems."
                },
                "authors": [
                    {
                        "name": "Bevan Koopman"
                    },
                    {
                        "name": "Guido Zuccon"
                    }
                ],
                "author_detail": {
                    "name": "Guido Zuccon"
                },
                "author": "Guido Zuccon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12295v1",
                "updated": "2025-07-16T14:47:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    47,
                    41,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:47:41Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    47,
                    41,
                    2,
                    197,
                    0
                ],
                "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding"
                },
                "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems."
                },
                "authors": [
                    {
                        "name": "Feng Xiao"
                    },
                    {
                        "name": "Jicong Fan"
                    }
                ],
                "author_detail": {
                    "name": "Jicong Fan"
                },
                "author": "Jicong Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12292v1",
                "updated": "2025-07-16T14:44:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    44,
                    29,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:44:29Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    44,
                    29,
                    2,
                    197,
                    0
                ],
                "title": "Efficient Calisthenics Skills Classification through Foreground Instance\n  Selection and Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Calisthenics Skills Classification through Foreground Instance\n  Selection and Depth Estimation"
                },
                "summary": "Calisthenics skill classification is the computer vision task of inferring\nthe skill performed by an athlete from images, enabling automatic performance\nassessment and personalized analytics. Traditional methods for calisthenics\nskill recognition are based on pose estimation methods to determine the\nposition of skeletal data from images, which is later fed to a classification\nalgorithm to infer the performed skill. Despite the progress in human pose\nestimation algorithms, they still involve high computational costs, long\ninference times, and complex setups, which limit the applicability of such\napproaches in real-time applications or mobile devices. This work proposes a\ndirect approach to calisthenics skill recognition, which leverages depth\nestimation and athlete patch retrieval to avoid the computationally expensive\nhuman pose estimation module. Using Depth Anything V2 for depth estimation and\nYOLOv10 for athlete localization, we segment the subject from the background\nrather than relying on traditional pose estimation techniques. This strategy\nincreases efficiency, reduces inference time, and improves classification\naccuracy. Our approach significantly outperforms skeleton-based methods,\nachieving 38.3x faster inference with RGB image patches and improved\nclassification accuracy with depth patches (0.837 vs. 0.815). Beyond these\nperformance gains, the modular design of our pipeline allows for flexible\nreplacement of components, enabling future enhancements and adaptation to\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calisthenics skill classification is the computer vision task of inferring\nthe skill performed by an athlete from images, enabling automatic performance\nassessment and personalized analytics. Traditional methods for calisthenics\nskill recognition are based on pose estimation methods to determine the\nposition of skeletal data from images, which is later fed to a classification\nalgorithm to infer the performed skill. Despite the progress in human pose\nestimation algorithms, they still involve high computational costs, long\ninference times, and complex setups, which limit the applicability of such\napproaches in real-time applications or mobile devices. This work proposes a\ndirect approach to calisthenics skill recognition, which leverages depth\nestimation and athlete patch retrieval to avoid the computationally expensive\nhuman pose estimation module. Using Depth Anything V2 for depth estimation and\nYOLOv10 for athlete localization, we segment the subject from the background\nrather than relying on traditional pose estimation techniques. This strategy\nincreases efficiency, reduces inference time, and improves classification\naccuracy. Our approach significantly outperforms skeleton-based methods,\nachieving 38.3x faster inference with RGB image patches and improved\nclassification accuracy with depth patches (0.837 vs. 0.815). Beyond these\nperformance gains, the modular design of our pipeline allows for flexible\nreplacement of components, enabling future enhancements and adaptation to\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Antonio Finocchiaro"
                    },
                    {
                        "name": "Giovanni Maria Farinella"
                    },
                    {
                        "name": "Antonino Furnari"
                    }
                ],
                "author_detail": {
                    "name": "Antonino Furnari"
                },
                "author": "Antonino Furnari",
                "arxiv_comment": "13 pages, 4 figures, In International Conference on Image Analysis\n  and Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12286v1",
                "updated": "2025-07-16T14:38:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    38,
                    27,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:38:27Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    38,
                    27,
                    2,
                    197,
                    0
                ],
                "title": "SHACL Validation in the Presence of Ontologies: Semantics and Rewriting\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHACL Validation in the Presence of Ontologies: Semantics and Rewriting\n  Techniques"
                },
                "summary": "SHACL and OWL are two prominent W3C standards for managing RDF data. These\nlanguages share many features, but they have one fundamental difference: OWL,\ndesigned for inferring facts from incomplete data, makes the open-world\nassumption, whereas SHACL is a constraint language that treats the data as\ncomplete and must be validated under the closed-world assumption. The\ncombination of both formalisms is very appealing and has been called for, but\ntheir semantic gap is a major challenge, semantically and computationally. In\nthis paper, we advocate a semantics for SHACL validation in the presence of\nontologies based on core universal models. We provide a technique for\nconstructing these models for ontologies in the rich data-tractable description\nlogic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to\ndevelop a rewriting technique that reduces SHACL validation in the presence of\nontologies to standard validation. Finally, we study the complexity of SHACL\nvalidation in the presence of ontologies, and show that even very simple\nontologies make the problem EXPTIME-complete, and PTIME-complete in data\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHACL and OWL are two prominent W3C standards for managing RDF data. These\nlanguages share many features, but they have one fundamental difference: OWL,\ndesigned for inferring facts from incomplete data, makes the open-world\nassumption, whereas SHACL is a constraint language that treats the data as\ncomplete and must be validated under the closed-world assumption. The\ncombination of both formalisms is very appealing and has been called for, but\ntheir semantic gap is a major challenge, semantically and computationally. In\nthis paper, we advocate a semantics for SHACL validation in the presence of\nontologies based on core universal models. We provide a technique for\nconstructing these models for ontologies in the rich data-tractable description\nlogic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to\ndevelop a rewriting technique that reduces SHACL validation in the presence of\nontologies to standard validation. Finally, we study the complexity of SHACL\nvalidation in the presence of ontologies, and show that even very simple\nontologies make the problem EXPTIME-complete, and PTIME-complete in data\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Anouk Oudshoorn"
                    },
                    {
                        "name": "Magdalena Ortiz"
                    },
                    {
                        "name": "Mantas Simkus"
                    }
                ],
                "author_detail": {
                    "name": "Mantas Simkus"
                },
                "author": "Mantas Simkus",
                "arxiv_comment": "36 pages, 6 figures, submitted to the journal of Artificial\n  Intelligence (AIJ)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14335v2",
                "updated": "2025-07-16T14:35:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    35,
                    17,
                    2,
                    197,
                    0
                ],
                "published": "2024-06-20T14:04:53Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    14,
                    4,
                    53,
                    3,
                    172,
                    0
                ],
                "title": "Linearly-Interpretable Concept Embedding Models for Text Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linearly-Interpretable Concept Embedding Models for Text Analysis"
                },
                "summary": "Despite their success, Large-Language Models (LLMs) still face criticism due\nto their lack of interpretability. Traditional post-hoc interpretation methods,\nbased on attention and gradient-based analysis, offer limited insights as they\nonly approximate the model's decision-making processes and have been proved to\nbe unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been\nlately proposed in the textual field to provide interpretable predictions based\non human-understandable concepts. However, CBMs still exhibit several\nlimitations due to their architectural constraints limiting their expressivity,\nto the absence of task-interpretability when employing non-linear task\npredictors and for requiring extensive annotations that are impractical for\nreal-world text data. In this paper, we address these challenges by proposing a\nnovel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the\ncurrent accuracy-interpretability trade-off. LICEMs classification accuracy is\nbetter than existing interpretable models and matches black-box ones. We show\nthat the explanations provided by our models are more interveneable and\ncausally consistent with respect to existing solutions. Finally, we show that\nLICEMs can be trained without requiring any concept supervision, as concepts\ncan be automatically predicted when using an LLM backbone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their success, Large-Language Models (LLMs) still face criticism due\nto their lack of interpretability. Traditional post-hoc interpretation methods,\nbased on attention and gradient-based analysis, offer limited insights as they\nonly approximate the model's decision-making processes and have been proved to\nbe unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been\nlately proposed in the textual field to provide interpretable predictions based\non human-understandable concepts. However, CBMs still exhibit several\nlimitations due to their architectural constraints limiting their expressivity,\nto the absence of task-interpretability when employing non-linear task\npredictors and for requiring extensive annotations that are impractical for\nreal-world text data. In this paper, we address these challenges by proposing a\nnovel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the\ncurrent accuracy-interpretability trade-off. LICEMs classification accuracy is\nbetter than existing interpretable models and matches black-box ones. We show\nthat the explanations provided by our models are more interveneable and\ncausally consistent with respect to existing solutions. Finally, we show that\nLICEMs can be trained without requiring any concept supervision, as concepts\ncan be automatically predicted when using an LLM backbone."
                },
                "authors": [
                    {
                        "name": "Francesco De Santis"
                    },
                    {
                        "name": "Philippe Bich"
                    },
                    {
                        "name": "Gabriele Ciravegna"
                    },
                    {
                        "name": "Pietro Barbiero"
                    },
                    {
                        "name": "Danilo Giordano"
                    },
                    {
                        "name": "Tania Cerquitelli"
                    }
                ],
                "author_detail": {
                    "name": "Tania Cerquitelli"
                },
                "author": "Tania Cerquitelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12284v2",
                "updated": "2025-07-17T12:55:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    55,
                    32,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T14:31:33Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    31,
                    33,
                    2,
                    197,
                    0
                ],
                "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks"
                },
                "summary": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures."
                },
                "authors": [
                    {
                        "name": "Artem Chervyakov"
                    },
                    {
                        "name": "Alexander Kharitonov"
                    },
                    {
                        "name": "Pavel Zadorozhny"
                    },
                    {
                        "name": "Adamenko Pavel"
                    },
                    {
                        "name": "Rodion Levichev"
                    },
                    {
                        "name": "Dmitrii Vorobev"
                    },
                    {
                        "name": "Dmitrii Salikhov"
                    },
                    {
                        "name": "Aidar Valeev"
                    },
                    {
                        "name": "Alena Pestova"
                    },
                    {
                        "name": "Maria Dziuba"
                    },
                    {
                        "name": "Ilseyar Alimova"
                    },
                    {
                        "name": "Artem Zavgorodnev"
                    },
                    {
                        "name": "Aleksandr Medvedev"
                    },
                    {
                        "name": "Stanislav Moiseev"
                    },
                    {
                        "name": "Elena Bruches"
                    },
                    {
                        "name": "Daniil Grebenkin"
                    },
                    {
                        "name": "Roman Derunets"
                    },
                    {
                        "name": "Vikulov Vladimir"
                    },
                    {
                        "name": "Anton Emelyanov"
                    },
                    {
                        "name": "Dmitrii Babaev"
                    },
                    {
                        "name": "Vladimir V. Ivanov"
                    },
                    {
                        "name": "Valentin Malykh"
                    },
                    {
                        "name": "Alena Fenogenova"
                    }
                ],
                "author_detail": {
                    "name": "Alena Fenogenova"
                },
                "author": "Alena Fenogenova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01942v2",
                "updated": "2025-07-16T14:27:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    27,
                    51,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-02T17:53:11Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    53,
                    11,
                    2,
                    183,
                    0
                ],
                "title": "Morphology and stellar populations of a candidate ultra-diffuse galaxy\n  in early Euclid and Rubin imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Morphology and stellar populations of a candidate ultra-diffuse galaxy\n  in early Euclid and Rubin imaging"
                },
                "summary": "We present multi-wavelength imaging and analysis of a low surface brightness\n(LSB) dwarf galaxy in the Extended Chandra Deep Field South (ECDFS),\nSMDG0333094-280938, with particular emphasis on data from the Euclid space\ntelescope and from the Vera C. Rubin Observatory. The galaxy is clumpy and\nblue, and appears to host globular clusters (GCs), suggesting a distance of\n~50-60 Mpc which would make the dwarf an ultra-diffuse galaxy (UDG). We carry\nout spectral energy distribution (SED) fitting from the far-ultraviolet to the\nnear-infrared, in order to estimate the galaxy age and metallicity. We infer a\nrecent peak of star formation that may have led to the formation of the UDG\nthrough feedback-driven expansion. This early analysis illustrates how Euclid\nand Rubin are poised to identify and characterize many thousands of UDGs and\nother LSB galaxies in the near future, including their GCs and stellar\npopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present multi-wavelength imaging and analysis of a low surface brightness\n(LSB) dwarf galaxy in the Extended Chandra Deep Field South (ECDFS),\nSMDG0333094-280938, with particular emphasis on data from the Euclid space\ntelescope and from the Vera C. Rubin Observatory. The galaxy is clumpy and\nblue, and appears to host globular clusters (GCs), suggesting a distance of\n~50-60 Mpc which would make the dwarf an ultra-diffuse galaxy (UDG). We carry\nout spectral energy distribution (SED) fitting from the far-ultraviolet to the\nnear-infrared, in order to estimate the galaxy age and metallicity. We infer a\nrecent peak of star formation that may have led to the formation of the UDG\nthrough feedback-driven expansion. This early analysis illustrates how Euclid\nand Rubin are poised to identify and characterize many thousands of UDGs and\nother LSB galaxies in the near future, including their GCs and stellar\npopulations."
                },
                "authors": [
                    {
                        "name": "Aaron J. Romanowsky"
                    },
                    {
                        "name": "Yimeng Tang"
                    },
                    {
                        "name": "Kevin A. Bundy"
                    }
                ],
                "author_detail": {
                    "name": "Kevin A. Bundy"
                },
                "author": "Kevin A. Bundy",
                "arxiv_doi": "10.3847/2515-5172/adee10",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2515-5172/adee10",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "published in RNAAS; 3 pages, 1 figure",
                "arxiv_journal_ref": "Research Notes of the American Astronomical Society, 9, 181 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11330v2",
                "updated": "2025-07-16T14:26:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    26,
                    34,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-15T14:03:55Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    14,
                    3,
                    55,
                    1,
                    196,
                    0
                ],
                "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach\n  Integrating Human and Large Language Model Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach\n  Integrating Human and Large Language Model Knowledge"
                },
                "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. One of the most common types of\nnovelty in academic papers is the introduction of new methods. In this paper,\nwe propose leveraging human knowledge and LLM to assist pretrained language\nmodels (PLMs, e.g. BERT etc.) in predicting the method novelty of papers.\nSpecifically, we extract sentences related to the novelty of the academic paper\nfrom peer review reports and use LLM to summarize the methodology section of\nthe academic paper, which are then used to fine-tune PLMs. In addition, we have\ndesigned a text-guided fusion module with novel Sparse-Attention to better\nintegrate human and LLM knowledge. We compared the method we proposed with a\nlarge number of baselines. Extensive experiments demonstrate that our method\nachieves superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. One of the most common types of\nnovelty in academic papers is the introduction of new methods. In this paper,\nwe propose leveraging human knowledge and LLM to assist pretrained language\nmodels (PLMs, e.g. BERT etc.) in predicting the method novelty of papers.\nSpecifically, we extract sentences related to the novelty of the academic paper\nfrom peer review reports and use LLM to summarize the methodology section of\nthe academic paper, which are then used to fine-tune PLMs. In addition, we have\ndesigned a text-guided fusion module with novel Sparse-Attention to better\nintegrate human and LLM knowledge. We compared the method we proposed with a\nlarge number of baselines. Extensive experiments demonstrate that our method\nachieves superior performance."
                },
                "authors": [
                    {
                        "name": "Wenqing Wu"
                    },
                    {
                        "name": "Chengzhi Zhang"
                    },
                    {
                        "name": "Yi Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhao"
                },
                "author": "Yi Zhao",
                "arxiv_doi": "10.1002/asi.70005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/asi.70005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.11330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Journal of the Association for Information Science and Technology,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10559v2",
                "updated": "2025-07-16T14:25:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    25,
                    7,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-02T15:50:09Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    50,
                    9,
                    2,
                    183,
                    0
                ],
                "title": "NLP Meets the World: Toward Improving Conversations With the Public\n  About Natural Language Processing Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP Meets the World: Toward Improving Conversations With the Public\n  About Natural Language Processing Research"
                },
                "summary": "Recent developments in large language models (LLMs) have been accompanied by\nrapidly growing public interest in natural language processing (NLP). This\nattention is reflected by major news venues, which sometimes invite NLP\nresearchers to share their knowledge and views with a wide audience.\nRecognizing the opportunities of the present, for both the research field and\nfor individual researchers, this paper shares recommendations for communicating\nwith a general audience about the capabilities and limitations of NLP. These\nrecommendations cover three themes: vague terminology as an obstacle to public\nunderstanding, unreasonable expectations as obstacles to sustainable growth,\nand ethical failures as obstacles to continued support. Published NLP research\nand popular news coverage are cited to illustrate these themes with examples.\nThe recommendations promote effective, transparent communication with the\ngeneral public about NLP, in order to strengthen public understanding and\nencourage support for research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in large language models (LLMs) have been accompanied by\nrapidly growing public interest in natural language processing (NLP). This\nattention is reflected by major news venues, which sometimes invite NLP\nresearchers to share their knowledge and views with a wide audience.\nRecognizing the opportunities of the present, for both the research field and\nfor individual researchers, this paper shares recommendations for communicating\nwith a general audience about the capabilities and limitations of NLP. These\nrecommendations cover three themes: vague terminology as an obstacle to public\nunderstanding, unreasonable expectations as obstacles to sustainable growth,\nand ethical failures as obstacles to continued support. Published NLP research\nand popular news coverage are cited to illustrate these themes with examples.\nThe recommendations promote effective, transparent communication with the\ngeneral public about NLP, in order to strengthen public understanding and\nencourage support for research."
                },
                "authors": [
                    {
                        "name": "Shomir Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Shomir Wilson"
                },
                "author": "Shomir Wilson",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12273v2",
                "updated": "2025-07-17T14:54:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    54,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T14:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    22,
                    0,
                    2,
                    197,
                    0
                ],
                "title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot"
                },
                "summary": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments."
                },
                "authors": [
                    {
                        "name": "Luca Garello"
                    },
                    {
                        "name": "Francesca Cocchella"
                    },
                    {
                        "name": "Alessandra Sciutti"
                    },
                    {
                        "name": "Manuel Catalano"
                    },
                    {
                        "name": "Francesco Rea"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Rea"
                },
                "author": "Francesco Rea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06915v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06915v6",
                "updated": "2025-07-16T14:21:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    21,
                    32,
                    2,
                    197,
                    0
                ],
                "published": "2024-02-10T09:28:03Z",
                "published_parsed": [
                    2024,
                    2,
                    10,
                    9,
                    28,
                    3,
                    5,
                    41,
                    0
                ],
                "title": "Detection and inference of changes in high-dimensional linear regression\n  with non-sparse structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection and inference of changes in high-dimensional linear regression\n  with non-sparse structures"
                },
                "summary": "For data segmentation in high-dimensional linear regression settings, the\nregression parameters are often assumed to be sparse segment-wise, which\nenables many existing methods to estimate the parameters locally via\n$\\ell_1$-regularised maximum likelihood-type estimation and then contrast them\nfor change point detection. Contrary to this common practice, we show that the\nexact sparsity of neither regression parameters nor their differences, a.k.a.\\\ndifferential parameters, is necessary for consistency in multiple change point\ndetection. In fact, both statistically and computationally, better efficiency\nis attained by a simple strategy that scans for large discrepancies in local\ncovariance between the regressors and the response. We go a step further and\npropose a suite of tools for directly inferring about the differential\nparameters post-segmentation, which are applicable even when the regression\nparameters themselves are non-sparse. Theoretical investigations are conducted\nunder general conditions permitting non-Gaussianity, temporal dependence and\nultra-high dimensionality. Numerical results from simulated and macroeconomic\ndatasets demonstrate the competitiveness and efficacy of the proposed methods.\nImplementation of all methods is provided in the R package \\texttt{inferchange}\non GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For data segmentation in high-dimensional linear regression settings, the\nregression parameters are often assumed to be sparse segment-wise, which\nenables many existing methods to estimate the parameters locally via\n$\\ell_1$-regularised maximum likelihood-type estimation and then contrast them\nfor change point detection. Contrary to this common practice, we show that the\nexact sparsity of neither regression parameters nor their differences, a.k.a.\\\ndifferential parameters, is necessary for consistency in multiple change point\ndetection. In fact, both statistically and computationally, better efficiency\nis attained by a simple strategy that scans for large discrepancies in local\ncovariance between the regressors and the response. We go a step further and\npropose a suite of tools for directly inferring about the differential\nparameters post-segmentation, which are applicable even when the regression\nparameters themselves are non-sparse. Theoretical investigations are conducted\nunder general conditions permitting non-Gaussianity, temporal dependence and\nultra-high dimensionality. Numerical results from simulated and macroeconomic\ndatasets demonstrate the competitiveness and efficacy of the proposed methods.\nImplementation of all methods is provided in the R package \\texttt{inferchange}\non GitHub."
                },
                "authors": [
                    {
                        "name": "Haeran Cho"
                    },
                    {
                        "name": "Tobias Kley"
                    },
                    {
                        "name": "Housen Li"
                    }
                ],
                "author_detail": {
                    "name": "Housen Li"
                },
                "author": "Housen Li",
                "arxiv_doi": "10.1093/jrsssb/qkaf029",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/jrsssb/qkaf029",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.06915v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06915v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11647v2",
                "updated": "2025-07-16T14:16:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    16,
                    47,
                    2,
                    197,
                    0
                ],
                "published": "2024-10-15T14:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    33,
                    23,
                    1,
                    289,
                    0
                ],
                "title": "Measuring Spiritual Values and Bias of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Spiritual Values and Bias of Large Language Models"
                },
                "summary": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias."
                },
                "authors": [
                    {
                        "name": "Songyuan Liu"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Runze Yan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Jiaying Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaying Lu"
                },
                "author": "Jiaying Lu",
                "arxiv_comment": "9 pages including appendix; 5 figures; 5 tables",
                "arxiv_journal_ref": "in Proceedings of KDD 2025 SciSoc LLM Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20996v2",
                "updated": "2025-07-16T14:16:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    16,
                    46,
                    2,
                    197,
                    0
                ],
                "published": "2025-05-27T10:31:21Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    10,
                    31,
                    21,
                    1,
                    147,
                    0
                ],
                "title": "Parameter inference of microlensed gravitational waves using neural\n  spline flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter inference of microlensed gravitational waves using neural\n  spline flows"
                },
                "summary": "When gravitational waves (GWs) propagate near massive objects, they undergo\ngravitational lensing that imprints lens model dependent modulations on the\nwaveform. This effect provides a powerful tool for cosmological and\nastrophysical studies. However, conventional Bayesian parameter inference\nmethods for GWs are computationally expensive, especially for lensed events\nwith additional lens parameters, necessitating more efficient approaches. In\nthis work, we explore the use of neural spline flows (NSFs) for posterior\ninference of microlensed GWs, and successfully apply NSFs to the inference of\n11-dimensional lens parameters. Our results demonstrate that compared with\ntraditional methods like Bilby dynesty that rely on Bayesian inference, the NSF\nnetwork we built not only achieves inference accuracy comparable to traditional\nmethods for the main parameters, but also can reduce the inference time from\napproximately 3 days to 0.8 s on average. Additionally, the network exhibits\nstrong generalization for the spin parameters of GW sources. It is anticipated\nto become a powerful tool for future low-latency searches for lensed GW\nsignals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When gravitational waves (GWs) propagate near massive objects, they undergo\ngravitational lensing that imprints lens model dependent modulations on the\nwaveform. This effect provides a powerful tool for cosmological and\nastrophysical studies. However, conventional Bayesian parameter inference\nmethods for GWs are computationally expensive, especially for lensed events\nwith additional lens parameters, necessitating more efficient approaches. In\nthis work, we explore the use of neural spline flows (NSFs) for posterior\ninference of microlensed GWs, and successfully apply NSFs to the inference of\n11-dimensional lens parameters. Our results demonstrate that compared with\ntraditional methods like Bilby dynesty that rely on Bayesian inference, the NSF\nnetwork we built not only achieves inference accuracy comparable to traditional\nmethods for the main parameters, but also can reduce the inference time from\napproximately 3 days to 0.8 s on average. Additionally, the network exhibits\nstrong generalization for the spin parameters of GW sources. It is anticipated\nto become a powerful tool for future low-latency searches for lensed GW\nsignals."
                },
                "authors": [
                    {
                        "name": "Zheng Qin"
                    },
                    {
                        "name": "Tian-Yang Sun"
                    },
                    {
                        "name": "Bo-Yuan Li"
                    },
                    {
                        "name": "Jing-Fei Zhang"
                    },
                    {
                        "name": "Xiao Guo"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21042v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21042v3",
                "updated": "2025-07-16T14:13:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    13,
                    44,
                    2,
                    197,
                    0
                ],
                "published": "2025-04-28T13:30:48Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    13,
                    30,
                    48,
                    0,
                    118,
                    0
                ],
                "title": "What's Pulling the Strings? Evaluating Integrity and Attribution in AI\n  Training and Inference through Concept Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What's Pulling the Strings? Evaluating Integrity and Attribution in AI\n  Training and Inference through Concept Shift"
                },
                "summary": "The growing adoption of artificial intelligence (AI) has amplified concerns\nabout trustworthiness, including integrity, privacy, robustness, and bias. To\nassess and attribute these threats, we propose ConceptLens, a generic framework\nthat leverages pre-trained multimodal models to identify the root causes of\nintegrity threats by analyzing Concept Shift in probing samples. ConceptLens\ndemonstrates strong detection performance for vanilla data poisoning attacks\nand uncovers vulnerabilities to bias injection, such as the generation of\ncovert advertisements through malicious concept shifts. It identifies privacy\nrisks in unaltered but high-risk samples, filters them before training, and\nprovides insights into model weaknesses arising from incomplete or imbalanced\ntraining data. Additionally, at the model level, it attributes concepts that\nthe target model is overly dependent on, identifies misleading concepts, and\nexplains how disrupting key concepts negatively impacts the model. Furthermore,\nit uncovers sociological biases in generative content, revealing disparities\nacross sociological contexts. Strikingly, ConceptLens reveals how safe training\nand inference data can be unintentionally and easily exploited, potentially\nundermining safety alignment. Our study informs actionable insights to breed\ntrust in AI systems, thereby speeding adoption and driving greater innovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of artificial intelligence (AI) has amplified concerns\nabout trustworthiness, including integrity, privacy, robustness, and bias. To\nassess and attribute these threats, we propose ConceptLens, a generic framework\nthat leverages pre-trained multimodal models to identify the root causes of\nintegrity threats by analyzing Concept Shift in probing samples. ConceptLens\ndemonstrates strong detection performance for vanilla data poisoning attacks\nand uncovers vulnerabilities to bias injection, such as the generation of\ncovert advertisements through malicious concept shifts. It identifies privacy\nrisks in unaltered but high-risk samples, filters them before training, and\nprovides insights into model weaknesses arising from incomplete or imbalanced\ntraining data. Additionally, at the model level, it attributes concepts that\nthe target model is overly dependent on, identifies misleading concepts, and\nexplains how disrupting key concepts negatively impacts the model. Furthermore,\nit uncovers sociological biases in generative content, revealing disparities\nacross sociological contexts. Strikingly, ConceptLens reveals how safe training\nand inference data can be unintentionally and easily exploited, potentially\nundermining safety alignment. Our study informs actionable insights to breed\ntrust in AI systems, thereby speeding adoption and driving greater innovation."
                },
                "authors": [
                    {
                        "name": "Jiamin Chang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Hammond Pearce"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Minhui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Minhui Xue"
                },
                "author": "Minhui Xue",
                "arxiv_comment": "Accepted to The ACM Conference on Computer and Communications\n  Security (CCS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21042v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21042v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12263v1",
                "updated": "2025-07-16T14:10:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    10,
                    0,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:10:00Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    10,
                    0,
                    2,
                    197,
                    0
                ],
                "title": "EEG-fused Digital Twin Brain for Autonomous Driving in Virtual Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEG-fused Digital Twin Brain for Autonomous Driving in Virtual Scenarios"
                },
                "summary": "Current methodologies typically integrate biophysical brain models with\nfunctional magnetic resonance imaging(fMRI) data - while offering\nmillimeter-scale spatial resolution (0.5-2 mm^3 voxels), these approaches\nsuffer from limited temporal resolution (>0.5 Hz) for tracking rapid neural\ndynamics during continuous tasks. Conversely, Electroencephalogram (EEG)\nprovides millisecond-scale temporal precision (<=1 ms sampling rate) for\nreal-time guidance of continuous task execution, albeit constrained by low\nspatial resolution. To reconcile these complementary modalities, we present a\ngeneralizable Bayesian inference framework that integrates\nhigh-spatial-resolution structural MRI(sMRI) with high-temporal-resolution EEG\nto construct a biologically realistic digital twin brain(DTB) model. The\nframework establishes voxel-wise mappings between millisecond-scale EEG and\nsMRI-derived spiking networks, while demonstrating its translational potential\nthrough a brain-inspired autonomous driving simulation. Our EEG-DTB model\nachieves capabilities: (1) Biologically-plausible EEG signal generation (0.88\nresting-state,0.60 task-state correlation), with simulated signals in\ntask-state yielding steering predictions outperforming both chance and\nempirical signals (p<0.05); (2) Successful autonomous driving in the CARLA\nsimulator using decoded steering angles. The proposed approach pioneers a new\nparadigm for studying sensorimotor integration and for mechanistic studies of\nperception-action cycles and the development of brain-inspired control systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current methodologies typically integrate biophysical brain models with\nfunctional magnetic resonance imaging(fMRI) data - while offering\nmillimeter-scale spatial resolution (0.5-2 mm^3 voxels), these approaches\nsuffer from limited temporal resolution (>0.5 Hz) for tracking rapid neural\ndynamics during continuous tasks. Conversely, Electroencephalogram (EEG)\nprovides millisecond-scale temporal precision (<=1 ms sampling rate) for\nreal-time guidance of continuous task execution, albeit constrained by low\nspatial resolution. To reconcile these complementary modalities, we present a\ngeneralizable Bayesian inference framework that integrates\nhigh-spatial-resolution structural MRI(sMRI) with high-temporal-resolution EEG\nto construct a biologically realistic digital twin brain(DTB) model. The\nframework establishes voxel-wise mappings between millisecond-scale EEG and\nsMRI-derived spiking networks, while demonstrating its translational potential\nthrough a brain-inspired autonomous driving simulation. Our EEG-DTB model\nachieves capabilities: (1) Biologically-plausible EEG signal generation (0.88\nresting-state,0.60 task-state correlation), with simulated signals in\ntask-state yielding steering predictions outperforming both chance and\nempirical signals (p<0.05); (2) Successful autonomous driving in the CARLA\nsimulator using decoded steering angles. The proposed approach pioneers a new\nparadigm for studying sensorimotor integration and for mechanistic studies of\nperception-action cycles and the development of brain-inspired control systems."
                },
                "authors": [
                    {
                        "name": "Yubo Hou"
                    },
                    {
                        "name": "Zhengxin Zhang"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Wenlian Lu"
                    },
                    {
                        "name": "Jianfeng Feng"
                    },
                    {
                        "name": "Taiping Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Taiping Zeng"
                },
                "author": "Taiping Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12262v1",
                "updated": "2025-07-16T14:09:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    9,
                    49,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:09:49Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    9,
                    49,
                    2,
                    197,
                    0
                ],
                "title": "A Framework for Nonstationary Gaussian Processes with Neural Network\n  Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Nonstationary Gaussian Processes with Neural Network\n  Parameters"
                },
                "summary": "Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset."
                },
                "authors": [
                    {
                        "name": "Zachary James"
                    },
                    {
                        "name": "Joseph Guinness"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Guinness"
                },
                "author": "Joseph Guinness",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12261v1",
                "updated": "2025-07-16T14:06:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    6,
                    51,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:06:51Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    6,
                    51,
                    2,
                    197,
                    0
                ],
                "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form\n  Clinical Notes"
                },
                "summary": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions."
                },
                "authors": [
                    {
                        "name": "Johann Frei"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Lisa Raithel"
                    },
                    {
                        "name": "Roland Roller"
                    },
                    {
                        "name": "Alexander Meyer"
                    },
                    {
                        "name": "Frank Kramer"
                    }
                ],
                "author_detail": {
                    "name": "Frank Kramer"
                },
                "author": "Frank Kramer",
                "arxiv_comment": "Submitted to EMNLP 2025 System Demonstrations | Code:\n  https://github.com/j-frei/Infherno | Video:\n  https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo:\n  https://infherno.misit-augsburg.de | HuggingFace Spaces:\n  https://huggingface.co/spaces/nfel/infherno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15460v4",
                "updated": "2025-07-16T14:04:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    4,
                    55,
                    2,
                    197,
                    0
                ],
                "published": "2024-10-20T18:18:23Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    18,
                    18,
                    23,
                    6,
                    294,
                    0
                ],
                "title": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model\n  Training"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent, concerns about\ntheir reliability, particularly due to hallucinations - factually inaccurate or\nirrelevant outputs - have grown. Our research investigates the relationship\nbetween the uncertainty in training dynamics and the emergence of\nhallucinations. Using models from the Pythia suite and several hallucination\ndetection metrics, we analyze hallucination trends and identify significant\nvariance during training. To address this, we propose \\textbf{Sensitivity\nDropout (SenD)}, a novel training protocol designed to reduce hallucination\nvariance during training by deterministically dropping embedding indices with\nsignificant variability. In addition, we develop an unsupervised hallucination\ndetection metric, Efficient EigenScore (EES), which approximates the\ntraditional EigenScore in 2x speed. This metric is integrated into our training\nprotocol, allowing SenD to be both computationally scalable and effective at\nreducing hallucination variance. SenD improves test-time reliability of Pythia\nand Meta's Llama models by up to 17\\% and enhances factual accuracy in\nWikipedia, Medical, Legal, and Coding domains without affecting downstream task\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent, concerns about\ntheir reliability, particularly due to hallucinations - factually inaccurate or\nirrelevant outputs - have grown. Our research investigates the relationship\nbetween the uncertainty in training dynamics and the emergence of\nhallucinations. Using models from the Pythia suite and several hallucination\ndetection metrics, we analyze hallucination trends and identify significant\nvariance during training. To address this, we propose \\textbf{Sensitivity\nDropout (SenD)}, a novel training protocol designed to reduce hallucination\nvariance during training by deterministically dropping embedding indices with\nsignificant variability. In addition, we develop an unsupervised hallucination\ndetection metric, Efficient EigenScore (EES), which approximates the\ntraditional EigenScore in 2x speed. This metric is integrated into our training\nprotocol, allowing SenD to be both computationally scalable and effective at\nreducing hallucination variance. SenD improves test-time reliability of Pythia\nand Meta's Llama models by up to 17\\% and enhances factual accuracy in\nWikipedia, Medical, Legal, and Coding domains without affecting downstream task\nperformance."
                },
                "authors": [
                    {
                        "name": "Shahrad Mohammadzadeh"
                    },
                    {
                        "name": "Juan David Guerra"
                    },
                    {
                        "name": "Marco Bonizzato"
                    },
                    {
                        "name": "Reihaneh Rabbany"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "arxiv_comment": "Accepted to ACL 2025, accepted to Safe Generative AI Workshop @\n  NeurIPS 2024. Camera-ready version for ACL 2025 (to appear). Submitted July\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12257v1",
                "updated": "2025-07-16T14:02:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    2,
                    21,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:02:21Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    2,
                    21,
                    2,
                    197,
                    0
                ],
                "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Causal Discovery in Real-World Time Series with Power-Laws"
                },
                "summary": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance."
                },
                "authors": [
                    {
                        "name": "Matteo Tusoni"
                    },
                    {
                        "name": "Giuseppe Masi"
                    },
                    {
                        "name": "Andrea Coletta"
                    },
                    {
                        "name": "Aldo Glielmo"
                    },
                    {
                        "name": "Viviana Arrigoni"
                    },
                    {
                        "name": "Novella Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Novella Bartolini"
                },
                "author": "Novella Bartolini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12252v1",
                "updated": "2025-07-16T13:59:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    59,
                    32,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:59:32Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    59,
                    32,
                    2,
                    197,
                    0
                ],
                "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Contextual ASR via Multi-grained Fusion with Large Language\n  Models"
                },
                "summary": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/."
                },
                "authors": [
                    {
                        "name": "Shilin Zhou"
                    },
                    {
                        "name": "Zhenghua Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhenghua Li"
                },
                "author": "Zhenghua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12251v1",
                "updated": "2025-07-16T13:59:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    59,
                    27,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:59:27Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    59,
                    27,
                    2,
                    197,
                    0
                ],
                "title": "Fast Variational Bayes for Large Spatial Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Variational Bayes for Large Spatial Data"
                },
                "summary": "Recent variational Bayes methods for geospatial regression, proposed as an\nalternative to computationally expensive Markov chain Monte Carlo (MCMC)\nsampling, have leveraged Nearest Neighbor Gaussian processes (NNGP) to achieve\nscalability. Yet, these variational methods remain inferior in accuracy and\nspeed compared to spNNGP, the state-of-the-art MCMC-based software for NNGP. We\nintroduce spVarBayes, a suite of fast variational Bayesian approaches for\nlarge-scale geospatial data analysis using NNGP. Our contributions are\nprimarily computational. We replace auto-differentiation with a combination of\ncalculus of variations, closed-form gradient updates, and linear response\ncorrections for improved variance estimation. We also accommodate covariates\n(fixed effects) in the model and offer inference on the variance parameters.\nSimulation experiments demonstrate that we achieve comparable accuracy to\nspNNGP but with reduced computational costs, and considerably outperform\nexisting variational inference methods in terms of both accuracy and speed.\nAnalysis of a large forest canopy height dataset illustrates the practical\nimplementation of proposed methods and shows that the inference results are\nconsistent with those obtained from the MCMC approach. The proposed methods are\nimplemented in publicly available Github R-package spVarBayes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent variational Bayes methods for geospatial regression, proposed as an\nalternative to computationally expensive Markov chain Monte Carlo (MCMC)\nsampling, have leveraged Nearest Neighbor Gaussian processes (NNGP) to achieve\nscalability. Yet, these variational methods remain inferior in accuracy and\nspeed compared to spNNGP, the state-of-the-art MCMC-based software for NNGP. We\nintroduce spVarBayes, a suite of fast variational Bayesian approaches for\nlarge-scale geospatial data analysis using NNGP. Our contributions are\nprimarily computational. We replace auto-differentiation with a combination of\ncalculus of variations, closed-form gradient updates, and linear response\ncorrections for improved variance estimation. We also accommodate covariates\n(fixed effects) in the model and offer inference on the variance parameters.\nSimulation experiments demonstrate that we achieve comparable accuracy to\nspNNGP but with reduced computational costs, and considerably outperform\nexisting variational inference methods in terms of both accuracy and speed.\nAnalysis of a large forest canopy height dataset illustrates the practical\nimplementation of proposed methods and shows that the inference results are\nconsistent with those obtained from the MCMC approach. The proposed methods are\nimplemented in publicly available Github R-package spVarBayes."
                },
                "authors": [
                    {
                        "name": "Jiafang Song"
                    },
                    {
                        "name": "Abhirup Datta"
                    }
                ],
                "author_detail": {
                    "name": "Abhirup Datta"
                },
                "author": "Abhirup Datta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12248v1",
                "updated": "2025-07-16T13:57:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    57,
                    50,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:57:50Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    57,
                    50,
                    2,
                    197,
                    0
                ],
                "title": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on\n  PathMNIST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on\n  PathMNIST"
                },
                "summary": "Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis."
                },
                "authors": [
                    {
                        "name": "Anida Nezovi"
                    },
                    {
                        "name": "Jalal Romano"
                    },
                    {
                        "name": "Nada Mari"
                    },
                    {
                        "name": "Medina Kapo"
                    },
                    {
                        "name": "Amila Akagi"
                    }
                ],
                "author_detail": {
                    "name": "Amila Akagi"
                },
                "author": "Amila Akagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12228v1",
                "updated": "2025-07-16T13:40:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    40,
                    46,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:40:46Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    40,
                    46,
                    2,
                    197,
                    0
                ],
                "title": "Quantum estimation of cosmological parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum estimation of cosmological parameters"
                },
                "summary": "Understanding how well future cosmological experiments can reconstruct the\nmechanism that generated primordial inhomogeneities is key to assessing the\nextent to which cosmology can inform fundamental physics. In this work, we\napply a quantum metrology tool - the quantum Fisher information - to the\nsqueezed quantum state describing cosmological perturbations at the end of\ninflation. This quantifies the ultimate precision achievable in parameter\nestimation, assuming ideal access to early-universe information. By comparing\nthe quantum Fisher information to its classical counterpart - derived from\nmeasurements of the curvature perturbation power spectrum alone (homodyne\nmeasurement) - we evaluate how close current observations come to this quantum\nlimit. Focusing on the tensor-to-scalar ratio as a case study, we find that the\ngap between classical and quantum Fisher information grows exponentially with\nthe number of e-folds a mode spends outside the horizon. This suggests the\nexistence of a highly efficient (but presently inaccessible) optimal\nmeasurement. Conversely, we show that accessing the decaying mode of\ninflationary perturbations is a necessary (but not sufficient) condition for\nexponentially improving the inference of the tensor-to-scalar ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how well future cosmological experiments can reconstruct the\nmechanism that generated primordial inhomogeneities is key to assessing the\nextent to which cosmology can inform fundamental physics. In this work, we\napply a quantum metrology tool - the quantum Fisher information - to the\nsqueezed quantum state describing cosmological perturbations at the end of\ninflation. This quantifies the ultimate precision achievable in parameter\nestimation, assuming ideal access to early-universe information. By comparing\nthe quantum Fisher information to its classical counterpart - derived from\nmeasurements of the curvature perturbation power spectrum alone (homodyne\nmeasurement) - we evaluate how close current observations come to this quantum\nlimit. Focusing on the tensor-to-scalar ratio as a case study, we find that the\ngap between classical and quantum Fisher information grows exponentially with\nthe number of e-folds a mode spends outside the horizon. This suggests the\nexistence of a highly efficient (but presently inaccessible) optimal\nmeasurement. Conversely, we show that accessing the decaying mode of\ninflationary perturbations is a necessary (but not sufficient) condition for\nexponentially improving the inference of the tensor-to-scalar ratio."
                },
                "authors": [
                    {
                        "name": "Micha Piotrak"
                    },
                    {
                        "name": "Thomas Colas"
                    },
                    {
                        "name": "Ana Alonso-Serrano"
                    },
                    {
                        "name": "Alessio Serafini"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Serafini"
                },
                "author": "Alessio Serafini",
                "arxiv_comment": "27 pages without appendices (41 pages in total), 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16994v2",
                "updated": "2025-07-16T13:35:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    35,
                    23,
                    2,
                    197,
                    0
                ],
                "published": "2025-02-24T09:28:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    28,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "FADE: Why Bad Descriptions Happen to Good Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FADE: Why Bad Descriptions Happen to Good Features"
                },
                "summary": "Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While this may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for automatically evaluating feature-to-description\nalignment. FADE evaluates alignment across four key metrics - Clarity,\nResponsiveness, Purity, and Faithfulness - and systematically quantifies the\ncauses of the misalignment between features and their descriptions. We apply\nFADE to analyze existing open-source feature descriptions and assess key\ncomponents of automated interpretability pipelines, aiming to enhance the\nquality of descriptions. Our findings highlight fundamental challenges in\ngenerating feature descriptions, particularly for SAEs compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While this may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for automatically evaluating feature-to-description\nalignment. FADE evaluates alignment across four key metrics - Clarity,\nResponsiveness, Purity, and Faithfulness - and systematically quantifies the\ncauses of the misalignment between features and their descriptions. We apply\nFADE to analyze existing open-source feature descriptions and assess key\ncomponents of automated interpretability pipelines, aiming to enhance the\nquality of descriptions. Our findings highlight fundamental challenges in\ngenerating feature descriptions, particularly for SAEs compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE"
                },
                "authors": [
                    {
                        "name": "Bruno Puri"
                    },
                    {
                        "name": "Aakriti Jain"
                    },
                    {
                        "name": "Elena Golimblevskaia"
                    },
                    {
                        "name": "Patrick Kahardipraja"
                    },
                    {
                        "name": "Thomas Wiegand"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Lapuschkin"
                },
                "author": "Sebastian Lapuschkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00584v2",
                "updated": "2025-07-16T13:33:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    33,
                    8,
                    2,
                    197,
                    0
                ],
                "published": "2025-04-01T09:39:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    39,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "Semantic Adapter for Universal Text Embeddings: Diagnosing and\n  Mitigating Negation Blindness to Enhance Universality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Adapter for Universal Text Embeddings: Diagnosing and\n  Mitigating Negation Blindness to Enhance Universality"
                },
                "summary": "Negation plays an important role in various natural language processing tasks\nsuch as Natural Language Inference and Sentiment Analysis tasks. Numerous prior\nstudies have found that contextual text embedding models such as BERT, ELMO,\nRoBERTa or XLNet face challenges in accurately understanding negation. Recent\nadvancements in universal text embeddings have demonstrated superior\nperformance over contextual text embeddings in various tasks. However, due to\nthe bias in popular evaluation benchmarks, the negation awareness capacity of\nthese models remains unclear. To bridge the gap in existing literature, an\nin-depth analysis is initiated in this work to study the negation awareness of\ncutting-edge universal text embedding models. Our findings reveal a significant\nlack of negation awareness in these models, often interpreting negated text\npairs as semantically similar. To efficiently deal with the conflict that\ndifferent tasks need different trade-offs between topic and negation\ninformation among other semantic information, a data-efficient and\ncomputational-efficient embedding re-weighting method is proposed without\nmodifying the parameters of text embedding models. The proposed solution is\nable to improve text embedding models' negation awareness significantly on both\nsimple negation understanding task and complex negation understanding task.\nFurthermore, the proposed solution can also significantly improve the negation\nawareness of Large Language Model based task-specific high dimensional\nuniversal text embeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negation plays an important role in various natural language processing tasks\nsuch as Natural Language Inference and Sentiment Analysis tasks. Numerous prior\nstudies have found that contextual text embedding models such as BERT, ELMO,\nRoBERTa or XLNet face challenges in accurately understanding negation. Recent\nadvancements in universal text embeddings have demonstrated superior\nperformance over contextual text embeddings in various tasks. However, due to\nthe bias in popular evaluation benchmarks, the negation awareness capacity of\nthese models remains unclear. To bridge the gap in existing literature, an\nin-depth analysis is initiated in this work to study the negation awareness of\ncutting-edge universal text embedding models. Our findings reveal a significant\nlack of negation awareness in these models, often interpreting negated text\npairs as semantically similar. To efficiently deal with the conflict that\ndifferent tasks need different trade-offs between topic and negation\ninformation among other semantic information, a data-efficient and\ncomputational-efficient embedding re-weighting method is proposed without\nmodifying the parameters of text embedding models. The proposed solution is\nable to improve text embedding models' negation awareness significantly on both\nsimple negation understanding task and complex negation understanding task.\nFurthermore, the proposed solution can also significantly improve the negation\nawareness of Large Language Model based task-specific high dimensional\nuniversal text embeddings."
                },
                "authors": [
                    {
                        "name": "Hongliu Cao"
                    }
                ],
                "author_detail": {
                    "name": "Hongliu Cao"
                },
                "author": "Hongliu Cao",
                "arxiv_comment": "Accepted in ECAI 2025 main track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00603v2",
                "updated": "2025-07-16T13:28:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    28,
                    57,
                    2,
                    197,
                    0
                ],
                "published": "2024-10-01T11:44:29Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    11,
                    44,
                    29,
                    1,
                    275,
                    0
                ],
                "title": "An Empirical Study of Large Language Models for Type and Call Graph\n  Analysis in Python and JavaScript",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Large Language Models for Type and Call Graph\n  Analysis in Python and JavaScript"
                },
                "summary": "Large Language Models (LLMs) are increasingly being explored for their\npotential in software engineering, particularly in static analysis tasks. In\nthis study, we investigate the potential of current LLMs to enhance call-graph\nanalysis and type inference for Python and JavaScript programs. We empirically\nevaluated 24 LLMs, including OpenAI's GPT series and open-source models like\nLLaMA and Mistral, using existing and newly developed benchmarks. Specifically,\nwe enhanced TypeEvalPy, a micro-benchmarking framework for type inference in\nPython, with auto-generation capabilities, expanding its scope from 860 to\n77,268 type annotations for Python. Additionally, we introduced SWARM-CG and\nSWARM-JS, comprehensive benchmarking suites for evaluating call-graph\nconstruction tools across multiple programming languages.\n  Our findings reveal a contrasting performance of LLMs in static analysis\ntasks. For call-graph generation, traditional static analysis tools such as\nPyCG for Python and Jelly for JavaScript consistently outperform LLMs. While\nadvanced models like mistral-large-it-2407-123b and gpt-4o show promise, they\nstill struggle with completeness and soundness in call-graph analysis across\nboth languages. In contrast, LLMs demonstrate a clear advantage in type\ninference for Python, surpassing traditional tools like HeaderGen and hybrid\napproaches such as HiTyper. These results suggest that, while LLMs hold promise\nin type inference, their limitations in call-graph analysis highlight the need\nfor further research. Our study provides a foundation for integrating LLMs into\nstatic analysis workflows, offering insights into their strengths and current\nlimitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being explored for their\npotential in software engineering, particularly in static analysis tasks. In\nthis study, we investigate the potential of current LLMs to enhance call-graph\nanalysis and type inference for Python and JavaScript programs. We empirically\nevaluated 24 LLMs, including OpenAI's GPT series and open-source models like\nLLaMA and Mistral, using existing and newly developed benchmarks. Specifically,\nwe enhanced TypeEvalPy, a micro-benchmarking framework for type inference in\nPython, with auto-generation capabilities, expanding its scope from 860 to\n77,268 type annotations for Python. Additionally, we introduced SWARM-CG and\nSWARM-JS, comprehensive benchmarking suites for evaluating call-graph\nconstruction tools across multiple programming languages.\n  Our findings reveal a contrasting performance of LLMs in static analysis\ntasks. For call-graph generation, traditional static analysis tools such as\nPyCG for Python and Jelly for JavaScript consistently outperform LLMs. While\nadvanced models like mistral-large-it-2407-123b and gpt-4o show promise, they\nstill struggle with completeness and soundness in call-graph analysis across\nboth languages. In contrast, LLMs demonstrate a clear advantage in type\ninference for Python, surpassing traditional tools like HeaderGen and hybrid\napproaches such as HiTyper. These results suggest that, while LLMs hold promise\nin type inference, their limitations in call-graph analysis highlight the need\nfor further research. Our study provides a foundation for integrating LLMs into\nstatic analysis workflows, offering insights into their strengths and current\nlimitations."
                },
                "authors": [
                    {
                        "name": "Ashwin Prasad Shivarpatna Venkatesh"
                    },
                    {
                        "name": "Rose Sunil"
                    },
                    {
                        "name": "Samkutty Sabu"
                    },
                    {
                        "name": "Amir M. Mir"
                    },
                    {
                        "name": "Sofia Reis"
                    },
                    {
                        "name": "Eric Bodden"
                    }
                ],
                "author_detail": {
                    "name": "Eric Bodden"
                },
                "author": "Eric Bodden",
                "arxiv_comment": "Accepted to be published in the EMSE journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12220v1",
                "updated": "2025-07-16T13:25:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    25,
                    50,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:25:50Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    25,
                    50,
                    2,
                    197,
                    0
                ],
                "title": "Data Synchronization at High Frequencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Synchronization at High Frequencies"
                },
                "summary": "Asynchronous trading in high-frequency financial markets introduces\nsignificant biases into econometric analysis, distorting risk estimates and\nleading to suboptimal portfolio decisions. Existing synchronization methods,\nsuch as the previous-tick approach, suffer from information loss and create\nartificial price staleness. We introduce a novel framework that recasts the\ndata synchronization challenge as a constrained matrix completion problem. Our\napproach recovers the potential matrix of high-frequency price increments by\nminimizing its nuclear norm -- capturing the underlying low-rank factor\nstructure -- subject to a large-scale linear system derived from observed,\nasynchronous price changes. Theoretically, we prove the existence and\nuniqueness of our estimator and establish its convergence rate. A key\ntheoretical insight is that our method accurately and robustly leverages\ninformation from both frequently and infrequently traded assets, overcoming a\ncritical difficulty of efficiency loss in traditional methods. Empirically,\nusing extensive simulations and a large panel of S&P 500 stocks, we demonstrate\nthat our method substantially outperforms established benchmarks. It not only\nachieves significantly lower synchronization errors, but also corrects the bias\nin systematic risk estimates (i.e., eigenvalues) and the estimate of betas\ncaused by stale prices. Crucially, portfolios constructed using our\nsynchronized data yield consistently and economically significant higher\nout-of-sample Sharpe ratios. Our framework provides a powerful tool for\nuncovering the true dynamics of asset prices, with direct implications for\nhigh-frequency risk management, algorithmic trading, and econometric inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous trading in high-frequency financial markets introduces\nsignificant biases into econometric analysis, distorting risk estimates and\nleading to suboptimal portfolio decisions. Existing synchronization methods,\nsuch as the previous-tick approach, suffer from information loss and create\nartificial price staleness. We introduce a novel framework that recasts the\ndata synchronization challenge as a constrained matrix completion problem. Our\napproach recovers the potential matrix of high-frequency price increments by\nminimizing its nuclear norm -- capturing the underlying low-rank factor\nstructure -- subject to a large-scale linear system derived from observed,\nasynchronous price changes. Theoretically, we prove the existence and\nuniqueness of our estimator and establish its convergence rate. A key\ntheoretical insight is that our method accurately and robustly leverages\ninformation from both frequently and infrequently traded assets, overcoming a\ncritical difficulty of efficiency loss in traditional methods. Empirically,\nusing extensive simulations and a large panel of S&P 500 stocks, we demonstrate\nthat our method substantially outperforms established benchmarks. It not only\nachieves significantly lower synchronization errors, but also corrects the bias\nin systematic risk estimates (i.e., eigenvalues) and the estimate of betas\ncaused by stale prices. Crucially, portfolios constructed using our\nsynchronized data yield consistently and economically significant higher\nout-of-sample Sharpe ratios. Our framework provides a powerful tool for\nuncovering the true dynamics of asset prices, with direct implications for\nhigh-frequency risk management, algorithmic trading, and econometric inference."
                },
                "authors": [
                    {
                        "name": "Xinbing Kong"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Bin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wu"
                },
                "author": "Bin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12215v1",
                "updated": "2025-07-16T13:19:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    19,
                    46,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:19:46Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    19,
                    46,
                    2,
                    197,
                    0
                ],
                "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese\n  Chess via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese\n  Chess via Reinforcement Learning"
                },
                "summary": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas."
                },
                "authors": [
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Shuochen Liu"
                    },
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jiayao Shi"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12207v1",
                "updated": "2025-07-16T13:07:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    7,
                    24,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:07:24Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    7,
                    24,
                    2,
                    197,
                    0
                ],
                "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics\n  via LLM-driven Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics\n  via LLM-driven Evolution"
                },
                "summary": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems."
                },
                "authors": [
                    {
                        "name": "Subin Lin"
                    },
                    {
                        "name": "Chuanbo Hua"
                    }
                ],
                "author_detail": {
                    "name": "Chuanbo Hua"
                },
                "author": "Chuanbo Hua",
                "arxiv_comment": "ICML 2025 CO-Build Workshop Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12205v1",
                "updated": "2025-07-16T13:04:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    4,
                    6,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:04:06Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    4,
                    6,
                    2,
                    197,
                    0
                ],
                "title": "Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed\n  Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed\n  Storage"
                },
                "summary": "Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance\nbottleneck in the local deployment of sparse Large Language Models (LLMs),\nwhere inference predominantly operates on workloads during the decoder phase\nwith a batch size of one. Existing SpMV kernels and sparse matrix formats,\noriginally designed for scientific computing, fail to exploit the unique\nstructure patterns inherent in sparse LLMs, resulting in suboptimal performance\nand excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized\nSpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a\nhierarchical block extraction algorithm that captures multiple granularities of\nblock structures within sparse LLMs, and (2) a novel compressed sparse format\n(EC-CSR) that employs delta indexing to reduce storage overhead and enhance\nmemory access efficiency. Evaluated on real sparse weight matrices from LLaMA\nand OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV\nlibraries and reduces storage overhead by up to 55.4% compared to CSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance\nbottleneck in the local deployment of sparse Large Language Models (LLMs),\nwhere inference predominantly operates on workloads during the decoder phase\nwith a batch size of one. Existing SpMV kernels and sparse matrix formats,\noriginally designed for scientific computing, fail to exploit the unique\nstructure patterns inherent in sparse LLMs, resulting in suboptimal performance\nand excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized\nSpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a\nhierarchical block extraction algorithm that captures multiple granularities of\nblock structures within sparse LLMs, and (2) a novel compressed sparse format\n(EC-CSR) that employs delta indexing to reduce storage overhead and enhance\nmemory access efficiency. Evaluated on real sparse weight matrices from LLaMA\nand OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV\nlibraries and reduces storage overhead by up to 55.4% compared to CSR."
                },
                "authors": [
                    {
                        "name": "Junqing Lin"
                    },
                    {
                        "name": "Jingwei Sun"
                    },
                    {
                        "name": "Mingge Lu"
                    },
                    {
                        "name": "Guangzhong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangzhong Sun"
                },
                "author": "Guangzhong Sun",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11423v2",
                "updated": "2025-07-16T13:02:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    2,
                    26,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-15T15:47:47Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    15,
                    47,
                    47,
                    1,
                    196,
                    0
                ],
                "title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer,\n  and Optimize?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer,\n  and Optimize?"
                },
                "summary": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities."
                },
                "authors": [
                    {
                        "name": "Yanjian Zhang"
                    },
                    {
                        "name": "Guillaume Wisniewski"
                    },
                    {
                        "name": "Nadi Tomeh"
                    },
                    {
                        "name": "Thierry Charnois"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Charnois"
                },
                "author": "Thierry Charnois",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12201v1",
                "updated": "2025-07-16T12:55:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    55,
                    58,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T12:55:58Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    55,
                    58,
                    2,
                    197,
                    0
                ],
                "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and\n  Reducing Hallucination in Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and\n  Reducing Hallucination in Generative Models"
                },
                "summary": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to hallucinations,\noften stemming from inaccuracies in score approximation. In this work, we\nreinterpret diffusion sampling through the lens of optimization and introduce\nRODS (Robust Optimization-inspired Diffusion Sampler), a novel method that\ndetects and corrects high-risk sampling steps using geometric cues from the\nloss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS improves both sampling fidelity and robustness, detecting\nover 70% of hallucinated samples and correcting more than 25%, all while\navoiding the introduction of new artifacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved state-of-the-art performance in generative\nmodeling, yet their sampling procedures remain vulnerable to hallucinations,\noften stemming from inaccuracies in score approximation. In this work, we\nreinterpret diffusion sampling through the lens of optimization and introduce\nRODS (Robust Optimization-inspired Diffusion Sampler), a novel method that\ndetects and corrects high-risk sampling steps using geometric cues from the\nloss landscape. RODS enforces smoother sampling trajectories and adaptively\nadjusts perturbations, reducing hallucinations without retraining and at\nminimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands\ndemonstrate that RODS improves both sampling fidelity and robustness, detecting\nover 70% of hallucinated samples and correcting more than 25%, all while\navoiding the introduction of new artifacts."
                },
                "authors": [
                    {
                        "name": "Yiqi Tian"
                    },
                    {
                        "name": "Pengfei Jin"
                    },
                    {
                        "name": "Mingze Yuan"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Bo Zeng"
                    },
                    {
                        "name": "Quanzheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Quanzheng Li"
                },
                "author": "Quanzheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12197v1",
                "updated": "2025-07-16T12:47:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    47,
                    9,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T12:47:09Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    47,
                    9,
                    2,
                    197,
                    0
                ],
                "title": "Quantize More, Lose Less: Autoregressive Generation from Residually\n  Quantized Speech Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize More, Lose Less: Autoregressive Generation from Residually\n  Quantized Speech Representations"
                },
                "summary": "Text-to-speech (TTS) synthesis has seen renewed progress under the discrete\nmodeling paradigm. Existing autoregressive approaches often rely on\nsingle-codebook representations, which suffer from significant information\nloss. Even with post-hoc refinement techniques such as flow matching, these\nmethods fail to recover fine-grained details (e.g., prosodic nuances,\nspeaker-specific timbres), especially in challenging scenarios like singing\nvoice or music synthesis. We propose QTTS, a novel TTS framework built upon our\nnew audio codec, QDAC. The core innovation of QDAC lies in its end-to-end\ntraining of an ASR-based auto-regressive network with a GAN, which achieves\nsuperior semantic feature disentanglement for scalable, near-lossless\ncompression. QTTS models these discrete codes using two innovative strategies:\nthe Hierarchical Parallel architecture, which uses a dual-AR structure to model\ninter-codebook dependencies for higher-quality synthesis, and the Delay\nMultihead approach, which employs parallelized prediction with a fixed delay to\naccelerate inference speed. Our experiments demonstrate that the proposed\nframework achieves higher synthesis quality and better preserves expressive\ncontent compared to baseline. This suggests that scaling up compression via\nmulti-codebook modeling is a promising direction for high-fidelity,\ngeneral-purpose speech and audio generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-speech (TTS) synthesis has seen renewed progress under the discrete\nmodeling paradigm. Existing autoregressive approaches often rely on\nsingle-codebook representations, which suffer from significant information\nloss. Even with post-hoc refinement techniques such as flow matching, these\nmethods fail to recover fine-grained details (e.g., prosodic nuances,\nspeaker-specific timbres), especially in challenging scenarios like singing\nvoice or music synthesis. We propose QTTS, a novel TTS framework built upon our\nnew audio codec, QDAC. The core innovation of QDAC lies in its end-to-end\ntraining of an ASR-based auto-regressive network with a GAN, which achieves\nsuperior semantic feature disentanglement for scalable, near-lossless\ncompression. QTTS models these discrete codes using two innovative strategies:\nthe Hierarchical Parallel architecture, which uses a dual-AR structure to model\ninter-codebook dependencies for higher-quality synthesis, and the Delay\nMultihead approach, which employs parallelized prediction with a fixed delay to\naccelerate inference speed. Our experiments demonstrate that the proposed\nframework achieves higher synthesis quality and better preserves expressive\ncontent compared to baseline. This suggests that scaling up compression via\nmulti-codebook modeling is a promising direction for high-fidelity,\ngeneral-purpose speech and audio generation."
                },
                "authors": [
                    {
                        "name": "Yichen Han"
                    },
                    {
                        "name": "Xiaoyang Hao"
                    },
                    {
                        "name": "Keming Chen"
                    },
                    {
                        "name": "Weibo Xiong"
                    },
                    {
                        "name": "Jun He"
                    },
                    {
                        "name": "Ruonan Zhang"
                    },
                    {
                        "name": "Junjie Cao"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Dongrui Zhang"
                    },
                    {
                        "name": "Hui Xia"
                    },
                    {
                        "name": "Huilei Fu"
                    },
                    {
                        "name": "Kai Jia"
                    },
                    {
                        "name": "Kaixuan Guo"
                    },
                    {
                        "name": "Mingli Jin"
                    },
                    {
                        "name": "Qingyun Meng"
                    },
                    {
                        "name": "Ruidong Ma"
                    },
                    {
                        "name": "Ruiqian Fang"
                    },
                    {
                        "name": "Shaotong Guo"
                    },
                    {
                        "name": "Xuhui Li"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Yulong Liu"
                    },
                    {
                        "name": "Yunfeng Li"
                    },
                    {
                        "name": "Yuyi Zhang"
                    },
                    {
                        "name": "Yuze Zhou"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Zhaowen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhaowen Chen"
                },
                "author": "Zhaowen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03564v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03564v2",
                "updated": "2025-07-16T12:36:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    36,
                    59,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-04T13:16:59Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    13,
                    16,
                    59,
                    4,
                    185,
                    0
                ],
                "title": "2.5D Object Detection for Intelligent Roadside Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D Object Detection for Intelligent Roadside Infrastructure"
                },
                "summary": "On-board sensors of autonomous vehicles can be obstructed, occluded, or\nlimited by restricted fields of view, complicating downstream driving\ndecisions. Intelligent roadside infrastructure perception systems, installed at\nelevated vantage points, can provide wide, unobstructed intersection coverage,\nsupplying a complementary information stream to autonomous vehicles via\nvehicle-to-everything (V2X) communication. However, conventional 3D\nobject-detection algorithms struggle to generalize under the domain shift\nintroduced by top-down perspectives and steep camera angles. We introduce a\n2.5D object detection framework, tailored specifically for infrastructure\nroadside-mounted cameras. Unlike conventional 2D or 3D object detection, we\nemploy a prediction approach to detect ground planes of vehicles as\nparallelograms in the image frame. The parallelogram preserves the planar\nposition, size, and orientation of objects while omitting their height, which\nis unnecessary for most downstream applications. For training, a mix of\nreal-world and synthetically generated scenes is leveraged. We evaluate\ngeneralizability on a held-out camera viewpoint and in adverse-weather\nscenarios absent from the training set. Our results show high detection\naccuracy, strong cross-viewpoint generalization, and robustness to diverse\nlighting and weather conditions. Model weights and inference code are provided\nat: https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-board sensors of autonomous vehicles can be obstructed, occluded, or\nlimited by restricted fields of view, complicating downstream driving\ndecisions. Intelligent roadside infrastructure perception systems, installed at\nelevated vantage points, can provide wide, unobstructed intersection coverage,\nsupplying a complementary information stream to autonomous vehicles via\nvehicle-to-everything (V2X) communication. However, conventional 3D\nobject-detection algorithms struggle to generalize under the domain shift\nintroduced by top-down perspectives and steep camera angles. We introduce a\n2.5D object detection framework, tailored specifically for infrastructure\nroadside-mounted cameras. Unlike conventional 2D or 3D object detection, we\nemploy a prediction approach to detect ground planes of vehicles as\nparallelograms in the image frame. The parallelogram preserves the planar\nposition, size, and orientation of objects while omitting their height, which\nis unnecessary for most downstream applications. For training, a mix of\nreal-world and synthetically generated scenes is leveraged. We evaluate\ngeneralizability on a held-out camera viewpoint and in adverse-weather\nscenarios absent from the training set. Our results show high detection\naccuracy, strong cross-viewpoint generalization, and robustness to diverse\nlighting and weather conditions. Model weights and inference code are provided\nat: https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection"
                },
                "authors": [
                    {
                        "name": "Nikolai Polley"
                    },
                    {
                        "name": "Yacin Boualili"
                    },
                    {
                        "name": "Ferdinand Mtsch"
                    },
                    {
                        "name": "Maximilian Zipfl"
                    },
                    {
                        "name": "Tobias Fleck"
                    },
                    {
                        "name": "J. Marius Zllner"
                    }
                ],
                "author_detail": {
                    "name": "J. Marius Zllner"
                },
                "author": "J. Marius Zllner",
                "arxiv_comment": "Accepted at 2025 IEEE 28th International Conference on Intelligent\n  Transportation Systems (ITSC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03564v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07682v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07682v4",
                "updated": "2025-07-16T12:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    27,
                    40,
                    2,
                    197,
                    0
                ],
                "published": "2024-12-10T17:13:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    13,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation"
                },
                "summary": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks."
                },
                "authors": [
                    {
                        "name": "Alfredo Garrachn Ruiz"
                    },
                    {
                        "name": "Toms de la Rosa"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo",
                "arxiv_comment": "13 pages, 12 tables, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07682v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07682v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08802v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08802v4",
                "updated": "2025-07-16T12:09:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    9,
                    34,
                    2,
                    197,
                    0
                ],
                "published": "2024-02-05T14:20:19Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    14,
                    20,
                    19,
                    0,
                    36,
                    0
                ],
                "title": "Governance of Generative Artificial Intelligence for Companies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Governance of Generative Artificial Intelligence for Companies"
                },
                "summary": "Generative Artificial Intelligence (GenAI), specifically large language\nmodels(LLMs) like ChatGPT, has swiftly entered organizations without adequate\ngovernance, posing both opportunities and risks. Despite extensive debates on\nGenAI's transformative nature and regulatory measures, limited research\naddresses organizational governance, encompassing technical and business\nperspectives. Although numerous frameworks for governance of AI exist, it is\nnot clear to what extent they apply to GenAI. Our review paper fills this gap\nby surveying recent works with the purpose of better understanding fundamental\ncharacteristics of GenAI and adjusting prior frameworks specifically towards\nGenAI governance within companies. To do so, it extends Nickerson's framework\ndevelopment processes to include prior conceptualizations. Our framework\noutlines the scope, objectives, and governance mechanisms tailored to harness\nbusiness opportunities as well as mitigate risks associated with GenAI\nintegration. Our research contributes a focused approach to GenAI governance,\noffering practical insights for companies navigating the challenges of GenAI\nadoption and highlighting research gaps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI), specifically large language\nmodels(LLMs) like ChatGPT, has swiftly entered organizations without adequate\ngovernance, posing both opportunities and risks. Despite extensive debates on\nGenAI's transformative nature and regulatory measures, limited research\naddresses organizational governance, encompassing technical and business\nperspectives. Although numerous frameworks for governance of AI exist, it is\nnot clear to what extent they apply to GenAI. Our review paper fills this gap\nby surveying recent works with the purpose of better understanding fundamental\ncharacteristics of GenAI and adjusting prior frameworks specifically towards\nGenAI governance within companies. To do so, it extends Nickerson's framework\ndevelopment processes to include prior conceptualizations. Our framework\noutlines the scope, objectives, and governance mechanisms tailored to harness\nbusiness opportunities as well as mitigate risks associated with GenAI\nintegration. Our research contributes a focused approach to GenAI governance,\noffering practical insights for companies navigating the challenges of GenAI\nadoption and highlighting research gaps."
                },
                "authors": [
                    {
                        "name": "Johannes Schneider"
                    },
                    {
                        "name": "Pauline Kuss"
                    },
                    {
                        "name": "Rene Abraham"
                    },
                    {
                        "name": "Christian Meske"
                    }
                ],
                "author_detail": {
                    "name": "Christian Meske"
                },
                "author": "Christian Meske",
                "arxiv_comment": "This paper is under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08802v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08802v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12171v1",
                "updated": "2025-07-16T12:05:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    5,
                    23,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T12:05:23Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    5,
                    23,
                    2,
                    197,
                    0
                ],
                "title": "Cosmic Cartography II: completing galaxy catalogs for gravitational-wave\n  cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic Cartography II: completing galaxy catalogs for gravitational-wave\n  cosmology"
                },
                "summary": "The dark siren method exploits the complementarity between gravitational-wave\nbinary coalescence signals and galaxy catalogs originating from the same\nregions of space. However, all galaxy catalogs are incomplete, i.e. they only\ninclude a subset of all galaxies, typically being biased towards the bright end\nof the luminosity distribution. This sub-selection systematically affects the\ndark siren inference of the Hubble constant $H_0$, so a completeness relation\nhas to be introduced that accounts for the missing objects. In the literature\nit is standard to assume that the missing galaxies are uniformly distributed\nacross the sky and that the galaxy magnitude distribution is known. In this\nwork we develop a novel method which improves upon these assumptions and\nreconstructs the underlying true galaxy field, respecting the spatial\ncorrelation of galaxies on large scales. In our method the true magnitude\ndistribution of galaxies is inferred alongside the spatial galaxy distribution.\nOur method results in an improved three-dimensional prior in redshift and sky\nposition for the host galaxy of a GW event, which is expected to make the\nresulting $H_0$ posterior more robust. Building on our previous work, we make a\nnumber of improvements, and validate our method on simulated data based on the\nMillennium simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dark siren method exploits the complementarity between gravitational-wave\nbinary coalescence signals and galaxy catalogs originating from the same\nregions of space. However, all galaxy catalogs are incomplete, i.e. they only\ninclude a subset of all galaxies, typically being biased towards the bright end\nof the luminosity distribution. This sub-selection systematically affects the\ndark siren inference of the Hubble constant $H_0$, so a completeness relation\nhas to be introduced that accounts for the missing objects. In the literature\nit is standard to assume that the missing galaxies are uniformly distributed\nacross the sky and that the galaxy magnitude distribution is known. In this\nwork we develop a novel method which improves upon these assumptions and\nreconstructs the underlying true galaxy field, respecting the spatial\ncorrelation of galaxies on large scales. In our method the true magnitude\ndistribution of galaxies is inferred alongside the spatial galaxy distribution.\nOur method results in an improved three-dimensional prior in redshift and sky\nposition for the host galaxy of a GW event, which is expected to make the\nresulting $H_0$ posterior more robust. Building on our previous work, we make a\nnumber of improvements, and validate our method on simulated data based on the\nMillennium simulation."
                },
                "authors": [
                    {
                        "name": "Konstantin Leyde"
                    },
                    {
                        "name": "Tessa Baker"
                    },
                    {
                        "name": "Wolfgang Enzi"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Enzi"
                },
                "author": "Wolfgang Enzi",
                "arxiv_comment": "We refer the busy reader to Fig. 2 for an overview of the method, and\n  to Sec. 4.1 with Figures7, 8 and 10 for the main results. 43 pages, 20\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05843v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05843v4",
                "updated": "2025-07-16T11:47:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    47,
                    45,
                    2,
                    197,
                    0
                ],
                "published": "2025-02-09T10:30:54Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    10,
                    30,
                    54,
                    6,
                    40,
                    0
                ],
                "title": "From Objects to Events: Unlocking Complex Visual Understanding in Object\n  Detectors via LLM-guided Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Objects to Events: Unlocking Complex Visual Understanding in Object\n  Detectors via LLM-guided Symbolic Reasoning"
                },
                "summary": "Current object detectors excel at entity localization and classification, yet\nexhibit inherent limitations in event recognition capabilities. This deficiency\narises from their architecture's emphasis on discrete object identification\nrather than modeling the compositional reasoning, inter-object correlations,\nand contextual semantics essential for comprehensive event understanding. To\naddress this challenge, we present a novel framework that expands the\ncapability of standard object detectors beyond mere object recognition to\ncomplex event understanding through LLM-guided symbolic reasoning. Our key\ninnovation lies in bridging the semantic gap between object detection and event\nunderstanding without requiring expensive task-specific training. The proposed\nplug-and-play framework interfaces with any open-vocabulary detector while\nextending their inherent capabilities across architectures. At its core, our\napproach combines (i) a symbolic regression mechanism exploring relationship\npatterns among detected entities and (ii) a LLM-guided strategically guiding\nthe search toward meaningful expressions. These discovered symbolic rules\ntransform low-level visual perception into interpretable event understanding,\nproviding a transparent reasoning path from objects to events with strong\ntransferability across domains.We compared our training-free framework against\nspecialized event recognition systems across diverse application domains.\nExperiments demonstrate that our framework enhances multiple object detector\narchitectures to recognize complex events such as illegal fishing activities\n(75% AUROC, +8.36% improvement), construction safety violations (+15.77%), and\nabnormal crowd behaviors (+23.16%). Code is available at\n\\href{https://github.com/MAC-AutoML/SymbolicDet}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current object detectors excel at entity localization and classification, yet\nexhibit inherent limitations in event recognition capabilities. This deficiency\narises from their architecture's emphasis on discrete object identification\nrather than modeling the compositional reasoning, inter-object correlations,\nand contextual semantics essential for comprehensive event understanding. To\naddress this challenge, we present a novel framework that expands the\ncapability of standard object detectors beyond mere object recognition to\ncomplex event understanding through LLM-guided symbolic reasoning. Our key\ninnovation lies in bridging the semantic gap between object detection and event\nunderstanding without requiring expensive task-specific training. The proposed\nplug-and-play framework interfaces with any open-vocabulary detector while\nextending their inherent capabilities across architectures. At its core, our\napproach combines (i) a symbolic regression mechanism exploring relationship\npatterns among detected entities and (ii) a LLM-guided strategically guiding\nthe search toward meaningful expressions. These discovered symbolic rules\ntransform low-level visual perception into interpretable event understanding,\nproviding a transparent reasoning path from objects to events with strong\ntransferability across domains.We compared our training-free framework against\nspecialized event recognition systems across diverse application domains.\nExperiments demonstrate that our framework enhances multiple object detector\narchitectures to recognize complex events such as illegal fishing activities\n(75% AUROC, +8.36% improvement), construction safety violations (+15.77%), and\nabnormal crowd behaviors (+23.16%). Code is available at\n\\href{https://github.com/MAC-AutoML/SymbolicDet}{here}."
                },
                "authors": [
                    {
                        "name": "Yuhui Zeng"
                    },
                    {
                        "name": "Haoxiang Wu"
                    },
                    {
                        "name": "Wenjie Nie"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Guangyao Chen"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05843v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05843v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15740v3",
                "updated": "2025-07-16T11:45:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    45,
                    2,
                    2,
                    197,
                    0
                ],
                "published": "2024-03-23T06:36:32Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    6,
                    36,
                    32,
                    5,
                    83,
                    0
                ],
                "title": "Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training"
                },
                "summary": "A primary concern regarding training large language models (LLMs) is whether\nthey abuse copyrighted online text. With the increasing training data scale and\nthe prevalence of LLMs in daily lives, two problems arise: \\textbf{1)} false\npositive membership inference results misled by similar examples; \\textbf{2)}\nmembership inference methods are usually too complex for end users to\nunderstand and use. To address these issues, we propose an alternative\n\\textit{insert-and-detect} methodology, advocating that web users and content\nplatforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{\\textbf{ghost\nsentences}} and a user-friendly last-$k$ words test, allowing end users to chat\nwith LLMs for membership inference. Ghost sentences consist primarily of unique\npassphrases of random natural words, which can come with customized elements to\nbypass possible filter rules. The last-$k$ words test requires a significant\nrepetition time of ghost sentences~($\\ge10$). For cases with fewer repetitions,\nwe designed an extra perplexity test, as LLMs exhibit high perplexity when\nencountering unnatural passphrases. We also conduct a comprehensive study on\nthe memorization and membership inference of ghost sentences, examining factors\nsuch as training data scales, model sizes, repetition times, insertion\npositions, wordlist of passphrases, alignment, \\textit{etc}. Our study shows\nthe possibility of applying ghost sentences in real scenarios and provides\ninstructions for the potential application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A primary concern regarding training large language models (LLMs) is whether\nthey abuse copyrighted online text. With the increasing training data scale and\nthe prevalence of LLMs in daily lives, two problems arise: \\textbf{1)} false\npositive membership inference results misled by similar examples; \\textbf{2)}\nmembership inference methods are usually too complex for end users to\nunderstand and use. To address these issues, we propose an alternative\n\\textit{insert-and-detect} methodology, advocating that web users and content\nplatforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{\\textbf{ghost\nsentences}} and a user-friendly last-$k$ words test, allowing end users to chat\nwith LLMs for membership inference. Ghost sentences consist primarily of unique\npassphrases of random natural words, which can come with customized elements to\nbypass possible filter rules. The last-$k$ words test requires a significant\nrepetition time of ghost sentences~($\\ge10$). For cases with fewer repetitions,\nwe designed an extra perplexity test, as LLMs exhibit high perplexity when\nencountering unnatural passphrases. We also conduct a comprehensive study on\nthe memorization and membership inference of ghost sentences, examining factors\nsuch as training data scales, model sizes, repetition times, insertion\npositions, wordlist of passphrases, alignment, \\textit{etc}. Our study shows\nthe possibility of applying ghost sentences in real scenarios and provides\ninstructions for the potential application."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Linchao Zhu"
                    },
                    {
                        "name": "Ruijie Quan"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "A technical report, work mainly done in the early of 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11579v2",
                "updated": "2025-07-16T11:39:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    39,
                    44,
                    2,
                    197,
                    0
                ],
                "published": "2025-03-14T16:45:23Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    45,
                    23,
                    4,
                    73,
                    0
                ],
                "title": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers"
                },
                "summary": "State-of-the-art transformer-based large multimodal models (LMMs) struggle to\nhandle hour-long video inputs due to the quadratic complexity of the causal\nself-attention operations, leading to high computational costs during training\nand inference. Existing token compression-based methods reduce the number of\nvideo tokens but often incur information loss and remain inefficient for\nextremely long sequences. In this paper, we explore an orthogonal direction to\nbuild a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to\nencode video tokens with linear complexity. Without any token reduction, VAMBA\ncan encode more than 1024 frames (640$\\times$360) on a single GPU, while\ntransformer-based models can only encode 256 frames. On long video input, VAMBA\nachieves at least 50% reduction in GPU memory usage during training and\ninference, and nearly doubles the speed per training step compared to\ntransformer-based LMMs. Our experimental results demonstrate that VAMBA\nimproves accuracy by 4.3% on the challenging hour-long video understanding\nbenchmark LVBench over prior efficient video LMMs, and maintains strong\nperformance on a broad spectrum of long and short video understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art transformer-based large multimodal models (LMMs) struggle to\nhandle hour-long video inputs due to the quadratic complexity of the causal\nself-attention operations, leading to high computational costs during training\nand inference. Existing token compression-based methods reduce the number of\nvideo tokens but often incur information loss and remain inefficient for\nextremely long sequences. In this paper, we explore an orthogonal direction to\nbuild a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to\nencode video tokens with linear complexity. Without any token reduction, VAMBA\ncan encode more than 1024 frames (640$\\times$360) on a single GPU, while\ntransformer-based models can only encode 256 frames. On long video input, VAMBA\nachieves at least 50% reduction in GPU memory usage during training and\ninference, and nearly doubles the speed per training step compared to\ntransformer-based LMMs. Our experimental results demonstrate that VAMBA\nimproves accuracy by 4.3% on the challenging hour-long video understanding\nbenchmark LVBench over prior efficient video LMMs, and maintains strong\nperformance on a broad spectrum of long and short video understanding tasks."
                },
                "authors": [
                    {
                        "name": "Weiming Ren"
                    },
                    {
                        "name": "Wentao Ma"
                    },
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Cong Wei"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "ICCV 2025 Camera Ready Version. Project Page:\n  https://tiger-ai-lab.github.io/Vamba/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12157v1",
                "updated": "2025-07-16T11:37:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    37,
                    33,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    37,
                    33,
                    2,
                    197,
                    0
                ],
                "title": "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data\n  Augmentation"
                },
                "summary": "Fine-grained image recognition (FGIR) aims to distinguish visually similar\nsub-categories within a broader class, such as identifying bird species. While\nmost existing FGIR methods rely on backbones pretrained on large-scale datasets\nlike ImageNet, this dependence limits adaptability to resource-constrained\nenvironments and hinders the development of task-specific architectures\ntailored to the unique challenges of FGIR.\n  In this work, we challenge the conventional reliance on pretrained models by\ndemonstrating that high-performance FGIR systems can be trained entirely from\nscratch. We introduce a novel training framework, TGDA, that integrates\ndata-aware augmentation with weak supervision via a fine-grained-aware teacher\nmodel, implemented through knowledge distillation. This framework unlocks the\ndesign of task-specific and hardware-aware architectures, including LRNets for\nlow-resolution FGIR and ViTFS, a family of Vision Transformers optimized for\nefficient inference.\n  Extensive experiments across three FGIR benchmarks over diverse settings\ninvolving low-resolution and high-resolution inputs show that our method\nconsistently matches or surpasses state-of-the-art pretrained counterparts. In\nparticular, in the low-resolution setting, LRNets trained with TGDA improve\naccuracy by up to 23\\% over prior methods while requiring up to 20.6x less\nparameters, lower FLOPs, and significantly less training data. Similarly,\nViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k\nwhile using 15.3x fewer trainable parameters and requiring orders of magnitudes\nless data. These results highlight TGDA's potential as an adaptable alternative\nto pretraining, paving the way for more efficient fine-grained vision systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained image recognition (FGIR) aims to distinguish visually similar\nsub-categories within a broader class, such as identifying bird species. While\nmost existing FGIR methods rely on backbones pretrained on large-scale datasets\nlike ImageNet, this dependence limits adaptability to resource-constrained\nenvironments and hinders the development of task-specific architectures\ntailored to the unique challenges of FGIR.\n  In this work, we challenge the conventional reliance on pretrained models by\ndemonstrating that high-performance FGIR systems can be trained entirely from\nscratch. We introduce a novel training framework, TGDA, that integrates\ndata-aware augmentation with weak supervision via a fine-grained-aware teacher\nmodel, implemented through knowledge distillation. This framework unlocks the\ndesign of task-specific and hardware-aware architectures, including LRNets for\nlow-resolution FGIR and ViTFS, a family of Vision Transformers optimized for\nefficient inference.\n  Extensive experiments across three FGIR benchmarks over diverse settings\ninvolving low-resolution and high-resolution inputs show that our method\nconsistently matches or surpasses state-of-the-art pretrained counterparts. In\nparticular, in the low-resolution setting, LRNets trained with TGDA improve\naccuracy by up to 23\\% over prior methods while requiring up to 20.6x less\nparameters, lower FLOPs, and significantly less training data. Similarly,\nViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k\nwhile using 15.3x fewer trainable parameters and requiring orders of magnitudes\nless data. These results highlight TGDA's potential as an adaptable alternative\nto pretraining, paving the way for more efficient fine-grained vision systems."
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Fernando Mikael"
                    },
                    {
                        "name": "Oswin Gosal"
                    },
                    {
                        "name": "Femiloye Oyerinde"
                    },
                    {
                        "name": "Hao-Chun Liang"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Main: 10 pages, 2 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09037v2",
                "updated": "2025-07-16T11:33:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    33,
                    35,
                    2,
                    197,
                    0
                ],
                "published": "2025-04-12T01:27:49Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    1,
                    27,
                    49,
                    5,
                    102,
                    0
                ],
                "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to\n  Reason, and Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to\n  Reason, and Agentic Systems"
                },
                "summary": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ..."
                },
                "authors": [
                    {
                        "name": "Zixuan Ke"
                    },
                    {
                        "name": "Fangkai Jiao"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Do Xuan Long"
                    },
                    {
                        "name": "Minzhi Li"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Peifeng Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "72 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12145v1",
                "updated": "2025-07-16T11:25:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    25,
                    3,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:25:03Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    25,
                    3,
                    2,
                    197,
                    0
                ],
                "title": "PRISM: Distributed Inference for Foundation Models at Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Distributed Inference for Foundation Models at Edge"
                },
                "summary": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Muhammad Azlan Qazi"
                    },
                    {
                        "name": "Alexandros Iosifidis"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12144v1",
                "updated": "2025-07-16T11:22:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    22,
                    18,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:22:18Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    22,
                    18,
                    2,
                    197,
                    0
                ],
                "title": "FourCastNet 3: A geometric approach to probabilistic machine-learning\n  weather forecasting at scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FourCastNet 3: A geometric approach to probabilistic machine-learning\n  weather forecasting at scale"
                },
                "summary": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions."
                },
                "authors": [
                    {
                        "name": "Boris Bonev"
                    },
                    {
                        "name": "Thorsten Kurth"
                    },
                    {
                        "name": "Ankur Mahesh"
                    },
                    {
                        "name": "Mauro Bisson"
                    },
                    {
                        "name": "Jean Kossaifi"
                    },
                    {
                        "name": "Karthik Kashinath"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "William D. Collins"
                    },
                    {
                        "name": "Michael S. Pritchard"
                    },
                    {
                        "name": "Alexander Keller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Keller"
                },
                "author": "Alexander Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "86-10, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.6.5; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12143v1",
                "updated": "2025-07-16T11:19:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    19,
                    28,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:19:28Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    19,
                    28,
                    2,
                    197,
                    0
                ],
                "title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as\n  Teachers, Students and Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as\n  Teachers, Students and Evaluators"
                },
                "summary": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable."
                },
                "authors": [
                    {
                        "name": "Pavel indel"
                    },
                    {
                        "name": "Ondej Bojar"
                    }
                ],
                "author_detail": {
                    "name": "Ondej Bojar"
                },
                "author": "Ondej Bojar",
                "arxiv_comment": "30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12142v1",
                "updated": "2025-07-16T11:17:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    17,
                    12,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:17:12Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    17,
                    12,
                    2,
                    197,
                    0
                ],
                "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA\n  Optimization"
                },
                "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications."
                },
                "authors": [
                    {
                        "name": "Vladimir Bogachev"
                    },
                    {
                        "name": "Vladimir Aletov"
                    },
                    {
                        "name": "Alexander Molozhavenko"
                    },
                    {
                        "name": "Denis Bobkov"
                    },
                    {
                        "name": "Vera Soboleva"
                    },
                    {
                        "name": "Aibek Alanov"
                    },
                    {
                        "name": "Maxim Rakhuba"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Rakhuba"
                },
                "author": "Maxim Rakhuba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 65F55, 53Z50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12133v1",
                "updated": "2025-07-16T11:02:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    2,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:02:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    2,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with\n  Optimized VMD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with\n  Optimized VMD"
                },
                "summary": "Device recognition is vital for security in wireless communication systems,\nparticularly for applications like access control. Radio Frequency Fingerprint\nIdentification (RFFI) offers a non-cryptographic solution by exploiting\nhardware-induced signal distortions. This paper proposes HyDRA, a Hybrid\nDual-mode RF Architecture that integrates an optimized Variational Mode\nDecomposition (VMD) with a novel architecture based on the fusion of\nConvolutional Neural Networks (CNNs), Transformers, and Mamba components,\ndesigned to support both closed-set and open-set classification tasks. The\noptimized VMD enhances preprocessing efficiency and classification accuracy by\nfixing center frequencies and using closed-form solutions. HyDRA employs the\nTransformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and\nthe Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting\nto varying conditions. Evaluation on public datasets demonstrates\nstate-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance\nin our proposed open-set classification method, effectively identifying\nunauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves\nmillisecond-level inference speed with low power consumption, providing a\npractical solution for real-time wireless authentication in real-world\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device recognition is vital for security in wireless communication systems,\nparticularly for applications like access control. Radio Frequency Fingerprint\nIdentification (RFFI) offers a non-cryptographic solution by exploiting\nhardware-induced signal distortions. This paper proposes HyDRA, a Hybrid\nDual-mode RF Architecture that integrates an optimized Variational Mode\nDecomposition (VMD) with a novel architecture based on the fusion of\nConvolutional Neural Networks (CNNs), Transformers, and Mamba components,\ndesigned to support both closed-set and open-set classification tasks. The\noptimized VMD enhances preprocessing efficiency and classification accuracy by\nfixing center frequencies and using closed-form solutions. HyDRA employs the\nTransformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and\nthe Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting\nto varying conditions. Evaluation on public datasets demonstrates\nstate-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance\nin our proposed open-set classification method, effectively identifying\nunauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves\nmillisecond-level inference speed with low power consumption, providing a\npractical solution for real-time wireless authentication in real-world\nenvironments."
                },
                "authors": [
                    {
                        "name": "Hanwen Liu"
                    },
                    {
                        "name": "Yuhe Huang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yanjie Zhai"
                    },
                    {
                        "name": "Jiaxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxuan Lu"
                },
                "author": "Jiaxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12126v1",
                "updated": "2025-07-16T10:49:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    49,
                    30,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T10:49:30Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    49,
                    30,
                    2,
                    197,
                    0
                ],
                "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation\n  for Unstructured Survey data Modeling and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation\n  for Unstructured Survey data Modeling and Analysis"
                },
                "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines."
                },
                "authors": [
                    {
                        "name": "Payal Bhattad"
                    },
                    {
                        "name": "Sai Manoj Pudukotai Dinakarrao"
                    },
                    {
                        "name": "Anju Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Anju Gupta"
                },
                "author": "Anju Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00513v3",
                "updated": "2025-07-16T10:35:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    35,
                    9,
                    2,
                    197,
                    0
                ],
                "published": "2025-04-01T08:03:40Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    3,
                    40,
                    1,
                    91,
                    0
                ],
                "title": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset"
                },
                "summary": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use."
                },
                "authors": [
                    {
                        "name": "Asma Yamani"
                    },
                    {
                        "name": "Malak Baslyman"
                    },
                    {
                        "name": "Moataz Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Moataz Ahmed"
                },
                "author": "Moataz Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12272v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12272v4",
                "updated": "2025-07-16T10:34:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    34,
                    26,
                    2,
                    197,
                    0
                ],
                "published": "2025-02-17T19:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    19,
                    16,
                    37,
                    0,
                    48,
                    0
                ],
                "title": "Learning to Reason at the Frontier of Learnability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason at the Frontier of Learnability"
                },
                "summary": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs."
                },
                "authors": [
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12272v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12272v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12116v1",
                "updated": "2025-07-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    33,
                    29,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T10:33:29Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    33,
                    29,
                    2,
                    197,
                    0
                ],
                "title": "Euclid preparation. Simulating thousands of Euclid spectroscopic skies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid preparation. Simulating thousands of Euclid spectroscopic skies"
                },
                "summary": "We present two extensive sets of 3500+1000 simulations of dark matter haloes\non the past light cone, and two corresponding sets of simulated (`mock') galaxy\ncatalogues that represent the Euclid spectroscopic sample. The simulations were\nproduced with the latest version of the PINOCCHIO code, and provide the\nlargest, public set of simulated skies. Mock galaxy catalogues were obtained by\npopulating haloes with galaxies using an halo occupation distribution (HOD)\nmodel extracted from the Flagship galaxy catalogue provided by Euclid\nCollaboration. The Geppetto set of 3500 simulated skies was obtained by tiling\na 1.2 Gpc/h box to cover a light-cone whose sky footprint is a circle of 30 deg\nradius, for an area of 2763 deg$^2$ and a minimum halo mass of\n$1.5\\times10^{11}$ Msun/h. The relatively small box size makes this set unfit\nfor measuring very large scales. The EuclidLargeBox set consists of 1000\nsimulations of 3.38 Gpc/h, with the same mass resolution and a footprint that\ncovers half of the sky, excluding the Milky Way zone of avoidance. From this we\nproduced a set of 1000 EuclidLargeMocks on the 30 deg radius footprint, whose\ncomoving volume is fully contained in the simulation box. We validated the two\nsets of catalogues by analysing number densities, power spectra, and 2-point\ncorrelation functions, showing that the Flagship spectroscopic catalogue is\nconsistent with being one of the realisations of the simulated sets, although\nwe noticed small deviations limited to the quadrupole at k>0.2 h/Mpc. We show\ncosmological parameter inference from these catalogues and demonstrate that\nusing one realisation of EuclidLargeMocks in place of the Flagship mock\nproduces the same posteriors, to within the expected shift given by sample\nvariance. These simulated skies will be used for the galaxy clustering analysis\nof Euclid's Data Release 1 (DR1).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present two extensive sets of 3500+1000 simulations of dark matter haloes\non the past light cone, and two corresponding sets of simulated (`mock') galaxy\ncatalogues that represent the Euclid spectroscopic sample. The simulations were\nproduced with the latest version of the PINOCCHIO code, and provide the\nlargest, public set of simulated skies. Mock galaxy catalogues were obtained by\npopulating haloes with galaxies using an halo occupation distribution (HOD)\nmodel extracted from the Flagship galaxy catalogue provided by Euclid\nCollaboration. The Geppetto set of 3500 simulated skies was obtained by tiling\na 1.2 Gpc/h box to cover a light-cone whose sky footprint is a circle of 30 deg\nradius, for an area of 2763 deg$^2$ and a minimum halo mass of\n$1.5\\times10^{11}$ Msun/h. The relatively small box size makes this set unfit\nfor measuring very large scales. The EuclidLargeBox set consists of 1000\nsimulations of 3.38 Gpc/h, with the same mass resolution and a footprint that\ncovers half of the sky, excluding the Milky Way zone of avoidance. From this we\nproduced a set of 1000 EuclidLargeMocks on the 30 deg radius footprint, whose\ncomoving volume is fully contained in the simulation box. We validated the two\nsets of catalogues by analysing number densities, power spectra, and 2-point\ncorrelation functions, showing that the Flagship spectroscopic catalogue is\nconsistent with being one of the realisations of the simulated sets, although\nwe noticed small deviations limited to the quadrupole at k>0.2 h/Mpc. We show\ncosmological parameter inference from these catalogues and demonstrate that\nusing one realisation of EuclidLargeMocks in place of the Flagship mock\nproduces the same posteriors, to within the expected shift given by sample\nvariance. These simulated skies will be used for the galaxy clustering analysis\nof Euclid's Data Release 1 (DR1)."
                },
                "authors": [
                    {
                        "name": "Euclid Collaboration"
                    },
                    {
                        "name": "P. Monaco"
                    },
                    {
                        "name": "G. Parimbelli"
                    },
                    {
                        "name": "M. Y. Elkhashab"
                    },
                    {
                        "name": "J. Salvalaggio"
                    },
                    {
                        "name": "T. Castro"
                    },
                    {
                        "name": "M. D. Lepinzan"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "E. Sefusatti"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "L. Tornatore"
                    },
                    {
                        "name": "G. E. Addison"
                    },
                    {
                        "name": "S. Bruton"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. de la Torre"
                    },
                    {
                        "name": "P. Fosalba"
                    },
                    {
                        "name": "G. Lavaux"
                    },
                    {
                        "name": "S. Lee"
                    },
                    {
                        "name": "K. Markovic"
                    },
                    {
                        "name": "K. S. McCarthy"
                    },
                    {
                        "name": "F. Passalacqua"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "C. Scarlata"
                    },
                    {
                        "name": "P. Tallada-Cresp"
                    },
                    {
                        "name": "M. Viel"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "F. Bernardeau"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "G. Caas-Herrera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "V. F. Cardone"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "A. M. Di Giorgio"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "F. Ducret"
                    },
                    {
                        "name": "C. A. J. Duncan"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "A. Ealet"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "R. Farinelli"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "N. Fourmanoit"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "L. Guzzo"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihnen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kmmel"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "E. Munari"
                    },
                    {
                        "name": "R. Nakajima"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "F. Rizzo"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "A. G. Snchez"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "M. Scodeggio"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "M. Seiffert"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "P. Simon"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "D. Tavagnacco"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "V. Allevato"
                    },
                    {
                        "name": "M. Ballardini"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "M. Calabrese"
                    },
                    {
                        "name": "A. Cappi"
                    },
                    {
                        "name": "D. Di Ferdinando"
                    },
                    {
                        "name": "J. A. Escartin Vigo"
                    },
                    {
                        "name": "G. Fabbian"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "J. Martn-Fleitas"
                    },
                    {
                        "name": "S. Matthew"
                    },
                    {
                        "name": "N. Mauri"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "A. Pezzotta"
                    },
                    {
                        "name": "M. Pntinen"
                    },
                    {
                        "name": "C. Porciani"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "M. Tenti"
                    },
                    {
                        "name": "M. Wiesmann"
                    },
                    {
                        "name": "Y. Akrami"
                    },
                    {
                        "name": "S. Alvi"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "S. Anselmi"
                    },
                    {
                        "name": "M. Archidiacono"
                    },
                    {
                        "name": "F. Atrio-Barandela"
                    },
                    {
                        "name": "S. Avila"
                    },
                    {
                        "name": "A. Balaguera-Antolinez"
                    },
                    {
                        "name": "P. Bergamini"
                    },
                    {
                        "name": "D. Bertacca"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "A. Blanchard"
                    },
                    {
                        "name": "L. Blot"
                    },
                    {
                        "name": "S. Borgani"
                    },
                    {
                        "name": "M. L. Brown"
                    },
                    {
                        "name": "A. Calabro"
                    },
                    {
                        "name": "B. Camacho Quevedo"
                    },
                    {
                        "name": "F. Caro"
                    },
                    {
                        "name": "C. S. Carvalho"
                    },
                    {
                        "name": "F. Cogato"
                    },
                    {
                        "name": "S. Conseil"
                    },
                    {
                        "name": "S. Contarini"
                    },
                    {
                        "name": "A. R. Cooray"
                    },
                    {
                        "name": "O. Cucciati"
                    },
                    {
                        "name": "S. Davini"
                    },
                    {
                        "name": "G. Desprez"
                    },
                    {
                        "name": "A. Daz-Snchez"
                    },
                    {
                        "name": "J. J. Diaz"
                    },
                    {
                        "name": "S. Di Domizio"
                    },
                    {
                        "name": "J. M. Diego"
                    },
                    {
                        "name": "A. Enia"
                    },
                    {
                        "name": "Y. Fang"
                    },
                    {
                        "name": "A. G. Ferrari"
                    },
                    {
                        "name": "A. Finoguenov"
                    },
                    {
                        "name": "F. Fontanot"
                    },
                    {
                        "name": "A. Franco"
                    },
                    {
                        "name": "K. Ganga"
                    },
                    {
                        "name": "J. Garca-Bellido"
                    },
                    {
                        "name": "T. Gasparetto"
                    },
                    {
                        "name": "V. Gautard"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "F. Giacomini"
                    },
                    {
                        "name": "F. Gianotti"
                    },
                    {
                        "name": "G. Gozaliasl"
                    },
                    {
                        "name": "M. Guidi"
                    },
                    {
                        "name": "C. M. Gutierrez"
                    },
                    {
                        "name": "A. Hall"
                    },
                    {
                        "name": "S. Hemmati"
                    },
                    {
                        "name": "C. Hernndez-Monteagudo"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "J. Hjorth"
                    },
                    {
                        "name": "S. Joudaki"
                    },
                    {
                        "name": "J. J. E. Kajava"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "V. Kansal"
                    },
                    {
                        "name": "D. Karagiannis"
                    },
                    {
                        "name": "K. Kiiveri"
                    },
                    {
                        "name": "C. C. Kirkpatrick"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "V. Le Brun"
                    },
                    {
                        "name": "J. Le Graet"
                    },
                    {
                        "name": "L. Legrand"
                    },
                    {
                        "name": "M. Lembo"
                    },
                    {
                        "name": "F. Lepori"
                    },
                    {
                        "name": "G. Leroy"
                    },
                    {
                        "name": "G. F. Lesci"
                    },
                    {
                        "name": "J. Lesgourgues"
                    },
                    {
                        "name": "L. Leuzzi"
                    },
                    {
                        "name": "T. I. Liaudat"
                    },
                    {
                        "name": "J. Macias-Perez"
                    },
                    {
                        "name": "G. Maggio"
                    },
                    {
                        "name": "M. Magliocchetti"
                    },
                    {
                        "name": "C. Mancini"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "R. Maoli"
                    },
                    {
                        "name": "C. J. A. P. Martins"
                    },
                    {
                        "name": "L. Maurin"
                    },
                    {
                        "name": "M. Miluzio"
                    },
                    {
                        "name": "A. Montoro"
                    },
                    {
                        "name": "C. Moretti"
                    },
                    {
                        "name": "G. Morgante"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "A. Navarro-Alsina"
                    },
                    {
                        "name": "S. Nesseris"
                    },
                    {
                        "name": "K. Paterson"
                    },
                    {
                        "name": "A. Pisani"
                    },
                    {
                        "name": "D. Potter"
                    },
                    {
                        "name": "S. Quai"
                    },
                    {
                        "name": "M. Radovich"
                    },
                    {
                        "name": "G. Rodighiero"
                    },
                    {
                        "name": "S. Sacquegna"
                    },
                    {
                        "name": "M. Sahln"
                    },
                    {
                        "name": "D. B. Sanders"
                    },
                    {
                        "name": "D. Sciotti"
                    },
                    {
                        "name": "E. Sellentin"
                    },
                    {
                        "name": "L. C. Smith"
                    },
                    {
                        "name": "J. G. Sorce"
                    },
                    {
                        "name": "K. Tanidis"
                    },
                    {
                        "name": "C. Tao"
                    },
                    {
                        "name": "G. Testera"
                    },
                    {
                        "name": "R. Teyssier"
                    },
                    {
                        "name": "S. Tosi"
                    },
                    {
                        "name": "A. Troja"
                    },
                    {
                        "name": "M. Tucci"
                    },
                    {
                        "name": "C. Valieri"
                    },
                    {
                        "name": "A. Venhola"
                    },
                    {
                        "name": "F. Vernizzi"
                    },
                    {
                        "name": "G. Verza"
                    },
                    {
                        "name": "P. Vielzeuf"
                    },
                    {
                        "name": "N. A. Walton"
                    }
                ],
                "author_detail": {
                    "name": "N. A. Walton"
                },
                "author": "N. A. Walton",
                "arxiv_affiliation": "Institute of Astronomy, University of Cambridge, Madingley Road, Cambridge CB3 0HA, UK",
                "arxiv_comment": "19 pages, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12104v1",
                "updated": "2025-07-16T10:20:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    20,
                    14,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T10:20:14Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    20,
                    14,
                    2,
                    197,
                    0
                ],
                "title": "From Static to Intelligent: Evolving SaaS Pricing with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Static to Intelligent: Evolving SaaS Pricing with LLMs"
                },
                "summary": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites."
                },
                "authors": [
                    {
                        "name": "Francisco Javier Cavero"
                    },
                    {
                        "name": "Juan C. Alonso"
                    },
                    {
                        "name": "Antonio Ruiz-Corts"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Ruiz-Corts"
                },
                "author": "Antonio Ruiz-Corts",
                "arxiv_comment": "12 pages. Accepted at the SOC4AI Workshop (Service-Oriented Computing\n  for AI Applications), held in conjunction with the 22nd International\n  Conference on Service-Oriented Computing (ICSOC 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12084v1",
                "updated": "2025-07-16T09:46:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    46,
                    58,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T09:46:58Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    46,
                    58,
                    2,
                    197,
                    0
                ],
                "title": "LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided\n  Seed Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided\n  Seed Generation"
                },
                "summary": "Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing\nremains an important approach to securing smart contracts. Even though mutation\nscheduling is a key factor influencing fuzzing effectiveness, existing fuzzers\nhave primarily explored seed scheduling and generation, while mutation\nscheduling has been rarely addressed by prior work. In this work, we propose a\nLarge Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing\nframework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and\nhybrid testing techniques. Key components of the proposed LLAMA include: (i) a\nhierarchical prompting strategy that guides LLMs to generate semantically valid\ninitial seeds, coupled with a lightweight pre-fuzzing phase to select\nhigh-potential inputs; (ii) a multi-feedback optimization mechanism that\nsimultaneously improves seed generation, seed selection, and mutation\nscheduling by leveraging runtime coverage and dependency feedback; and (iii) an\nevolutionary fuzzing engine that dynamically adjusts mutation operator\nprobabilities based on effectiveness, while incorporating symbolic execution to\nescape stagnation and uncover deeper vulnerabilities. Our experiments\ndemonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage\nand vulnerability detection. Specifically, it achieves 91% instruction coverage\nand 90% branch coverage, while detecting 132 out of 148 known vulnerabilities\nacross diverse categories. These results highlight LLAMA's effectiveness,\nadaptability, and practicality in real-world smart contract security testing\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing\nremains an important approach to securing smart contracts. Even though mutation\nscheduling is a key factor influencing fuzzing effectiveness, existing fuzzers\nhave primarily explored seed scheduling and generation, while mutation\nscheduling has been rarely addressed by prior work. In this work, we propose a\nLarge Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing\nframework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and\nhybrid testing techniques. Key components of the proposed LLAMA include: (i) a\nhierarchical prompting strategy that guides LLMs to generate semantically valid\ninitial seeds, coupled with a lightweight pre-fuzzing phase to select\nhigh-potential inputs; (ii) a multi-feedback optimization mechanism that\nsimultaneously improves seed generation, seed selection, and mutation\nscheduling by leveraging runtime coverage and dependency feedback; and (iii) an\nevolutionary fuzzing engine that dynamically adjusts mutation operator\nprobabilities based on effectiveness, while incorporating symbolic execution to\nescape stagnation and uncover deeper vulnerabilities. Our experiments\ndemonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage\nand vulnerability detection. Specifically, it achieves 91% instruction coverage\nand 90% branch coverage, while detecting 132 out of 148 known vulnerabilities\nacross diverse categories. These results highlight LLAMA's effectiveness,\nadaptability, and practicality in real-world smart contract security testing\nscenarios."
                },
                "authors": [
                    {
                        "name": "Keke Gai"
                    },
                    {
                        "name": "Haochen Liang"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Liehuang Zhu"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12079v1",
                "updated": "2025-07-16T09:39:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    39,
                    56,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T09:39:56Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    39,
                    56,
                    2,
                    197,
                    0
                ],
                "title": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method\n  for Active Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method\n  for Active Learning"
                },
                "summary": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems."
                },
                "authors": [
                    {
                        "name": "Tosin Adewumi"
                    },
                    {
                        "name": "Foteini Simistira Liwicki"
                    },
                    {
                        "name": "Marcus Liwicki"
                    },
                    {
                        "name": "Viktor Gardelli"
                    },
                    {
                        "name": "Lama Alkhaled"
                    },
                    {
                        "name": "Hamam Mokayed"
                    }
                ],
                "author_detail": {
                    "name": "Hamam Mokayed"
                },
                "author": "Hamam Mokayed",
                "arxiv_comment": "This paper was accepted for the special issue AI for Education by the\n  IEEE Signal Processing Magazine journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11278v2",
                "updated": "2025-07-16T09:32:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    32,
                    35,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-15T12:55:00Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    12,
                    55,
                    0,
                    1,
                    196,
                    0
                ],
                "title": "The fault in our sirens: Hierarchical diagnosis of waveform systematics\n  in Hubble-Lematre constant measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fault in our sirens: Hierarchical diagnosis of waveform systematics\n  in Hubble-Lematre constant measurements"
                },
                "summary": "Cosmological inference using a population of binary black-hole mergers,\ncombined with a galaxy catalog, presents an exciting opportunity for precision\ncosmology with the possibility of resolving the Hubble tension. However, the\naccuracy of these measurements heavily relies on the quality of the model used\nto infer the binary parameters, including the model of the gravitational-wave\nsignal. We use state-of-the-art waveform models to explore the impact of\ninaccurate modeling in measuring the Hubble-Lema\\^itre constant for the\nupcoming and future ground-based gravitational-wave observatories. We diagnose\nthe presence of inaccuracies within a hierarchical population-analysis\nframework, without a priori knowing the true value of the parameter, by\nassessing the consistency of the distribution of individual posteriors in\nrelation to their measurement errors. Our findings indicate that even a small\nhigh-mass, spin-precessing subpopulation -- comprising as little as 5\\% of the\npopulation generating the events observed by the LIGO-Virgo-KAGRA Collaboration\nso far -- can result in an unreliable measurement of the Hubble-Lema\\^itre\nconstant in the upcoming observing runs of these detectors, with even more\npronounced effects expected in future facilities on the ground.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological inference using a population of binary black-hole mergers,\ncombined with a galaxy catalog, presents an exciting opportunity for precision\ncosmology with the possibility of resolving the Hubble tension. However, the\naccuracy of these measurements heavily relies on the quality of the model used\nto infer the binary parameters, including the model of the gravitational-wave\nsignal. We use state-of-the-art waveform models to explore the impact of\ninaccurate modeling in measuring the Hubble-Lema\\^itre constant for the\nupcoming and future ground-based gravitational-wave observatories. We diagnose\nthe presence of inaccuracies within a hierarchical population-analysis\nframework, without a priori knowing the true value of the parameter, by\nassessing the consistency of the distribution of individual posteriors in\nrelation to their measurement errors. Our findings indicate that even a small\nhigh-mass, spin-precessing subpopulation -- comprising as little as 5\\% of the\npopulation generating the events observed by the LIGO-Virgo-KAGRA Collaboration\nso far -- can result in an unreliable measurement of the Hubble-Lema\\^itre\nconstant in the upcoming observing runs of these detectors, with even more\npronounced effects expected in future facilities on the ground."
                },
                "authors": [
                    {
                        "name": "Arnab Dhani"
                    },
                    {
                        "name": "Jonathan Gair"
                    },
                    {
                        "name": "Alessandra Buonanno"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Buonanno"
                },
                "author": "Alessandra Buonanno",
                "arxiv_comment": "fixed minor typo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12059v1",
                "updated": "2025-07-16T09:16:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    16,
                    36,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T09:16:36Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    16,
                    36,
                    2,
                    197,
                    0
                ],
                "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal\n  Directions, Revisited",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Ability of Large Language Models to Reason about Cardinal\n  Directions, Revisited"
                },
                "summary": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24."
                },
                "authors": [
                    {
                        "name": "Anthony G Cohn"
                    },
                    {
                        "name": "Robert E Blackwell"
                    }
                ],
                "author_detail": {
                    "name": "Robert E Blackwell"
                },
                "author": "Robert E Blackwell",
                "arxiv_comment": "8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12049v1",
                "updated": "2025-07-16T09:10:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    10,
                    38,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T09:10:38Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    10,
                    38,
                    2,
                    197,
                    0
                ],
                "title": "MoViAD: Modular Visual Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoViAD: Modular Visual Anomaly Detection"
                },
                "summary": "VAD is a critical field in machine learning focused on identifying deviations\nfrom normal patterns in images, often challenged by the scarcity of anomalous\ndata and the need for unsupervised training. To accelerate research and\ndeployment in this domain, we introduce MoViAD, a comprehensive and highly\nmodular library designed to provide fast and easy access to state-of-the-art\nVAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array\nof scenarios, including continual, semi-supervised, few-shots, noisy, and many\nmore. In addition, it addresses practical deployment challenges through\ndedicated Edge and IoT settings, offering optimized models and backbones, along\nwith quantization and compression utilities for efficient on-device execution\nand distributed inference. MoViAD integrates a selection of backbones, robust\nevaluation VAD metrics (pixel-level and image-level) and useful profiling tools\nfor efficiency analysis. The library is designed for fast, effortless\ndeployment, enabling machine learning engineers to easily use it for their\nspecific setup with custom models, datasets, and backbones. At the same time,\nit offers the flexibility and extensibility researchers need to develop and\nexperiment with new methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAD is a critical field in machine learning focused on identifying deviations\nfrom normal patterns in images, often challenged by the scarcity of anomalous\ndata and the need for unsupervised training. To accelerate research and\ndeployment in this domain, we introduce MoViAD, a comprehensive and highly\nmodular library designed to provide fast and easy access to state-of-the-art\nVAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array\nof scenarios, including continual, semi-supervised, few-shots, noisy, and many\nmore. In addition, it addresses practical deployment challenges through\ndedicated Edge and IoT settings, offering optimized models and backbones, along\nwith quantization and compression utilities for efficient on-device execution\nand distributed inference. MoViAD integrates a selection of backbones, robust\nevaluation VAD metrics (pixel-level and image-level) and useful profiling tools\nfor efficiency analysis. The library is designed for fast, effortless\ndeployment, enabling machine learning engineers to easily use it for their\nspecific setup with custom models, datasets, and backbones. At the same time,\nit offers the flexibility and extensibility researchers need to develop and\nexperiment with new methods."
                },
                "authors": [
                    {
                        "name": "Manuel Barusco"
                    },
                    {
                        "name": "Francesco Borsatti"
                    },
                    {
                        "name": "Arianna Stropeni"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    }
                ],
                "author_detail": {
                    "name": "Gian Antonio Susto"
                },
                "author": "Gian Antonio Susto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11167v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11167v3",
                "updated": "2025-07-16T08:58:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    8,
                    58,
                    13,
                    2,
                    197,
                    0
                ],
                "published": "2025-03-14T08:12:28Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    8,
                    12,
                    28,
                    4,
                    73,
                    0
                ],
                "title": "Neurons: Emulating the Human Visual Cortex Improves Fidelity and\n  Interpretability in fMRI-to-Video Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurons: Emulating the Human Visual Cortex Improves Fidelity and\n  Interpretability in fMRI-to-Video Reconstruction"
                },
                "summary": "Decoding visual stimuli from neural activity is essential for understanding\nthe human brain. While fMRI methods have successfully reconstructed static\nimages, fMRI-to-video reconstruction faces challenges due to the need for\ncapturing spatiotemporal dynamics like motion and scene transitions. Recent\napproaches have improved semantic and perceptual alignment but struggle to\nintegrate coarse fMRI data with detailed visual features. Inspired by the\nhierarchical organization of the visual system, we propose NEURONS, a novel\nframework that decouples learning into four correlated sub-tasks: key object\nsegmentation, concept recognition, scene description, and blurry video\nreconstruction. This approach simulates the visual cortex's functional\nspecialization, allowing the model to capture diverse video content. In the\ninference stage, NEURONS generates robust conditioning signals for a\npre-trained text-to-video diffusion model to reconstruct the videos. Extensive\nexperiments demonstrate that NEURONS outperforms state-of-the-art baselines,\nachieving solid improvements in video consistency (26.6%) and semantic-level\naccuracy (19.1%). Notably, NEURONS shows a strong functional correlation with\nthe visual cortex, highlighting its potential for brain-computer interfaces and\nclinical applications. Code and model weights are available at:\nhttps://github.com/xmed-lab/NEURONS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding visual stimuli from neural activity is essential for understanding\nthe human brain. While fMRI methods have successfully reconstructed static\nimages, fMRI-to-video reconstruction faces challenges due to the need for\ncapturing spatiotemporal dynamics like motion and scene transitions. Recent\napproaches have improved semantic and perceptual alignment but struggle to\nintegrate coarse fMRI data with detailed visual features. Inspired by the\nhierarchical organization of the visual system, we propose NEURONS, a novel\nframework that decouples learning into four correlated sub-tasks: key object\nsegmentation, concept recognition, scene description, and blurry video\nreconstruction. This approach simulates the visual cortex's functional\nspecialization, allowing the model to capture diverse video content. In the\ninference stage, NEURONS generates robust conditioning signals for a\npre-trained text-to-video diffusion model to reconstruct the videos. Extensive\nexperiments demonstrate that NEURONS outperforms state-of-the-art baselines,\nachieving solid improvements in video consistency (26.6%) and semantic-level\naccuracy (19.1%). Notably, NEURONS shows a strong functional correlation with\nthe visual cortex, highlighting its potential for brain-computer interfaces and\nclinical applications. Code and model weights are available at:\nhttps://github.com/xmed-lab/NEURONS."
                },
                "authors": [
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Qixiang Zhang"
                    },
                    {
                        "name": "Lehan Wang"
                    },
                    {
                        "name": "Xuanqi Huang"
                    },
                    {
                        "name": "Xiaomeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaomeng Li"
                },
                "author": "Xiaomeng Li",
                "arxiv_comment": "Accepted by ICCV 2025, camera ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11167v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11167v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12039v2",
                "updated": "2025-07-17T15:27:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    27,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T08:56:19Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    8,
                    56,
                    19,
                    2,
                    197,
                    0
                ],
                "title": "A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans"
                },
                "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity."
                },
                "authors": [
                    {
                        "name": "Anca Dinu"
                    },
                    {
                        "name": "Andra-Maria Florescu"
                    },
                    {
                        "name": "Alina Resceanu"
                    }
                ],
                "author_detail": {
                    "name": "Alina Resceanu"
                },
                "author": "Alina Resceanu",
                "arxiv_comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10024v2",
                "updated": "2025-07-16T08:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    8,
                    53,
                    53,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-14T08:06:12Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    6,
                    12,
                    0,
                    195,
                    0
                ],
                "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles"
                },
                "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research."
                },
                "authors": [
                    {
                        "name": "Shaolun Ruan"
                    },
                    {
                        "name": "Rui Sheng"
                    },
                    {
                        "name": "Xiaolin Wen"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Tim Dwyer"
                    },
                    {
                        "name": "Jiannan Li"
                    }
                ],
                "author_detail": {
                    "name": "Jiannan Li"
                },
                "author": "Jiannan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.19982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19982v2",
                "updated": "2025-07-16T17:52:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    52,
                    44,
                    2,
                    197,
                    0
                ],
                "published": "2025-04-28T16:57:17Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    57,
                    17,
                    0,
                    118,
                    0
                ],
                "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining\n  Turn-Level Precision with Dialogue-Level Comparisons"
                },
                "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research."
                },
                "authors": [
                    {
                        "name": "Emre Can Acikgoz"
                    },
                    {
                        "name": "Carl Guo"
                    },
                    {
                        "name": "Suvodip Dey"
                    },
                    {
                        "name": "Akul Datta"
                    },
                    {
                        "name": "Takyoung Kim"
                    },
                    {
                        "name": "Gokhan Tur"
                    },
                    {
                        "name": "Dilek Hakkani-Tr"
                    }
                ],
                "author_detail": {
                    "name": "Dilek Hakkani-Tr"
                },
                "author": "Dilek Hakkani-Tr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12448v1",
                "updated": "2025-07-16T17:40:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    40,
                    58,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:40:58Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    40,
                    58,
                    2,
                    197,
                    0
                ],
                "title": "Jenga-Krotov algorithm: Efficient compilation of multi-qubit gates for\n  exchange-only qubits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga-Krotov algorithm: Efficient compilation of multi-qubit gates for\n  exchange-only qubits"
                },
                "summary": "Exchange-only (EO) qubits, implemented in triple-quantum-dot systems, offer a\ncompelling platform for scalable semiconductor-based quantum computing by\nenabling universal control through purely exchange interactions. While\nhigh-fidelity single- and two-qubit gates have been demonstrated, the synthesis\nof efficient multi-qubit operations -- such as the Toffoli gate -- remains a\nkey bottleneck. Conventional gate decompositions into elementary operations\nlead to prohibitively long and error-prone pulse sequences, limiting practical\ndeployment. In this work, we introduce a gradient-based optimization algorithm,\nJenga-Krotov (JK), tailored to discover compact, high-fidelity EO gate\nsequences. Applying JK to the Toffoli gate, we reduce the number of required\nexchange unitaries from 216 (in standard decomposition) to 92, and compress the\ntime steps required from 162 to 50, all while maintaining target fidelity.\nUnder realistic noise, the accumulated gate error from our optimized sequence\nis an order of magnitude lower than that of conventional approaches. These\nresults demonstrate that the JK algorithm is a general and scalable strategy\nfor multi-qubit gate synthesis in EO architectures, potentially facilitating\nrealization of multi-qubit algorithms on semiconductor platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exchange-only (EO) qubits, implemented in triple-quantum-dot systems, offer a\ncompelling platform for scalable semiconductor-based quantum computing by\nenabling universal control through purely exchange interactions. While\nhigh-fidelity single- and two-qubit gates have been demonstrated, the synthesis\nof efficient multi-qubit operations -- such as the Toffoli gate -- remains a\nkey bottleneck. Conventional gate decompositions into elementary operations\nlead to prohibitively long and error-prone pulse sequences, limiting practical\ndeployment. In this work, we introduce a gradient-based optimization algorithm,\nJenga-Krotov (JK), tailored to discover compact, high-fidelity EO gate\nsequences. Applying JK to the Toffoli gate, we reduce the number of required\nexchange unitaries from 216 (in standard decomposition) to 92, and compress the\ntime steps required from 162 to 50, all while maintaining target fidelity.\nUnder realistic noise, the accumulated gate error from our optimized sequence\nis an order of magnitude lower than that of conventional approaches. These\nresults demonstrate that the JK algorithm is a general and scalable strategy\nfor multi-qubit gate synthesis in EO architectures, potentially facilitating\nrealization of multi-qubit algorithms on semiconductor platforms."
                },
                "authors": [
                    {
                        "name": "Jiahao Wu"
                    },
                    {
                        "name": "Guanjie He"
                    },
                    {
                        "name": "Wenyuan Zhuo"
                    },
                    {
                        "name": "Quan Fu"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "15+11 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12443v1",
                "updated": "2025-07-16T17:29:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    29,
                    15,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:29:15Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    29,
                    15,
                    2,
                    197,
                    0
                ],
                "title": "LLM-Based Config Synthesis requires Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Config Synthesis requires Disambiguation"
                },
                "summary": "Beyond hallucinations, another problem in program synthesis using LLMs is\nambiguity in user intent. We illustrate the ambiguity problem in a networking\ncontext for LLM-based incremental configuration synthesis of route-maps and\nACLs. These structures frequently overlap in header space, making the relative\npriority of actions impossible for the LLM to infer without user interaction.\nMeasurements in a large cloud identify complex ACLs with 100's of overlaps,\nshowing ambiguity is a real problem. We propose a prototype system, Clarify,\nwhich uses an LLM augmented with a new module called a Disambiguator that helps\nelicit user intent. On a small synthetic workload, Clarify incrementally\nsynthesizes routing policies after disambiguation and then verifies them. Our\ntreatment of ambiguities is useful more generally when the intent of updates\ncan be correctly synthesized by LLMs, but their integration is ambiguous and\ncan lead to different global behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond hallucinations, another problem in program synthesis using LLMs is\nambiguity in user intent. We illustrate the ambiguity problem in a networking\ncontext for LLM-based incremental configuration synthesis of route-maps and\nACLs. These structures frequently overlap in header space, making the relative\npriority of actions impossible for the LLM to infer without user interaction.\nMeasurements in a large cloud identify complex ACLs with 100's of overlaps,\nshowing ambiguity is a real problem. We propose a prototype system, Clarify,\nwhich uses an LLM augmented with a new module called a Disambiguator that helps\nelicit user intent. On a small synthetic workload, Clarify incrementally\nsynthesizes routing policies after disambiguation and then verifies them. Our\ntreatment of ambiguities is useful more generally when the intent of updates\ncan be correctly synthesized by LLMs, but their integration is ambiguous and\ncan lead to different global behaviors."
                },
                "authors": [
                    {
                        "name": "Rajdeep Mondal"
                    },
                    {
                        "name": "Nikolaj Bjorner"
                    },
                    {
                        "name": "Todd Millstein"
                    },
                    {
                        "name": "Alan Tang"
                    },
                    {
                        "name": "George Varghese"
                    }
                ],
                "author_detail": {
                    "name": "George Varghese"
                },
                "author": "George Varghese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09850v3",
                "updated": "2025-07-16T17:16:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    16,
                    18,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-14T01:14:50Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    1,
                    14,
                    50,
                    0,
                    195,
                    0
                ],
                "title": "The Challenge of Teaching Reasoning to LLMs Without RL or Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Challenge of Teaching Reasoning to LLMs Without RL or Distillation"
                },
                "summary": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective."
                },
                "authors": [
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Branislav Kisacanin"
                    },
                    {
                        "name": "George Armstrong"
                    },
                    {
                        "name": "Shubham Toshniwal"
                    },
                    {
                        "name": "Ivan Moshkov"
                    },
                    {
                        "name": "Alexan Ayrapetyan"
                    },
                    {
                        "name": "Sadegh Mahdavi"
                    },
                    {
                        "name": "Dan Zhao"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Dragan Masulovic"
                    },
                    {
                        "name": "Marius Stanean"
                    },
                    {
                        "name": "Advaith Avadhanam"
                    },
                    {
                        "name": "Max Wang"
                    },
                    {
                        "name": "Ashmit Dutta"
                    },
                    {
                        "name": "Shitij Govil"
                    },
                    {
                        "name": "Sri Yanamandara"
                    },
                    {
                        "name": "Mihir Tandon"
                    },
                    {
                        "name": "Sriram Ananthakrishnan"
                    },
                    {
                        "name": "Vedant Rathi"
                    },
                    {
                        "name": "David Zhang"
                    },
                    {
                        "name": "Joonseok Kang"
                    },
                    {
                        "name": "Leon Luo"
                    },
                    {
                        "name": "Titu Andreescu"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Igor Gitman"
                    }
                ],
                "author_detail": {
                    "name": "Igor Gitman"
                },
                "author": "Igor Gitman",
                "arxiv_comment": "Accepted at the Second AI for Math Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12426v1",
                "updated": "2025-07-16T17:15:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    15,
                    6,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:15:06Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    15,
                    6,
                    2,
                    197,
                    0
                ],
                "title": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for\n  Spatio-Temporal Action Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for\n  Spatio-Temporal Action Recognition"
                },
                "summary": "The landscape of video recognition has evolved significantly, shifting from\ntraditional Convolutional Neural Networks (CNNs) to Transformer-based\narchitectures for improved accuracy. While 3D CNNs have been effective at\ncapturing spatiotemporal dynamics, recent Transformer models leverage\nself-attention to model long-range spatial and temporal dependencies. Despite\nachieving state-of-the-art performance on major benchmarks, Transformers remain\ncomputationally expensive, particularly with dense video data. To address this,\nwe propose a lightweight Video Focal Modulation Network, DVFL-Net, which\ndistills spatiotemporal knowledge from a large pre-trained teacher into a\ncompact nano student model, enabling efficient on-device deployment. DVFL-Net\nutilizes knowledge distillation and spatial-temporal feature modulation to\nsignificantly reduce computation while preserving high recognition performance.\nWe employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal\nfocal modulation to effectively transfer both local and global context from the\nVideo-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate\nDVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it\nagainst recent state-of-the-art methods in Human Action Recognition (HAR).\nAdditionally, we conduct a detailed ablation study analyzing the impact of\nforward KL divergence. The results confirm the superiority of DVFL-Net in\nachieving an optimal balance between performance and efficiency, demonstrating\nlower memory usage, reduced GFLOPs, and strong accuracy, making it a practical\nsolution for real-time HAR applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The landscape of video recognition has evolved significantly, shifting from\ntraditional Convolutional Neural Networks (CNNs) to Transformer-based\narchitectures for improved accuracy. While 3D CNNs have been effective at\ncapturing spatiotemporal dynamics, recent Transformer models leverage\nself-attention to model long-range spatial and temporal dependencies. Despite\nachieving state-of-the-art performance on major benchmarks, Transformers remain\ncomputationally expensive, particularly with dense video data. To address this,\nwe propose a lightweight Video Focal Modulation Network, DVFL-Net, which\ndistills spatiotemporal knowledge from a large pre-trained teacher into a\ncompact nano student model, enabling efficient on-device deployment. DVFL-Net\nutilizes knowledge distillation and spatial-temporal feature modulation to\nsignificantly reduce computation while preserving high recognition performance.\nWe employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal\nfocal modulation to effectively transfer both local and global context from the\nVideo-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate\nDVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it\nagainst recent state-of-the-art methods in Human Action Recognition (HAR).\nAdditionally, we conduct a detailed ablation study analyzing the impact of\nforward KL divergence. The results confirm the superiority of DVFL-Net in\nachieving an optimal balance between performance and efficiency, demonstrating\nlower memory usage, reduced GFLOPs, and strong accuracy, making it a practical\nsolution for real-time HAR applications."
                },
                "authors": [
                    {
                        "name": "Hayat Ullah"
                    },
                    {
                        "name": "Muhammad Ali Shafique"
                    },
                    {
                        "name": "Abbas Khan"
                    },
                    {
                        "name": "Arslan Munir"
                    }
                ],
                "author_detail": {
                    "name": "Arslan Munir"
                },
                "author": "Arslan Munir",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12425v1",
                "updated": "2025-07-16T17:13:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    13,
                    6,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:13:06Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    13,
                    6,
                    2,
                    197,
                    0
                ],
                "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Retrieval-Augmented Generation for Structured Enterprise and\n  Internal Data"
                },
                "summary": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot"
                },
                "authors": [
                    {
                        "name": "Chandana Cheerla"
                    }
                ],
                "author_detail": {
                    "name": "Chandana Cheerla"
                },
                "author": "Chandana Cheerla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12415v1",
                "updated": "2025-07-16T17:05:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    5,
                    17,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T17:05:17Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    5,
                    17,
                    2,
                    197,
                    0
                ],
                "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?"
                },
                "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field."
                },
                "authors": [
                    {
                        "name": "Xinyi He"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Mingzhe Du"
                    },
                    {
                        "name": "Lin Yan"
                    },
                    {
                        "name": "Zhijie Fan"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zejian Yuan"
                    },
                    {
                        "name": "Zejun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zejun Ma"
                },
                "author": "Zejun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08218v2",
                "updated": "2025-07-16T16:57:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    57,
                    48,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-10T23:47:05Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    23,
                    47,
                    5,
                    3,
                    191,
                    0
                ],
                "title": "Simple Mechanistic Explanations for Out-Of-Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Mechanistic Explanations for Out-Of-Context Reasoning"
                },
                "summary": "Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment."
                },
                "authors": [
                    {
                        "name": "Atticus Wang"
                    },
                    {
                        "name": "Joshua Engels"
                    },
                    {
                        "name": "Oliver Clive-Griffin"
                    },
                    {
                        "name": "Senthooran Rajamanoharan"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "ICML 2025 Workshop R2-FM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23175v2",
                "updated": "2025-07-16T16:49:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    49,
                    21,
                    2,
                    197,
                    0
                ],
                "published": "2025-03-29T18:09:36Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    18,
                    9,
                    36,
                    5,
                    88,
                    0
                ],
                "title": "Large Language Models are Unreliable for Cyber Threat Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Unreliable for Cyber Threat Intelligence"
                },
                "summary": "Several recent works have argued that Large Language Models (LLMs) can be\nused to tame the data deluge in the cybersecurity field, by improving the\nautomation of Cyber Threat Intelligence (CTI) tasks. This work presents an\nevaluation methodology that other than allowing to test LLMs on CTI tasks when\nusing zero-shot learning, few-shot learning and fine-tuning, also allows to\nquantify their consistency and their confidence level. We run experiments with\nthree state-of-the-art LLMs and a dataset of 350 threat intelligence reports\nand present new evidence of potential security risks in relying on LLMs for\nCTI. We show how LLMs cannot guarantee sufficient performance on real-size\nreports while also being inconsistent and overconfident. Few-shot learning and\nfine-tuning only partially improve the results, thus posing doubts about the\npossibility of using LLMs for CTI scenarios, where labelled datasets are\nlacking and where confidence is a fundamental factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several recent works have argued that Large Language Models (LLMs) can be\nused to tame the data deluge in the cybersecurity field, by improving the\nautomation of Cyber Threat Intelligence (CTI) tasks. This work presents an\nevaluation methodology that other than allowing to test LLMs on CTI tasks when\nusing zero-shot learning, few-shot learning and fine-tuning, also allows to\nquantify their consistency and their confidence level. We run experiments with\nthree state-of-the-art LLMs and a dataset of 350 threat intelligence reports\nand present new evidence of potential security risks in relying on LLMs for\nCTI. We show how LLMs cannot guarantee sufficient performance on real-size\nreports while also being inconsistent and overconfident. Few-shot learning and\nfine-tuning only partially improve the results, thus posing doubts about the\npossibility of using LLMs for CTI scenarios, where labelled datasets are\nlacking and where confidence is a fundamental factor."
                },
                "authors": [
                    {
                        "name": "Emanuele Mezzi"
                    },
                    {
                        "name": "Fabio Massacci"
                    },
                    {
                        "name": "Katja Tuma"
                    }
                ],
                "author_detail": {
                    "name": "Katja Tuma"
                },
                "author": "Katja Tuma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12391v1",
                "updated": "2025-07-16T16:37:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    37,
                    13,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:37:13Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    37,
                    13,
                    2,
                    197,
                    0
                ],
                "title": "Assessing the Value of Visual Input: A Benchmark of Multimodal Large\n  Language Models for Robotic Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Value of Visual Input: A Benchmark of Multimodal Large\n  Language Models for Robotic Path Planning"
                },
                "summary": "Large Language Models (LLMs) show potential for enhancing robotic path\nplanning. This paper assesses visual input's utility for multimodal LLMs in\nsuch tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on\ngenerating valid and optimal paths in 2D grid environments, simulating\nsimplified robotic planning, comparing text-only versus text-plus-visual inputs\nacross varying model sizes and grid complexities. Our results indicate moderate\nsuccess rates on simpler small grids, where visual input or few-shot text\nprompting offered some benefits. However, performance significantly degraded on\nlarger grids, highlighting a scalability challenge. While larger models\ngenerally achieved higher average success, the visual modality was not\nuniversally dominant over well-structured text for these multimodal systems,\nand successful paths on simpler grids were generally of high quality. These\nresults indicate current limitations in robust spatial reasoning, constraint\nadherence, and scalable multimodal integration, identifying areas for future\nLLM development in robotic path planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show potential for enhancing robotic path\nplanning. This paper assesses visual input's utility for multimodal LLMs in\nsuch tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on\ngenerating valid and optimal paths in 2D grid environments, simulating\nsimplified robotic planning, comparing text-only versus text-plus-visual inputs\nacross varying model sizes and grid complexities. Our results indicate moderate\nsuccess rates on simpler small grids, where visual input or few-shot text\nprompting offered some benefits. However, performance significantly degraded on\nlarger grids, highlighting a scalability challenge. While larger models\ngenerally achieved higher average success, the visual modality was not\nuniversally dominant over well-structured text for these multimodal systems,\nand successful paths on simpler grids were generally of high quality. These\nresults indicate current limitations in robust spatial reasoning, constraint\nadherence, and scalable multimodal integration, identifying areas for future\nLLM development in robotic path planning."
                },
                "authors": [
                    {
                        "name": "Jacinto Colan"
                    },
                    {
                        "name": "Ana Davila"
                    },
                    {
                        "name": "Yasuhisa Hasegawa"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhisa Hasegawa"
                },
                "author": "Yasuhisa Hasegawa",
                "arxiv_comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)",
                "arxiv_journal_ref": "2025 SICE Festival with Annual Conference (SICE FES)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12387v1",
                "updated": "2025-07-16T16:33:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    33,
                    44,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:33:44Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    33,
                    44,
                    2,
                    197,
                    0
                ],
                "title": "Towards dislocation-driven quantum interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards dislocation-driven quantum interconnects"
                },
                "summary": "A central problem in the deployment of quantum technologies is the\nrealization of robust architectures for quantum interconnects. We propose to\nengineer interconnects in semiconductors and insulators by patterning spin\nqubits at dislocations, thus forming quasi one-dimensional lines of entangled\npoint defects. To gain insight into the feasibility and control of\ndislocation-driven interconnects, we investigate the optical cycle and\ncoherence properties of nitrogen-vacancy (NV) centers in diamond, in proximity\nof dislocations, using a combination of advanced first-principles calculations.\nWe show that one can engineer spin defects with properties similar to those of\ntheir bulk counterparts, including charge stability and a favorable optical\ncycle, and that NV centers close to dislocations have much improved coherence\nproperties. Finally, we predict optically detected magnetic resonance spectra\nthat may facilitate the experimental identification of specific defect\nconfigurations. Our results provide a theoretical foundation for the\nengineering of one-dimensional arrays of spin defects in the solid state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central problem in the deployment of quantum technologies is the\nrealization of robust architectures for quantum interconnects. We propose to\nengineer interconnects in semiconductors and insulators by patterning spin\nqubits at dislocations, thus forming quasi one-dimensional lines of entangled\npoint defects. To gain insight into the feasibility and control of\ndislocation-driven interconnects, we investigate the optical cycle and\ncoherence properties of nitrogen-vacancy (NV) centers in diamond, in proximity\nof dislocations, using a combination of advanced first-principles calculations.\nWe show that one can engineer spin defects with properties similar to those of\ntheir bulk counterparts, including charge stability and a favorable optical\ncycle, and that NV centers close to dislocations have much improved coherence\nproperties. Finally, we predict optically detected magnetic resonance spectra\nthat may facilitate the experimental identification of specific defect\nconfigurations. Our results provide a theoretical foundation for the\nengineering of one-dimensional arrays of spin defects in the solid state."
                },
                "authors": [
                    {
                        "name": "Cunzhi Zhang"
                    },
                    {
                        "name": "Victor Wen-zhe Yu"
                    },
                    {
                        "name": "Yu Jin"
                    },
                    {
                        "name": "Jonah Nagura"
                    },
                    {
                        "name": "Sevim Polat Genlik"
                    },
                    {
                        "name": "Maryam Ghazisaeidi"
                    },
                    {
                        "name": "Giulia Galli"
                    }
                ],
                "author_detail": {
                    "name": "Giulia Galli"
                },
                "author": "Giulia Galli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12378v1",
                "updated": "2025-07-16T16:27:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    27,
                    5,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:27:05Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    27,
                    5,
                    2,
                    197,
                    0
                ],
                "title": "Developing Visual Augmented Q&A System using Scalable Vision Embedding\n  Retrieval & Late Interaction Re-ranker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing Visual Augmented Q&A System using Scalable Vision Embedding\n  Retrieval & Late Interaction Re-ranker"
                },
                "summary": "Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises."
                },
                "authors": [
                    {
                        "name": "Rachna Saxena"
                    },
                    {
                        "name": "Abhijeet Kumar"
                    },
                    {
                        "name": "Suresh Shanmugam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Shanmugam"
                },
                "author": "Suresh Shanmugam",
                "arxiv_comment": "Presented at NLP@IR workshop at SIGIR conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12372v1",
                "updated": "2025-07-16T16:21:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    21,
                    1,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:21:01Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    21,
                    1,
                    2,
                    197,
                    0
                ],
                "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User\n  Demographics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User\n  Demographics"
                },
                "summary": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes."
                },
                "authors": [
                    {
                        "name": "Meysam Alizadeh"
                    },
                    {
                        "name": "Fabrizio Gilardi"
                    },
                    {
                        "name": "Zeynab Samei"
                    },
                    {
                        "name": "Mohsen Mosleh"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Mosleh"
                },
                "author": "Mohsen Mosleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12370v1",
                "updated": "2025-07-16T16:15:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    15,
                    25,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:15:25Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    15,
                    25,
                    2,
                    197,
                    0
                ],
                "title": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests\n  through Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests\n  through Debate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant capabilities in\nunderstanding and generating human language, contributing to more natural\ninteractions with complex systems. However, they face challenges such as\nambiguity in user requests processed by LLMs. To address these challenges, this\npaper introduces and evaluates a multi-agent debate framework designed to\nenhance detection and resolution capabilities beyond single models. The\nframework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and\nMistral-7B variants) and a dataset with diverse ambiguities. The debate\nframework markedly enhanced the performance of Llama3-8B and Mistral-7B\nvariants over their individual baselines, with Mistral-7B-led debates achieving\na notable 76.7% success rate and proving particularly effective for complex\nambiguities and efficient consensus. While acknowledging varying model\nresponses to collaborative strategies, these findings underscore the debate\nframework's value as a targeted method for augmenting LLM capabilities. This\nwork offers important insights for developing more robust and adaptive language\nunderstanding systems by showing how structured debates can lead to improved\nclarity in interactive systems."
                },
                "authors": [
                    {
                        "name": "Ana Davila"
                    },
                    {
                        "name": "Jacinto Colan"
                    },
                    {
                        "name": "Yasuhisa Hasegawa"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhisa Hasegawa"
                },
                "author": "Yasuhisa Hasegawa",
                "arxiv_comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)",
                "arxiv_journal_ref": "2025 SICE Festival with Annual Conference (SICE FES)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12367v1",
                "updated": "2025-07-16T16:10:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    10,
                    42,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T16:10:42Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    16,
                    10,
                    42,
                    2,
                    197,
                    0
                ],
                "title": "GitChameleon: Evaluating AI Code Generation Against Python Library\n  Version Incompatibilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GitChameleon: Evaluating AI Code Generation Against Python Library\n  Version Incompatibilities"
                },
                "summary": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark."
                },
                "authors": [
                    {
                        "name": "Diganta Misra"
                    },
                    {
                        "name": "Nizar Islah"
                    },
                    {
                        "name": "Victor May"
                    },
                    {
                        "name": "Brice Rauby"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Justine Gehring"
                    },
                    {
                        "name": "Antonio Orvieto"
                    },
                    {
                        "name": "Muawiz Chaudhary"
                    },
                    {
                        "name": "Eilif B. Muller"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Samira Ebrahimi Kahou"
                    },
                    {
                        "name": "Massimo Caccia"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Caccia"
                },
                "author": "Massimo Caccia",
                "arxiv_comment": "Version 2 of the dataset from: arXiv:2411.05830",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23603v2",
                "updated": "2025-07-16T15:51:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    51,
                    30,
                    2,
                    197,
                    0
                ],
                "published": "2025-06-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    8,
                    8,
                    15,
                    0,
                    181,
                    0
                ],
                "title": "SoK: Semantic Privacy in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Semantic Privacy in Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs."
                },
                "authors": [
                    {
                        "name": "Baihe Ma"
                    },
                    {
                        "name": "Yanna Jiang"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Guangsheng Yu"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Caijun Sun"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Xuelei Qi"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Ren Ping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ren Ping Liu"
                },
                "author": "Ren Ping Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09477v2",
                "updated": "2025-07-16T15:44:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    44,
                    18,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-13T03:29:41Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    3,
                    29,
                    41,
                    6,
                    194,
                    0
                ],
                "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs"
                },
                "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning."
                },
                "authors": [
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yuyao Yang"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Yaozu Wu"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Yusheng Zhao"
                    },
                    {
                        "name": "Chunkit Chan"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Zhongfen Deng"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Dongyuan Li"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "submitted to ARR May",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12344v1",
                "updated": "2025-07-16T15:38:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    38,
                    7,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T15:38:07Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    38,
                    7,
                    2,
                    197,
                    0
                ],
                "title": "Improving Lightweight Weed Detection via Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Lightweight Weed Detection via Knowledge Distillation"
                },
                "summary": "Weed detection is a critical component of precision agriculture, facilitating\ntargeted herbicide application and reducing environmental impact. However,\ndeploying accurate object detection models on resource-limited platforms\nremains challenging, particularly when differentiating visually similar weed\nspecies commonly encountered in plant phenotyping applications. In this work,\nwe investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative\nDistillation (MGD) to enhance the performance of lightweight models for\nreal-time smart spraying systems. Utilizing YOLO11x as the teacher model and\nYOLO11n as both reference and student, both CWD and MGD effectively transfer\nknowledge from the teacher to the student model. Our experiments, conducted on\na real-world dataset comprising sugar beet crops and four weed types (Cirsium,\nConvolvulus, Fallopia, and Echinochloa), consistently show increased AP50\nacross all classes. The distilled CWD student model achieves a notable\nimprovement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without\nincreasing model complexity. Additionally, we validate real-time deployment\nfeasibility by evaluating the student YOLO11n model on Jetson Orin Nano and\nRaspberry Pi 5 embedded devices, performing five independent runs to evaluate\nperformance stability across random seeds. These findings confirm CWD and MGD\nas an effective, efficient, and practical approach for improving deep\nlearning-based weed detection accuracy in precision agriculture and plant\nphenotyping scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weed detection is a critical component of precision agriculture, facilitating\ntargeted herbicide application and reducing environmental impact. However,\ndeploying accurate object detection models on resource-limited platforms\nremains challenging, particularly when differentiating visually similar weed\nspecies commonly encountered in plant phenotyping applications. In this work,\nwe investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative\nDistillation (MGD) to enhance the performance of lightweight models for\nreal-time smart spraying systems. Utilizing YOLO11x as the teacher model and\nYOLO11n as both reference and student, both CWD and MGD effectively transfer\nknowledge from the teacher to the student model. Our experiments, conducted on\na real-world dataset comprising sugar beet crops and four weed types (Cirsium,\nConvolvulus, Fallopia, and Echinochloa), consistently show increased AP50\nacross all classes. The distilled CWD student model achieves a notable\nimprovement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without\nincreasing model complexity. Additionally, we validate real-time deployment\nfeasibility by evaluating the student YOLO11n model on Jetson Orin Nano and\nRaspberry Pi 5 embedded devices, performing five independent runs to evaluate\nperformance stability across random seeds. These findings confirm CWD and MGD\nas an effective, efficient, and practical approach for improving deep\nlearning-based weed detection accuracy in precision agriculture and plant\nphenotyping scenarios."
                },
                "authors": [
                    {
                        "name": "Ahmet Ouz Saltk"
                    },
                    {
                        "name": "Max Voigt"
                    },
                    {
                        "name": "Sourav Modak"
                    },
                    {
                        "name": "Mike Beckworth"
                    },
                    {
                        "name": "Anthony Stein"
                    }
                ],
                "author_detail": {
                    "name": "Anthony Stein"
                },
                "author": "Anthony Stein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10644v2",
                "updated": "2025-07-16T15:30:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    30,
                    42,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-14T16:47:19Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    16,
                    47,
                    19,
                    0,
                    195,
                    0
                ],
                "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web\n  of Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web\n  of Agents"
                },
                "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA."
                },
                "authors": [
                    {
                        "name": "Tatiana Petrova"
                    },
                    {
                        "name": "Boris Bliznioukov"
                    },
                    {
                        "name": "Aleksandr Puzikov"
                    },
                    {
                        "name": "Radu State"
                    }
                ],
                "author_detail": {
                    "name": "Radu State"
                },
                "arxiv_affiliation": "SEDAN SnT, University of Luxembourg, Luxembourg, Luxembourg",
                "author": "Radu State",
                "arxiv_comment": "33 pages, 9 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10628v2",
                "updated": "2025-07-16T15:30:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    30,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-14T08:10:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    10,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement\n  Learning"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for facilitating the self-improvement of large language\nmodels (LLMs), particularly in the domain of complex reasoning tasks. However,\nprevailing on-policy RL methods often contend with significant training\ninstability and inefficiency. This is primarily due to a capacity-difficulty\nmismatch, where the complexity of training data frequently outpaces the model's\ncurrent capabilities, leading to critically sparse reward signals and stalled\nlearning progress. This challenge is particularly acute for smaller, more\nresource-efficient LLMs. To overcome this, we introduce the Guided Hybrid\nPolicy Optimization (GHPO), a novel difficulty-aware reinforcement learning\nframework. GHPO dynamically calibrates task difficulty by employing adaptive\nprompt refinement to provide targeted guidance. This unique approach adaptively\nbalances direct imitation learning for problems currently beyond the model's\nreach with exploration-based reinforcement learning for more manageable tasks,\neffectively creating a smooth and optimized learning curriculum. Extensive\nexperiments demonstrate that GHPO achieves an average performance gain of\napproximately 5% across six challenging mathematics benchmarks, consistently\noutperforming strong on-policy reinforcement learning and curriculum learning\nbaselines. Further analysis confirms that our framework significantly enhances\nboth training stability and final reasoning performance, thus offering a\nscalable and efficient solution for developing powerful and robust reasoning\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for facilitating the self-improvement of large language\nmodels (LLMs), particularly in the domain of complex reasoning tasks. However,\nprevailing on-policy RL methods often contend with significant training\ninstability and inefficiency. This is primarily due to a capacity-difficulty\nmismatch, where the complexity of training data frequently outpaces the model's\ncurrent capabilities, leading to critically sparse reward signals and stalled\nlearning progress. This challenge is particularly acute for smaller, more\nresource-efficient LLMs. To overcome this, we introduce the Guided Hybrid\nPolicy Optimization (GHPO), a novel difficulty-aware reinforcement learning\nframework. GHPO dynamically calibrates task difficulty by employing adaptive\nprompt refinement to provide targeted guidance. This unique approach adaptively\nbalances direct imitation learning for problems currently beyond the model's\nreach with exploration-based reinforcement learning for more manageable tasks,\neffectively creating a smooth and optimized learning curriculum. Extensive\nexperiments demonstrate that GHPO achieves an average performance gain of\napproximately 5% across six challenging mathematics benchmarks, consistently\noutperforming strong on-policy reinforcement learning and curriculum learning\nbaselines. Further analysis confirms that our framework significantly enhances\nboth training stability and final reasoning performance, thus offering a\nscalable and efficient solution for developing powerful and robust reasoning\nmodels."
                },
                "authors": [
                    {
                        "name": "Ziru Liu"
                    },
                    {
                        "name": "Cheng Gong"
                    },
                    {
                        "name": "Xinyu Fu"
                    },
                    {
                        "name": "Yaofang Liu"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Shoubo Hu"
                    },
                    {
                        "name": "Suiyun Zhang"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Dandan Tu"
                    }
                ],
                "author_detail": {
                    "name": "Dandan Tu"
                },
                "author": "Dandan Tu",
                "arxiv_comment": "Code avaiable at https://github.com/hkgc-1/GHPO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00265v2",
                "updated": "2025-07-16T15:16:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    16,
                    27,
                    2,
                    197,
                    0
                ],
                "published": "2024-10-31T23:54:21Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    23,
                    54,
                    21,
                    3,
                    305,
                    0
                ],
                "title": "Quantifying calibration error in modern neural networks through evidence\n  based theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying calibration error in modern neural networks through evidence\n  based theory"
                },
                "summary": "Trustworthiness in neural networks is crucial for their deployment in\ncritical applications, where reliability, confidence, and uncertainty play\npivotal roles in decision-making. Traditional performance metrics such as\naccuracy and precision fail to capture these aspects, particularly in cases\nwhere models exhibit overconfidence. To address these limitations, this paper\nintroduces a novel framework for quantifying the trustworthiness of neural\nnetworks by incorporating subjective logic into the evaluation of Expected\nCalibration Error (ECE). This method provides a comprehensive measure of trust,\ndisbelief, and uncertainty by clustering predicted probabilities and fusing\nopinions using appropriate fusion operators. We demonstrate the effectiveness\nof this approach through experiments on MNIST and CIFAR-10 datasets, where\npost-calibration results indicate improved trustworthiness. The proposed\nframework offers a more interpretable and nuanced assessment of AI models, with\npotential applications in sensitive domains such as healthcare and autonomous\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness in neural networks is crucial for their deployment in\ncritical applications, where reliability, confidence, and uncertainty play\npivotal roles in decision-making. Traditional performance metrics such as\naccuracy and precision fail to capture these aspects, particularly in cases\nwhere models exhibit overconfidence. To address these limitations, this paper\nintroduces a novel framework for quantifying the trustworthiness of neural\nnetworks by incorporating subjective logic into the evaluation of Expected\nCalibration Error (ECE). This method provides a comprehensive measure of trust,\ndisbelief, and uncertainty by clustering predicted probabilities and fusing\nopinions using appropriate fusion operators. We demonstrate the effectiveness\nof this approach through experiments on MNIST and CIFAR-10 datasets, where\npost-calibration results indicate improved trustworthiness. The proposed\nframework offers a more interpretable and nuanced assessment of AI models, with\npotential applications in sensitive domains such as healthcare and autonomous\nsystems."
                },
                "authors": [
                    {
                        "name": "Koffi Ismael Ouattara"
                    }
                ],
                "author_detail": {
                    "name": "Koffi Ismael Ouattara"
                },
                "author": "Koffi Ismael Ouattara",
                "arxiv_comment": "Accepted at FUSION 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04602v2",
                "updated": "2025-07-16T15:12:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    12,
                    42,
                    2,
                    197,
                    0
                ],
                "published": "2025-06-05T03:40:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    40,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "MVP-Shapley: Feature-based Modeling for Evaluating the Most Valuable\n  Player in Basketball",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVP-Shapley: Feature-based Modeling for Evaluating the Most Valuable\n  Player in Basketball"
                },
                "summary": "The burgeoning growth of the esports and multiplayer online gaming community\nhas highlighted the critical importance of evaluating the Most Valuable Player\n(MVP). The establishment of an explainable and practical MVP evaluation method\nis very challenging. In our study, we specifically focus on play-by-play data,\nwhich records related events during the game, such as assists and points. We\naim to address the challenges by introducing a new MVP evaluation framework,\ndenoted as \\oursys, which leverages Shapley values. This approach encompasses\nfeature processing, win-loss model training, Shapley value allocation, and MVP\nranking determination based on players' contributions. Additionally, we\noptimize our algorithm to align with expert voting results from the perspective\nof causality. Finally, we substantiated the efficacy of our method through\nvalidation using the NBA dataset and the Dunk City Dynasty dataset and\nimplemented online deployment in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The burgeoning growth of the esports and multiplayer online gaming community\nhas highlighted the critical importance of evaluating the Most Valuable Player\n(MVP). The establishment of an explainable and practical MVP evaluation method\nis very challenging. In our study, we specifically focus on play-by-play data,\nwhich records related events during the game, such as assists and points. We\naim to address the challenges by introducing a new MVP evaluation framework,\ndenoted as \\oursys, which leverages Shapley values. This approach encompasses\nfeature processing, win-loss model training, Shapley value allocation, and MVP\nranking determination based on players' contributions. Additionally, we\noptimize our algorithm to align with expert voting results from the perspective\nof causality. Finally, we substantiated the efficacy of our method through\nvalidation using the NBA dataset and the Dunk City Dynasty dataset and\nimplemented online deployment in the industry."
                },
                "authors": [
                    {
                        "name": "Haifeng Sun"
                    },
                    {
                        "name": "Yu Xiong"
                    },
                    {
                        "name": "Runze Wu"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Changjie Fan"
                    },
                    {
                        "name": "Shaojie Tang"
                    },
                    {
                        "name": "Xiang-Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang-Yang Li"
                },
                "author": "Xiang-Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12314v1",
                "updated": "2025-07-16T15:09:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    9,
                    13,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T15:09:13Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    9,
                    13,
                    2,
                    197,
                    0
                ],
                "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack"
                },
                "summary": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures."
                },
                "authors": [
                    {
                        "name": "Zihao Xue"
                    },
                    {
                        "name": "Zhen Bi"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Zhenlin Hu"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zhenfang Liu"
                    },
                    {
                        "name": "Qing Sheng"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Jungang Lou"
                    }
                ],
                "author_detail": {
                    "name": "Jungang Lou"
                },
                "author": "Jungang Lou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12308v1",
                "updated": "2025-07-16T15:05:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    5,
                    30,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T15:05:30Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    15,
                    5,
                    30,
                    2,
                    197,
                    0
                ],
                "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and\n  Summarization"
                },
                "summary": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL."
                },
                "authors": [
                    {
                        "name": "Prashanth Vijayaraghavan"
                    },
                    {
                        "name": "Apoorva Nitsure"
                    },
                    {
                        "name": "Charles Mackin"
                    },
                    {
                        "name": "Luyao Shi"
                    },
                    {
                        "name": "Stefano Ambrogio"
                    },
                    {
                        "name": "Arvind Haran"
                    },
                    {
                        "name": "Viresh Paruthi"
                    },
                    {
                        "name": "Ali Elzein"
                    },
                    {
                        "name": "Dan Coops"
                    },
                    {
                        "name": "David Beymer"
                    },
                    {
                        "name": "Tyler Baldwin"
                    },
                    {
                        "name": "Ehsan Degan"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Degan"
                },
                "author": "Ehsan Degan",
                "arxiv_doi": "10.1145/3670474.3685966",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3670474.3685966",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.12308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12296v1",
                "updated": "2025-07-16T14:49:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    49,
                    45,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:49:45Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    49,
                    45,
                    2,
                    197,
                    0
                ],
                "title": "Humans are more gullible than LLMs in believing common psychological\n  myths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans are more gullible than LLMs in believing common psychological\n  myths"
                },
                "summary": "Despite widespread debunking, many psychological myths remain deeply\nentrenched. This paper investigates whether Large Language Models (LLMs) mimic\nhuman behaviour of myth belief and explores methods to mitigate such\ntendencies. Using 50 popular psychological myths, we evaluate myth belief\nacross multiple LLMs under different prompting strategies, including\nretrieval-augmented generation and swaying prompts. Results show that LLMs\nexhibit significantly lower myth belief rates than humans, though user\nprompting can influence responses. RAG proves effective in reducing myth belief\nand reveals latent debiasing potential within LLMs. Our findings contribute to\nthe emerging field of Machine Psychology and highlight how cognitive science\nmethods can inform the evaluation and development of LLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite widespread debunking, many psychological myths remain deeply\nentrenched. This paper investigates whether Large Language Models (LLMs) mimic\nhuman behaviour of myth belief and explores methods to mitigate such\ntendencies. Using 50 popular psychological myths, we evaluate myth belief\nacross multiple LLMs under different prompting strategies, including\nretrieval-augmented generation and swaying prompts. Results show that LLMs\nexhibit significantly lower myth belief rates than humans, though user\nprompting can influence responses. RAG proves effective in reducing myth belief\nand reveals latent debiasing potential within LLMs. Our findings contribute to\nthe emerging field of Machine Psychology and highlight how cognitive science\nmethods can inform the evaluation and development of LLM-based systems."
                },
                "authors": [
                    {
                        "name": "Bevan Koopman"
                    },
                    {
                        "name": "Guido Zuccon"
                    }
                ],
                "author_detail": {
                    "name": "Guido Zuccon"
                },
                "author": "Guido Zuccon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12295v1",
                "updated": "2025-07-16T14:47:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    47,
                    41,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:47:41Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    47,
                    41,
                    2,
                    197,
                    0
                ],
                "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding"
                },
                "summary": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems."
                },
                "authors": [
                    {
                        "name": "Feng Xiao"
                    },
                    {
                        "name": "Jicong Fan"
                    }
                ],
                "author_detail": {
                    "name": "Jicong Fan"
                },
                "author": "Jicong Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14335v2",
                "updated": "2025-07-16T14:35:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    35,
                    17,
                    2,
                    197,
                    0
                ],
                "published": "2024-06-20T14:04:53Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    14,
                    4,
                    53,
                    3,
                    172,
                    0
                ],
                "title": "Linearly-Interpretable Concept Embedding Models for Text Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linearly-Interpretable Concept Embedding Models for Text Analysis"
                },
                "summary": "Despite their success, Large-Language Models (LLMs) still face criticism due\nto their lack of interpretability. Traditional post-hoc interpretation methods,\nbased on attention and gradient-based analysis, offer limited insights as they\nonly approximate the model's decision-making processes and have been proved to\nbe unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been\nlately proposed in the textual field to provide interpretable predictions based\non human-understandable concepts. However, CBMs still exhibit several\nlimitations due to their architectural constraints limiting their expressivity,\nto the absence of task-interpretability when employing non-linear task\npredictors and for requiring extensive annotations that are impractical for\nreal-world text data. In this paper, we address these challenges by proposing a\nnovel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the\ncurrent accuracy-interpretability trade-off. LICEMs classification accuracy is\nbetter than existing interpretable models and matches black-box ones. We show\nthat the explanations provided by our models are more interveneable and\ncausally consistent with respect to existing solutions. Finally, we show that\nLICEMs can be trained without requiring any concept supervision, as concepts\ncan be automatically predicted when using an LLM backbone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their success, Large-Language Models (LLMs) still face criticism due\nto their lack of interpretability. Traditional post-hoc interpretation methods,\nbased on attention and gradient-based analysis, offer limited insights as they\nonly approximate the model's decision-making processes and have been proved to\nbe unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been\nlately proposed in the textual field to provide interpretable predictions based\non human-understandable concepts. However, CBMs still exhibit several\nlimitations due to their architectural constraints limiting their expressivity,\nto the absence of task-interpretability when employing non-linear task\npredictors and for requiring extensive annotations that are impractical for\nreal-world text data. In this paper, we address these challenges by proposing a\nnovel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the\ncurrent accuracy-interpretability trade-off. LICEMs classification accuracy is\nbetter than existing interpretable models and matches black-box ones. We show\nthat the explanations provided by our models are more interveneable and\ncausally consistent with respect to existing solutions. Finally, we show that\nLICEMs can be trained without requiring any concept supervision, as concepts\ncan be automatically predicted when using an LLM backbone."
                },
                "authors": [
                    {
                        "name": "Francesco De Santis"
                    },
                    {
                        "name": "Philippe Bich"
                    },
                    {
                        "name": "Gabriele Ciravegna"
                    },
                    {
                        "name": "Pietro Barbiero"
                    },
                    {
                        "name": "Danilo Giordano"
                    },
                    {
                        "name": "Tania Cerquitelli"
                    }
                ],
                "author_detail": {
                    "name": "Tania Cerquitelli"
                },
                "author": "Tania Cerquitelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12284v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12284v2",
                "updated": "2025-07-17T12:55:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    12,
                    55,
                    32,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T14:31:33Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    31,
                    33,
                    2,
                    197,
                    0
                ],
                "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERA Code: A Unified Framework for Evaluating Code Generation Across\n  Tasks"
                },
                "summary": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures."
                },
                "authors": [
                    {
                        "name": "Artem Chervyakov"
                    },
                    {
                        "name": "Alexander Kharitonov"
                    },
                    {
                        "name": "Pavel Zadorozhny"
                    },
                    {
                        "name": "Adamenko Pavel"
                    },
                    {
                        "name": "Rodion Levichev"
                    },
                    {
                        "name": "Dmitrii Vorobev"
                    },
                    {
                        "name": "Dmitrii Salikhov"
                    },
                    {
                        "name": "Aidar Valeev"
                    },
                    {
                        "name": "Alena Pestova"
                    },
                    {
                        "name": "Maria Dziuba"
                    },
                    {
                        "name": "Ilseyar Alimova"
                    },
                    {
                        "name": "Artem Zavgorodnev"
                    },
                    {
                        "name": "Aleksandr Medvedev"
                    },
                    {
                        "name": "Stanislav Moiseev"
                    },
                    {
                        "name": "Elena Bruches"
                    },
                    {
                        "name": "Daniil Grebenkin"
                    },
                    {
                        "name": "Roman Derunets"
                    },
                    {
                        "name": "Vikulov Vladimir"
                    },
                    {
                        "name": "Anton Emelyanov"
                    },
                    {
                        "name": "Dmitrii Babaev"
                    },
                    {
                        "name": "Vladimir V. Ivanov"
                    },
                    {
                        "name": "Valentin Malykh"
                    },
                    {
                        "name": "Alena Fenogenova"
                    }
                ],
                "author_detail": {
                    "name": "Alena Fenogenova"
                },
                "author": "Alena Fenogenova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12284v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11330v2",
                "updated": "2025-07-16T14:26:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    26,
                    34,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-15T14:03:55Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    14,
                    3,
                    55,
                    1,
                    196,
                    0
                ],
                "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach\n  Integrating Human and Large Language Model Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach\n  Integrating Human and Large Language Model Knowledge"
                },
                "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. One of the most common types of\nnovelty in academic papers is the introduction of new methods. In this paper,\nwe propose leveraging human knowledge and LLM to assist pretrained language\nmodels (PLMs, e.g. BERT etc.) in predicting the method novelty of papers.\nSpecifically, we extract sentences related to the novelty of the academic paper\nfrom peer review reports and use LLM to summarize the methodology section of\nthe academic paper, which are then used to fine-tune PLMs. In addition, we have\ndesigned a text-guided fusion module with novel Sparse-Attention to better\nintegrate human and LLM knowledge. We compared the method we proposed with a\nlarge number of baselines. Extensive experiments demonstrate that our method\nachieves superior performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. One of the most common types of\nnovelty in academic papers is the introduction of new methods. In this paper,\nwe propose leveraging human knowledge and LLM to assist pretrained language\nmodels (PLMs, e.g. BERT etc.) in predicting the method novelty of papers.\nSpecifically, we extract sentences related to the novelty of the academic paper\nfrom peer review reports and use LLM to summarize the methodology section of\nthe academic paper, which are then used to fine-tune PLMs. In addition, we have\ndesigned a text-guided fusion module with novel Sparse-Attention to better\nintegrate human and LLM knowledge. We compared the method we proposed with a\nlarge number of baselines. Extensive experiments demonstrate that our method\nachieves superior performance."
                },
                "authors": [
                    {
                        "name": "Wenqing Wu"
                    },
                    {
                        "name": "Chengzhi Zhang"
                    },
                    {
                        "name": "Yi Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhao"
                },
                "author": "Yi Zhao",
                "arxiv_doi": "10.1002/asi.70005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/asi.70005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.11330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Journal of the Association for Information Science and Technology,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10559v2",
                "updated": "2025-07-16T14:25:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    25,
                    7,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-02T15:50:09Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    50,
                    9,
                    2,
                    183,
                    0
                ],
                "title": "NLP Meets the World: Toward Improving Conversations With the Public\n  About Natural Language Processing Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP Meets the World: Toward Improving Conversations With the Public\n  About Natural Language Processing Research"
                },
                "summary": "Recent developments in large language models (LLMs) have been accompanied by\nrapidly growing public interest in natural language processing (NLP). This\nattention is reflected by major news venues, which sometimes invite NLP\nresearchers to share their knowledge and views with a wide audience.\nRecognizing the opportunities of the present, for both the research field and\nfor individual researchers, this paper shares recommendations for communicating\nwith a general audience about the capabilities and limitations of NLP. These\nrecommendations cover three themes: vague terminology as an obstacle to public\nunderstanding, unreasonable expectations as obstacles to sustainable growth,\nand ethical failures as obstacles to continued support. Published NLP research\nand popular news coverage are cited to illustrate these themes with examples.\nThe recommendations promote effective, transparent communication with the\ngeneral public about NLP, in order to strengthen public understanding and\nencourage support for research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in large language models (LLMs) have been accompanied by\nrapidly growing public interest in natural language processing (NLP). This\nattention is reflected by major news venues, which sometimes invite NLP\nresearchers to share their knowledge and views with a wide audience.\nRecognizing the opportunities of the present, for both the research field and\nfor individual researchers, this paper shares recommendations for communicating\nwith a general audience about the capabilities and limitations of NLP. These\nrecommendations cover three themes: vague terminology as an obstacle to public\nunderstanding, unreasonable expectations as obstacles to sustainable growth,\nand ethical failures as obstacles to continued support. Published NLP research\nand popular news coverage are cited to illustrate these themes with examples.\nThe recommendations promote effective, transparent communication with the\ngeneral public about NLP, in order to strengthen public understanding and\nencourage support for research."
                },
                "authors": [
                    {
                        "name": "Shomir Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Shomir Wilson"
                },
                "author": "Shomir Wilson",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12273v2",
                "updated": "2025-07-17T14:54:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    54,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T14:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    22,
                    0,
                    2,
                    197,
                    0
                ],
                "title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction\n  with an Agentic Robot"
                },
                "summary": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments."
                },
                "authors": [
                    {
                        "name": "Luca Garello"
                    },
                    {
                        "name": "Francesca Cocchella"
                    },
                    {
                        "name": "Alessandra Sciutti"
                    },
                    {
                        "name": "Manuel Catalano"
                    },
                    {
                        "name": "Francesco Rea"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Rea"
                },
                "author": "Francesco Rea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12269v2",
                "updated": "2025-07-17T07:11:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    7,
                    11,
                    14,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T14:19:44Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    19,
                    44,
                    2,
                    197,
                    0
                ],
                "title": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust\n  Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in\n  Extremely Preterm Infants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust\n  Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in\n  Extremely Preterm Infants"
                },
                "summary": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of\nextremely low birth weight infants. Defined by oxygen dependence at 36 weeks\npostmenstrual age, it causes lifelong respiratory complications. However,\npreventive interventions carry severe risks, including neurodevelopmental\nimpairment, ventilator-induced lung injury, and systemic complications.\nTherefore, early BPD prognosis and prediction of BPD outcome is crucial to\navoid unnecessary toxicity in low risk infants. Admission radiographs of\nextremely preterm infants are routinely acquired within 24h of life and could\nserve as a non-invasive prognostic tool. In this work, we developed and\ninvestigated a deep learning approach using chest X-rays from 163 extremely\nlow-birth-weight infants ($\\leq$32 weeks gestation, 401-999g) obtained within\n24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult\nchest radiographs, employing progressive layer freezing with discriminative\nlearning rates to prevent overfitting and evaluated a CutMix augmentation and\nlinear probing. For moderate/severe BPD outcome prediction, our best performing\nmodel with progressive freezing, linear probing and CutMix achieved an AUROC of\n0.78 $\\pm$ 0.10, balanced accuracy of 0.69 $\\pm$ 0.10, and an F1-score of 0.67\n$\\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet\ninitialization (p = 0.031) which confirms domain-specific pretraining to be\nimportant for BPD outcome prediction. Routine IRDS grades showed limited\nprognostic value (AUROC 0.57 $\\pm$ 0.11), confirming the need of learned\nmarkers. Our approach demonstrates that domain-specific pretraining enables\naccurate BPD prediction from routine day-1 radiographs. Through progressive\nfreezing and linear probing, the method remains computationally feasible for\nsite-level implementation and future federated learning deployments."
                },
                "authors": [
                    {
                        "name": "Sybelle Goedicke-Fritz"
                    },
                    {
                        "name": "Michelle Bous"
                    },
                    {
                        "name": "Annika Engel"
                    },
                    {
                        "name": "Matthias Flotho"
                    },
                    {
                        "name": "Pascal Hirsch"
                    },
                    {
                        "name": "Hannah Wittig"
                    },
                    {
                        "name": "Dino Milanovic"
                    },
                    {
                        "name": "Dominik Mohr"
                    },
                    {
                        "name": "Mathias Kaspar"
                    },
                    {
                        "name": "Sogand Nemat"
                    },
                    {
                        "name": "Dorothea Kerner"
                    },
                    {
                        "name": "Arno Bcker"
                    },
                    {
                        "name": "Andreas Keller"
                    },
                    {
                        "name": "Sascha Meyer"
                    },
                    {
                        "name": "Michael Zemlin"
                    },
                    {
                        "name": "Philipp Flotho"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Flotho"
                },
                "arxiv_affiliation": "Helmholtz Institute for Pharmaceutical Research Saarland",
                "author": "Philipp Flotho",
                "arxiv_comment": "S.G.-F., M.B., and A.E. contributed equally to this work and share\n  first authorship. M.Z. and P.F. contributed equally to this work and share\n  senior authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11647v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11647v2",
                "updated": "2025-07-16T14:16:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    16,
                    47,
                    2,
                    197,
                    0
                ],
                "published": "2024-10-15T14:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    33,
                    23,
                    1,
                    289,
                    0
                ],
                "title": "Measuring Spiritual Values and Bias of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Spiritual Values and Bias of Large Language Models"
                },
                "summary": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias."
                },
                "authors": [
                    {
                        "name": "Songyuan Liu"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Runze Yan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Jiaying Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaying Lu"
                },
                "author": "Jiaying Lu",
                "arxiv_comment": "9 pages including appendix; 5 figures; 5 tables",
                "arxiv_journal_ref": "in Proceedings of KDD 2025 SciSoc LLM Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11647v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11647v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11212v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11212v5",
                "updated": "2025-07-16T14:15:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    15,
                    6,
                    2,
                    197,
                    0
                ],
                "published": "2024-01-20T11:37:44Z",
                "published_parsed": [
                    2024,
                    1,
                    20,
                    11,
                    37,
                    44,
                    5,
                    20,
                    0
                ],
                "title": "Programming Distributed Collective Processes in the eXchange Calculus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Distributed Collective Processes in the eXchange Calculus"
                },
                "summary": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Audrito"
                    },
                    {
                        "name": "Roberto Casadei"
                    },
                    {
                        "name": "Ferruccio Damiani"
                    },
                    {
                        "name": "Gianluca Torta"
                    },
                    {
                        "name": "Mirko Viroli"
                    }
                ],
                "author_detail": {
                    "name": "Mirko Viroli"
                },
                "author": "Mirko Viroli",
                "arxiv_comment": "41 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11212v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11212v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.3; F.1.1; F.4.3; I.2.11; J.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12261v1",
                "updated": "2025-07-16T14:06:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    6,
                    51,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T14:06:51Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    6,
                    51,
                    2,
                    197,
                    0
                ],
                "title": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form\n  Clinical Notes"
                },
                "summary": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For clinical data integration and healthcare services, the HL7 FHIR standard\nhas established itself as a desirable format for interoperability between\ncomplex health data. Previous attempts at automating the translation from\nfree-form clinical notes into structured FHIR resources rely on modular,\nrule-based systems or LLMs with instruction tuning and constrained decoding.\nSince they frequently suffer from limited generalizability and structural\ninconformity, we propose an end-to-end framework powered by LLM agents, code\nexecution, and healthcare terminology database tools to address these issues.\nOur solution, called Infherno, is designed to adhere to the FHIR document\nschema and competes well with a human baseline in predicting FHIR resources\nfrom unstructured text. The implementation features a front end for custom and\nsynthetic data and both local and proprietary models, supporting clinical data\nintegration processes and interoperability across institutions."
                },
                "authors": [
                    {
                        "name": "Johann Frei"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Lisa Raithel"
                    },
                    {
                        "name": "Roland Roller"
                    },
                    {
                        "name": "Alexander Meyer"
                    },
                    {
                        "name": "Frank Kramer"
                    }
                ],
                "author_detail": {
                    "name": "Frank Kramer"
                },
                "author": "Frank Kramer",
                "arxiv_comment": "Submitted to EMNLP 2025 System Demonstrations | Code:\n  https://github.com/j-frei/Infherno | Video:\n  https://www.youtube.com/watch?v=kyj5C2ivbMw | Demo:\n  https://infherno.misit-augsburg.de | HuggingFace Spaces:\n  https://huggingface.co/spaces/nfel/infherno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15460v4",
                "updated": "2025-07-16T14:04:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    4,
                    55,
                    2,
                    197,
                    0
                ],
                "published": "2024-10-20T18:18:23Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    18,
                    18,
                    23,
                    6,
                    294,
                    0
                ],
                "title": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detox: Sensitivity Dropout (SenD) for Large Language Model\n  Training"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent, concerns about\ntheir reliability, particularly due to hallucinations - factually inaccurate or\nirrelevant outputs - have grown. Our research investigates the relationship\nbetween the uncertainty in training dynamics and the emergence of\nhallucinations. Using models from the Pythia suite and several hallucination\ndetection metrics, we analyze hallucination trends and identify significant\nvariance during training. To address this, we propose \\textbf{Sensitivity\nDropout (SenD)}, a novel training protocol designed to reduce hallucination\nvariance during training by deterministically dropping embedding indices with\nsignificant variability. In addition, we develop an unsupervised hallucination\ndetection metric, Efficient EigenScore (EES), which approximates the\ntraditional EigenScore in 2x speed. This metric is integrated into our training\nprotocol, allowing SenD to be both computationally scalable and effective at\nreducing hallucination variance. SenD improves test-time reliability of Pythia\nand Meta's Llama models by up to 17\\% and enhances factual accuracy in\nWikipedia, Medical, Legal, and Coding domains without affecting downstream task\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent, concerns about\ntheir reliability, particularly due to hallucinations - factually inaccurate or\nirrelevant outputs - have grown. Our research investigates the relationship\nbetween the uncertainty in training dynamics and the emergence of\nhallucinations. Using models from the Pythia suite and several hallucination\ndetection metrics, we analyze hallucination trends and identify significant\nvariance during training. To address this, we propose \\textbf{Sensitivity\nDropout (SenD)}, a novel training protocol designed to reduce hallucination\nvariance during training by deterministically dropping embedding indices with\nsignificant variability. In addition, we develop an unsupervised hallucination\ndetection metric, Efficient EigenScore (EES), which approximates the\ntraditional EigenScore in 2x speed. This metric is integrated into our training\nprotocol, allowing SenD to be both computationally scalable and effective at\nreducing hallucination variance. SenD improves test-time reliability of Pythia\nand Meta's Llama models by up to 17\\% and enhances factual accuracy in\nWikipedia, Medical, Legal, and Coding domains without affecting downstream task\nperformance."
                },
                "authors": [
                    {
                        "name": "Shahrad Mohammadzadeh"
                    },
                    {
                        "name": "Juan David Guerra"
                    },
                    {
                        "name": "Marco Bonizzato"
                    },
                    {
                        "name": "Reihaneh Rabbany"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "arxiv_comment": "Accepted to ACL 2025, accepted to Safe Generative AI Workshop @\n  NeurIPS 2024. Camera-ready version for ACL 2025 (to appear). Submitted July\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12252v1",
                "updated": "2025-07-16T13:59:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    59,
                    32,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:59:32Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    59,
                    32,
                    2,
                    197,
                    0
                ],
                "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Contextual ASR via Multi-grained Fusion with Large Language\n  Models"
                },
                "summary": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/."
                },
                "authors": [
                    {
                        "name": "Shilin Zhou"
                    },
                    {
                        "name": "Zhenghua Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhenghua Li"
                },
                "author": "Zhenghua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12248v1",
                "updated": "2025-07-16T13:57:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    57,
                    50,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:57:50Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    57,
                    50,
                    2,
                    197,
                    0
                ],
                "title": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on\n  PathMNIST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on\n  PathMNIST"
                },
                "summary": "Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has significantly advanced the field of medical image\nclassification, particularly with the adoption of Convolutional Neural Networks\n(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer\nunique advantages in model development and deployment. However, their\ncomparative performance in medical imaging tasks remains underexplored. This\nstudy presents a comprehensive analysis of CNN implementations across these\nframeworks, using the PathMNIST dataset as a benchmark. We evaluate training\nefficiency, classification accuracy and inference speed to assess their\nsuitability for real-world applications. Our findings highlight the trade-offs\nbetween computational speed and model accuracy, offering valuable insights for\nresearchers and practitioners in medical image analysis."
                },
                "authors": [
                    {
                        "name": "Anida Nezovi"
                    },
                    {
                        "name": "Jalal Romano"
                    },
                    {
                        "name": "Nada Mari"
                    },
                    {
                        "name": "Medina Kapo"
                    },
                    {
                        "name": "Amila Akagi"
                    }
                ],
                "author_detail": {
                    "name": "Amila Akagi"
                },
                "author": "Amila Akagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16994v2",
                "updated": "2025-07-16T13:35:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    35,
                    23,
                    2,
                    197,
                    0
                ],
                "published": "2025-02-24T09:28:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    9,
                    28,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "FADE: Why Bad Descriptions Happen to Good Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FADE: Why Bad Descriptions Happen to Good Features"
                },
                "summary": "Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While this may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for automatically evaluating feature-to-description\nalignment. FADE evaluates alignment across four key metrics - Clarity,\nResponsiveness, Purity, and Faithfulness - and systematically quantifies the\ncauses of the misalignment between features and their descriptions. We apply\nFADE to analyze existing open-source feature descriptions and assess key\ncomponents of automated interpretability pipelines, aiming to enhance the\nquality of descriptions. Our findings highlight fundamental challenges in\ngenerating feature descriptions, particularly for SAEs compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in mechanistic interpretability have highlighted the\npotential of automating interpretability pipelines in analyzing the latent\nrepresentations within LLMs. While this may enhance our understanding of\ninternal mechanisms, the field lacks standardized evaluation methods for\nassessing the validity of discovered features. We attempt to bridge this gap by\nintroducing FADE: Feature Alignment to Description Evaluation, a scalable\nmodel-agnostic framework for automatically evaluating feature-to-description\nalignment. FADE evaluates alignment across four key metrics - Clarity,\nResponsiveness, Purity, and Faithfulness - and systematically quantifies the\ncauses of the misalignment between features and their descriptions. We apply\nFADE to analyze existing open-source feature descriptions and assess key\ncomponents of automated interpretability pipelines, aiming to enhance the\nquality of descriptions. Our findings highlight fundamental challenges in\ngenerating feature descriptions, particularly for SAEs compared to MLP neurons,\nproviding insights into the limitations and future directions of automated\ninterpretability. We release FADE as an open-source package at:\nhttps://github.com/brunibrun/FADE"
                },
                "authors": [
                    {
                        "name": "Bruno Puri"
                    },
                    {
                        "name": "Aakriti Jain"
                    },
                    {
                        "name": "Elena Golimblevskaia"
                    },
                    {
                        "name": "Patrick Kahardipraja"
                    },
                    {
                        "name": "Thomas Wiegand"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Lapuschkin"
                },
                "author": "Sebastian Lapuschkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00603v2",
                "updated": "2025-07-16T13:28:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    28,
                    57,
                    2,
                    197,
                    0
                ],
                "published": "2024-10-01T11:44:29Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    11,
                    44,
                    29,
                    1,
                    275,
                    0
                ],
                "title": "An Empirical Study of Large Language Models for Type and Call Graph\n  Analysis in Python and JavaScript",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Large Language Models for Type and Call Graph\n  Analysis in Python and JavaScript"
                },
                "summary": "Large Language Models (LLMs) are increasingly being explored for their\npotential in software engineering, particularly in static analysis tasks. In\nthis study, we investigate the potential of current LLMs to enhance call-graph\nanalysis and type inference for Python and JavaScript programs. We empirically\nevaluated 24 LLMs, including OpenAI's GPT series and open-source models like\nLLaMA and Mistral, using existing and newly developed benchmarks. Specifically,\nwe enhanced TypeEvalPy, a micro-benchmarking framework for type inference in\nPython, with auto-generation capabilities, expanding its scope from 860 to\n77,268 type annotations for Python. Additionally, we introduced SWARM-CG and\nSWARM-JS, comprehensive benchmarking suites for evaluating call-graph\nconstruction tools across multiple programming languages.\n  Our findings reveal a contrasting performance of LLMs in static analysis\ntasks. For call-graph generation, traditional static analysis tools such as\nPyCG for Python and Jelly for JavaScript consistently outperform LLMs. While\nadvanced models like mistral-large-it-2407-123b and gpt-4o show promise, they\nstill struggle with completeness and soundness in call-graph analysis across\nboth languages. In contrast, LLMs demonstrate a clear advantage in type\ninference for Python, surpassing traditional tools like HeaderGen and hybrid\napproaches such as HiTyper. These results suggest that, while LLMs hold promise\nin type inference, their limitations in call-graph analysis highlight the need\nfor further research. Our study provides a foundation for integrating LLMs into\nstatic analysis workflows, offering insights into their strengths and current\nlimitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being explored for their\npotential in software engineering, particularly in static analysis tasks. In\nthis study, we investigate the potential of current LLMs to enhance call-graph\nanalysis and type inference for Python and JavaScript programs. We empirically\nevaluated 24 LLMs, including OpenAI's GPT series and open-source models like\nLLaMA and Mistral, using existing and newly developed benchmarks. Specifically,\nwe enhanced TypeEvalPy, a micro-benchmarking framework for type inference in\nPython, with auto-generation capabilities, expanding its scope from 860 to\n77,268 type annotations for Python. Additionally, we introduced SWARM-CG and\nSWARM-JS, comprehensive benchmarking suites for evaluating call-graph\nconstruction tools across multiple programming languages.\n  Our findings reveal a contrasting performance of LLMs in static analysis\ntasks. For call-graph generation, traditional static analysis tools such as\nPyCG for Python and Jelly for JavaScript consistently outperform LLMs. While\nadvanced models like mistral-large-it-2407-123b and gpt-4o show promise, they\nstill struggle with completeness and soundness in call-graph analysis across\nboth languages. In contrast, LLMs demonstrate a clear advantage in type\ninference for Python, surpassing traditional tools like HeaderGen and hybrid\napproaches such as HiTyper. These results suggest that, while LLMs hold promise\nin type inference, their limitations in call-graph analysis highlight the need\nfor further research. Our study provides a foundation for integrating LLMs into\nstatic analysis workflows, offering insights into their strengths and current\nlimitations."
                },
                "authors": [
                    {
                        "name": "Ashwin Prasad Shivarpatna Venkatesh"
                    },
                    {
                        "name": "Rose Sunil"
                    },
                    {
                        "name": "Samkutty Sabu"
                    },
                    {
                        "name": "Amir M. Mir"
                    },
                    {
                        "name": "Sofia Reis"
                    },
                    {
                        "name": "Eric Bodden"
                    }
                ],
                "author_detail": {
                    "name": "Eric Bodden"
                },
                "author": "Eric Bodden",
                "arxiv_comment": "Accepted to be published in the EMSE journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10577v2",
                "updated": "2025-07-16T13:25:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    25,
                    34,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-11T10:08:05Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    10,
                    8,
                    5,
                    4,
                    192,
                    0
                ],
                "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos\n  and influence opinions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos\n  and influence opinions"
                },
                "summary": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space."
                },
                "authors": [
                    {
                        "name": "Ccile Log"
                    },
                    {
                        "name": "Rehan Ghori"
                    }
                ],
                "author_detail": {
                    "name": "Rehan Ghori"
                },
                "author": "Rehan Ghori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12215v1",
                "updated": "2025-07-16T13:19:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    19,
                    46,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:19:46Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    19,
                    46,
                    2,
                    197,
                    0
                ],
                "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese\n  Chess via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese\n  Chess via Reinforcement Learning"
                },
                "summary": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas."
                },
                "authors": [
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Shuochen Liu"
                    },
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jiayao Shi"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12207v1",
                "updated": "2025-07-16T13:07:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    7,
                    24,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:07:24Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    7,
                    24,
                    2,
                    197,
                    0
                ],
                "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics\n  via LLM-driven Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics\n  via LLM-driven Evolution"
                },
                "summary": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems."
                },
                "authors": [
                    {
                        "name": "Subin Lin"
                    },
                    {
                        "name": "Chuanbo Hua"
                    }
                ],
                "author_detail": {
                    "name": "Chuanbo Hua"
                },
                "author": "Chuanbo Hua",
                "arxiv_comment": "ICML 2025 CO-Build Workshop Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12205v1",
                "updated": "2025-07-16T13:04:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    4,
                    6,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T13:04:06Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    4,
                    6,
                    2,
                    197,
                    0
                ],
                "title": "Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed\n  Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed\n  Storage"
                },
                "summary": "Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance\nbottleneck in the local deployment of sparse Large Language Models (LLMs),\nwhere inference predominantly operates on workloads during the decoder phase\nwith a batch size of one. Existing SpMV kernels and sparse matrix formats,\noriginally designed for scientific computing, fail to exploit the unique\nstructure patterns inherent in sparse LLMs, resulting in suboptimal performance\nand excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized\nSpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a\nhierarchical block extraction algorithm that captures multiple granularities of\nblock structures within sparse LLMs, and (2) a novel compressed sparse format\n(EC-CSR) that employs delta indexing to reduce storage overhead and enhance\nmemory access efficiency. Evaluated on real sparse weight matrices from LLaMA\nand OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV\nlibraries and reduces storage overhead by up to 55.4% compared to CSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance\nbottleneck in the local deployment of sparse Large Language Models (LLMs),\nwhere inference predominantly operates on workloads during the decoder phase\nwith a batch size of one. Existing SpMV kernels and sparse matrix formats,\noriginally designed for scientific computing, fail to exploit the unique\nstructure patterns inherent in sparse LLMs, resulting in suboptimal performance\nand excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized\nSpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a\nhierarchical block extraction algorithm that captures multiple granularities of\nblock structures within sparse LLMs, and (2) a novel compressed sparse format\n(EC-CSR) that employs delta indexing to reduce storage overhead and enhance\nmemory access efficiency. Evaluated on real sparse weight matrices from LLaMA\nand OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV\nlibraries and reduces storage overhead by up to 55.4% compared to CSR."
                },
                "authors": [
                    {
                        "name": "Junqing Lin"
                    },
                    {
                        "name": "Jingwei Sun"
                    },
                    {
                        "name": "Mingge Lu"
                    },
                    {
                        "name": "Guangzhong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangzhong Sun"
                },
                "author": "Guangzhong Sun",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11423v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11423v2",
                "updated": "2025-07-16T13:02:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    13,
                    2,
                    26,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-15T15:47:47Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    15,
                    47,
                    47,
                    1,
                    196,
                    0
                ],
                "title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer,\n  and Optimize?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer,\n  and Optimize?"
                },
                "summary": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities."
                },
                "authors": [
                    {
                        "name": "Yanjian Zhang"
                    },
                    {
                        "name": "Guillaume Wisniewski"
                    },
                    {
                        "name": "Nadi Tomeh"
                    },
                    {
                        "name": "Thierry Charnois"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Charnois"
                },
                "author": "Thierry Charnois",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11423v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15026v2",
                "updated": "2025-07-16T12:59:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    59,
                    2,
                    2,
                    197,
                    0
                ],
                "published": "2025-06-17T23:59:06Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    23,
                    59,
                    6,
                    1,
                    168,
                    0
                ],
                "title": "Algorithmic Approaches to Enhance Safety in Autonomous Vehicles:\n  Minimizing Lane Changes and Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic Approaches to Enhance Safety in Autonomous Vehicles:\n  Minimizing Lane Changes and Merging"
                },
                "summary": "The rapid advancements in autonomous vehicle (AV) technology promise enhanced\nsafety and operational efficiency. However, frequent lane changes and merging\nmaneuvers continue to pose significant safety risks and disrupt traffic flow.\nThis paper introduces the Minimizing Lane Change Algorithm (MLCA), a\nstate-machine-based approach designed to reduce unnecessary lane changes,\nthereby enhancing both traffic safety and efficiency. The MLCA algorithm\nprioritizes maintaining lane stability unless safety-critical conditions\nnecessitate a lane change. The algorithm's effectiveness was evaluated through\nsimulations conducted on the SUMO platform, comparing its performance against\nestablished models, including LC2017 and MOBIL. Results demonstrate substantial\nreductions in lane changes and collisions, leading to smoother traffic flow and\nimproved safety metrics. Additionally, the study highlights the MLCA's\nadaptability to various traffic densities and roadway configurations,\nshowcasing its potential for wide-scale deployment in real-world AV systems.\nFuture work aims to validate these findings in more complex scenarios using the\nCARLA simulator, which will enable the testing of the algorithm under more\ndynamic and high-fidelity conditions, such as urban traffic environments with\ndiverse road users. Moreover, the integration of cybersecurity measures for\nvehicle-to-vehicle (V2V) communication will be explored to ensure robust and\nsecure data exchange, further enhancing the reliability and safety of AV\noperations. This research contributes to the broader goal of developing\nintelligent traffic systems that optimize both individual vehicle performance\nand overall traffic network efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in autonomous vehicle (AV) technology promise enhanced\nsafety and operational efficiency. However, frequent lane changes and merging\nmaneuvers continue to pose significant safety risks and disrupt traffic flow.\nThis paper introduces the Minimizing Lane Change Algorithm (MLCA), a\nstate-machine-based approach designed to reduce unnecessary lane changes,\nthereby enhancing both traffic safety and efficiency. The MLCA algorithm\nprioritizes maintaining lane stability unless safety-critical conditions\nnecessitate a lane change. The algorithm's effectiveness was evaluated through\nsimulations conducted on the SUMO platform, comparing its performance against\nestablished models, including LC2017 and MOBIL. Results demonstrate substantial\nreductions in lane changes and collisions, leading to smoother traffic flow and\nimproved safety metrics. Additionally, the study highlights the MLCA's\nadaptability to various traffic densities and roadway configurations,\nshowcasing its potential for wide-scale deployment in real-world AV systems.\nFuture work aims to validate these findings in more complex scenarios using the\nCARLA simulator, which will enable the testing of the algorithm under more\ndynamic and high-fidelity conditions, such as urban traffic environments with\ndiverse road users. Moreover, the integration of cybersecurity measures for\nvehicle-to-vehicle (V2V) communication will be explored to ensure robust and\nsecure data exchange, further enhancing the reliability and safety of AV\noperations. This research contributes to the broader goal of developing\nintelligent traffic systems that optimize both individual vehicle performance\nand overall traffic network efficiency."
                },
                "authors": [
                    {
                        "name": "Seyed Moein Abtahi"
                    },
                    {
                        "name": "Akramul Azim"
                    }
                ],
                "author_detail": {
                    "name": "Akramul Azim"
                },
                "author": "Akramul Azim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09467v2",
                "updated": "2025-07-16T12:48:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    48,
                    21,
                    2,
                    197,
                    0
                ],
                "published": "2025-02-13T16:31:50Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    31,
                    50,
                    3,
                    44,
                    0
                ],
                "title": "Just Trial Once: Ongoing Causal Validation of Machine Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Trial Once: Ongoing Causal Validation of Machine Learning Models"
                },
                "summary": "Machine learning (ML) models are increasingly used as decision-support tools\nin high-risk domains. Evaluating the causal impact of deploying such models can\nbe done with a randomized controlled trial (RCT) that randomizes users to ML\nvs. control groups and assesses the effect on relevant outcomes. However, ML\nmodels are inevitably updated over time, and we often lack evidence for the\ncausal impact of these updates. While the causal effect could be repeatedly\nvalidated with ongoing RCTs, such experiments are expensive and time-consuming\nto run. In this work, we present an alternative solution: using only data from\na prior RCT, we give conditions under which the causal impact of a new ML model\ncan be precisely bounded or estimated, even if it was not included in the RCT.\nOur assumptions incorporate two realistic constraints: ML predictions are often\ndeterministic, and their impacts depend on user trust in the model. Based on\nour analysis, we give recommendations for trial designs that maximize our\nability to assess future versions of an ML model. Our hope is that our trial\ndesign recommendations will save practitioners time and resources while\nallowing for quicker deployments of updates to ML models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models are increasingly used as decision-support tools\nin high-risk domains. Evaluating the causal impact of deploying such models can\nbe done with a randomized controlled trial (RCT) that randomizes users to ML\nvs. control groups and assesses the effect on relevant outcomes. However, ML\nmodels are inevitably updated over time, and we often lack evidence for the\ncausal impact of these updates. While the causal effect could be repeatedly\nvalidated with ongoing RCTs, such experiments are expensive and time-consuming\nto run. In this work, we present an alternative solution: using only data from\na prior RCT, we give conditions under which the causal impact of a new ML model\ncan be precisely bounded or estimated, even if it was not included in the RCT.\nOur assumptions incorporate two realistic constraints: ML predictions are often\ndeterministic, and their impacts depend on user trust in the model. Based on\nour analysis, we give recommendations for trial designs that maximize our\nability to assess future versions of an ML model. Our hope is that our trial\ndesign recommendations will save practitioners time and resources while\nallowing for quicker deployments of updates to ML models."
                },
                "authors": [
                    {
                        "name": "Jacob M. Chen"
                    },
                    {
                        "name": "Michael Oberst"
                    }
                ],
                "author_detail": {
                    "name": "Michael Oberst"
                },
                "author": "Michael Oberst",
                "arxiv_comment": "26 pages. In proceedings of the 41st Conference on Uncertainty in\n  Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12196v1",
                "updated": "2025-07-16T12:46:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    46,
                    4,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T12:46:04Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    46,
                    4,
                    2,
                    197,
                    0
                ],
                "title": "Selective Quantization Tuning for ONNX Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Quantization Tuning for ONNX Models"
                },
                "summary": "Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model."
                },
                "authors": [
                    {
                        "name": "Nikolaos Louloudakis"
                    },
                    {
                        "name": "Ajitha Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Ajitha Rajan"
                },
                "author": "Ajitha Rajan",
                "arxiv_comment": "5 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07682v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07682v4",
                "updated": "2025-07-16T12:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    27,
                    40,
                    2,
                    197,
                    0
                ],
                "published": "2024-12-10T17:13:35Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    13,
                    35,
                    1,
                    345,
                    0
                ],
                "title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation"
                },
                "summary": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks."
                },
                "authors": [
                    {
                        "name": "Alfredo Garrachn Ruiz"
                    },
                    {
                        "name": "Toms de la Rosa"
                    },
                    {
                        "name": "Daniel Borrajo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Borrajo"
                },
                "author": "Daniel Borrajo",
                "arxiv_comment": "13 pages, 12 tables, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07682v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07682v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08802v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08802v4",
                "updated": "2025-07-16T12:09:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    12,
                    9,
                    34,
                    2,
                    197,
                    0
                ],
                "published": "2024-02-05T14:20:19Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    14,
                    20,
                    19,
                    0,
                    36,
                    0
                ],
                "title": "Governance of Generative Artificial Intelligence for Companies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Governance of Generative Artificial Intelligence for Companies"
                },
                "summary": "Generative Artificial Intelligence (GenAI), specifically large language\nmodels(LLMs) like ChatGPT, has swiftly entered organizations without adequate\ngovernance, posing both opportunities and risks. Despite extensive debates on\nGenAI's transformative nature and regulatory measures, limited research\naddresses organizational governance, encompassing technical and business\nperspectives. Although numerous frameworks for governance of AI exist, it is\nnot clear to what extent they apply to GenAI. Our review paper fills this gap\nby surveying recent works with the purpose of better understanding fundamental\ncharacteristics of GenAI and adjusting prior frameworks specifically towards\nGenAI governance within companies. To do so, it extends Nickerson's framework\ndevelopment processes to include prior conceptualizations. Our framework\noutlines the scope, objectives, and governance mechanisms tailored to harness\nbusiness opportunities as well as mitigate risks associated with GenAI\nintegration. Our research contributes a focused approach to GenAI governance,\noffering practical insights for companies navigating the challenges of GenAI\nadoption and highlighting research gaps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI), specifically large language\nmodels(LLMs) like ChatGPT, has swiftly entered organizations without adequate\ngovernance, posing both opportunities and risks. Despite extensive debates on\nGenAI's transformative nature and regulatory measures, limited research\naddresses organizational governance, encompassing technical and business\nperspectives. Although numerous frameworks for governance of AI exist, it is\nnot clear to what extent they apply to GenAI. Our review paper fills this gap\nby surveying recent works with the purpose of better understanding fundamental\ncharacteristics of GenAI and adjusting prior frameworks specifically towards\nGenAI governance within companies. To do so, it extends Nickerson's framework\ndevelopment processes to include prior conceptualizations. Our framework\noutlines the scope, objectives, and governance mechanisms tailored to harness\nbusiness opportunities as well as mitigate risks associated with GenAI\nintegration. Our research contributes a focused approach to GenAI governance,\noffering practical insights for companies navigating the challenges of GenAI\nadoption and highlighting research gaps."
                },
                "authors": [
                    {
                        "name": "Johannes Schneider"
                    },
                    {
                        "name": "Pauline Kuss"
                    },
                    {
                        "name": "Rene Abraham"
                    },
                    {
                        "name": "Christian Meske"
                    }
                ],
                "author_detail": {
                    "name": "Christian Meske"
                },
                "author": "Christian Meske",
                "arxiv_comment": "This paper is under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08802v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08802v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05843v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05843v4",
                "updated": "2025-07-16T11:47:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    47,
                    45,
                    2,
                    197,
                    0
                ],
                "published": "2025-02-09T10:30:54Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    10,
                    30,
                    54,
                    6,
                    40,
                    0
                ],
                "title": "From Objects to Events: Unlocking Complex Visual Understanding in Object\n  Detectors via LLM-guided Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Objects to Events: Unlocking Complex Visual Understanding in Object\n  Detectors via LLM-guided Symbolic Reasoning"
                },
                "summary": "Current object detectors excel at entity localization and classification, yet\nexhibit inherent limitations in event recognition capabilities. This deficiency\narises from their architecture's emphasis on discrete object identification\nrather than modeling the compositional reasoning, inter-object correlations,\nand contextual semantics essential for comprehensive event understanding. To\naddress this challenge, we present a novel framework that expands the\ncapability of standard object detectors beyond mere object recognition to\ncomplex event understanding through LLM-guided symbolic reasoning. Our key\ninnovation lies in bridging the semantic gap between object detection and event\nunderstanding without requiring expensive task-specific training. The proposed\nplug-and-play framework interfaces with any open-vocabulary detector while\nextending their inherent capabilities across architectures. At its core, our\napproach combines (i) a symbolic regression mechanism exploring relationship\npatterns among detected entities and (ii) a LLM-guided strategically guiding\nthe search toward meaningful expressions. These discovered symbolic rules\ntransform low-level visual perception into interpretable event understanding,\nproviding a transparent reasoning path from objects to events with strong\ntransferability across domains.We compared our training-free framework against\nspecialized event recognition systems across diverse application domains.\nExperiments demonstrate that our framework enhances multiple object detector\narchitectures to recognize complex events such as illegal fishing activities\n(75% AUROC, +8.36% improvement), construction safety violations (+15.77%), and\nabnormal crowd behaviors (+23.16%). Code is available at\n\\href{https://github.com/MAC-AutoML/SymbolicDet}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current object detectors excel at entity localization and classification, yet\nexhibit inherent limitations in event recognition capabilities. This deficiency\narises from their architecture's emphasis on discrete object identification\nrather than modeling the compositional reasoning, inter-object correlations,\nand contextual semantics essential for comprehensive event understanding. To\naddress this challenge, we present a novel framework that expands the\ncapability of standard object detectors beyond mere object recognition to\ncomplex event understanding through LLM-guided symbolic reasoning. Our key\ninnovation lies in bridging the semantic gap between object detection and event\nunderstanding without requiring expensive task-specific training. The proposed\nplug-and-play framework interfaces with any open-vocabulary detector while\nextending their inherent capabilities across architectures. At its core, our\napproach combines (i) a symbolic regression mechanism exploring relationship\npatterns among detected entities and (ii) a LLM-guided strategically guiding\nthe search toward meaningful expressions. These discovered symbolic rules\ntransform low-level visual perception into interpretable event understanding,\nproviding a transparent reasoning path from objects to events with strong\ntransferability across domains.We compared our training-free framework against\nspecialized event recognition systems across diverse application domains.\nExperiments demonstrate that our framework enhances multiple object detector\narchitectures to recognize complex events such as illegal fishing activities\n(75% AUROC, +8.36% improvement), construction safety violations (+15.77%), and\nabnormal crowd behaviors (+23.16%). Code is available at\n\\href{https://github.com/MAC-AutoML/SymbolicDet}{here}."
                },
                "authors": [
                    {
                        "name": "Yuhui Zeng"
                    },
                    {
                        "name": "Haoxiang Wu"
                    },
                    {
                        "name": "Wenjie Nie"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Guangyao Chen"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05843v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05843v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15740v3",
                "updated": "2025-07-16T11:45:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    45,
                    2,
                    2,
                    197,
                    0
                ],
                "published": "2024-03-23T06:36:32Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    6,
                    36,
                    32,
                    5,
                    83,
                    0
                ],
                "title": "Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training"
                },
                "summary": "A primary concern regarding training large language models (LLMs) is whether\nthey abuse copyrighted online text. With the increasing training data scale and\nthe prevalence of LLMs in daily lives, two problems arise: \\textbf{1)} false\npositive membership inference results misled by similar examples; \\textbf{2)}\nmembership inference methods are usually too complex for end users to\nunderstand and use. To address these issues, we propose an alternative\n\\textit{insert-and-detect} methodology, advocating that web users and content\nplatforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{\\textbf{ghost\nsentences}} and a user-friendly last-$k$ words test, allowing end users to chat\nwith LLMs for membership inference. Ghost sentences consist primarily of unique\npassphrases of random natural words, which can come with customized elements to\nbypass possible filter rules. The last-$k$ words test requires a significant\nrepetition time of ghost sentences~($\\ge10$). For cases with fewer repetitions,\nwe designed an extra perplexity test, as LLMs exhibit high perplexity when\nencountering unnatural passphrases. We also conduct a comprehensive study on\nthe memorization and membership inference of ghost sentences, examining factors\nsuch as training data scales, model sizes, repetition times, insertion\npositions, wordlist of passphrases, alignment, \\textit{etc}. Our study shows\nthe possibility of applying ghost sentences in real scenarios and provides\ninstructions for the potential application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A primary concern regarding training large language models (LLMs) is whether\nthey abuse copyrighted online text. With the increasing training data scale and\nthe prevalence of LLMs in daily lives, two problems arise: \\textbf{1)} false\npositive membership inference results misled by similar examples; \\textbf{2)}\nmembership inference methods are usually too complex for end users to\nunderstand and use. To address these issues, we propose an alternative\n\\textit{insert-and-detect} methodology, advocating that web users and content\nplatforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{\\textbf{ghost\nsentences}} and a user-friendly last-$k$ words test, allowing end users to chat\nwith LLMs for membership inference. Ghost sentences consist primarily of unique\npassphrases of random natural words, which can come with customized elements to\nbypass possible filter rules. The last-$k$ words test requires a significant\nrepetition time of ghost sentences~($\\ge10$). For cases with fewer repetitions,\nwe designed an extra perplexity test, as LLMs exhibit high perplexity when\nencountering unnatural passphrases. We also conduct a comprehensive study on\nthe memorization and membership inference of ghost sentences, examining factors\nsuch as training data scales, model sizes, repetition times, insertion\npositions, wordlist of passphrases, alignment, \\textit{etc}. Our study shows\nthe possibility of applying ghost sentences in real scenarios and provides\ninstructions for the potential application."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Linchao Zhu"
                    },
                    {
                        "name": "Ruijie Quan"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "A technical report, work mainly done in the early of 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12158v1",
                "updated": "2025-07-16T11:37:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    37,
                    37,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:37:37Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    37,
                    37,
                    2,
                    197,
                    0
                ],
                "title": "Probabilistic Safety Verification for an Autonomous Ground Vehicle: A\n  Situation Coverage Grid Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Safety Verification for an Autonomous Ground Vehicle: A\n  Situation Coverage Grid Approach"
                },
                "summary": "As industrial autonomous ground vehicles are increasingly deployed in\nsafety-critical environments, ensuring their safe operation under diverse\nconditions is paramount. This paper presents a novel approach for their safety\nverification based on systematic situation extraction, probabilistic modelling\nand verification. We build upon the concept of a situation coverage grid, which\nexhaustively enumerates environmental configurations relevant to the vehicle's\noperation. This grid is augmented with quantitative probabilistic data\ncollected from situation-based system testing, capturing probabilistic\ntransitions between situations. We then generate a probabilistic model that\nencodes the dynamics of both normal and unsafe system behaviour. Safety\nproperties extracted from hazard analysis and formalised in temporal logic are\nverified through probabilistic model checking against this model. The results\ndemonstrate that our approach effectively identifies high-risk situations,\nprovides quantitative safety guarantees, and supports compliance with\nregulatory standards, thereby contributing to the robust deployment of\nautonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As industrial autonomous ground vehicles are increasingly deployed in\nsafety-critical environments, ensuring their safe operation under diverse\nconditions is paramount. This paper presents a novel approach for their safety\nverification based on systematic situation extraction, probabilistic modelling\nand verification. We build upon the concept of a situation coverage grid, which\nexhaustively enumerates environmental configurations relevant to the vehicle's\noperation. This grid is augmented with quantitative probabilistic data\ncollected from situation-based system testing, capturing probabilistic\ntransitions between situations. We then generate a probabilistic model that\nencodes the dynamics of both normal and unsafe system behaviour. Safety\nproperties extracted from hazard analysis and formalised in temporal logic are\nverified through probabilistic model checking against this model. The results\ndemonstrate that our approach effectively identifies high-risk situations,\nprovides quantitative safety guarantees, and supports compliance with\nregulatory standards, thereby contributing to the robust deployment of\nautonomous systems."
                },
                "authors": [
                    {
                        "name": "Nawshin Mannan Proma"
                    },
                    {
                        "name": "Gricel Vzquez"
                    },
                    {
                        "name": "Sepeedeh Shahbeigi"
                    },
                    {
                        "name": "Arjun Badyal"
                    },
                    {
                        "name": "Victoria Hodge"
                    }
                ],
                "author_detail": {
                    "name": "Victoria Hodge"
                },
                "author": "Victoria Hodge",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09037v2",
                "updated": "2025-07-16T11:33:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    33,
                    35,
                    2,
                    197,
                    0
                ],
                "published": "2025-04-12T01:27:49Z",
                "published_parsed": [
                    2025,
                    4,
                    12,
                    1,
                    27,
                    49,
                    5,
                    102,
                    0
                ],
                "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to\n  Reason, and Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to\n  Reason, and Agentic Systems"
                },
                "summary": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a fundamental cognitive process that enables logical inference,\nproblem-solving, and decision-making. With the rapid advancement of large\nlanguage models (LLMs), reasoning has emerged as a key capability that\ndistinguishes advanced AI systems from conventional models that empower\nchatbots. In this survey, we categorize existing methods along two orthogonal\ndimensions: (1) Regimes, which define the stage at which reasoning is achieved\n(either at inference time or through dedicated training); and (2)\nArchitectures, which determine the components involved in the reasoning\nprocess, distinguishing between standalone LLMs and agentic compound systems\nthat incorporate external tools, and multi-agent collaborations. Within each\ndimension, we analyze two key perspectives: (1) Input level, which focuses on\ntechniques that construct high-quality prompts that the LLM condition on; and\n(2) Output level, which methods that refine multiple sampled candidates to\nenhance reasoning quality. This categorization provides a systematic\nunderstanding of the evolving landscape of LLM reasoning, highlighting emerging\ntrends such as the shift from inference-scaling to learning-to-reason (e.g.,\nDeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep\nResearch, Manus Agent). Additionally, we cover a broad spectrum of learning\nalgorithms, from supervised fine-tuning to reinforcement learning such as PPO\nand GRPO, and the training of reasoners and verifiers. We also examine key\ndesigns of agentic workflows, from established patterns like\ngenerator-evaluator and LLM debate to recent innovations. ..."
                },
                "authors": [
                    {
                        "name": "Zixuan Ke"
                    },
                    {
                        "name": "Fangkai Jiao"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Do Xuan Long"
                    },
                    {
                        "name": "Minzhi Li"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Peifeng Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "72 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23836v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23836v3",
                "updated": "2025-07-16T11:25:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    25,
                    40,
                    2,
                    197,
                    0
                ],
                "published": "2025-05-28T12:03:09Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    3,
                    9,
                    2,
                    148,
                    0
                ],
                "title": "Large Language Models Often Know When They Are Being Evaluated",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Often Know When They Are Being Evaluated"
                },
                "summary": "If AI models can detect when they are being evaluated, the effectiveness of\nevaluations might be compromised. For example, models could have systematically\ndifferent behavior during evaluations, leading to less reliable benchmarks for\ndeployment and governance decisions. We investigate whether frontier language\nmodels can accurately classify transcripts based on whether they originate from\nevaluations or real-world deployment, a capability we call evaluation\nawareness. To achieve this, we construct a diverse benchmark of 1,000 prompts\nand transcripts from 61 distinct datasets. These span public benchmarks (e.g.,\nMMLU, SWEBench), real-world deployment interactions, and agent trajectories\nfrom scaffolding frameworks (e.g., web-browsing agents). Frontier models\nclearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches\nan AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of\n$0.92$). Furthermore, both AI models and humans are better at identifying\nevaluations in agentic settings compared to chat settings. Additionally, we\ntest whether models can identify the purpose of the evaluation. Under\nmultiple-choice and open-ended questioning, AI models far outperform random\nchance in identifying what an evaluation is testing for. Our results indicate\nthat frontier models already exhibit a substantial, though not yet superhuman,\nlevel of evaluation-awareness. We recommend tracking this capability in future\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If AI models can detect when they are being evaluated, the effectiveness of\nevaluations might be compromised. For example, models could have systematically\ndifferent behavior during evaluations, leading to less reliable benchmarks for\ndeployment and governance decisions. We investigate whether frontier language\nmodels can accurately classify transcripts based on whether they originate from\nevaluations or real-world deployment, a capability we call evaluation\nawareness. To achieve this, we construct a diverse benchmark of 1,000 prompts\nand transcripts from 61 distinct datasets. These span public benchmarks (e.g.,\nMMLU, SWEBench), real-world deployment interactions, and agent trajectories\nfrom scaffolding frameworks (e.g., web-browsing agents). Frontier models\nclearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches\nan AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of\n$0.92$). Furthermore, both AI models and humans are better at identifying\nevaluations in agentic settings compared to chat settings. Additionally, we\ntest whether models can identify the purpose of the evaluation. Under\nmultiple-choice and open-ended questioning, AI models far outperform random\nchance in identifying what an evaluation is testing for. Our results indicate\nthat frontier models already exhibit a substantial, though not yet superhuman,\nlevel of evaluation-awareness. We recommend tracking this capability in future\nmodels."
                },
                "authors": [
                    {
                        "name": "Joe Needham"
                    },
                    {
                        "name": "Giles Edkins"
                    },
                    {
                        "name": "Govind Pimpale"
                    },
                    {
                        "name": "Henning Bartsch"
                    },
                    {
                        "name": "Marius Hobbhahn"
                    }
                ],
                "author_detail": {
                    "name": "Marius Hobbhahn"
                },
                "author": "Marius Hobbhahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23836v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23836v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12145v1",
                "updated": "2025-07-16T11:25:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    25,
                    3,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:25:03Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    25,
                    3,
                    2,
                    197,
                    0
                ],
                "title": "PRISM: Distributed Inference for Foundation Models at Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Distributed Inference for Foundation Models at Edge"
                },
                "summary": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Muhammad Azlan Qazi"
                    },
                    {
                        "name": "Alexandros Iosifidis"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12143v1",
                "updated": "2025-07-16T11:19:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    19,
                    28,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:19:28Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    19,
                    28,
                    2,
                    197,
                    0
                ],
                "title": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as\n  Teachers, Students and Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as\n  Teachers, Students and Evaluators"
                },
                "summary": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELOQUENT is a set of shared tasks that aims to create easily testable\nhigh-level criteria for evaluating generative language models. Sensemaking is\none such shared task.\n  In Sensemaking, we try to assess how well generative models ``make sense out\nof a given text'' in three steps inspired by exams in a classroom setting: (1)\nTeacher systems should prepare a set of questions, (2) Student systems should\nanswer these questions, and (3) Evaluator systems should score these answers,\nall adhering rather strictly to a given set of input materials.\n  We report on the 2025 edition of Sensemaking, where we had 7 sources of test\nmaterials (fact-checking analyses of statements, textbooks, transcribed\nrecordings of a lecture, and educational videos) spanning English, German,\nUkrainian, and Czech languages.\n  This year, 4 teams participated, providing us with 2 Teacher submissions, 2\nStudent submissions, and 2 Evaluator submissions. We added baselines for\nTeacher and Student using commercial large language model systems. We devised a\nfully automatic evaluation procedure, which we compare to a minimalistic manual\nevaluation.\n  We were able to make some interesting observations. For the first task, the\ncreation of questions, better evaluation strategies will still have to be\ndevised because it is difficult to discern the quality of the various candidate\nquestion sets. In the second task, question answering, the LLMs examined\noverall perform acceptably, but restricting their answers to the given input\ntexts remains problematic. In the third task, evaluation of question answers,\nour adversarial tests reveal that systems using the LLM-as-a-Judge paradigm\nerroneously rate both garbled question-answer pairs and answers to mixed-up\nquestions as acceptable."
                },
                "authors": [
                    {
                        "name": "Pavel indel"
                    },
                    {
                        "name": "Ondej Bojar"
                    }
                ],
                "author_detail": {
                    "name": "Ondej Bojar"
                },
                "author": "Ondej Bojar",
                "arxiv_comment": "30 pages, 7 figures, CLEF 2025 Conference and Labs of the Evaluation\n  Forum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12142v1",
                "updated": "2025-07-16T11:17:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    17,
                    12,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:17:12Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    17,
                    12,
                    2,
                    197,
                    0
                ],
                "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA\n  Optimization"
                },
                "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications."
                },
                "authors": [
                    {
                        "name": "Vladimir Bogachev"
                    },
                    {
                        "name": "Vladimir Aletov"
                    },
                    {
                        "name": "Alexander Molozhavenko"
                    },
                    {
                        "name": "Denis Bobkov"
                    },
                    {
                        "name": "Vera Soboleva"
                    },
                    {
                        "name": "Aibek Alanov"
                    },
                    {
                        "name": "Maxim Rakhuba"
                    }
                ],
                "author_detail": {
                    "name": "Maxim Rakhuba"
                },
                "author": "Maxim Rakhuba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 65F55, 53Z50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03132v2",
                "updated": "2025-07-16T11:15:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    15,
                    36,
                    2,
                    197,
                    0
                ],
                "published": "2025-02-05T12:49:26Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    49,
                    26,
                    2,
                    36,
                    0
                ],
                "title": "SPARK: A Modular Benchmark for Humanoid Robot Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARK: A Modular Benchmark for Humanoid Robot Safety"
                },
                "summary": "This paper introduces the Safe Protective and Assistive Robot Kit (SPARK), a\ncomprehensive benchmark designed to ensure safety in humanoid autonomy and\nteleoperation. Humanoid robots pose significant safety risks due to their\nphysical capabilities of interacting with complex environments. The physical\nstructures of humanoid robots further add complexity to the design of general\nsafety solutions. To facilitate safe deployment of complex robot systems, SPARK\ncan be used as a toolbox that comes with state-of-the-art safe control\nalgorithms in a modular and composable robot control framework. Users can\neasily configure safety criteria and sensitivity levels to optimize the balance\nbetween safety and performance. To accelerate humanoid safety research and\ndevelopment, SPARK provides simulation benchmarks that compare safety\napproaches in a variety of environments, tasks, and robot models. Furthermore,\nSPARK allows quick deployment of synthesized safe controllers on real robots.\nFor hardware deployment, SPARK supports Apple Vision Pro (AVP) or a Motion\nCapture System as external sensors, while offering interfaces for seamless\nintegration with alternative hardware setups at the same time. This paper\ndemonstrates SPARK's capability with both simulation experiments and case\nstudies with a Unitree G1 humanoid robot. Leveraging these advantages of SPARK,\nusers and researchers can significantly improve the safety of their humanoid\nsystems as well as accelerate relevant research. The open source code is\navailable at: https://github.com/intelligent-control-lab/spark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Safe Protective and Assistive Robot Kit (SPARK), a\ncomprehensive benchmark designed to ensure safety in humanoid autonomy and\nteleoperation. Humanoid robots pose significant safety risks due to their\nphysical capabilities of interacting with complex environments. The physical\nstructures of humanoid robots further add complexity to the design of general\nsafety solutions. To facilitate safe deployment of complex robot systems, SPARK\ncan be used as a toolbox that comes with state-of-the-art safe control\nalgorithms in a modular and composable robot control framework. Users can\neasily configure safety criteria and sensitivity levels to optimize the balance\nbetween safety and performance. To accelerate humanoid safety research and\ndevelopment, SPARK provides simulation benchmarks that compare safety\napproaches in a variety of environments, tasks, and robot models. Furthermore,\nSPARK allows quick deployment of synthesized safe controllers on real robots.\nFor hardware deployment, SPARK supports Apple Vision Pro (AVP) or a Motion\nCapture System as external sensors, while offering interfaces for seamless\nintegration with alternative hardware setups at the same time. This paper\ndemonstrates SPARK's capability with both simulation experiments and case\nstudies with a Unitree G1 humanoid robot. Leveraging these advantages of SPARK,\nusers and researchers can significantly improve the safety of their humanoid\nsystems as well as accelerate relevant research. The open source code is\navailable at: https://github.com/intelligent-control-lab/spark."
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Kai S. Yun"
                    },
                    {
                        "name": "Yikuan Fang"
                    },
                    {
                        "name": "Sebin Jung"
                    },
                    {
                        "name": "Feihan Li"
                    },
                    {
                        "name": "Bowei Li"
                    },
                    {
                        "name": "Weiye Zhao"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "arxiv_comment": "Presented at IFAC Symposium on Robotics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12132v1",
                "updated": "2025-07-16T11:00:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    0,
                    46,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T11:00:46Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    11,
                    0,
                    46,
                    2,
                    197,
                    0
                ],
                "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition\n  Using Wi-Fi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition\n  Using Wi-Fi"
                },
                "summary": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications."
                },
                "authors": [
                    {
                        "name": "Navid Hasanzadeh"
                    },
                    {
                        "name": "Shahrokh Valaee"
                    }
                ],
                "author_detail": {
                    "name": "Shahrokh Valaee"
                },
                "author": "Shahrokh Valaee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12126v1",
                "updated": "2025-07-16T10:49:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    49,
                    30,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T10:49:30Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    49,
                    30,
                    2,
                    197,
                    0
                ],
                "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation\n  for Unstructured Survey data Modeling and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation\n  for Unstructured Survey data Modeling and Analysis"
                },
                "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text data augmentation is a widely used strategy for mitigating data sparsity\nin natural language processing (NLP), particularly in low-resource settings\nwhere limited samples hinder effective semantic modeling. While augmentation\ncan improve input diversity and downstream interpretability, existing\ntechniques often lack mechanisms to ensure semantic preservation during\nlarge-scale or iterative generation, leading to redundancy and instability.\nThis work introduces a principled evaluation framework for large language model\n(LLM) based text augmentation, comprising two components: (1) Scalability\nAnalysis, which measures semantic consistency as augmentation volume increases,\nand (2) Iterative Augmentation with Summarization Refinement (IASR), which\nevaluates semantic drift across recursive paraphrasing cycles. Empirical\nevaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the\nbest balance of semantic fidelity, diversity, and generation efficiency.\nApplied to a real-world topic modeling task using BERTopic with GPT-enhanced\nfew-shot labeling, the proposed approach results in a 400% increase in topic\ngranularity and complete elimination of topic overlaps. These findings\nvalidated the utility of the proposed frameworks for structured evaluation of\nLLM-based augmentation in practical NLP pipelines."
                },
                "authors": [
                    {
                        "name": "Payal Bhattad"
                    },
                    {
                        "name": "Sai Manoj Pudukotai Dinakarrao"
                    },
                    {
                        "name": "Anju Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Anju Gupta"
                },
                "author": "Anju Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00513v3",
                "updated": "2025-07-16T10:35:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    35,
                    9,
                    2,
                    197,
                    0
                ],
                "published": "2025-04-01T08:03:40Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    8,
                    3,
                    40,
                    1,
                    91,
                    0
                ],
                "title": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for User Stories in AI Systems: UStAI Dataset"
                },
                "summary": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI systems are gaining widespread adoption across various sectors and\ndomains. Creating high-quality AI system requirements is crucial for aligning\nthe AI system with business goals and consumer values and for social\nresponsibility. However, with the uncertain nature of AI systems and the heavy\nreliance on sensitive data, more research is needed to address the elicitation\nand analysis of AI systems requirements. With the proprietary nature of many AI\nsystems, there is a lack of open-source requirements artifacts and technical\nrequirements documents for AI systems, limiting broader research and\ninvestigation. With Large Language Models (LLMs) emerging as a promising\nalternative to human-generated text, this paper investigates the potential use\nof LLMs to generate user stories for AI systems based on abstracts from\nscholarly papers. We conducted an empirical evaluation using three LLMs and\ngenerated $1260$ user stories from $42$ abstracts from $26$ domains. We assess\ntheir quality using the Quality User Story (QUS) framework. Moreover, we\nidentify relevant non-functional requirements (NFRs) and ethical principles.\nOur analysis demonstrates that the investigated LLMs can generate user stories\ninspired by the needs of various stakeholders, offering a promising approach\nfor generating user stories for research purposes and for aiding in the early\nrequirements elicitation phase of AI systems. We have compiled and curated a\ncollection of stories generated by various LLMs into a dataset (UStAI), which\nis now publicly available for use."
                },
                "authors": [
                    {
                        "name": "Asma Yamani"
                    },
                    {
                        "name": "Malak Baslyman"
                    },
                    {
                        "name": "Moataz Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Moataz Ahmed"
                },
                "author": "Moataz Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12272v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12272v4",
                "updated": "2025-07-16T10:34:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    34,
                    26,
                    2,
                    197,
                    0
                ],
                "published": "2025-02-17T19:16:37Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    19,
                    16,
                    37,
                    0,
                    48,
                    0
                ],
                "title": "Learning to Reason at the Frontier of Learnability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason at the Frontier of Learnability"
                },
                "summary": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs."
                },
                "authors": [
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Jakob Foerster"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Foerster"
                },
                "author": "Jakob Foerster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12272v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12272v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12104v1",
                "updated": "2025-07-16T10:20:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    20,
                    14,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T10:20:14Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    10,
                    20,
                    14,
                    2,
                    197,
                    0
                ],
                "title": "From Static to Intelligent: Evolving SaaS Pricing with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Static to Intelligent: Evolving SaaS Pricing with LLMs"
                },
                "summary": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SaaS paradigm has revolutionized software distribution by offering\nflexible pricing options to meet diverse customer needs. However, the rapid\nexpansion of the SaaS market has introduced significant complexity for DevOps\nteams, who must manually manage and evolve pricing structures, an approach that\nis both time-consuming and prone to errors. The absence of automated tools for\npricing analysis restricts the ability to efficiently evaluate, optimize, and\nscale these models. This paper proposes leveraging intelligent pricing\n(iPricing), dynamic, machine-readable pricing models, as a solution to these\nchallenges. Intelligent pricing enables competitive analysis, streamlines\noperational decision-making, and supports continuous pricing evolution in\nresponse to market dynamics, leading to improved efficiency and accuracy. We\npresent an LLM-driven approach that automates the transformation of static HTML\npricing into iPricing, significantly improving efficiency and consistency while\nminimizing human error. Our implementation, AI4Pricing2Yaml, features a basic\nInformation Extractor that uses web scraping and LLMs technologies to extract\nessential pricing components, plans, features, usage limits, and add-ons, from\nSaaS websites. Validation against a dataset of 30 distinct commercial SaaS,\nencompassing over 150 intelligent pricings, demonstrates the system's\neffectiveness in extracting the desired elements across all steps. However,\nchallenges remain in addressing hallucinations, complex structures, and dynamic\ncontent. This work highlights the potential of automating intelligent pricing\ntransformation to streamline SaaS pricing management, offering implications for\nimproved consistency and scalability in an increasingly intricate pricing\nlandscape. Future research will focus on refining extraction capabilities and\nenhancing the system's adaptability to a wider range of SaaS websites."
                },
                "authors": [
                    {
                        "name": "Francisco Javier Cavero"
                    },
                    {
                        "name": "Juan C. Alonso"
                    },
                    {
                        "name": "Antonio Ruiz-Corts"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Ruiz-Corts"
                },
                "author": "Antonio Ruiz-Corts",
                "arxiv_comment": "12 pages. Accepted at the SOC4AI Workshop (Service-Oriented Computing\n  for AI Applications), held in conjunction with the 22nd International\n  Conference on Service-Oriented Computing (ICSOC 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12084v1",
                "updated": "2025-07-16T09:46:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    46,
                    58,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T09:46:58Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    46,
                    58,
                    2,
                    197,
                    0
                ],
                "title": "LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided\n  Seed Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided\n  Seed Generation"
                },
                "summary": "Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing\nremains an important approach to securing smart contracts. Even though mutation\nscheduling is a key factor influencing fuzzing effectiveness, existing fuzzers\nhave primarily explored seed scheduling and generation, while mutation\nscheduling has been rarely addressed by prior work. In this work, we propose a\nLarge Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing\nframework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and\nhybrid testing techniques. Key components of the proposed LLAMA include: (i) a\nhierarchical prompting strategy that guides LLMs to generate semantically valid\ninitial seeds, coupled with a lightweight pre-fuzzing phase to select\nhigh-potential inputs; (ii) a multi-feedback optimization mechanism that\nsimultaneously improves seed generation, seed selection, and mutation\nscheduling by leveraging runtime coverage and dependency feedback; and (iii) an\nevolutionary fuzzing engine that dynamically adjusts mutation operator\nprobabilities based on effectiveness, while incorporating symbolic execution to\nescape stagnation and uncover deeper vulnerabilities. Our experiments\ndemonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage\nand vulnerability detection. Specifically, it achieves 91% instruction coverage\nand 90% branch coverage, while detecting 132 out of 148 known vulnerabilities\nacross diverse categories. These results highlight LLAMA's effectiveness,\nadaptability, and practicality in real-world smart contract security testing\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing\nremains an important approach to securing smart contracts. Even though mutation\nscheduling is a key factor influencing fuzzing effectiveness, existing fuzzers\nhave primarily explored seed scheduling and generation, while mutation\nscheduling has been rarely addressed by prior work. In this work, we propose a\nLarge Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing\nframework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and\nhybrid testing techniques. Key components of the proposed LLAMA include: (i) a\nhierarchical prompting strategy that guides LLMs to generate semantically valid\ninitial seeds, coupled with a lightweight pre-fuzzing phase to select\nhigh-potential inputs; (ii) a multi-feedback optimization mechanism that\nsimultaneously improves seed generation, seed selection, and mutation\nscheduling by leveraging runtime coverage and dependency feedback; and (iii) an\nevolutionary fuzzing engine that dynamically adjusts mutation operator\nprobabilities based on effectiveness, while incorporating symbolic execution to\nescape stagnation and uncover deeper vulnerabilities. Our experiments\ndemonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage\nand vulnerability detection. Specifically, it achieves 91% instruction coverage\nand 90% branch coverage, while detecting 132 out of 148 known vulnerabilities\nacross diverse categories. These results highlight LLAMA's effectiveness,\nadaptability, and practicality in real-world smart contract security testing\nscenarios."
                },
                "authors": [
                    {
                        "name": "Keke Gai"
                    },
                    {
                        "name": "Haochen Liang"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Liehuang Zhu"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12079v1",
                "updated": "2025-07-16T09:39:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    39,
                    56,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T09:39:56Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    39,
                    56,
                    2,
                    197,
                    0
                ],
                "title": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method\n  for Active Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Findings of MEGA: Maths Explanation with LLMs using the Socratic Method\n  for Active Learning"
                },
                "summary": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an intervention study on the effects of the combined\nmethods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3)\nsimplified gamification and (4) formative feedback on university students'\nMaths learning driven by large language models (LLMs). We call our approach\nMathematics Explanations through Games by AI LLMs (MEGA). Some students\nstruggle with Maths and as a result avoid Math-related discipline or subjects\ndespite the importance of Maths across many fields, including signal\nprocessing. Oftentimes, students' Maths difficulties stem from suboptimal\npedagogy. We compared the MEGA method to the traditional step-by-step (CoT)\nmethod to ascertain which is better by using a within-group design after\nrandomly assigning questions for the participants, who are university students.\nSamples (n=60) were randomly drawn from each of the two test sets of the Grade\nSchool Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH)\ndatasets, based on the error margin of 11%, the confidence level of 90%, and a\nmanageable number of samples for the student evaluators. These samples were\nused to evaluate two capable LLMs at length (Generative Pretrained Transformer\n4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for\ncapability. The results showed that students agree in more instances that the\nMEGA method is experienced as better for learning for both datasets. It is even\nmuch better than the CoT (47.5% compared to 26.67%) in the more difficult MATH\ndataset, indicating that MEGA is better at explaining difficult Maths problems."
                },
                "authors": [
                    {
                        "name": "Tosin Adewumi"
                    },
                    {
                        "name": "Foteini Simistira Liwicki"
                    },
                    {
                        "name": "Marcus Liwicki"
                    },
                    {
                        "name": "Viktor Gardelli"
                    },
                    {
                        "name": "Lama Alkhaled"
                    },
                    {
                        "name": "Hamam Mokayed"
                    }
                ],
                "author_detail": {
                    "name": "Hamam Mokayed"
                },
                "author": "Hamam Mokayed",
                "arxiv_comment": "This paper was accepted for the special issue AI for Education by the\n  IEEE Signal Processing Magazine journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12059v1",
                "updated": "2025-07-16T09:16:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    16,
                    36,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T09:16:36Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    16,
                    36,
                    2,
                    197,
                    0
                ],
                "title": "Evaluating the Ability of Large Language Models to Reason about Cardinal\n  Directions, Revisited",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Ability of Large Language Models to Reason about Cardinal\n  Directions, Revisited"
                },
                "summary": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the abilities of 28 Large language Models (LLMs) to reason\nabout cardinal directions (CDs) using a benchmark generated from a set of\ntemplates, extensively testing an LLM's ability to determine the correct CD\ngiven a particular scenario. The templates allow for a number of degrees of\nvariation such as means of locomotion of the agent involved, and whether set in\nthe first, second or third person. Even the newer Large Reasoning Models are\nunable to reliably determine the correct CD for all questions. This paper\nsummarises and extends earlier work presented at COSIT-24."
                },
                "authors": [
                    {
                        "name": "Anthony G Cohn"
                    },
                    {
                        "name": "Robert E Blackwell"
                    }
                ],
                "author_detail": {
                    "name": "Robert E Blackwell"
                },
                "author": "Robert E Blackwell",
                "arxiv_comment": "8 pages, 5 figures. Accepted at QR 2025 : 38th International Workshop\n  on Qualitative Reasoning at IJCAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12049v1",
                "updated": "2025-07-16T09:10:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    10,
                    38,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T09:10:38Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    9,
                    10,
                    38,
                    2,
                    197,
                    0
                ],
                "title": "MoViAD: Modular Visual Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoViAD: Modular Visual Anomaly Detection"
                },
                "summary": "VAD is a critical field in machine learning focused on identifying deviations\nfrom normal patterns in images, often challenged by the scarcity of anomalous\ndata and the need for unsupervised training. To accelerate research and\ndeployment in this domain, we introduce MoViAD, a comprehensive and highly\nmodular library designed to provide fast and easy access to state-of-the-art\nVAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array\nof scenarios, including continual, semi-supervised, few-shots, noisy, and many\nmore. In addition, it addresses practical deployment challenges through\ndedicated Edge and IoT settings, offering optimized models and backbones, along\nwith quantization and compression utilities for efficient on-device execution\nand distributed inference. MoViAD integrates a selection of backbones, robust\nevaluation VAD metrics (pixel-level and image-level) and useful profiling tools\nfor efficiency analysis. The library is designed for fast, effortless\ndeployment, enabling machine learning engineers to easily use it for their\nspecific setup with custom models, datasets, and backbones. At the same time,\nit offers the flexibility and extensibility researchers need to develop and\nexperiment with new methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAD is a critical field in machine learning focused on identifying deviations\nfrom normal patterns in images, often challenged by the scarcity of anomalous\ndata and the need for unsupervised training. To accelerate research and\ndeployment in this domain, we introduce MoViAD, a comprehensive and highly\nmodular library designed to provide fast and easy access to state-of-the-art\nVAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array\nof scenarios, including continual, semi-supervised, few-shots, noisy, and many\nmore. In addition, it addresses practical deployment challenges through\ndedicated Edge and IoT settings, offering optimized models and backbones, along\nwith quantization and compression utilities for efficient on-device execution\nand distributed inference. MoViAD integrates a selection of backbones, robust\nevaluation VAD metrics (pixel-level and image-level) and useful profiling tools\nfor efficiency analysis. The library is designed for fast, effortless\ndeployment, enabling machine learning engineers to easily use it for their\nspecific setup with custom models, datasets, and backbones. At the same time,\nit offers the flexibility and extensibility researchers need to develop and\nexperiment with new methods."
                },
                "authors": [
                    {
                        "name": "Manuel Barusco"
                    },
                    {
                        "name": "Francesco Borsatti"
                    },
                    {
                        "name": "Arianna Stropeni"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    }
                ],
                "author_detail": {
                    "name": "Gian Antonio Susto"
                },
                "author": "Gian Antonio Susto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12039v2",
                "updated": "2025-07-17T15:27:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    15,
                    27,
                    29,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T08:56:19Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    8,
                    56,
                    19,
                    2,
                    197,
                    0
                ],
                "title": "A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Approach to Assessing Linguistic Creativity of Large\n  Language Models and Humans"
                },
                "summary": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The following paper introduces a general linguistic creativity test for\nhumans and Large Language Models (LLMs). The test consists of various tasks\naimed at assessing their ability to generate new original words and phrases\nbased on word formation processes (derivation and compounding) and on\nmetaphorical language use. We administered the test to 24 humans and to an\nequal number of LLMs, and we automatically evaluated their answers using OCSAI\ntool for three criteria: Originality, Elaboration, and Flexibility. The results\nshow that LLMs not only outperformed humans in all the assessed criteria, but\ndid better in six out of the eight test tasks. We then computed the uniqueness\nof the individual answers, which showed some minor differences between humans\nand LLMs. Finally, we performed a short manual analysis of the dataset, which\nrevealed that humans are more inclined towards E(extending)-creativity, while\nLLMs favor F(ixed)-creativity."
                },
                "authors": [
                    {
                        "name": "Anca Dinu"
                    },
                    {
                        "name": "Andra-Maria Florescu"
                    },
                    {
                        "name": "Alina Resceanu"
                    }
                ],
                "author_detail": {
                    "name": "Alina Resceanu"
                },
                "author": "Alina Resceanu",
                "arxiv_comment": "Accepted for presentation at KES 2025. To appear in Procedia Computer\n  Science (Elsevier)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10024v2",
                "updated": "2025-07-16T08:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    8,
                    53,
                    53,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-14T08:06:12Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    6,
                    12,
                    0,
                    195,
                    0
                ],
                "title": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative Study for LLM-assisted Design Study Process: Strategies,\n  Challenges, and Roles"
                },
                "summary": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design studies aim to create visualization solutions for real-world problems\nof different application domains. Recently, the emergence of large language\nmodels (LLMs) has introduced new opportunities to enhance the design study\nprocess, providing capabilities such as creative problem-solving, data\nhandling, and insightful analysis. However, despite their growing popularity,\nthere remains a lack of systematic understanding of how LLMs can effectively\nassist researchers in visualization-specific design studies. In this paper, we\nconducted a multi-stage qualitative study to fill this gap, involving 30 design\nstudy researchers from diverse backgrounds and expertise levels. Through\nin-depth interviews and carefully-designed questionnaires, we investigated\nstrategies for utilizing LLMs, the challenges encountered, and the practices\nused to overcome them. We further compiled and summarized the roles that LLMs\ncan play across different stages of the design study process. Our findings\nhighlight practical implications to inform visualization practitioners, and\nprovide a framework for leveraging LLMs to enhance the design study process in\nvisualization research."
                },
                "authors": [
                    {
                        "name": "Shaolun Ruan"
                    },
                    {
                        "name": "Rui Sheng"
                    },
                    {
                        "name": "Xiaolin Wen"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Tim Dwyer"
                    },
                    {
                        "name": "Jiannan Li"
                    }
                ],
                "author_detail": {
                    "name": "Jiannan Li"
                },
                "author": "Jiannan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12032v1",
                "updated": "2025-07-16T08:48:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    8,
                    48,
                    4,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T08:48:04Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    8,
                    48,
                    4,
                    2,
                    197,
                    0
                ],
                "title": "ARRC: Explainable, Workflow-Integrated Recommender for Sustainable\n  Resource Optimization Across the Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARRC: Explainable, Workflow-Integrated Recommender for Sustainable\n  Resource Optimization Across the Edge-Cloud Continuum"
                },
                "summary": "Achieving sustainable, explainable, and maintainable automation for resource\noptimization is a core challenge across the edge-cloud continuum. Persistent\noverprovisioning and operational complexity often stem from heterogeneous\nplatforms and layered abstractions, while systems lacking explainability and\nmaintainability become fragile, impede safe recovery, and accumulate technical\ndebt. Existing solutions are frequently reactive, limited to single abstraction\nlayers, or require intrusive platform changes, leaving efficiency and\nmaintainability gains unrealized.\n  This paper addresses safe, transparent, and low-effort resource optimization\nin dynamic, multi-tenant edge-cloud systems, without disrupting operator\nworkflows or increasing technical debt. We introduce ARRC, a recommender system\nrooted in software engineering design principles, which delivers explainable,\ncross-layer resource recommendations directly into operator workflows (such as\ntickets and GitOps pull requests). ARRC encapsulates optimization logic in\nspecialized, auditable agents coordinated via a shared interface, supporting\nmaintainability and extensibility through transparency and the ability to\ninspect both recommendations and their rationale.\n  Empirical evaluation in a multi-region industrial deployment shows that ARRC\nreduces operator workload by over 50%, improves compute utilization by up to\n7.7x, and maintains error rates below 5%, with most benefits achieved through\nincremental, operator-approved changes. This demonstrates that explainable,\nrecommendation-based architectures can achieve sustainable efficiency and\nmaintainability improvements at production scale.\n  ARRC provides an empirically evaluated framework for integrating explainable,\nworkflow-driven automation into resource management, intended to advance best\npractices for robust, maintainable, and transparent edge-cloud continuum\nplatforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving sustainable, explainable, and maintainable automation for resource\noptimization is a core challenge across the edge-cloud continuum. Persistent\noverprovisioning and operational complexity often stem from heterogeneous\nplatforms and layered abstractions, while systems lacking explainability and\nmaintainability become fragile, impede safe recovery, and accumulate technical\ndebt. Existing solutions are frequently reactive, limited to single abstraction\nlayers, or require intrusive platform changes, leaving efficiency and\nmaintainability gains unrealized.\n  This paper addresses safe, transparent, and low-effort resource optimization\nin dynamic, multi-tenant edge-cloud systems, without disrupting operator\nworkflows or increasing technical debt. We introduce ARRC, a recommender system\nrooted in software engineering design principles, which delivers explainable,\ncross-layer resource recommendations directly into operator workflows (such as\ntickets and GitOps pull requests). ARRC encapsulates optimization logic in\nspecialized, auditable agents coordinated via a shared interface, supporting\nmaintainability and extensibility through transparency and the ability to\ninspect both recommendations and their rationale.\n  Empirical evaluation in a multi-region industrial deployment shows that ARRC\nreduces operator workload by over 50%, improves compute utilization by up to\n7.7x, and maintains error rates below 5%, with most benefits achieved through\nincremental, operator-approved changes. This demonstrates that explainable,\nrecommendation-based architectures can achieve sustainable efficiency and\nmaintainability improvements at production scale.\n  ARRC provides an empirically evaluated framework for integrating explainable,\nworkflow-driven automation into resource management, intended to advance best\npractices for robust, maintainable, and transparent edge-cloud continuum\nplatforms."
                },
                "authors": [
                    {
                        "name": "Brian-Frederik Jahnke"
                    },
                    {
                        "name": "Ren Brinkhege"
                    },
                    {
                        "name": "Jan Peter Meyer"
                    },
                    {
                        "name": "Daniel Tebernum"
                    },
                    {
                        "name": "Falk Howar"
                    }
                ],
                "author_detail": {
                    "name": "Falk Howar"
                },
                "author": "Falk Howar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08506v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08506v3",
                "updated": "2025-07-16T08:29:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    8,
                    29,
                    2,
                    2,
                    197,
                    0
                ],
                "published": "2025-03-11T14:56:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    56,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper\n  Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper\n  Reviews"
                },
                "summary": "Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic paper review is a critical yet time-consuming task within the\nresearch community. With the increasing volume of academic publications,\nautomating the review process has become a significant challenge. The primary\nissue lies in generating comprehensive, accurate, and reasoning-consistent\nreview comments that align with human reviewers' judgments. In this paper, we\naddress this challenge by proposing ReviewAgents, a framework that leverages\nlarge language models (LLMs) to generate academic paper reviews. We first\nintroduce a novel dataset, Review-CoT, consisting of 142k review comments,\ndesigned for training LLM agents. This dataset emulates the structured\nreasoning process of human reviewers-summarizing the paper, referencing\nrelevant works, identifying strengths and weaknesses, and generating a review\nconclusion. Building upon this, we train LLM reviewer agents capable of\nstructured reasoning using a relevant-paper-aware training method. Furthermore,\nwe construct ReviewAgents, a multi-role, multi-LLM agent review framework, to\nenhance the review comment generation process. Additionally, we propose\nReviewBench, a benchmark for evaluating the review comments generated by LLMs.\nOur experimental results on ReviewBench demonstrate that while existing LLMs\nexhibit a certain degree of potential for automating the review process, there\nremains a gap when compared to human-generated reviews. Moreover, our\nReviewAgents framework further narrows this gap, outperforming advanced LLMs in\ngenerating review comments."
                },
                "authors": [
                    {
                        "name": "Xian Gao"
                    },
                    {
                        "name": "Jiacheng Ruan"
                    },
                    {
                        "name": "Zongyun Zhang"
                    },
                    {
                        "name": "Jingsheng Gao"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Yuzhuo Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhuo Fu"
                },
                "author": "Yuzhuo Fu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08506v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08506v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00691v2",
                "updated": "2025-07-16T07:55:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    55,
                    10,
                    2,
                    197,
                    0
                ],
                "published": "2025-01-01T01:06:58Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    1,
                    6,
                    58,
                    2,
                    1,
                    0
                ],
                "title": "Labels Generated by Large Language Models Help Measure People's Empathy\n  in Vitro",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Labels Generated by Large Language Models Help Measure People's Empathy\n  in Vitro"
                },
                "summary": "Large language models (LLMs) have revolutionised many fields, with\nLLM-as-a-service (LLMSaaS) offering accessible, general-purpose solutions\nwithout costly task-specific training. In contrast to the widely studied prompt\nengineering for directly solving tasks (in vivo), this paper explores LLMs'\npotential for in-vitro applications: using LLM-generated labels to improve\nsupervised training of mainstream models. We examine two strategies - (1) noisy\nlabel correction and (2) training data augmentation - in empathy computing, an\nemerging task to predict psychology-based questionnaire outcomes from inputs\nlike textual narratives. Crowdsourced datasets in this domain often suffer from\nnoisy labels that misrepresent underlying empathy. We show that replacing or\nsupplementing these crowdsourced labels with LLM-generated labels, developed\nusing psychology-based scale-aware prompts, achieves statistically significant\naccuracy improvements. Notably, the RoBERTa pre-trained language model (PLM)\ntrained with noise-reduced labels yields a state-of-the-art Pearson correlation\ncoefficient of 0.648 on the public NewsEmp benchmarks. This paper further\nanalyses evaluation metric selection and demographic biases to help guide the\nfuture development of more equitable empathy computing models. Code and\nLLM-generated labels are available at\nhttps://github.com/hasan-rakibul/LLMPathy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionised many fields, with\nLLM-as-a-service (LLMSaaS) offering accessible, general-purpose solutions\nwithout costly task-specific training. In contrast to the widely studied prompt\nengineering for directly solving tasks (in vivo), this paper explores LLMs'\npotential for in-vitro applications: using LLM-generated labels to improve\nsupervised training of mainstream models. We examine two strategies - (1) noisy\nlabel correction and (2) training data augmentation - in empathy computing, an\nemerging task to predict psychology-based questionnaire outcomes from inputs\nlike textual narratives. Crowdsourced datasets in this domain often suffer from\nnoisy labels that misrepresent underlying empathy. We show that replacing or\nsupplementing these crowdsourced labels with LLM-generated labels, developed\nusing psychology-based scale-aware prompts, achieves statistically significant\naccuracy improvements. Notably, the RoBERTa pre-trained language model (PLM)\ntrained with noise-reduced labels yields a state-of-the-art Pearson correlation\ncoefficient of 0.648 on the public NewsEmp benchmarks. This paper further\nanalyses evaluation metric selection and demographic biases to help guide the\nfuture development of more equitable empathy computing models. Code and\nLLM-generated labels are available at\nhttps://github.com/hasan-rakibul/LLMPathy."
                },
                "authors": [
                    {
                        "name": "Md Rakibul Hasan"
                    },
                    {
                        "name": "Yue Yao"
                    },
                    {
                        "name": "Md Zakir Hossain"
                    },
                    {
                        "name": "Aneesh Krishna"
                    },
                    {
                        "name": "Imre Rudas"
                    },
                    {
                        "name": "Shafin Rahman"
                    },
                    {
                        "name": "Tom Gedeon"
                    }
                ],
                "author_detail": {
                    "name": "Tom Gedeon"
                },
                "author": "Tom Gedeon",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12000v2",
                "updated": "2025-07-17T02:34:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    2,
                    34,
                    42,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T07:55:06Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    55,
                    6,
                    2,
                    197,
                    0
                ],
                "title": "DSSD: Efficient Edge-Device LLM Deployment and Collaborative Inference\n  via Distributed Split Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSSD: Efficient Edge-Device LLM Deployment and Collaborative Inference\n  via Distributed Split Speculative Decoding"
                },
                "summary": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing"
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11997v1",
                "updated": "2025-07-16T07:50:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    50,
                    43,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T07:50:43Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    50,
                    43,
                    2,
                    197,
                    0
                ],
                "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection"
                },
                "summary": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods."
                },
                "authors": [
                    {
                        "name": "Tairan Huang"
                    },
                    {
                        "name": "Yili Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yili Wang"
                },
                "author": "Yili Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00548v2",
                "updated": "2025-07-16T07:46:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    46,
                    59,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-01T08:13:00Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    13,
                    0,
                    1,
                    182,
                    0
                ],
                "title": "A waveform and time digitization mainboard prototype for the hybrid\n  digital optical module of TRIDENT neutrino experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A waveform and time digitization mainboard prototype for the hybrid\n  digital optical module of TRIDENT neutrino experiment"
                },
                "summary": "The TRIDENT (Tropical Deep-sea Neutrino Telescope) experiment is a\nnext-generation underwater neutrino observatory planned for deployment in the\nWest Pacific Ocean, designed to detect astrophysical neutrinos through\nCherenkov radiation. The full-scale detector will consist of approximately 1000\nvertical strings, each equipped with 20 hybrid digital optical modules (hDOMs)\ncontaining both photomultiplier tubes (PMTs) and silicon photomultipliers\n(SiPMs) for comprehensive light detection. This paper presents a\ncustom-designed digitization mainboard prototype for the hDOM, featuring\nsimultaneous 32-channel PMT waveform digitization at 125 MS/s using commercial\nanalog-to-digital converters (ADCs) and 56-channel high-precision time\nmeasurement through field-programmable gate array (FPGA)-implemented\ntime-to-digital converters (TDCs). The system demonstrates excellent\nperformance in single photoelectron (SPE) detection with clear pedestal\nseparation, maintains linear response up to 240 PEs, and achieves\nsub-nanosecond timing resolution for PMT or SiPM pulse edges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The TRIDENT (Tropical Deep-sea Neutrino Telescope) experiment is a\nnext-generation underwater neutrino observatory planned for deployment in the\nWest Pacific Ocean, designed to detect astrophysical neutrinos through\nCherenkov radiation. The full-scale detector will consist of approximately 1000\nvertical strings, each equipped with 20 hybrid digital optical modules (hDOMs)\ncontaining both photomultiplier tubes (PMTs) and silicon photomultipliers\n(SiPMs) for comprehensive light detection. This paper presents a\ncustom-designed digitization mainboard prototype for the hDOM, featuring\nsimultaneous 32-channel PMT waveform digitization at 125 MS/s using commercial\nanalog-to-digital converters (ADCs) and 56-channel high-precision time\nmeasurement through field-programmable gate array (FPGA)-implemented\ntime-to-digital converters (TDCs). The system demonstrates excellent\nperformance in single photoelectron (SPE) detection with clear pedestal\nseparation, maintains linear response up to 240 PEs, and achieves\nsub-nanosecond timing resolution for PMT or SiPM pulse edges."
                },
                "authors": [
                    {
                        "name": "Guangping Zhang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Donglian Xu"
                    }
                ],
                "author_detail": {
                    "name": "Donglian Xu"
                },
                "author": "Donglian Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04715v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04715v6",
                "updated": "2025-07-16T07:46:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    46,
                    43,
                    2,
                    197,
                    0
                ],
                "published": "2025-03-06T18:58:29Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    18,
                    58,
                    29,
                    3,
                    65,
                    0
                ],
                "title": "Predictable Scale: Part I, Step Law -- Optimal Hyperparameter Scaling\n  Law in Large Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictable Scale: Part I, Step Law -- Optimal Hyperparameter Scaling\n  Law in Large Language Model Pretraining"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well\\text{-}established, yet their effective deployment\nnecessitates careful hyperparameter optimization. Although existing methods\nhave explored the influence of hyperparameters on model performance, a\nprincipled and generalizable framework across model architectures and data\nrecipes remains absent. In this study, we conduct an unprecedented empirical\ninvestigation\\text{-} training over 3,700 LLMs from scratch across 100 trillion\ntokens, consuming nearly one million NVIDIA H800 GPU hours to establish a\nuniversal Scaling Law for hyperparameter optimization in LLM Pre-training,\ncalled \\textbf{Step Law}. We empirically observe that, under fixed model size\n($N$) and dataset size ($D$), the hyperparameter landscape exhibits convexity\nwith a broad optimum, substantially reducing the complexity of hyperparameter\nsearch. Building on this insight, we formally define and empirically validate\nthe Step Law: The optimal learning rate follows a power-law relationship with\n$N$ and $D$, while the optimal batch size is primarily influenced by $D$ and\nremains largely invariant to $N$.Notably, our estimated optima deviate from the\nglobal best performance found via exhaustive search by merely \\textbf{0.094\\%}\non the test set. To our best known, Step Law is the \\textbf{first} that unifies\ndifferent model shapes and structures, such as Mixture-of-Experts models and\ndense transformers, as well as establishes optimal hyperparameter scaling laws\nacross diverse data recipes. We contribute a universal, plug-and-play optimal\nhyperparameter tool for the community, which is expected to advance efficient\nLLM training at scale. All experimental code, data and checkpoints are publicly\navailable at\n\\href{https://github.com/step-law/steplaw}{https://github.com/step-law/steplaw}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well\\text{-}established, yet their effective deployment\nnecessitates careful hyperparameter optimization. Although existing methods\nhave explored the influence of hyperparameters on model performance, a\nprincipled and generalizable framework across model architectures and data\nrecipes remains absent. In this study, we conduct an unprecedented empirical\ninvestigation\\text{-} training over 3,700 LLMs from scratch across 100 trillion\ntokens, consuming nearly one million NVIDIA H800 GPU hours to establish a\nuniversal Scaling Law for hyperparameter optimization in LLM Pre-training,\ncalled \\textbf{Step Law}. We empirically observe that, under fixed model size\n($N$) and dataset size ($D$), the hyperparameter landscape exhibits convexity\nwith a broad optimum, substantially reducing the complexity of hyperparameter\nsearch. Building on this insight, we formally define and empirically validate\nthe Step Law: The optimal learning rate follows a power-law relationship with\n$N$ and $D$, while the optimal batch size is primarily influenced by $D$ and\nremains largely invariant to $N$.Notably, our estimated optima deviate from the\nglobal best performance found via exhaustive search by merely \\textbf{0.094\\%}\non the test set. To our best known, Step Law is the \\textbf{first} that unifies\ndifferent model shapes and structures, such as Mixture-of-Experts models and\ndense transformers, as well as establishes optimal hyperparameter scaling laws\nacross diverse data recipes. We contribute a universal, plug-and-play optimal\nhyperparameter tool for the community, which is expected to advance efficient\nLLM training at scale. All experimental code, data and checkpoints are publicly\navailable at\n\\href{https://github.com/step-law/steplaw}{https://github.com/step-law/steplaw}."
                },
                "authors": [
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shijie Xuyang"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Zhenyu Ding"
                    },
                    {
                        "name": "Haoying Wang"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04715v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04715v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11988v2",
                "updated": "2025-07-17T03:34:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    3,
                    34,
                    27,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-16T07:38:28Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    38,
                    28,
                    2,
                    197,
                    0
                ],
                "title": "Aime: Towards Fully-Autonomous Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aime: Towards Fully-Autonomous Multi-Agent Framework"
                },
                "summary": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Yexuan Shi"
                    },
                    {
                        "name": "Mingyu Wang"
                    },
                    {
                        "name": "Yunxiang Cao"
                    },
                    {
                        "name": "Hongjie Lai"
                    },
                    {
                        "name": "Junjian Lan"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Jie Geng"
                    },
                    {
                        "name": "Zhenan Li"
                    },
                    {
                        "name": "Zihao Xia"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Wenbo Duan"
                    },
                    {
                        "name": "Yuanshuo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanshuo Zhu"
                },
                "author": "Yuanshuo Zhu",
                "arxiv_comment": "14 pages, 1 figures,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11987v1",
                "updated": "2025-07-16T07:37:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    37,
                    23,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T07:37:23Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    37,
                    23,
                    2,
                    197,
                    0
                ],
                "title": "Formal Verification of Neural Certificates Done Dynamically",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Neural Certificates Done Dynamically"
                },
                "summary": "Neural certificates have emerged as a powerful tool in cyber-physical systems\ncontrol, providing witnesses of correctness. These certificates, such as\nbarrier functions, often learned alongside control policies, once verified,\nserve as mathematical proofs of system safety. However, traditional formal\nverification of their defining conditions typically faces scalability\nchallenges due to exhaustive state-space exploration. To address this\nchallenge, we propose a lightweight runtime monitoring framework that\nintegrates real-time verification and does not require access to the underlying\ncontrol policy. Our monitor observes the system during deployment and performs\non-the-fly verification of the certificate over a lookahead region to ensure\nsafety within a finite prediction horizon. We instantiate this framework for\nReLU-based control barrier functions and demonstrate its practical\neffectiveness in a case study. Our approach enables timely detection of safety\nviolations and incorrect certificates with minimal overhead, providing an\neffective but lightweight alternative to the static verification of the\ncertificates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural certificates have emerged as a powerful tool in cyber-physical systems\ncontrol, providing witnesses of correctness. These certificates, such as\nbarrier functions, often learned alongside control policies, once verified,\nserve as mathematical proofs of system safety. However, traditional formal\nverification of their defining conditions typically faces scalability\nchallenges due to exhaustive state-space exploration. To address this\nchallenge, we propose a lightweight runtime monitoring framework that\nintegrates real-time verification and does not require access to the underlying\ncontrol policy. Our monitor observes the system during deployment and performs\non-the-fly verification of the certificate over a lookahead region to ensure\nsafety within a finite prediction horizon. We instantiate this framework for\nReLU-based control barrier functions and demonstrate its practical\neffectiveness in a case study. Our approach enables timely detection of safety\nviolations and incorrect certificates with minimal overhead, providing an\neffective but lightweight alternative to the static verification of the\ncertificates."
                },
                "authors": [
                    {
                        "name": "Thomas A. Henzinger"
                    },
                    {
                        "name": "Konstantin Kueffner"
                    },
                    {
                        "name": "Emily Yu"
                    }
                ],
                "author_detail": {
                    "name": "Emily Yu"
                },
                "author": "Emily Yu",
                "arxiv_comment": "Accepted at RV'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11078v2",
                "updated": "2025-07-16T07:26:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    26,
                    24,
                    2,
                    197,
                    0
                ],
                "published": "2025-02-16T11:02:37Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    2,
                    37,
                    6,
                    47,
                    0
                ],
                "title": "DEEPER Insight into Your User: Directed Persona Refinement for Dynamic\n  Persona Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEEPER Insight into Your User: Directed Persona Refinement for Dynamic\n  Persona Modeling"
                },
                "summary": "To advance personalized applications such as recommendation systems and user\nbehavior prediction, recent research increasingly adopts large language models\n(LLMs) for human -readable persona modeling. In dynamic real -world scenarios,\neffective persona modeling necessitates leveraging streaming behavior data to\ncontinually optimize user personas. However, existing methods -whether\nregenerating personas or incrementally extending them with new behaviors -often\nfail to achieve sustained improvements in persona quality or future behavior\nprediction accuracy. To address this, we propose DEEPER, a novel approach for\ndynamic persona modeling that enables continual persona optimization.\nSpecifically, we enhance the model's direction -search capability through an\niterative reinforcement learning framework, allowing it to automatically\nidentify effective update directions and optimize personas using discrepancies\nbetween user behaviors and model predictions. Extensive experiments on dynamic\npersona modeling involving 4800 users across 10 domains highlight the superior\npersona optimization capabilities of DEEPER, delivering an impressive 32.2%\naverage reduction in user behavior prediction error over four update rounds\n-outperforming the best baseline by a remarkable 22.92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To advance personalized applications such as recommendation systems and user\nbehavior prediction, recent research increasingly adopts large language models\n(LLMs) for human -readable persona modeling. In dynamic real -world scenarios,\neffective persona modeling necessitates leveraging streaming behavior data to\ncontinually optimize user personas. However, existing methods -whether\nregenerating personas or incrementally extending them with new behaviors -often\nfail to achieve sustained improvements in persona quality or future behavior\nprediction accuracy. To address this, we propose DEEPER, a novel approach for\ndynamic persona modeling that enables continual persona optimization.\nSpecifically, we enhance the model's direction -search capability through an\niterative reinforcement learning framework, allowing it to automatically\nidentify effective update directions and optimize personas using discrepancies\nbetween user behaviors and model predictions. Extensive experiments on dynamic\npersona modeling involving 4800 users across 10 domains highlight the superior\npersona optimization capabilities of DEEPER, delivering an impressive 32.2%\naverage reduction in user behavior prediction error over four update rounds\n-outperforming the best baseline by a remarkable 22.92%."
                },
                "authors": [
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Chengyu Du"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Jinghan Xu"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Zulong Chen"
                    },
                    {
                        "name": "Liangyue Li"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11981v1",
                "updated": "2025-07-16T07:25:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    25,
                    27,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T07:25:27Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    25,
                    27,
                    2,
                    197,
                    0
                ],
                "title": "Simplifications are Absolutists: How Simplified Language Reduces Word\n  Sense Awareness in LLM-Generated Definitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplifications are Absolutists: How Simplified Language Reduces Word\n  Sense Awareness in LLM-Generated Definitions"
                },
                "summary": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can provide accurate word definitions and\nexplanations for any context. However, the scope of the definition changes for\ndifferent target groups, like children or language learners. This is especially\nrelevant for homonyms, words with multiple meanings, where oversimplification\nmight risk information loss by omitting key senses, potentially misleading\nusers who trust LLM outputs. We investigate how simplification impacts homonym\ndefinition quality across three target groups: Normal, Simple, and ELI5. Using\ntwo novel evaluation datasets spanning multiple languages, we test DeepSeek v3,\nLlama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge\nand human annotations. Our results show that simplification drastically\ndegrades definition completeness by neglecting polysemy, increasing the risk of\nmisunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization\nsubstantially improves homonym response quality across all prompt types. These\nfindings highlight the need to balance simplicity and completeness in\neducational NLP to ensure reliable, context-aware definitions for all learners."
                },
                "authors": [
                    {
                        "name": "Lukas Ellinger"
                    },
                    {
                        "name": "Miriam Anschtz"
                    },
                    {
                        "name": "Georg Groh"
                    }
                ],
                "author_detail": {
                    "name": "Georg Groh"
                },
                "author": "Georg Groh",
                "arxiv_comment": "Accepted by RANLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11979v1",
                "updated": "2025-07-16T07:21:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    21,
                    59,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T07:21:59Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    21,
                    59,
                    2,
                    197,
                    0
                ],
                "title": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation\n  of Trust and Interpersonal Closeness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Based Large Language Model Agent Simulation for Mutual Evaluation\n  of Trust and Interpersonal Closeness"
                },
                "summary": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as powerful tools for simulating\ncomplex social phenomena using human-like agents with specific traits. In human\nsocieties, value similarity is important for building trust and close\nrelationships; however, it remains unexplored whether this principle holds true\nin artificial societies comprising LLM agents. Therefore, this study\ninvestigates the influence of value similarity on relationship-building among\nLLM agents through two experiments. First, in a preliminary experiment, we\nevaluated the controllability of values in LLMs to identify the most effective\nmodel and prompt design for controlling the values. Subsequently, in the main\nexperiment, we generated pairs of LLM agents imbued with specific values and\nanalyzed their mutual evaluations of trust and interpersonal closeness\nfollowing a dialogue. The experiments were conducted in English and Japanese to\ninvestigate language dependence. The results confirmed that pairs of agents\nwith higher value similarity exhibited greater mutual trust and interpersonal\ncloseness. Our findings demonstrate that the LLM agent simulation serves as a\nvalid testbed for social science theories, contributes to elucidating the\nmechanisms by which values influence relationship building, and provides a\nfoundation for inspiring new theories and insights into the social sciences."
                },
                "authors": [
                    {
                        "name": "Yuki Sakamoto"
                    },
                    {
                        "name": "Takahisa Uchida"
                    },
                    {
                        "name": "Hiroshi Ishiguro"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshi Ishiguro"
                },
                "author": "Hiroshi Ishiguro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11974v1",
                "updated": "2025-07-16T07:16:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    16,
                    51,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T07:16:51Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    16,
                    51,
                    2,
                    197,
                    0
                ],
                "title": "A Review of Generative AI in Aquaculture: Foundations, Applications, and\n  Future Directions for Smart and Sustainable Farming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Generative AI in Aquaculture: Foundations, Applications, and\n  Future Directions for Smart and Sustainable Farming"
                },
                "summary": "Generative Artificial Intelligence (GAI) has rapidly emerged as a\ntransformative force in aquaculture, enabling intelligent synthesis of\nmultimodal data, including text, images, audio, and simulation outputs for\nsmarter, more adaptive decision-making. As the aquaculture industry shifts\ntoward data-driven, automation and digital integration operations under the\nAquaculture 4.0 paradigm, GAI models offer novel opportunities across\nenvironmental monitoring, robotics, disease diagnostics, infrastructure\nplanning, reporting, and market analysis. This review presents the first\ncomprehensive synthesis of GAI applications in aquaculture, encompassing\nfoundational architectures (e.g., diffusion models, transformers, and retrieval\naugmented generation), experimental systems, pilot deployments, and real-world\nuse cases. We highlight GAI's growing role in enabling underwater perception,\ndigital twin modeling, and autonomous planning for remotely operated vehicle\n(ROV) missions. We also provide an updated application taxonomy that spans\nsensing, control, optimization, communication, and regulatory compliance.\nBeyond technical capabilities, we analyze key limitations, including limited\ndata availability, real-time performance constraints, trust and explainability,\nenvironmental costs, and regulatory uncertainty. This review positions GAI not\nmerely as a tool but as a critical enabler of smart, resilient, and\nenvironmentally aligned aquaculture systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GAI) has rapidly emerged as a\ntransformative force in aquaculture, enabling intelligent synthesis of\nmultimodal data, including text, images, audio, and simulation outputs for\nsmarter, more adaptive decision-making. As the aquaculture industry shifts\ntoward data-driven, automation and digital integration operations under the\nAquaculture 4.0 paradigm, GAI models offer novel opportunities across\nenvironmental monitoring, robotics, disease diagnostics, infrastructure\nplanning, reporting, and market analysis. This review presents the first\ncomprehensive synthesis of GAI applications in aquaculture, encompassing\nfoundational architectures (e.g., diffusion models, transformers, and retrieval\naugmented generation), experimental systems, pilot deployments, and real-world\nuse cases. We highlight GAI's growing role in enabling underwater perception,\ndigital twin modeling, and autonomous planning for remotely operated vehicle\n(ROV) missions. We also provide an updated application taxonomy that spans\nsensing, control, optimization, communication, and regulatory compliance.\nBeyond technical capabilities, we analyze key limitations, including limited\ndata availability, real-time performance constraints, trust and explainability,\nenvironmental costs, and regulatory uncertainty. This review positions GAI not\nmerely as a tool but as a critical enabler of smart, resilient, and\nenvironmentally aligned aquaculture systems."
                },
                "authors": [
                    {
                        "name": "Waseem Akram"
                    },
                    {
                        "name": "Muhayy Ud Din"
                    },
                    {
                        "name": "Lyes Saad Soud"
                    },
                    {
                        "name": "Irfan Hussain"
                    }
                ],
                "author_detail": {
                    "name": "Irfan Hussain"
                },
                "author": "Irfan Hussain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11972v1",
                "updated": "2025-07-16T07:15:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    15,
                    59,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T07:15:59Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    15,
                    59,
                    2,
                    197,
                    0
                ],
                "title": "Graph Representations for Reading Comprehension Analysis using Large\n  Language Model and Eye-Tracking Biomarker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Representations for Reading Comprehension Analysis using Large\n  Language Model and Eye-Tracking Biomarker"
                },
                "summary": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies."
                },
                "authors": [
                    {
                        "name": "Yuhong Zhang"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Shilai Yang"
                    },
                    {
                        "name": "Yuchen Xu"
                    },
                    {
                        "name": "Gert Cauwenberghs"
                    },
                    {
                        "name": "Tzyy-Ping Jung"
                    }
                ],
                "author_detail": {
                    "name": "Tzyy-Ping Jung"
                },
                "author": "Tzyy-Ping Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06490v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06490v6",
                "updated": "2025-07-16T07:12:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    12,
                    33,
                    2,
                    197,
                    0
                ],
                "published": "2024-09-09T13:27:53Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    27,
                    53,
                    0,
                    253,
                    0
                ],
                "title": "UAVDB: Point-Guided Masks for UAV Detection and Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAVDB: Point-Guided Masks for UAV Detection and Segmentation"
                },
                "summary": "The widespread deployment of Unmanned Aerial Vehicles (UAVs) in surveillance,\nsecurity, and airspace monitoring demands accurate and scalable detection\nsolutions. However, progress is hindered by the lack of large-scale,\nhigh-resolution datasets with precise and cost-effective annotations. We\npresent UAVDB, a new benchmark dataset for UAV detection and segmentation,\nbuilt upon a point-guided weak supervision pipeline. As its foundation, UAVDB\nleverages trajectory point annotations and RGB video frames from the multi-view\ndrone tracking dataset, captured by fixed-camera setups. We introduce an\nefficient annotation method, Patch Intensity Convergence (PIC), which generates\nhigh-fidelity bounding boxes directly from these trajectory points, eliminating\nmanual labeling while maintaining accurate spatial localization. We further\nderive instance segmentation masks from these bounding boxes using the second\nversion of the Segment Anything Model (SAM2), enabling rich multi-task\nannotations with minimal supervision. UAVDB captures UAVs at diverse scales,\nfrom visible objects to near-single-pixel instances, under challenging\nenvironmental conditions. Particularly, PIC is lightweight and readily\npluggable into other point-guided scenarios, making it easy to scale up dataset\ngeneration across domains. We quantitatively compare PIC against existing\nannotation techniques, demonstrating superior Intersection over Union (IoU)\naccuracy and annotation efficiency. Finally, we benchmark several\nstate-of-the-art (SOTA) YOLO-series detectors on UAVDB, establishing strong\nbaselines for future research. The source code is available at\nhttps://github.com/wish44165/UAVDB .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of Unmanned Aerial Vehicles (UAVs) in surveillance,\nsecurity, and airspace monitoring demands accurate and scalable detection\nsolutions. However, progress is hindered by the lack of large-scale,\nhigh-resolution datasets with precise and cost-effective annotations. We\npresent UAVDB, a new benchmark dataset for UAV detection and segmentation,\nbuilt upon a point-guided weak supervision pipeline. As its foundation, UAVDB\nleverages trajectory point annotations and RGB video frames from the multi-view\ndrone tracking dataset, captured by fixed-camera setups. We introduce an\nefficient annotation method, Patch Intensity Convergence (PIC), which generates\nhigh-fidelity bounding boxes directly from these trajectory points, eliminating\nmanual labeling while maintaining accurate spatial localization. We further\nderive instance segmentation masks from these bounding boxes using the second\nversion of the Segment Anything Model (SAM2), enabling rich multi-task\nannotations with minimal supervision. UAVDB captures UAVs at diverse scales,\nfrom visible objects to near-single-pixel instances, under challenging\nenvironmental conditions. Particularly, PIC is lightweight and readily\npluggable into other point-guided scenarios, making it easy to scale up dataset\ngeneration across domains. We quantitatively compare PIC against existing\nannotation techniques, demonstrating superior Intersection over Union (IoU)\naccuracy and annotation efficiency. Finally, we benchmark several\nstate-of-the-art (SOTA) YOLO-series detectors on UAVDB, establishing strong\nbaselines for future research. The source code is available at\nhttps://github.com/wish44165/UAVDB ."
                },
                "authors": [
                    {
                        "name": "Yu-Hsi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Hsi Chen"
                },
                "author": "Yu-Hsi Chen",
                "arxiv_comment": "11 pages, 5 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06490v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06490v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13789v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13789v3",
                "updated": "2025-07-16T07:09:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    9,
                    5,
                    2,
                    197,
                    0
                ],
                "published": "2024-11-21T02:22:35Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    22,
                    35,
                    3,
                    326,
                    0
                ],
                "title": "LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display\n  Advertisement Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display\n  Advertisement Recommender System"
                },
                "summary": "Display advertising provides significant value to advertisers, publishers,\nand users. Traditional display advertising systems utilize a multi-stage\narchitecture consisting of retrieval, coarse ranking, and final ranking.\nHowever, conventional retrieval methods rely on ID-based learning to rank\nmechanisms and fail to adequately utilize the content information of ads, which\nhampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world\nknowledge of LLMs. However, three key challenges arise when attempting to\nmaximize the effectiveness of LLMs: \"How to capture user interests\", \"How to\nbridge the knowledge gap between LLMs and advertising system\", and \"How to\nefficiently deploy LLMs\". To overcome these challenges, we introduce a novel\nLLM-based framework called LLM Empowered Display ADvertisement REcommender\nsystem (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware\nPrompt Engineering introduces multi-faceted knowledge and designs intent-aware\n<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'\npersonal interests. (2) The Advertising-Specific Knowledge Alignment\nincorporates auxiliary fine-tuning tasks and Direct Preference Optimization\n(DPO) to align LLMs with ad semantic and business value. (3) The Efficient\nSystem Deployment deploys LEADRE in an online environment by integrating both\nlatency-tolerant and latency-sensitive service. Extensive offline experiments\ndemonstrate the effectiveness of LEADRE and validate the contributions of\nindividual modules. Online A/B test shows that LEADRE leads to a 1.57% and\n1.17% GMV lift for serviced users on WeChat Channels and Moments separately.\nLEADRE has been deployed on both platforms, serving tens of billions of\nrequests each day.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Display advertising provides significant value to advertisers, publishers,\nand users. Traditional display advertising systems utilize a multi-stage\narchitecture consisting of retrieval, coarse ranking, and final ranking.\nHowever, conventional retrieval methods rely on ID-based learning to rank\nmechanisms and fail to adequately utilize the content information of ads, which\nhampers their ability to provide diverse recommendation lists.\n  To address this limitation, we propose leveraging the extensive world\nknowledge of LLMs. However, three key challenges arise when attempting to\nmaximize the effectiveness of LLMs: \"How to capture user interests\", \"How to\nbridge the knowledge gap between LLMs and advertising system\", and \"How to\nefficiently deploy LLMs\". To overcome these challenges, we introduce a novel\nLLM-based framework called LLM Empowered Display ADvertisement REcommender\nsystem (LEADRE). LEADRE consists of three core modules: (1) The Intent-Aware\nPrompt Engineering introduces multi-faceted knowledge and designs intent-aware\n<Prompt, Response> pairs that fine-tune LLMs to generate ads tailored to users'\npersonal interests. (2) The Advertising-Specific Knowledge Alignment\nincorporates auxiliary fine-tuning tasks and Direct Preference Optimization\n(DPO) to align LLMs with ad semantic and business value. (3) The Efficient\nSystem Deployment deploys LEADRE in an online environment by integrating both\nlatency-tolerant and latency-sensitive service. Extensive offline experiments\ndemonstrate the effectiveness of LEADRE and validate the contributions of\nindividual modules. Online A/B test shows that LEADRE leads to a 1.57% and\n1.17% GMV lift for serviced users on WeChat Channels and Moments separately.\nLEADRE has been deployed on both platforms, serving tens of billions of\nrequests each day."
                },
                "authors": [
                    {
                        "name": "Fengxin Li"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Chao Zhou"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xiaoxiang Deng"
                    },
                    {
                        "name": "Wei Xue"
                    },
                    {
                        "name": "Dapeng Liu"
                    },
                    {
                        "name": "Lei Xiao"
                    },
                    {
                        "name": "Haijie Gu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Hongyan Liu"
                    },
                    {
                        "name": "Biao Qin"
                    },
                    {
                        "name": "Jun He"
                    }
                ],
                "author_detail": {
                    "name": "Jun He"
                },
                "author": "Jun He",
                "arxiv_comment": "Accepted by VLDB 2025 Industrial Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13789v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13789v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10972v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10972v3",
                "updated": "2025-07-16T07:09:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    9,
                    2,
                    2,
                    197,
                    0
                ],
                "published": "2025-06-12T17:59:23Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    17,
                    59,
                    23,
                    3,
                    163,
                    0
                ],
                "title": "Predictable Scale: Part II, Farseer: A Refined Scaling Law in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictable Scale: Part II, Farseer: A Refined Scaling Law in Large\n  Language Models"
                },
                "summary": "Training Large Language Models (LLMs) is prohibitively expensive, creating a\ncritical scaling gap where insights from small-scale experiments often fail to\ntransfer to resource-intensive production systems, thereby hindering efficient\ninnovation. To bridge this, we introduce Farseer, a novel and refined scaling\nlaw offering enhanced predictive accuracy across scales. By systematically\nconstructing a model loss surface $L(N,D)$, Farseer achieves a significantly\nbetter fit to empirical data than prior laws (e.g., Chinchilla's law). Our\nmethodology yields accurate, robust, and highly generalizable predictions,\ndemonstrating excellent extrapolation capabilities, improving upon Chinchilla's\nlaw by reducing extrapolation error by 433\\%. This allows for the reliable\nevaluation of competing training strategies across all $(N,D)$ settings,\nenabling conclusions from small-scale ablation studies to be confidently\nextrapolated to predict large-scale performance. Furthermore, Farseer provides\nnew insights into optimal compute allocation, better reflecting the nuanced\ndemands of modern LLM training. To validate our approach, we trained an\nextensive suite of approximately 1,000 LLMs across diverse scales and\nconfigurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are\ncomprehensively open-sourcing all models, data, results, and logs at\nhttps://github.com/Farseer-Scaling-Law/Farseer to foster further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) is prohibitively expensive, creating a\ncritical scaling gap where insights from small-scale experiments often fail to\ntransfer to resource-intensive production systems, thereby hindering efficient\ninnovation. To bridge this, we introduce Farseer, a novel and refined scaling\nlaw offering enhanced predictive accuracy across scales. By systematically\nconstructing a model loss surface $L(N,D)$, Farseer achieves a significantly\nbetter fit to empirical data than prior laws (e.g., Chinchilla's law). Our\nmethodology yields accurate, robust, and highly generalizable predictions,\ndemonstrating excellent extrapolation capabilities, improving upon Chinchilla's\nlaw by reducing extrapolation error by 433\\%. This allows for the reliable\nevaluation of competing training strategies across all $(N,D)$ settings,\nenabling conclusions from small-scale ablation studies to be confidently\nextrapolated to predict large-scale performance. Furthermore, Farseer provides\nnew insights into optimal compute allocation, better reflecting the nuanced\ndemands of modern LLM training. To validate our approach, we trained an\nextensive suite of approximately 1,000 LLMs across diverse scales and\nconfigurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are\ncomprehensively open-sourcing all models, data, results, and logs at\nhttps://github.com/Farseer-Scaling-Law/Farseer to foster further research."
                },
                "authors": [
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Zhenyu Ding"
                    },
                    {
                        "name": "Haoying Wang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shijie Xuyang"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "34",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10972v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11968v1",
                "updated": "2025-07-16T07:02:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    2,
                    15,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T07:02:15Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    7,
                    2,
                    15,
                    2,
                    197,
                    0
                ],
                "title": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on\n  Short Videos for Content Appropriateness Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on\n  Short Videos for Content Appropriateness Evaluation"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are increasingly used for content\nmoderation, yet their robustness in short-form video contexts remains\nunderexplored. Current safety evaluations often rely on unimodal attacks,\nfailing to address combined attack vulnerabilities. In this paper, we introduce\na comprehensive framework for evaluating the tri-modal safety of MLLMs. First,\nwe present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising\ndiverse short-form videos with human-guided synthetic adversarial attacks.\nSecond, we propose ChimeraBreak, a novel tri-modal attack strategy that\nsimultaneously challenges visual, auditory, and semantic reasoning pathways.\nExtensive experiments on state-of-the-art MLLMs reveal significant\nvulnerabilities with high Attack Success Rates (ASR). Our findings uncover\ndistinct failure modes, showing model biases toward misclassifying benign or\npolicy-violating content. We assess results using LLM-as-a-judge, demonstrating\nattack reasoning efficacy. Our dataset and findings provide crucial insights\nfor developing more robust and safe MLLMs."
                },
                "authors": [
                    {
                        "name": "Sahid Hossain Mustakim"
                    },
                    {
                        "name": "S M Jishanul Islam"
                    },
                    {
                        "name": "Ummay Maria Muna"
                    },
                    {
                        "name": "Montasir Chowdhury"
                    },
                    {
                        "name": "Mohammed Jawwadul Islam"
                    },
                    {
                        "name": "Sadia Ahmmed"
                    },
                    {
                        "name": "Tashfia Sikder"
                    },
                    {
                        "name": "Syed Tasdid Azam Dhrubo"
                    },
                    {
                        "name": "Swakkhar Shatabda"
                    }
                ],
                "author_detail": {
                    "name": "Swakkhar Shatabda"
                },
                "author": "Swakkhar Shatabda",
                "arxiv_comment": "Accepted as long paper, SVU Workshop at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11966v1",
                "updated": "2025-07-16T06:58:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    58,
                    2,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:58:02Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    58,
                    2,
                    2,
                    197,
                    0
                ],
                "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation"
                },
                "summary": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance."
                },
                "authors": [
                    {
                        "name": "Ziyu Ge"
                    },
                    {
                        "name": "Gabriel Chua"
                    },
                    {
                        "name": "Leanne Tan"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02445v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02445v5",
                "updated": "2025-07-16T06:56:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    56,
                    8,
                    2,
                    197,
                    0
                ],
                "published": "2025-03-04T09:40:00Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    40,
                    0,
                    1,
                    63,
                    0
                ],
                "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modeling"
                },
                "summary": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by up to 12% on MSE and 6% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by up to 12% on MSE and 6% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yu-Hao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Goran Nenadic"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "ICML 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02445v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02445v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22913v2",
                "updated": "2025-07-16T06:54:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    54,
                    34,
                    2,
                    197,
                    0
                ],
                "published": "2025-03-28T23:43:33Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    23,
                    43,
                    33,
                    4,
                    87,
                    0
                ],
                "title": "Resona: Improving Context Copying in Linear Recurrence Models with\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resona: Improving Context Copying in Linear Recurrence Models with\n  Retrieval"
                },
                "summary": "Recent shifts in the space of large language model (LLM) research have shown\nan increasing focus on novel architectures to compete with prototypical\nTransformer-based models that have long dominated this space. Linear recurrent\nmodels have proven to be a viable competitor due to their computational\nefficiency. However, such models still demonstrate a sizable gap compared to\nTransformers in terms of in-context learning among other tasks that require\nrecalling information from a context. In this work, we introduce Resona, a\nsimple and scalable framework for augmenting linear recurrent models with\nretrieval. Resona augments models with the ability to integrate retrieved\ninformation from the provided input context, enabling tailored behavior to\ndiverse task requirements. Experiments on a variety of linear recurrent models\ndemonstrate that Resona-augmented models observe significant performance gains\non a variety of synthetic as well as real-world natural language tasks,\nhighlighting its ability to act as a general purpose method to improve the\nin-context learning and language modeling abilities of linear recurrent LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent shifts in the space of large language model (LLM) research have shown\nan increasing focus on novel architectures to compete with prototypical\nTransformer-based models that have long dominated this space. Linear recurrent\nmodels have proven to be a viable competitor due to their computational\nefficiency. However, such models still demonstrate a sizable gap compared to\nTransformers in terms of in-context learning among other tasks that require\nrecalling information from a context. In this work, we introduce Resona, a\nsimple and scalable framework for augmenting linear recurrent models with\nretrieval. Resona augments models with the ability to integrate retrieved\ninformation from the provided input context, enabling tailored behavior to\ndiverse task requirements. Experiments on a variety of linear recurrent models\ndemonstrate that Resona-augmented models observe significant performance gains\non a variety of synthetic as well as real-world natural language tasks,\nhighlighting its ability to act as a general purpose method to improve the\nin-context learning and language modeling abilities of linear recurrent LLMs."
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Linrui Ma"
                    },
                    {
                        "name": "Jerry Huang"
                    },
                    {
                        "name": "Peng Lu"
                    },
                    {
                        "name": "Prasanna Parthasarathi"
                    },
                    {
                        "name": "Xiao-Wen Chang"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Yufei Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Cui"
                },
                "author": "Yufei Cui",
                "arxiv_comment": "Comments: Accepted at COLM 2025 (Conference on Learning with\n  Machines)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03034v2",
                "updated": "2025-07-16T06:52:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    52,
                    55,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-03T02:45:51Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    2,
                    45,
                    51,
                    3,
                    184,
                    0
                ],
                "title": "Rethinking Data Protection in the (Generative) Artificial Intelligence\n  Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Data Protection in the (Generative) Artificial Intelligence\n  Era"
                },
                "summary": "The (generative) artificial intelligence (AI) era has profoundly reshaped the\nmeaning and value of data. No longer confined to static content, data now\npermeates every stage of the AI lifecycle from the training samples that shape\nmodel parameters to the prompts and outputs that drive real-world model\ndeployment. This shift renders traditional notions of data protection\ninsufficient, while the boundaries of what needs safeguarding remain poorly\ndefined. Failing to safeguard data in AI systems can inflict societal and\nindividual, underscoring the urgent need to clearly delineate the scope of and\nrigorously enforce data protection. In this perspective, we propose a\nfour-level taxonomy, including non-usability, privacy preservation,\ntraceability, and deletability, that captures the diverse protection needs\narising in modern (generative) AI models and systems. Our framework offers a\nstructured understanding of the trade-offs between data utility and control,\nspanning the entire AI pipeline, including training datasets, model weights,\nsystem prompts, and AI-generated content. We analyze representative technical\napproaches at each level and reveal regulatory blind spots that leave critical\nassets exposed. By offering a structured lens to align future AI technologies\nand governance with trustworthy data practices, we underscore the urgency of\nrethinking data protection for modern AI techniques and provide timely guidance\nfor developers, researchers, and regulators alike.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The (generative) artificial intelligence (AI) era has profoundly reshaped the\nmeaning and value of data. No longer confined to static content, data now\npermeates every stage of the AI lifecycle from the training samples that shape\nmodel parameters to the prompts and outputs that drive real-world model\ndeployment. This shift renders traditional notions of data protection\ninsufficient, while the boundaries of what needs safeguarding remain poorly\ndefined. Failing to safeguard data in AI systems can inflict societal and\nindividual, underscoring the urgent need to clearly delineate the scope of and\nrigorously enforce data protection. In this perspective, we propose a\nfour-level taxonomy, including non-usability, privacy preservation,\ntraceability, and deletability, that captures the diverse protection needs\narising in modern (generative) AI models and systems. Our framework offers a\nstructured understanding of the trade-offs between data utility and control,\nspanning the entire AI pipeline, including training datasets, model weights,\nsystem prompts, and AI-generated content. We analyze representative technical\napproaches at each level and reveal regulatory blind spots that leave critical\nassets exposed. By offering a structured lens to align future AI technologies\nand governance with trustworthy data practices, we underscore the urgency of\nrethinking data protection for modern AI techniques and provide timely guidance\nfor developers, researchers, and regulators alike."
                },
                "authors": [
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Yu He"
                    },
                    {
                        "name": "Junfeng Guo"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "arxiv_comment": "Perspective paper for a broader scientific audience. The first two\n  authors contributed equally to this paper. 13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11959v1",
                "updated": "2025-07-16T06:44:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    44,
                    14,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:44:14Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    44,
                    14,
                    2,
                    197,
                    0
                ],
                "title": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoTPTQ: A Two-step Power-of-Two Post-training for LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language processing (NLP) tasks. However, their deployment is\nchallenging due to the substantial computational resources required.\nPower-of-two (PoT) quantization is a general tool to counteract this\ndifficulty. Albeit previous works on PoT quantization can be efficiently\ndequantized on CPUs using fixed-point addition, it showed less effectiveness on\nGPUs. The reason is entanglement of the sign bit and sequential bit\nmanipulations needed for dequantization. We propose a novel POT quantization\nframework for LLM weights that (i) outperforms state-of-the-art accuracy in\nextremely low-precision number formats, and (ii) enables faster inference\nthrough more efficient dequantization. To maintain the accuracy of the\nquantized model, we introduce a two-step post-training algorithm: (i)\ninitialize the quantization scales with a robust starting point, and (ii)\nrefine these scales using a minimal calibration set. The performance of our PoT\npost-training algorithm surpasses the current state-of-the-art in integer\nquantization, particularly at low precisions such as 2- and 3-bit formats. Our\nPoT quantization accelerates the dequantization step required for the floating\npoint inference and leads to $3.67\\times$ speed up on a NVIDIA V100, and\n$1.63\\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization."
                },
                "authors": [
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Vahid Partovi Nia"
                    },
                    {
                        "name": "Peng Lu"
                    },
                    {
                        "name": "Jerry Huang"
                    },
                    {
                        "name": "Xiao-Wen Chang"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Yufei Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Cui"
                },
                "author": "Yufei Cui",
                "arxiv_comment": "Accepted at ECAI 2025 (European Conference on Artificial\n  Intelligence)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11954v1",
                "updated": "2025-07-16T06:41:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    41,
                    3,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:41:03Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    41,
                    3,
                    2,
                    197,
                    0
                ],
                "title": "The benefits of query-based KGQA systems for complex and temporal\n  questions in LLM era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The benefits of query-based KGQA systems for complex and temporal\n  questions in LLM era"
                },
                "summary": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System"
                },
                "authors": [
                    {
                        "name": "Artem Alekseev"
                    },
                    {
                        "name": "Mikhail Chaichuk"
                    },
                    {
                        "name": "Miron Butko"
                    },
                    {
                        "name": "Alexander Panchenko"
                    },
                    {
                        "name": "Elena Tutubalina"
                    },
                    {
                        "name": "Oleg Somov"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Somov"
                },
                "author": "Oleg Somov",
                "arxiv_doi": "10.1007/978-3-031-97141-9_29",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97141-9_29",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.11954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 3 figures, 7 tables",
                "arxiv_journal_ref": "Lecture Notes in Computer Science, vol 15836. Springer, Cham.,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13026v2",
                "updated": "2025-07-16T06:31:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    31,
                    29,
                    2,
                    197,
                    0
                ],
                "published": "2025-03-17T10:29:08Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    10,
                    29,
                    8,
                    0,
                    76,
                    0
                ],
                "title": "HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with\n  Large Multimodal Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiMTok: Learning Hierarchical Mask Tokens for Image Segmentation with\n  Large Multimodal Model"
                },
                "summary": "The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of large multimodal models (LMMs) has attracted\nsignificant interest from the image segmentation community. To align with the\nnext-token-prediction paradigm, current LMM-driven segmentation methods either\nuse object boundary points to represent masks or introduce special segmentation\ntokens, whose hidden states are decoded by a segmentation model requiring the\noriginal image as input. However, these approaches often suffer from inadequate\nmask representation and complex architectures, limiting the potential of LMMs.\nIn this work, we propose the Hierarchical Mask Tokenizer (HiMTok), which\nrepresents segmentation masks with up to 32 tokens and eliminates the need for\nthe original image during mask de-tokenization. HiMTok allows for compact and\ncoarse-to-fine mask representations, aligning well with the LLM\nnext-token-prediction paradigm and facilitating the direct acquisition of\nsegmentation capabilities. We develop a 3-stage training recipe for progressive\nlearning of segmentation and visual capabilities, featuring a hierarchical mask\nloss for effective coarse-to-fine learning. Additionally, we enable\nbidirectional information flow, allowing conversion between bounding boxes and\nmask tokens to fully leverage multi-task training potential. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross various segmentation tasks,while also enhancing visual grounding and\nmaintaining overall visual understanding."
                },
                "authors": [
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Changxu Cheng"
                    },
                    {
                        "name": "Lingfeng Wang"
                    },
                    {
                        "name": "Senda Chen"
                    },
                    {
                        "name": "Wuyue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wuyue Zhao"
                },
                "author": "Wuyue Zhao",
                "arxiv_comment": "Accepted by ICCV 2025; the code is at\n  https://github.com/yayafengzi/LMM-HiMTok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11942v1",
                "updated": "2025-07-16T06:16:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    16,
                    6,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:16:06Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    16,
                    6,
                    2,
                    197,
                    0
                ],
                "title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt\n  Compression"
                },
                "summary": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Liu"
                },
                "author": "Guoming Liu",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11939v1",
                "updated": "2025-07-16T06:09:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    9,
                    2,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:09:02Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    9,
                    2,
                    2,
                    197,
                    0
                ],
                "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual\n  Chart Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual\n  Chart Question Answering"
                },
                "summary": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models."
                },
                "authors": [
                    {
                        "name": "Yichen Xu"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Qin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Qin Jin"
                },
                "author": "Qin Jin",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11935v1",
                "updated": "2025-07-16T05:58:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    5,
                    58,
                    45,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T05:58:45Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    5,
                    58,
                    45,
                    2,
                    197,
                    0
                ],
                "title": "Native-AI Empowered Scalable Architectures and Solutions for Future\n  Non-Terrestrial Networks: An Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Native-AI Empowered Scalable Architectures and Solutions for Future\n  Non-Terrestrial Networks: An Overview"
                },
                "summary": "As the path toward 6G networks is being charted, the emerging applications\nhave motivated evolutions of network architectures to realize the efficient,\nreliable, and flexible wireless networks. Among the potential architectures,\nthe non-terrestrial network (NTN) and open radio access network (ORAN) have\nreceived increasing interest from both academia and industry. Although the\ndeployment of NTNs ensures coverage, enhances spectral efficiency, and improves\nthe resilience of wireless networks. The high altitude and mobility of NTN\npresent new challenges in the development and operations (DevOps) lifecycle,\nhindering intelligent and scalable network management due to the lack of native\nartificial intelligence (AI) capability. With the advantages of ORAN in\ndisaggregation, openness, virtualization, and intelligence, several works\npropose integrating ORAN principles into the NTN, focusing mainly on ORAN\ndeployment options based on transparent and regenerative systems. However, a\nholistic view of how to effectively combine ORAN and NTN throughout the DevOps\nlifecycle is still missing, especially regarding how intelligent ORAN addresses\nthe scalability challenges in NTN. Motivated by this, in this paper, we first\nprovide the background knowledge about ORAN and NTN, outline the\nstate-of-the-art research on ORAN for NTNs, and present the DevOps challenges\nthat motivate the adoption of ORAN solutions. We then propose the ORAN-based\nNTN framework, discussing its features and architectures in detail. These\ninclude the discussion about flexible fronthaul split, RAN intelligent\ncontrollers (RICs) enhancement for distributed learning, scalable deployment\narchitecture, and multi-domain service management. Finally, the future research\ndirections, including combinations of the ORAN-based NTN framework and other\nenabling technologies and schemes, as well as the candidate use cases, are\nhighlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the path toward 6G networks is being charted, the emerging applications\nhave motivated evolutions of network architectures to realize the efficient,\nreliable, and flexible wireless networks. Among the potential architectures,\nthe non-terrestrial network (NTN) and open radio access network (ORAN) have\nreceived increasing interest from both academia and industry. Although the\ndeployment of NTNs ensures coverage, enhances spectral efficiency, and improves\nthe resilience of wireless networks. The high altitude and mobility of NTN\npresent new challenges in the development and operations (DevOps) lifecycle,\nhindering intelligent and scalable network management due to the lack of native\nartificial intelligence (AI) capability. With the advantages of ORAN in\ndisaggregation, openness, virtualization, and intelligence, several works\npropose integrating ORAN principles into the NTN, focusing mainly on ORAN\ndeployment options based on transparent and regenerative systems. However, a\nholistic view of how to effectively combine ORAN and NTN throughout the DevOps\nlifecycle is still missing, especially regarding how intelligent ORAN addresses\nthe scalability challenges in NTN. Motivated by this, in this paper, we first\nprovide the background knowledge about ORAN and NTN, outline the\nstate-of-the-art research on ORAN for NTNs, and present the DevOps challenges\nthat motivate the adoption of ORAN solutions. We then propose the ORAN-based\nNTN framework, discussing its features and architectures in detail. These\ninclude the discussion about flexible fronthaul split, RAN intelligent\ncontrollers (RICs) enhancement for distributed learning, scalable deployment\narchitecture, and multi-domain service management. Finally, the future research\ndirections, including combinations of the ORAN-based NTN framework and other\nenabling technologies and schemes, as well as the candidate use cases, are\nhighlighted."
                },
                "authors": [
                    {
                        "name": "Jikang Deng"
                    },
                    {
                        "name": "Fizza Hassan"
                    },
                    {
                        "name": "Hui Zhou"
                    },
                    {
                        "name": "Saad Al-Ahmadi"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    },
                    {
                        "name": "Daniel B. Da Costa"
                    }
                ],
                "author_detail": {
                    "name": "Daniel B. Da Costa"
                },
                "author": "Daniel B. Da Costa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11932v1",
                "updated": "2025-07-16T05:54:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    5,
                    54,
                    37,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T05:54:37Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    5,
                    54,
                    37,
                    2,
                    197,
                    0
                ],
                "title": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization\n  Capabilities of Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperphantasia: A Benchmark for Evaluating the Mental Visualization\n  Capabilities of Multimodal LLMs"
                },
                "summary": "Mental visualization, the ability to construct and manipulate visual\nrepresentations internally, is a core component of human cognition and plays a\nvital role in tasks involving reasoning, prediction, and abstraction. Despite\nthe rapid progress of Multimodal Large Language Models (MLLMs), current\nbenchmarks primarily assess passive visual perception, offering limited insight\ninto the more active capability of internally constructing visual patterns to\nsupport problem solving. Yet mental visualization is a critical cognitive skill\nin humans, supporting abilities such as spatial navigation, predicting physical\ntrajectories, and solving complex visual problems through imaginative\nsimulation. To bridge this gap, we introduce Hyperphantasia, a synthetic\nbenchmark designed to evaluate the mental visualization abilities of MLLMs\nthrough four carefully constructed puzzles. Each task is procedurally generated\nand presented at three difficulty levels, enabling controlled analysis of model\nperformance across increasing complexity. Our comprehensive evaluation of\nstate-of-the-art models reveals a substantial gap between the performance of\nhumans and MLLMs. Additionally, we explore the potential of reinforcement\nlearning to improve visual simulation capabilities. Our findings suggest that\nwhile some models exhibit partial competence in recognizing visual patterns,\nrobust mental visualization remains an open challenge for current MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental visualization, the ability to construct and manipulate visual\nrepresentations internally, is a core component of human cognition and plays a\nvital role in tasks involving reasoning, prediction, and abstraction. Despite\nthe rapid progress of Multimodal Large Language Models (MLLMs), current\nbenchmarks primarily assess passive visual perception, offering limited insight\ninto the more active capability of internally constructing visual patterns to\nsupport problem solving. Yet mental visualization is a critical cognitive skill\nin humans, supporting abilities such as spatial navigation, predicting physical\ntrajectories, and solving complex visual problems through imaginative\nsimulation. To bridge this gap, we introduce Hyperphantasia, a synthetic\nbenchmark designed to evaluate the mental visualization abilities of MLLMs\nthrough four carefully constructed puzzles. Each task is procedurally generated\nand presented at three difficulty levels, enabling controlled analysis of model\nperformance across increasing complexity. Our comprehensive evaluation of\nstate-of-the-art models reveals a substantial gap between the performance of\nhumans and MLLMs. Additionally, we explore the potential of reinforcement\nlearning to improve visual simulation capabilities. Our findings suggest that\nwhile some models exhibit partial competence in recognizing visual patterns,\nrobust mental visualization remains an open challenge for current MLLMs."
                },
                "authors": [
                    {
                        "name": "Mohammad Shahab Sepehri"
                    },
                    {
                        "name": "Berk Tinaz"
                    },
                    {
                        "name": "Zalan Fabian"
                    },
                    {
                        "name": "Mahdi Soltanolkotabi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Soltanolkotabi"
                },
                "author": "Mahdi Soltanolkotabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]