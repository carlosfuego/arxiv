[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.26730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26730v1",
                "updated": "2025-10-30T17:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:29:27Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference"
                },
                "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints."
                },
                "authors": [
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Runxin Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26692v1",
                "updated": "2025-10-30T16:59:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    43,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:59:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi Linear: An Expressive, Efficient Attention Architecture"
                },
                "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Chengyin Liu"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Weizhou Liu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Yizhi Zhang"
                    },
                    {
                        "name": "T. Y. Liu"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Shengjun Fang"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Zhengtao Wang"
                    },
                    {
                        "name": "Chao Hong"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Guanduo Chen"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Siyuan Pan"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Jiawen Tao"
                    },
                    {
                        "name": "Guohong Fu"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Yulun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Du"
                },
                "author": "Yulun Du",
                "arxiv_comment": "Kimi Linear tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20499v2",
                "updated": "2025-10-30T13:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    31,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-23T12:39:59Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming"
                },
                "summary": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset."
                },
                "authors": [
                    {
                        "name": "Akif Çördük"
                    },
                    {
                        "name": "Piotr Sielski"
                    },
                    {
                        "name": "Alice Boucher"
                    },
                    {
                        "name": "Kumar Aatish"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Aatish"
                },
                "author": "Kumar Aatish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26486v1",
                "updated": "2025-10-30T13:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks"
                },
                "summary": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    },
                    {
                        "name": "Guadalupe Correa-Cabrera"
                    }
                ],
                "author_detail": {
                    "name": "Guadalupe Correa-Cabrera"
                },
                "author": "Guadalupe Correa-Cabrera",
                "arxiv_comment": "Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25160v2",
                "updated": "2025-10-30T08:52:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    52,
                    17,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T04:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    29,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "Model-Document Protocol for AI Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Document Protocol for AI Search"
                },
                "summary": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v2",
                "updated": "2025-10-30T08:46:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    46,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "arxiv_comment": "Withdrawn by the authors to better delineate the related work from\n  the paper's original contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26234v1",
                "updated": "2025-10-30T08:12:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T08:12:53Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "title": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS"
                },
                "summary": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies."
                },
                "authors": [
                    {
                        "name": "Mathis Engelbart"
                    },
                    {
                        "name": "Mike Kosek"
                    },
                    {
                        "name": "Lars Eggert"
                    },
                    {
                        "name": "Jörg Ott"
                    }
                ],
                "author_detail": {
                    "name": "Jörg Ott"
                },
                "author": "Jörg Ott",
                "arxiv_doi": "10.1145/3772356.3772416",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772356.3772416",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.26234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "HotNets 2025",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25600v2",
                "updated": "2025-10-30T03:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    43,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T15:10:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    10,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models"
                },
                "summary": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation."
                },
                "authors": [
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Sihao Liu"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26104v1",
                "updated": "2025-10-30T03:30:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T03:30:12Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "title": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender"
                },
                "summary": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests."
                },
                "authors": [
                    {
                        "name": "Zhaoqi Zhang"
                    },
                    {
                        "name": "Haolei Pei"
                    },
                    {
                        "name": "Jun Guo"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Yufei Feng"
                    },
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v2",
                "updated": "2025-10-29T21:56:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    56,
                    19,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25979v1",
                "updated": "2025-10-29T21:26:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T21:26:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache"
                },
                "summary": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Shangye Chen"
                    },
                    {
                        "name": "Cyril Guyot"
                    },
                    {
                        "name": "Filip Blagojevic"
                    },
                    {
                        "name": "Hyeran Jeon"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "arxiv_affiliation": "University of California, Merced, USA",
                "author": "Dong Li",
                "arxiv_comment": "10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25977v1",
                "updated": "2025-10-29T21:22:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T21:22:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium"
                },
                "summary": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Jierui Xu"
                    },
                    {
                        "name": "Weichu Yang"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "arxiv_affiliation": "University of California, Merced",
                "author": "Dong Li",
                "arxiv_comment": "12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25695v1",
                "updated": "2025-10-29T17:00:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:00:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "title": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate"
                },
                "summary": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices."
                },
                "authors": [
                    {
                        "name": "Emerson J. Hollar"
                    },
                    {
                        "name": "Esmat Farzana"
                    }
                ],
                "author_detail": {
                    "name": "Esmat Farzana"
                },
                "author": "Esmat Farzana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25604v1",
                "updated": "2025-10-29T15:12:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:12:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Quickest Change Point Detection with Measurements over a Lossy Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quickest Change Point Detection with Measurements over a Lossy Link"
                },
                "summary": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime."
                },
                "authors": [
                    {
                        "name": "Krishna Chaythanya KV"
                    },
                    {
                        "name": "Saqib Abbas Baba"
                    },
                    {
                        "name": "Anurag Kumar"
                    },
                    {
                        "name": "Arpan Chattopadhyay"
                    },
                    {
                        "name": "Rajesh Sundaresan"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sundaresan"
                },
                "author": "Rajesh Sundaresan",
                "arxiv_comment": "17 pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25590v1",
                "updated": "2025-10-29T14:58:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:58:37Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
                },
                "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved."
                },
                "authors": [
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Bangyin Xiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "26 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v2",
                "updated": "2025-10-29T14:46:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    46,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hüger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25412v1",
                "updated": "2025-10-29T11:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:29:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Serve Programs, Not Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serve Programs, Not Prompts"
                },
                "summary": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3713082.3730398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25152v1",
                "updated": "2025-10-29T04:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T04:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "title": "Off-Centered WoS-Type Solvers with Statistical Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Centered WoS-Type Solvers with Statistical Weighting"
                },
                "summary": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems."
                },
                "authors": [
                    {
                        "name": "Anchang Bao"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Enya Shen"
                    },
                    {
                        "name": "Jianmin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Wang"
                },
                "author": "Jianmin Wang",
                "arxiv_comment": "SIGGRAPH Asia 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25122v1",
                "updated": "2025-10-29T03:00:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T03:00:36Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies"
                },
                "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Jiahong Chen"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Chuwei Cai"
                    },
                    {
                        "name": "Jinghui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jinghui Lu"
                },
                "author": "Jinghui Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24824v1",
                "updated": "2025-10-28T15:35:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:35:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling"
                },
                "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Xiang Luo"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Fan Xia"
                    },
                    {
                        "name": "Tianqi Zhang"
                    },
                    {
                        "name": "Hongrui Zhan"
                    },
                    {
                        "name": "Zheng Zhong"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Siyuan Qiao"
                    },
                    {
                        "name": "Xingyan Bin"
                    }
                ],
                "author_detail": {
                    "name": "Xingyan Bin"
                },
                "author": "Xingyan Bin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24359v1",
                "updated": "2025-10-28T12:28:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:28:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine"
                },
                "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual."
                },
                "authors": [
                    {
                        "name": "Pedram Fard"
                    },
                    {
                        "name": "Alaleh Azhir"
                    },
                    {
                        "name": "Neguine Rezaii"
                    },
                    {
                        "name": "Jiazi Tian"
                    },
                    {
                        "name": "Hossein Estiri"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Estiri"
                },
                "author": "Hossein Estiri",
                "arxiv_comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24273v1",
                "updated": "2025-10-28T10:32:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:32:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALS: Sparse Attention in Latent Space for KV cache Compression"
                },
                "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively."
                },
                "authors": [
                    {
                        "name": "Junlin Mu"
                    },
                    {
                        "name": "Hantao Huang"
                    },
                    {
                        "name": "Jihang Zhang"
                    },
                    {
                        "name": "Minghui Yu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24051v1",
                "updated": "2025-10-28T04:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T04:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Pie: A Programmable Serving System for Emerging LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: A Programmable Serving System for Emerging LLM Applications"
                },
                "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Zhiyao Ma"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3731569.3764814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.24051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v4",
                "updated": "2025-10-28T04:00:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    0,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v2",
                "updated": "2025-10-27T21:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    21,
                    48,
                    48,
                    0,
                    300,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12362v2",
                "updated": "2025-10-27T17:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    17,
                    31,
                    15,
                    0,
                    300,
                    0
                ],
                "published": "2024-04-18T17:45:19Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    45,
                    19,
                    3,
                    109,
                    0
                ],
                "title": "KV-weights are all you need for skipless transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-weights are all you need for skipless transformers"
                },
                "summary": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks."
                },
                "authors": [
                    {
                        "name": "Nils Graef"
                    }
                ],
                "author_detail": {
                    "name": "Nils Graef"
                },
                "author": "Nils Graef",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v3",
                "updated": "2025-10-27T16:20:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    20,
                    28,
                    0,
                    300,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_doi": "10.1145/3721462.3770776",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721462.3770776",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Middleware '25",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v2",
                "updated": "2025-10-27T14:59:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    59,
                    46,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving"
                },
                "summary": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Accepted in a computer science workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v2",
                "updated": "2025-10-27T11:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    11,
                    55,
                    7,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gaëtan Hadjeres"
                    },
                    {
                        "name": "Gaël Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_doi": "10.5334/TISMIR.251",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5334/TISMIR.251",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transactions of the International Society for Music Information\n  Retrieval, 8(1): 334-352 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22876v1",
                "updated": "2025-10-26T23:59:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec."
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22765v1",
                "updated": "2025-10-26T17:28:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T17:28:05Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval"
                },
                "summary": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Binxiao Xu"
                    },
                    {
                        "name": "Junyu Feng"
                    },
                    {
                        "name": "Ruichuan An"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v3",
                "updated": "2025-10-26T13:31:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    13,
                    31,
                    41,
                    6,
                    299,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22556v1",
                "updated": "2025-10-26T07:17:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T07:17:10Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size"
                },
                "summary": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length."
                },
                "authors": [
                    {
                        "name": "Jinhan Chen"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Shilong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shilong Wang"
                },
                "author": "Shilong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v3",
                "updated": "2025-10-26T04:25:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    4,
                    25,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23657v1",
                "updated": "2025-10-26T01:25:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T01:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "title": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops"
                },
                "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture."
                },
                "authors": [
                    {
                        "name": "Saklain Niam"
                    },
                    {
                        "name": "Tashfiqur Rahman"
                    },
                    {
                        "name": "Md. Amjad Patwary"
                    },
                    {
                        "name": "Mukarram Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Mukarram Hossain"
                },
                "author": "Mukarram Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22467v1",
                "updated": "2025-10-26T00:50:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T00:50:12Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "title": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints"
                },
                "summary": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore)."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v3",
                "updated": "2025-10-25T14:12:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    14,
                    12,
                    56,
                    5,
                    298,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "38 pages, 9 figures, 17 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23649v1",
                "updated": "2025-10-25T11:43:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models"
                },
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK."
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22145v1",
                "updated": "2025-10-25T03:34:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T03:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "title": "Fundamental Limits of Coded Caching with Fixed Subpacketization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Coded Caching with Fixed Subpacketization"
                },
                "summary": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Youlong Wu"
                    },
                    {
                        "name": "Jinyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyan Wang"
                },
                "author": "Jinyan Wang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v3",
                "updated": "2025-10-25T02:29:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    2,
                    29,
                    47,
                    5,
                    298,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v4",
                "updated": "2025-10-25T00:33:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    0,
                    33,
                    14,
                    5,
                    298,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22049v1",
                "updated": "2025-10-24T22:17:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T22:17:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "title": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders"
                },
                "summary": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users."
                },
                "authors": [
                    {
                        "name": "Zhimin Chen"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Ka Chun Mo"
                    },
                    {
                        "name": "Yunjiang Jiang"
                    },
                    {
                        "name": "Jane H. Lee"
                    },
                    {
                        "name": "Shouwei Chen"
                    },
                    {
                        "name": "Khushhall Chandra Mahajan"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Kai Ren"
                    },
                    {
                        "name": "Jinhui Li"
                    },
                    {
                        "name": "Wen-Yun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Yun Yang"
                },
                "author": "Wen-Yun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21696v1",
                "updated": "2025-10-24T17:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character"
                },
                "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Xibin Song"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Project page: https://wolfball.github.io/bachvid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20787v2",
                "updated": "2025-10-24T16:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    56,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T17:53:03Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    17,
                    53,
                    3,
                    3,
                    296,
                    0
                ],
                "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction"
                },
                "summary": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v2",
                "updated": "2025-10-24T14:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    55,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22922v2",
                "updated": "2025-10-24T11:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-28T15:15:31Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    15,
                    15,
                    31,
                    5,
                    179,
                    0
                ],
                "title": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling"
                },
                "summary": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass."
                },
                "authors": [
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "6 pages, 9 figures including tables. Short theoretical and practical\n  paper on improved dynamic programming for weighted job scheduling with\n  linear-time preprocessing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21361v1",
                "updated": "2025-10-24T11:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs."
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v6",
                "updated": "2025-10-24T08:41:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    41,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025.\n  https://neurips.cc/virtual/2025/poster/116615",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19240v2",
                "updated": "2025-10-24T08:35:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    35,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development"
                },
                "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."
                },
                "authors": [
                    {
                        "name": "Behnam Agahi"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v2",
                "updated": "2025-10-24T05:39:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    5,
                    39,
                    3,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v2",
                "updated": "2025-10-24T04:48:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    4,
                    48,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v5",
                "updated": "2025-10-23T23:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    23,
                    35,
                    32,
                    3,
                    296,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page:\n  https://github.com/tensorgi/TPA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v2",
                "updated": "2025-10-23T21:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    31,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v3",
                "updated": "2025-10-23T19:45:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    19,
                    45,
                    39,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    },
                    {
                        "name": "Panagiotis Karras"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Karras"
                },
                "author": "Panagiotis Karras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v3",
                "updated": "2025-10-23T18:52:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    18,
                    52,
                    25,
                    3,
                    296,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20707v1",
                "updated": "2025-10-23T16:17:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T16:17:47Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models"
                },
                "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Yuchao Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Our code is available at https://github.com/xuyang-liu16/MixKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v2",
                "updated": "2025-10-23T15:26:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    15,
                    26,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "arxiv_comment": "Accepted to ALENEX`26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v4",
                "updated": "2025-10-23T14:23:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    14,
                    23,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "35 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20878v1",
                "updated": "2025-10-23T12:28:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:28:58Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "title": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement"
                },
                "summary": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Danying Ge"
                    },
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Yixue Yang"
                    },
                    {
                        "name": "Weixing Ji"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Ji"
                },
                "author": "Weixing Ji",
                "arxiv_comment": "13 pages,16 figures,2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; E.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21865v1",
                "updated": "2025-10-23T10:35:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:35:35Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "title": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis"
                },
                "summary": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization."
                },
                "authors": [
                    {
                        "name": "F. I. Qowy"
                    }
                ],
                "author_detail": {
                    "name": "F. I. Qowy"
                },
                "author": "F. I. Qowy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20400v1",
                "updated": "2025-10-23T10:06:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:06:48Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels"
                },
                "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline."
                },
                "authors": [
                    {
                        "name": "Rubén Langarita"
                    },
                    {
                        "name": "Jesús Alastruey-Benedé"
                    },
                    {
                        "name": "Pablo Ibáñez-Marín"
                    },
                    {
                        "name": "Santiago Marco-Sola"
                    },
                    {
                        "name": "Miquel Moretó"
                    },
                    {
                        "name": "Adrià Armejach"
                    }
                ],
                "author_detail": {
                    "name": "Adrià Armejach"
                },
                "author": "Adrià Armejach",
                "arxiv_comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v2",
                "updated": "2025-10-23T09:55:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    55,
                    50,
                    3,
                    296,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "Accepted in TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v2",
                "updated": "2025-10-23T09:09:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    9,
                    15,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v2",
                "updated": "2025-10-23T08:29:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    29,
                    11,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20230v1",
                "updated": "2025-10-23T05:22:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T05:22:09Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "title": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$"
                },
                "summary": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Chenchao Xu"
                    },
                    {
                        "name": "Zhimian Wu"
                    },
                    {
                        "name": "Huachen Rao"
                    },
                    {
                        "name": "Zhaoyang Shan"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Guanghan Cao"
                    },
                    {
                        "name": "Michael Smidman"
                    },
                    {
                        "name": "Ming Shi"
                    },
                    {
                        "name": "Huiqiu Yuan"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Xianhui Chen"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Yu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yu Song"
                },
                "author": "Yu Song",
                "arxiv_comment": "submitted to journal in July 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v4",
                "updated": "2025-10-23T00:47:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    47,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "Accepted to ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v2",
                "updated": "2025-10-23T00:40:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    40,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v2",
                "updated": "2025-10-22T23:56:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    56,
                    45,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    }
                ],
                "author_detail": {
                    "name": "Fariha Tasmin"
                },
                "author": "Fariha Tasmin",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19670v1",
                "updated": "2025-10-22T15:16:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation"
                },
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "19 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.4; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v3",
                "updated": "2025-10-22T14:33:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    33,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19875v1",
                "updated": "2025-10-22T09:42:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:42:29Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention"
                },
                "summary": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "José Luis Redondo García"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Konstantina Palla"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24761v1",
                "updated": "2025-10-22T07:50:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    7,
                    50,
                    6,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T07:50:06Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    7,
                    50,
                    6,
                    2,
                    295,
                    0
                ],
                "title": "ODataX: A Progressive Evolution of the Open Data Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ODataX: A Progressive Evolution of the Open Data Protocol"
                },
                "summary": "The Open Data Protocol (OData) provides a standardized approach for building\nand consuming RESTful APIs with rich query capabilities. Despite its power and\nmaturity, OData adoption remains confined primarily to enterprise environments,\nparticularly within Microsoft and SAP ecosystems. This paper analyzes the key\nbarriers preventing wider OData adoption and introduces ODataX, an evolved\nversion of the protocol designed to address these limitations. ODataX maintains\nbackward compatibility with OData v4 while introducing progressive complexity\ndisclosure through simplified query syntax, built-in performance guardrails via\nquery cost estimation, and enhanced caching mechanisms. This work aims to\nbridge the gap between enterprise-grade query standardization and the\nsimplicity demanded by modern web development practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Data Protocol (OData) provides a standardized approach for building\nand consuming RESTful APIs with rich query capabilities. Despite its power and\nmaturity, OData adoption remains confined primarily to enterprise environments,\nparticularly within Microsoft and SAP ecosystems. This paper analyzes the key\nbarriers preventing wider OData adoption and introduces ODataX, an evolved\nversion of the protocol designed to address these limitations. ODataX maintains\nbackward compatibility with OData v4 while introducing progressive complexity\ndisclosure through simplified query syntax, built-in performance guardrails via\nquery cost estimation, and enhanced caching mechanisms. This work aims to\nbridge the gap between enterprise-grade query standardization and the\nsimplicity demanded by modern web development practices."
                },
                "authors": [
                    {
                        "name": "Anirudh Ganesh"
                    },
                    {
                        "name": "Nitin Sood"
                    }
                ],
                "author_detail": {
                    "name": "Nitin Sood"
                },
                "author": "Nitin Sood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19264v1",
                "updated": "2025-10-22T05:47:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T05:47:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAPRAD: LLM-Assisted PRotocol Attack Discovery"
                },
                "summary": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations."
                },
                "authors": [
                    {
                        "name": "R. Can Aygun"
                    },
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    },
                    {
                        "name": "Leonard Kleinrock"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Kleinrock"
                },
                "arxiv_affiliation": "UCLA",
                "author": "Leonard Kleinrock",
                "arxiv_comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)",
                "arxiv_journal_ref": "Published in IFIP Networking 2025 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19183v1",
                "updated": "2025-10-22T02:41:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:41:07Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning"
                },
                "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Fengyuan Sun"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Xinhao Xu"
                    },
                    {
                        "name": "Dandan Zheng"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19171v1",
                "updated": "2025-10-22T02:09:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:09:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG"
                },
                "summary": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference."
                },
                "authors": [
                    {
                        "name": "Jihwan Bang"
                    },
                    {
                        "name": "Juntae Lee"
                    },
                    {
                        "name": "Seunghan Yang"
                    },
                    {
                        "name": "Sungha Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sungha Choi"
                },
                "author": "Sungha Choi",
                "arxiv_comment": "Accepted at NeurIPS 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v3",
                "updated": "2025-10-21T22:37:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    22,
                    37,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v3",
                "updated": "2025-10-21T21:07:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    21,
                    7,
                    17,
                    1,
                    294,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "arxiv_doi": "10.1145/3772052.3772208",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772208",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the version accepted to ACM SoCC 2025. The title has been\n  updated to match the published version",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15878v2",
                "updated": "2025-10-21T16:32:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    32,
                    50,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-21T16:10:26Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    10,
                    26,
                    3,
                    233,
                    0
                ],
                "title": "Putting the Context back into Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putting the Context back into Memory"
                },
                "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices."
                },
                "authors": [
                    {
                        "name": "David A. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "David A. Roberts"
                },
                "author": "David A. Roberts",
                "arxiv_comment": "Fixed errors in paragraph numbering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18716v1",
                "updated": "2025-10-21T15:17:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:17:37Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Siyong Jian"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14576v3",
                "updated": "2025-10-21T15:13:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    13,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2024-01-26T00:27:00Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    0,
                    27,
                    0,
                    4,
                    26,
                    0
                ],
                "title": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints"
                },
                "summary": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings."
                },
                "authors": [
                    {
                        "name": "Steven W. D. Chien"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Artur Podobas"
                    },
                    {
                        "name": "Niclas Jansson"
                    },
                    {
                        "name": "Stefano Markidis"
                    },
                    {
                        "name": "Michio Honda"
                    }
                ],
                "author_detail": {
                    "name": "Michio Honda"
                },
                "author": "Michio Honda",
                "arxiv_doi": "10.1145/3772052.3772212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SoCC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18672v1",
                "updated": "2025-10-21T14:25:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:25:51Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study"
                },
                "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Junpan Wu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v1",
                "updated": "2025-10-21T12:39:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18546v1",
                "updated": "2025-10-21T11:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval"
                },
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon."
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v2",
                "updated": "2025-10-21T10:33:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    33,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching"
                },
                "summary": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v2",
                "updated": "2025-10-21T10:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    8,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18413v1",
                "updated": "2025-10-21T08:44:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T08:44:47Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference"
                },
                "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity."
                },
                "authors": [
                    {
                        "name": "Siyuan Yan"
                    },
                    {
                        "name": "Guo-Qing Jiang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    },
                    {
                        "name": "Ran Zhu"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Jingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Xu"
                },
                "author": "Jingwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v5",
                "updated": "2025-10-21T06:47:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    47,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was first submitted for review on Sept. 5, 2024, and the\n  initial version was uploaded to Arxiv on Sept. 30, 2024. The latest version\n  has accepted for publication by IEEE Transactions on Information Forensics\n  and Security (TIFS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v3",
                "updated": "2025-10-21T06:30:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    30,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Steffen Christgau"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18269v1",
                "updated": "2025-10-21T03:39:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T03:39:41Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "title": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding"
                },
                "summary": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth."
                },
                "authors": [
                    {
                        "name": "Xueyi Chen"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Kele Shao"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17777v1",
                "updated": "2025-10-20T17:35:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:35:47Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference"
                },
                "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability."
                },
                "authors": [
                    {
                        "name": "Samir Khaki"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Konstantinos N. Plataniotis"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17238v1",
                "updated": "2025-10-20T07:27:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17098v1",
                "updated": "2025-10-20T02:04:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T02:04:18Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models"
                },
                "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."
                },
                "authors": [
                    {
                        "name": "Elias Hossain"
                    },
                    {
                        "name": "Swayamjit Saha"
                    },
                    {
                        "name": "Somshubhra Roy"
                    },
                    {
                        "name": "Ravi Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Prasad"
                },
                "author": "Ravi Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17045v1",
                "updated": "2025-10-19T23:17:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T23:17:13Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    23,
                    17,
                    13,
                    6,
                    292,
                    0
                ],
                "title": "Video Reasoning without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Reasoning without Training"
                },
                "summary": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video reasoning using Large Multimodal Models (LMMs) relies on costly\nreinforcement learning (RL) and verbose chain-of-thought, resulting in\nsubstantial computational overhead during both training and inference.\nMoreover, the mechanisms that control the thinking process in these reasoning\nmodels are very limited. In this paper, using entropy of the model's output as\na signal, we discover that the high-quality models go through a series of\nmicro-explorations and micro-exploitations which keep the reasoning process\ngrounded (i.e., avoid excessive randomness while the model is exploring or\nthinking through an answer). We further observe that once this \"thinking\"\nprocess is over, more accurate models demonstrate a better convergence by\nreducing the entropy significantly via a final exploitation phase (i.e., a more\ncertain convergence towards a solution trajectory). We then use these novel,\ntheoretically-grounded insights to tune the model's behavior directly at\ninference, without using any RL or supervised fine-tuning. Specifically, during\ninference, our proposed approach called V-Reason (Video-Reason) adapts the\nvalue cache of the LMM via a few optimization steps on a small, trainable\ncontroller using an entropy-based objective, i.e., no supervision from any\ndataset or RL is necessary. This tuning improves the model's micro-exploration\nand exploitation behavior during inference. Our experiments show that our\nproposed method achieves significant improvements over the base\ninstruction-tuned models across several video reasoning datasets, narrowing the\ngap with RL-trained models to within 0.6% average accuracy without any\ntraining, while offering massive efficiency benefits: output tokens are reduced\nby 58.6% compared to the RL model."
                },
                "authors": [
                    {
                        "name": "Deepak Sridhar"
                    },
                    {
                        "name": "Kartikeya Bhardwaj"
                    },
                    {
                        "name": "Jeya Pradha Jeyaraj"
                    },
                    {
                        "name": "Nuno Vasconcelos"
                    },
                    {
                        "name": "Ankita Nayak"
                    },
                    {
                        "name": "Harris Teague"
                    }
                ],
                "author_detail": {
                    "name": "Harris Teague"
                },
                "author": "Harris Teague",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16871v1",
                "updated": "2025-10-19T15:13:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T15:13:25Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    15,
                    13,
                    25,
                    6,
                    292,
                    0
                ],
                "title": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addendum: Systematic Evaluation of Randomized Cache Designs against\n  Cache Occupancy"
                },
                "summary": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the main text published at USENIX Security 2025, we presented a systematic\nanalysis of the role of cache occupancy in the design considerations for\nrandomized caches (from the perspectives of performance and security). On the\nperformance front, we presented a uniform benchmarking strategy that allows for\na fair comparison among different randomized cache designs. Likewise, from the\nsecurity perspective, we presented three threat assumptions: (1) covert\nchannels; (2) process fingerprinting side-channel; and (3) AES key recovery.\nThe main takeaway of our work is an open problem of designing a randomized\ncache of comparable efficiency with modern set-associative LLCs, while still\nresisting both contention-based and occupancy-based attacks. This note is meant\nas an addendum to the main text in light of the observations made in [2]. To\nsummarize, the authors in [2] argue that (1) L1d cache size plays a role in\nadversarial success, and that (2) a patched version of MIRAGE with randomized\ninitial seeding of global eviction map prevents leakage of AES key. We discuss\nthe same in this addendum."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16805v1",
                "updated": "2025-10-19T12:16:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "published": "2025-10-19T12:16:40Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    16,
                    40,
                    6,
                    292,
                    0
                ],
                "title": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Precision Quantization for Language Models: Techniques and\n  Prospects"
                },
                "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid scaling of language models (LMs) has resulted in unprecedented\ncomputational, memory, and energy requirements, making their training and\ndeployment increasingly unsustainable. Quantization has emerged as an essential\ncompression technique to reduce model size, alleviate memory bottlenecks, and\naccelerate inference. However, while uniform low-bit quantization (e.g., INT8,\nINT4) provides significant efficiency gains, it can degrade accuracy in\nsensitive components of transformer-based LMs. Mixed-precision quantization\noffers a promising alternative by selectively allocating precision across\nlayers or within tensors to balance efficiency and accuracy. This survey\nprovides a comprehensive overview of Mixed-Precision quantization frameworks\nfor LMs (MXPLMs). We first review quantization fundamentals, including uniform\nand non-uniform quantizers, quantization granularity, and methods widely used\nin post-training quantization. We then categorize and compare recent MXPLM\nframeworks according to their bit allocation strategies and precision\nconfigurations across weights, activations, and key-value caches. A comparative\nanalysis highlights differences in perplexity, zero-shot task performance, and\ndeployment trade-offs. Furthermore, we contrast MXPLMs with earlier\nmixed-precision quantization methods for deep neural networks, identifying\nstrategies that transfer and those that face challenges in the LM setting.\nFinally, we summarize open issues and future directions, including\nhardware-aware design, activation quantization, and scalable optimization\nmethods for billion-parameter models. By consolidating recent advances, this\nwork serves as a reference for understanding the current landscape and research\nprospects of mixed-precision quantization for large-scale language models."
                },
                "authors": [
                    {
                        "name": "Mariam Rakka"
                    },
                    {
                        "name": "Marios Fournarakis"
                    },
                    {
                        "name": "Olga Krestinskaya"
                    },
                    {
                        "name": "Jinane Bazzi"
                    },
                    {
                        "name": "Khaled N. Salama"
                    },
                    {
                        "name": "Fadi Kurdahi"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "arxiv_comment": "46 pages, 6 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v2",
                "updated": "2025-10-18T11:29:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    11,
                    29,
                    52,
                    5,
                    291,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models. Nevertheless,\nminimizing the accuracy degradation caused by ultra-low-bit KV cache\nquantization remains a significant challenge. While scalar quantization is\nconstrained by 1-bit bound, vector quantization exploits intra-vector\ncorrelations and enables sub-bit regimes, making it more suitable for\nultra-low-bit quantization. To further mitigate quantization-induced\ndegradation, we reveal that the degradation is highly uneven across tokens in\nattention quality. To investigate this unevenness, we introduce anchor score to\nmeasure each token's sensitivity to quantization. Our analysis and experiments\nshow that preserving a small subset (1\\%) of tokens with the highest Anchor\nScore significantly mitigates accuracy loss under aggressive quantization.\n  We propose AnTKV, a dual-stage framework that leverages anchor token-aware\nvector quantization to compress the KV cache. It combines offline token-aware\ncentroids learning and online anchor token selection to balance compression and\naccuracy. To enable efficient deployment, we design an online anchor token\nselection kernel compatible with FlashAttention. It allows LLaMA3-8B to scale\nto 840K tokens on a single 80GB A100, while delivering up to $3.5\\times$ higher\ndecoding throughput over the FP16 baseline. Experiments demonstrate that AnTKV\nmatches or surpasses prior methods at 4-bit, and significantly reduce\nperplexity under ultra-low-bit quantization, achieving 6.32 at 1-bit on\nMistral-7B, compared to 7.25 for CQ and 15.36 for KVQuant."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v2",
                "updated": "2025-10-18T06:04:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    6,
                    4,
                    53,
                    5,
                    291,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM\n  Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) achieves formal data privacy protection but suffers from\nsignificant latency overhead, especially for long input sequences. While\nkey-value (KV) cache eviction and sparse attention algorithms have been\nproposed for efficient LLM inference in plaintext, they are not designed for\nMPC and cannot benefit private LLM inference directly. In this paper, we\npropose an accurate and MPC-friendly KV cache eviction framework, dubbed\nMPCache, building on the observation that historical tokens in a long sequence\nmay have different effects on the downstream decoding. Hence, MPCache combines\na look-once static eviction algorithm to discard unimportant KV cache and a\nquery-aware dynamic selection algorithm to activate only a small subset of KV\ncache for attention computation. MPCache further incorporates a series of\noptimizations for efficient dynamic KV cache selection, including MPC-friendly\nsimilarity approximation, hierarchical KV cache clustering, and cross-layer\nindex-sharing strategy. Extensive experiments demonstrate that MPCache\nconsistently outperforms prior-art KV cache eviction baselines across different\ngeneration tasks and achieves 1.8 ~ 2.01x and 3.39 ~ 8.37x decoding latency and\ncommunication reduction on different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Tao Wei"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v3",
                "updated": "2025-10-18T02:48:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    2,
                    48,
                    35,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16292v1",
                "updated": "2025-10-18T01:31:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T01:31:14Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    1,
                    31,
                    14,
                    5,
                    291,
                    0
                ],
                "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value\n  Weight Compression in Low-Precision Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) are integral to tasks such as image captioning\nand visual question answering, but their high computational cost, driven by\nlarge memory footprints and processing time, limits their scalability and\nreal-time applicability. In this work, we propose leveraging Singular-Value\nDecomposition (SVD) over the joint query (Q), key (K), and value (V) weight\nmatrices to reduce KV cache size and computational overhead. We in addition\nintroduce an efficient rank allocation strategy that dynamically adjusts the\nSVD rank based on its impact on VLM accuracy, achieving a significant reduction\nin both memory usage and computational cost. Finally, we extend this approach\nby applying quantization to both VLM weights and activations, resulting in a\nhighly efficient VLM. Our method outperforms previous approaches that rely\nsolely on quantization or SVD by achieving more than $10\\%$ accuracy\nimprovement while consuming less hardware cost, making it better for real-time\ndeployment on resource-constrained devices. We open source our code at\n\\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}."
                },
                "authors": [
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Haiyu Wang"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "arxiv_comment": "Accepted as Spotlight paper by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16276v1",
                "updated": "2025-10-18T00:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "published": "2025-10-18T00:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    0,
                    21,
                    45,
                    5,
                    291,
                    0
                ],
                "title": "What Limits Agentic Systems Efficiency?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Limits Agentic Systems Efficiency?"
                },
                "summary": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have\ndemonstrated strong reasoning capabilities. To further enhance LLM\ncapabilities, recent agentic systems, such as Deep Research, incorporate web\ninteractions into LLM reasoning to mitigate uncertainties and reduce potential\nerrors. However, existing research predominantly focuses on reasoning\nperformance, often neglecting the efficiency of agentic systems. In this work,\nwe present a comprehensive empirical study that identifies efficiency\nbottlenecks in web-interactive agentic systems. We decompose end-to-end latency\ninto two primary components: LLM API latency and web environment latency. We\nconduct a comprehensive empirical study across 15 models and 5 providers to\ndemonstrate high variability in API-based agentic systems. We observe that web\nenvironment latency can contribute as much as 53.7% to the overall latency in a\nweb-based agentic system. To improve latency, we propose SpecCache, a caching\nframework augmented with speculative execution that can reduce web environment\noverhead. Extensive evaluations on two standard benchmarks show that our\napproach improves the cache hit rate by up to 58x compared to a random caching\nstrategy, while reducing web environment overhead by up to 3.2x, without\ndegrading agentic system performance."
                },
                "authors": [
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Anand Jayarajan"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "27 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.26796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26796v1",
                "updated": "2025-10-30T17:59:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    59,
                    39,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:59:39Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    59,
                    39,
                    3,
                    303,
                    0
                ],
                "title": "SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting"
                },
                "summary": "Immersive applications call for synthesizing spatiotemporal 4D content from\ncasual videos without costly 3D supervision. Existing video-to-4D methods\ntypically rely on manually annotated camera poses, which are labor-intensive\nand brittle for in-the-wild footage. Recent warp-then-inpaint approaches\nmitigate the need for pose labels by warping input frames along a novel camera\ntrajectory and using an inpainting model to fill missing regions, thereby\ndepicting the 4D scene from diverse viewpoints. However, this\ntrajectory-to-trajectory formulation often entangles camera motion with scene\ndynamics and complicates both modeling and inference. We introduce SEE4D, a\npose-free, trajectory-to-camera framework that replaces explicit trajectory\nprediction with rendering to a bank of fixed virtual cameras, thereby\nseparating camera control from scene modeling. A view-conditional video\ninpainting model is trained to learn a robust geometry prior by denoising\nrealistically synthesized warped images and to inpaint occluded or missing\nregions across virtual viewpoints, eliminating the need for explicit 3D\nannotations. Building on this inpainting core, we design a spatiotemporal\nautoregressive inference pipeline that traverses virtual-camera splines and\nextends videos with overlapping windows, enabling coherent generation at\nbounded per-step complexity. We validate See4D on cross-view video generation\nand sparse reconstruction benchmarks. Across quantitative metrics and\nqualitative assessments, our method achieves superior generalization and\nimproved performance relative to pose- or trajectory-conditioned baselines,\nadvancing practical 4D world modeling from casual videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Immersive applications call for synthesizing spatiotemporal 4D content from\ncasual videos without costly 3D supervision. Existing video-to-4D methods\ntypically rely on manually annotated camera poses, which are labor-intensive\nand brittle for in-the-wild footage. Recent warp-then-inpaint approaches\nmitigate the need for pose labels by warping input frames along a novel camera\ntrajectory and using an inpainting model to fill missing regions, thereby\ndepicting the 4D scene from diverse viewpoints. However, this\ntrajectory-to-trajectory formulation often entangles camera motion with scene\ndynamics and complicates both modeling and inference. We introduce SEE4D, a\npose-free, trajectory-to-camera framework that replaces explicit trajectory\nprediction with rendering to a bank of fixed virtual cameras, thereby\nseparating camera control from scene modeling. A view-conditional video\ninpainting model is trained to learn a robust geometry prior by denoising\nrealistically synthesized warped images and to inpaint occluded or missing\nregions across virtual viewpoints, eliminating the need for explicit 3D\nannotations. Building on this inpainting core, we design a spatiotemporal\nautoregressive inference pipeline that traverses virtual-camera splines and\nextends videos with overlapping windows, enabling coherent generation at\nbounded per-step complexity. We validate See4D on cross-view video generation\nand sparse reconstruction benchmarks. Across quantitative metrics and\nqualitative assessments, our method achieves superior generalization and\nimproved performance relative to pose- or trajectory-conditioned baselines,\nadvancing practical 4D world modeling from casual videos."
                },
                "authors": [
                    {
                        "name": "Dongyue Lu"
                    },
                    {
                        "name": "Ao Liang"
                    },
                    {
                        "name": "Tianxin Huang"
                    },
                    {
                        "name": "Xiao Fu"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Baorui Ma"
                    },
                    {
                        "name": "Liang Pan"
                    },
                    {
                        "name": "Wei Yin"
                    },
                    {
                        "name": "Lingdong Kong"
                    },
                    {
                        "name": "Wei Tsang Ooi"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "arxiv_comment": "26 pages; 21 figures; 3 tables; project page:\n  https://see-4d.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26790v1",
                "updated": "2025-10-30T17:58:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    58,
                    26,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:58:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    58,
                    26,
                    3,
                    303,
                    0
                ],
                "title": "Gistify! Codebase-Level Understanding via Runtime Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gistify! Codebase-Level Understanding via Runtime Execution"
                },
                "summary": "As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces."
                },
                "authors": [
                    {
                        "name": "Hyunji Lee"
                    },
                    {
                        "name": "Minseon Kim"
                    },
                    {
                        "name": "Chinmay Singh"
                    },
                    {
                        "name": "Matheus Pereira"
                    },
                    {
                        "name": "Atharv Sonwane"
                    },
                    {
                        "name": "Isadora White"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Zhengyan Shi"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Marc-Alexandre Côté"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Lucas Caccia"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Caccia"
                },
                "author": "Lucas Caccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26788v1",
                "updated": "2025-10-30T17:58:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    58,
                    11,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:58:11Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    58,
                    11,
                    3,
                    303,
                    0
                ],
                "title": "Defeating the Training-Inference Mismatch via FP16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defeating the Training-Inference Mismatch via FP16"
                },
                "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Xiangxin Zhou"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26786v1",
                "updated": "2025-10-30T17:57:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    57,
                    40,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:57:40Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    57,
                    40,
                    3,
                    303,
                    0
                ],
                "title": "HEIR: Learning Graph-Based Motion Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEIR: Learning Graph-Based Motion Hierarchies"
                },
                "summary": "Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/"
                },
                "authors": [
                    {
                        "name": "Cheng Zheng"
                    },
                    {
                        "name": "William Koch"
                    },
                    {
                        "name": "Baiang Li"
                    },
                    {
                        "name": "Felix Heide"
                    }
                ],
                "author_detail": {
                    "name": "Felix Heide"
                },
                "author": "Felix Heide",
                "arxiv_comment": "Code link: https://github.com/princeton-computational-imaging/HEIR",
                "arxiv_journal_ref": "Advances in Neural Information Processing Systems 38 (NeurIPS\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26784v1",
                "updated": "2025-10-30T17:57:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    57,
                    17,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:57:17Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    57,
                    17,
                    3,
                    303,
                    0
                ],
                "title": "LLMs Process Lists With General Filter Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Process Lists With General Filter Heads"
                },
                "summary": "We investigate the mechanisms underlying a range of list-processing tasks in\nLLMs, and we find that LLMs have learned to encode a compact, causal\nrepresentation of a general filtering operation that mirrors the generic\n\"filter\" function of functional programming. Using causal mediation analysis on\na diverse set of list-processing tasks, we find that a small number of\nattention heads, which we dub filter heads, encode a compact representation of\nthe filtering predicate in their query states at certain tokens. We demonstrate\nthat this predicate representation is general and portable: it can be extracted\nand reapplied to execute the same filtering operation on different collections,\npresented in different formats, languages, or even in tasks. However, we also\nidentify situations where transformer LMs can exploit a different strategy for\nfiltering: eagerly evaluating if an item satisfies the predicate and storing\nthis intermediate result as a flag directly in the item representations. Our\nresults reveal that transformer LMs can develop human-interpretable\nimplementations of abstract computational operations that generalize in ways\nthat are surprisingly similar to strategies used in traditional functional\nprogramming patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the mechanisms underlying a range of list-processing tasks in\nLLMs, and we find that LLMs have learned to encode a compact, causal\nrepresentation of a general filtering operation that mirrors the generic\n\"filter\" function of functional programming. Using causal mediation analysis on\na diverse set of list-processing tasks, we find that a small number of\nattention heads, which we dub filter heads, encode a compact representation of\nthe filtering predicate in their query states at certain tokens. We demonstrate\nthat this predicate representation is general and portable: it can be extracted\nand reapplied to execute the same filtering operation on different collections,\npresented in different formats, languages, or even in tasks. However, we also\nidentify situations where transformer LMs can exploit a different strategy for\nfiltering: eagerly evaluating if an item satisfies the predicate and storing\nthis intermediate result as a flag directly in the item representations. Our\nresults reveal that transformer LMs can develop human-interpretable\nimplementations of abstract computational operations that generalize in ways\nthat are surprisingly similar to strategies used in traditional functional\nprogramming patterns."
                },
                "authors": [
                    {
                        "name": "Arnab Sen Sharma"
                    },
                    {
                        "name": "Giordano Rogers"
                    },
                    {
                        "name": "Natalie Shapira"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "arxiv_comment": "Code and data at https://filter.baulab.info/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26783v1",
                "updated": "2025-10-30T17:56:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    56,
                    47,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:56:47Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    56,
                    47,
                    3,
                    303,
                    0
                ],
                "title": "A Unified Theory for Causal Inference: Direct Debiased Machine Learning\n  via Bregman-Riesz Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Theory for Causal Inference: Direct Debiased Machine Learning\n  via Bregman-Riesz Regression"
                },
                "summary": "This note introduces a unified theory for causal inference that integrates\nRiesz regression, covariate balancing, density-ratio estimation (DRE), targeted\nmaximum likelihood estimation (TMLE), and the matching estimator in average\ntreatment effect (ATE) estimation. In ATE estimation, the balancing weights and\nthe regression functions of the outcome play important roles, where the\nbalancing weights are referred to as the Riesz representer, bias-correction\nterm, and clever covariates, depending on the context. Riesz regression,\ncovariate balancing, DRE, and the matching estimator are methods for estimating\nthe balancing weights, where Riesz regression is essentially equivalent to DRE\nin the ATE context, the matching estimator is a special case of DRE, and DRE is\nin a dual relationship with covariate balancing. TMLE is a method for\nconstructing regression function estimators such that the leading bias term\nbecomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density\nRatio Estimation and Riesz Regression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This note introduces a unified theory for causal inference that integrates\nRiesz regression, covariate balancing, density-ratio estimation (DRE), targeted\nmaximum likelihood estimation (TMLE), and the matching estimator in average\ntreatment effect (ATE) estimation. In ATE estimation, the balancing weights and\nthe regression functions of the outcome play important roles, where the\nbalancing weights are referred to as the Riesz representer, bias-correction\nterm, and clever covariates, depending on the context. Riesz regression,\ncovariate balancing, DRE, and the matching estimator are methods for estimating\nthe balancing weights, where Riesz regression is essentially equivalent to DRE\nin the ATE context, the matching estimator is a special case of DRE, and DRE is\nin a dual relationship with covariate balancing. TMLE is a method for\nconstructing regression function estimators such that the leading bias term\nbecomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density\nRatio Estimation and Riesz Regression."
                },
                "authors": [
                    {
                        "name": "Masahiro Kato"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Kato"
                },
                "author": "Masahiro Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26781v1",
                "updated": "2025-10-30T17:56:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    56,
                    31,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:56:31Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    56,
                    31,
                    3,
                    303,
                    0
                ],
                "title": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartAB: A Benchmark for Chart Grounding & Dense Alignment"
                },
                "summary": "Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charts play an important role in visualization, reasoning, data analysis, and\nthe exchange of ideas among humans. However, existing vision-language models\n(VLMs) still lack accurate perception of details and struggle to extract\nfine-grained structures from charts. Such limitations in chart grounding also\nhinder their ability to compare multiple charts and reason over them. In this\npaper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a\ncomprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting\ntabular data, localizing visualization elements, and recognizing various\nattributes from charts of diverse types and complexities. We design a JSON\ntemplate to facilitate the calculation of evaluation metrics specifically\ntailored for each grounding task. By incorporating a novel two-stage inference\nworkflow, the benchmark can further evaluate VLMs' capability to align and\ncompare elements/attributes across two charts. Our analysis of evaluations on\nseveral recent VLMs reveals new insights into their perception biases,\nweaknesses, robustness, and hallucinations in chart understanding. These\nfindings highlight the fine-grained discrepancies among VLMs in chart\nunderstanding tasks and point to specific skills that need to be strengthened\nin current models."
                },
                "authors": [
                    {
                        "name": "Aniruddh Bansal"
                    },
                    {
                        "name": "Davit Soselia"
                    },
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26776v1",
                "updated": "2025-10-30T17:55:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    55,
                    19,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:55:19Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    55,
                    19,
                    3,
                    303,
                    0
                ],
                "title": "Faithful and Fast Influence Function via Advanced Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faithful and Fast Influence Function via Advanced Sampling"
                },
                "summary": "How can we explain the influence of training data on black-box models?\nInfluence functions (IFs) offer a post-hoc solution by utilizing gradients and\nHessians. However, computing the Hessian for an entire dataset is\nresource-intensive, necessitating a feasible alternative. A common approach\ninvolves randomly sampling a small subset of the training data, but this method\noften results in highly inconsistent IF estimates due to the high variance in\nsample configurations. To address this, we propose two advanced sampling\ntechniques based on features and logits. These samplers select a small yet\nrepresentative subset of the entire dataset by considering the stochastic\ndistribution of features or logits, thereby enhancing the accuracy of IF\nestimations. We validate our approach through class removal experiments, a\ntypical application of IFs, using the F1-score to measure how effectively the\nmodel forgets the removed class while maintaining inference consistency on the\nremaining classes. Our method reduces computation time by 30.1% and memory\nusage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we explain the influence of training data on black-box models?\nInfluence functions (IFs) offer a post-hoc solution by utilizing gradients and\nHessians. However, computing the Hessian for an entire dataset is\nresource-intensive, necessitating a feasible alternative. A common approach\ninvolves randomly sampling a small subset of the training data, but this method\noften results in highly inconsistent IF estimates due to the high variance in\nsample configurations. To address this, we propose two advanced sampling\ntechniques based on features and logits. These samplers select a small yet\nrepresentative subset of the entire dataset by considering the stochastic\ndistribution of features or logits, thereby enhancing the accuracy of IF\nestimations. We validate our approach through class removal experiments, a\ntypical application of IFs, using the F1-score to measure how effectively the\nmodel forgets the removed class while maintaining inference consistency on the\nremaining classes. Our method reduces computation time by 30.1% and memory\nusage by 42.2%, or improves the F1-score by 2.5% compared to the baseline."
                },
                "authors": [
                    {
                        "name": "Jungyeon Koh"
                    },
                    {
                        "name": "Hyeonsu Lyu"
                    },
                    {
                        "name": "Jonggyu Jang"
                    },
                    {
                        "name": "Hyun Jong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Jong Yang"
                },
                "author": "Hyun Jong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26774v1",
                "updated": "2025-10-30T17:55:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    55,
                    1,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:55:01Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    55,
                    1,
                    3,
                    303,
                    0
                ],
                "title": "Compact Accretion Disks in the Aftermath of Tidal Disruption Events:\n  Parameter Inference from Joint X-ray Spectra and UV/Optical Photometry\n  Fitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Accretion Disks in the Aftermath of Tidal Disruption Events:\n  Parameter Inference from Joint X-ray Spectra and UV/Optical Photometry\n  Fitting"
                },
                "summary": "We present a multi-wavelength analysis of 14 tidal disruption events\n(TDEs)-including an off-nuclear event associated with an ultra-compact dwarf\ngalaxy-selected for having available thermal X-ray spectra during their\nlate-time UV/optical plateau phase. We show that at these stages, the full\nspectral energy distribution - X-ray spectra and UV/optical photometry - is\nwell described by a compact, yet standard accretion disk, the same disk which\npowers the X-rays at all times. By fitting up to three three epochs per source\nwith a fully relativistic disk model, we show that many system properties can\nbe reliably recovered, including importantly the black hole mass\n($M_{\\bullet}$). These accretion-based $M_{\\bullet}$ values, which in this\nsample span nearly three orders of magnitude, are consistent with galactic\nscaling relations but are significantly more precise (68\\% credible interval $\n< \\pm 0.3$ dex) and physically motivated. Expected accretion scaling relations\n(e.g., $L_{Bol}^{ disk} / L_{Edd} \\propto T_p^4 \\propto M_{\\bullet}^{-1}$),\nTDE-specific physics correlations ($L_{plat} \\propto M_{\\bullet}^{2/3}$ and\n$R_{out}/r_g \\propto M_{\\bullet}^{-2/3}$) and black hole-host galaxy\ncorrelations ($M_{\\bullet}$-$M_{\\star}$ and $M_{\\bullet}$-$\\sigma_{\\star}$)\nnaturally emerge from the data and, for the first time, are self-consistently\nextended into the intermediate-mass (IMBH, $M_{\\bullet} < 10^{5}$) regime. We\ndiscuss the implications of these results for TDE physics and modeling. We also\nreview and discuss different methods for $M_{\\bullet}$ inference in TDEs, and\nfind that approaches based on physical models of the early-time UV/optical\nemission are not able to recover (at a statistically significant level) black\nhole-host galaxy scalings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a multi-wavelength analysis of 14 tidal disruption events\n(TDEs)-including an off-nuclear event associated with an ultra-compact dwarf\ngalaxy-selected for having available thermal X-ray spectra during their\nlate-time UV/optical plateau phase. We show that at these stages, the full\nspectral energy distribution - X-ray spectra and UV/optical photometry - is\nwell described by a compact, yet standard accretion disk, the same disk which\npowers the X-rays at all times. By fitting up to three three epochs per source\nwith a fully relativistic disk model, we show that many system properties can\nbe reliably recovered, including importantly the black hole mass\n($M_{\\bullet}$). These accretion-based $M_{\\bullet}$ values, which in this\nsample span nearly three orders of magnitude, are consistent with galactic\nscaling relations but are significantly more precise (68\\% credible interval $\n< \\pm 0.3$ dex) and physically motivated. Expected accretion scaling relations\n(e.g., $L_{Bol}^{ disk} / L_{Edd} \\propto T_p^4 \\propto M_{\\bullet}^{-1}$),\nTDE-specific physics correlations ($L_{plat} \\propto M_{\\bullet}^{2/3}$ and\n$R_{out}/r_g \\propto M_{\\bullet}^{-2/3}$) and black hole-host galaxy\ncorrelations ($M_{\\bullet}$-$M_{\\star}$ and $M_{\\bullet}$-$\\sigma_{\\star}$)\nnaturally emerge from the data and, for the first time, are self-consistently\nextended into the intermediate-mass (IMBH, $M_{\\bullet} < 10^{5}$) regime. We\ndiscuss the implications of these results for TDE physics and modeling. We also\nreview and discuss different methods for $M_{\\bullet}$ inference in TDEs, and\nfind that approaches based on physical models of the early-time UV/optical\nemission are not able to recover (at a statistically significant level) black\nhole-host galaxy scalings."
                },
                "authors": [
                    {
                        "name": "M. Guolo"
                    },
                    {
                        "name": "A. Mummery"
                    },
                    {
                        "name": "S. van Velzen"
                    },
                    {
                        "name": "S. Gezari"
                    },
                    {
                        "name": "M. Nicholl"
                    },
                    {
                        "name": "Y. Yao"
                    },
                    {
                        "name": "M. Karmen"
                    },
                    {
                        "name": "Y. Ajay"
                    },
                    {
                        "name": "T. Wevers"
                    },
                    {
                        "name": "N. LeBaron"
                    },
                    {
                        "name": "R. Chornock"
                    }
                ],
                "author_detail": {
                    "name": "R. Chornock"
                },
                "author": "R. Chornock",
                "arxiv_comment": "Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26771v1",
                "updated": "2025-10-30T17:53:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    53,
                    42,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:53:42Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    53,
                    42,
                    3,
                    303,
                    0
                ],
                "title": "STaMP: Sequence Transformation and Mixed Precision for Low-Precision\n  Activation Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STaMP: Sequence Transformation and Mixed Precision for Low-Precision\n  Activation Quantization"
                },
                "summary": "Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Riccardo Del Chiaro"
                    },
                    {
                        "name": "Boris van Breugel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Markus Nagel"
                    }
                ],
                "author_detail": {
                    "name": "Markus Nagel"
                },
                "author": "Markus Nagel",
                "arxiv_comment": "10 pages main text, 8 pages supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26769v1",
                "updated": "2025-10-30T17:52:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    52,
                    39,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:52:39Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    52,
                    39,
                    3,
                    303,
                    0
                ],
                "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models"
                },
                "summary": "This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces SteerVLM, a lightweight steering module designed to\nguide Vision-Language Models (VLMs) towards outputs that better adhere to\ndesired instructions. Our approach learns from the latent embeddings of paired\nprompts encoding target and converse behaviors to dynamically adjust\nactivations connecting the language modality with image context. This allows\nfor fine-grained, inference-time control over complex output semantics without\nmodifying model weights while preserving performance on off-target tasks. Our\nsteering module requires learning parameters equal to 0.14% of the original\nVLM's size. Our steering module gains model control through dimension-wise\nactivation modulation and adaptive steering across layers without requiring\npre-extracted static vectors or manual tuning of intervention points.\nFurthermore, we introduce VNIA (Visual Narrative Intent Alignment), a\nmultimodal dataset specifically created to facilitate the development and\nevaluation of VLM steering techniques. Our method outperforms existing\nintervention techniques on steering and hallucination mitigation benchmarks for\nVLMs and proposes a robust solution for multimodal model control through\nactivation engineering."
                },
                "authors": [
                    {
                        "name": "Anushka Sivakumar"
                    },
                    {
                        "name": "Andrew Zhang"
                    },
                    {
                        "name": "Zaber Hakim"
                    },
                    {
                        "name": "Chris Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Chris Thomas"
                },
                "author": "Chris Thomas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26768v1",
                "updated": "2025-10-30T17:52:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    52,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:52:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    52,
                    2,
                    3,
                    303,
                    0
                ],
                "title": "AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions"
                },
                "summary": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/"
                },
                "authors": [
                    {
                        "name": "Shengnan An"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yehao Lin"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Xinxuan Lv"
                    },
                    {
                        "name": "Dan Ma"
                    },
                    {
                        "name": "Xuanlin Wang"
                    },
                    {
                        "name": "Ziwen Wang"
                    },
                    {
                        "name": "Shuang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuang Zhou"
                },
                "arxiv_affiliation": "Alphabetical order by last name",
                "author": "Shuang Zhou",
                "arxiv_comment": "14 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26767v1",
                "updated": "2025-10-30T17:51:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    51,
                    46,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:51:46Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    51,
                    46,
                    3,
                    303,
                    0
                ],
                "title": "Unbiased Primordial Gravitational Wave Inference from the CMB with SMICA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbiased Primordial Gravitational Wave Inference from the CMB with SMICA"
                },
                "summary": "The detection of primordial gravitational waves in Cosmic Microwave\nBackground B-mode polarization observations requires accurate and robust\nsubtraction of astrophysical contamination. We show, using a blind Spectral\nMatching Independent Component Analysis, that it is possible to infer unbiased\nestimates of the primordial B-mode signal from ground-based observations of a\nsmall patch of sky even for highly complex foreground contamination. This work,\noriginally performed in the context of configuration studies for a future\nCMB-S4 observatory, is highly relevant for the analysis of observations by the\ncurrent generation of CMB experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of primordial gravitational waves in Cosmic Microwave\nBackground B-mode polarization observations requires accurate and robust\nsubtraction of astrophysical contamination. We show, using a blind Spectral\nMatching Independent Component Analysis, that it is possible to infer unbiased\nestimates of the primordial B-mode signal from ground-based observations of a\nsmall patch of sky even for highly complex foreground contamination. This work,\noriginally performed in the context of configuration studies for a future\nCMB-S4 observatory, is highly relevant for the analysis of observations by the\ncurrent generation of CMB experiments."
                },
                "authors": [
                    {
                        "name": "Alexander Steier"
                    },
                    {
                        "name": "Shamik Ghosh"
                    },
                    {
                        "name": "Jacques Delabrouille"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Delabrouille"
                },
                "author": "Jacques Delabrouille",
                "arxiv_comment": "17 pages, 9 figures, to be published in JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09391v2",
                "updated": "2025-10-30T17:41:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    41,
                    15,
                    3,
                    303,
                    0
                ],
                "published": "2025-06-11T04:44:46Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    4,
                    44,
                    46,
                    2,
                    162,
                    0
                ],
                "title": "Comparing human and LLM politeness strategies in free production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing human and LLM politeness strategies in free production"
                },
                "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Robert D. Hawkins"
                    }
                ],
                "author_detail": {
                    "name": "Robert D. Hawkins"
                },
                "author": "Robert D. Hawkins",
                "arxiv_comment": "25 pages, 5 figures | EMNLP 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26742v1",
                "updated": "2025-10-30T17:38:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    38,
                    14,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:38:14Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    38,
                    14,
                    3,
                    303,
                    0
                ],
                "title": "Running VLAs at Real-time Speed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running VLAs at Real-time Speed"
                },
                "summary": "In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate\nand at most 480Hz trajectory frequency using a single consumer GPU. This\nenables dynamic and real-time tasks that were previously believed to be\nunattainable by large VLA models. To achieve it, we introduce a bag of\nstrategies to eliminate the overheads in model inference. The real-world\nexperiment shows that the pi0 policy with our strategy achieves a 100% success\nrate in grasping a falling pen task. Based on the results, we further propose a\nfull streaming inference framework for real-time robot control of VLA. Code is\navailable at https://github.com/Dexmal/realtime-vla.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate\nand at most 480Hz trajectory frequency using a single consumer GPU. This\nenables dynamic and real-time tasks that were previously believed to be\nunattainable by large VLA models. To achieve it, we introduce a bag of\nstrategies to eliminate the overheads in model inference. The real-world\nexperiment shows that the pi0 policy with our strategy achieves a 100% success\nrate in grasping a falling pen task. Based on the results, we further propose a\nfull streaming inference framework for real-time robot control of VLA. Code is\navailable at https://github.com/Dexmal/realtime-vla."
                },
                "authors": [
                    {
                        "name": "Yunchao Ma"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yunhuan Yang"
                    },
                    {
                        "name": "Tiancai Wang"
                    },
                    {
                        "name": "Haoqiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Haoqiang Fan"
                },
                "author": "Haoqiang Fan",
                "arxiv_comment": "Code is available at https://github.com/Dexmal/realtime-vla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09205v3",
                "updated": "2025-10-30T17:37:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    37,
                    55,
                    3,
                    303,
                    0
                ],
                "published": "2025-03-12T09:48:38Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    48,
                    38,
                    2,
                    71,
                    0
                ],
                "title": "Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model"
                },
                "summary": "Integrating audio and visual data for training multimodal foundational models\nremains a challenge. The Audio-Video Vector Alignment (AVVA) framework\naddresses this by considering AV scene alignment beyond mere temporal\nsynchronization, and leveraging Large Language Models (LLMs) for data curation.\nAVVA implements a scoring mechanism for selecting aligned training data\nsegments. It integrates Whisper, a speech-based foundation model, for audio and\nDINOv2 for video analysis in a dual-encoder structure with contrastive learning\non AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the\neffectiveness of the proposed model architecture and data curation approach.\nAVVA achieves a significant improvement in top-k accuracies for video-to-audio\nretrieval on all datasets compared to DenseAV, while using only 192 hrs of\ncurated training data. Furthermore, an ablation study indicates that the data\ncuration process effectively trades data quality for data quantity, yielding\nincreases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,\ncompared to training on the full spectrum of uncurated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating audio and visual data for training multimodal foundational models\nremains a challenge. The Audio-Video Vector Alignment (AVVA) framework\naddresses this by considering AV scene alignment beyond mere temporal\nsynchronization, and leveraging Large Language Models (LLMs) for data curation.\nAVVA implements a scoring mechanism for selecting aligned training data\nsegments. It integrates Whisper, a speech-based foundation model, for audio and\nDINOv2 for video analysis in a dual-encoder structure with contrastive learning\non AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the\neffectiveness of the proposed model architecture and data curation approach.\nAVVA achieves a significant improvement in top-k accuracies for video-to-audio\nretrieval on all datasets compared to DenseAV, while using only 192 hrs of\ncurated training data. Furthermore, an ablation study indicates that the data\ncuration process effectively trades data quality for data quantity, yielding\nincreases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,\ncompared to training on the full spectrum of uncurated data."
                },
                "authors": [
                    {
                        "name": "Ali Vosoughi"
                    },
                    {
                        "name": "Dimitra Emmanouilidou"
                    },
                    {
                        "name": "Hannes Gamper"
                    }
                ],
                "author_detail": {
                    "name": "Hannes Gamper"
                },
                "author": "Hannes Gamper",
                "arxiv_comment": "5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T, 68T45, 68T10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26740v1",
                "updated": "2025-10-30T17:37:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    37,
                    51,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:37:51Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    37,
                    51,
                    3,
                    303,
                    0
                ],
                "title": "A General Incentives-Based Framework for Fairness in Multi-agent\n  Resource Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Incentives-Based Framework for Fairness in Multi-agent\n  Resource Allocation"
                },
                "summary": "We introduce the General Incentives-based Framework for Fairness (GIFF), a\nnovel approach for fair multi-agent resource allocation that infers fair\ndecision-making from standard value functions. In resource-constrained\nsettings, agents optimizing for efficiency often create inequitable outcomes.\nOur approach leverages the action-value (Q-)function to balance efficiency and\nfairness without requiring additional training. Specifically, our method\ncomputes a local fairness gain for each action and introduces a counterfactual\nadvantage correction term to discourage over-allocation to already well-off\nagents. This approach is formalized within a centralized control setting, where\nan arbitrator uses the GIFF-modified Q-values to solve an allocation problem.\n  Empirical evaluations across diverse domains, including dynamic ridesharing,\nhomelessness prevention, and a complex job allocation task-demonstrate that our\nframework consistently outperforms strong baselines and can discover\nfar-sighted, equitable policies. The framework's effectiveness is supported by\na theoretical foundation; we prove its fairness surrogate is a principled lower\nbound on the true fairness improvement and that its trade-off parameter offers\nmonotonic tuning. Our findings establish GIFF as a robust and principled\nframework for leveraging standard reinforcement learning components to achieve\nmore equitable outcomes in complex multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the General Incentives-based Framework for Fairness (GIFF), a\nnovel approach for fair multi-agent resource allocation that infers fair\ndecision-making from standard value functions. In resource-constrained\nsettings, agents optimizing for efficiency often create inequitable outcomes.\nOur approach leverages the action-value (Q-)function to balance efficiency and\nfairness without requiring additional training. Specifically, our method\ncomputes a local fairness gain for each action and introduces a counterfactual\nadvantage correction term to discourage over-allocation to already well-off\nagents. This approach is formalized within a centralized control setting, where\nan arbitrator uses the GIFF-modified Q-values to solve an allocation problem.\n  Empirical evaluations across diverse domains, including dynamic ridesharing,\nhomelessness prevention, and a complex job allocation task-demonstrate that our\nframework consistently outperforms strong baselines and can discover\nfar-sighted, equitable policies. The framework's effectiveness is supported by\na theoretical foundation; we prove its fairness surrogate is a principled lower\nbound on the true fairness improvement and that its trade-off parameter offers\nmonotonic tuning. Our findings establish GIFF as a robust and principled\nframework for leveraging standard reinforcement learning components to achieve\nmore equitable outcomes in complex multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Ashwin Kumar"
                    },
                    {
                        "name": "William Yeoh"
                    }
                ],
                "author_detail": {
                    "name": "William Yeoh"
                },
                "author": "William Yeoh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06078v2",
                "updated": "2025-10-30T17:35:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    35,
                    26,
                    3,
                    303,
                    0
                ],
                "published": "2025-07-08T15:17:24Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    15,
                    17,
                    24,
                    1,
                    189,
                    0
                ],
                "title": "ScoreAdv: Score-based Targeted Generation of Natural Adversarial\n  Examples via Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScoreAdv: Score-based Targeted Generation of Natural Adversarial\n  Examples via Diffusion Models"
                },
                "summary": "Despite the success of deep learning across various domains, it remains\nvulnerable to adversarial attacks. Although many existing adversarial attack\nmethods achieve high success rates, they typically rely on $\\ell_{p}$-norm\nperturbation constraints, which do not align with human perceptual\ncapabilities. Consequently, researchers have shifted their focus toward\ngenerating natural, unrestricted adversarial examples (UAEs). GAN-based\napproaches suffer from inherent limitations, such as poor image quality due to\ninstability and mode collapse. Meanwhile, diffusion models have been employed\nfor UAE generation, but they still rely on iterative PGD perturbation\ninjection, without fully leveraging their central denoising capabilities. In\nthis paper, we introduce a novel approach for generating UAEs based on\ndiffusion models, named ScoreAdv. This method incorporates an interpretable\nadversarial guidance mechanism to gradually shift the sampling distribution\ntowards the adversarial distribution, while using an interpretable saliency map\nto inject the visual information of a reference image into the generated\nsamples. Notably, our method is capable of generating an unlimited number of\nnatural adversarial examples and can attack not only classification models but\nalso retrieval models. We conduct extensive experiments on ImageNet and CelebA\ndatasets, validating the performance of ScoreAdv across ten target models in\nboth black-box and white-box settings. Our results demonstrate that ScoreAdv\nachieves state-of-the-art attack success rates and image quality, while\nmaintaining inference efficiency. Furthermore, the dynamic balance between\ndenoising and adversarial perturbation enables ScoreAdv to remain robust even\nunder defensive measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of deep learning across various domains, it remains\nvulnerable to adversarial attacks. Although many existing adversarial attack\nmethods achieve high success rates, they typically rely on $\\ell_{p}$-norm\nperturbation constraints, which do not align with human perceptual\ncapabilities. Consequently, researchers have shifted their focus toward\ngenerating natural, unrestricted adversarial examples (UAEs). GAN-based\napproaches suffer from inherent limitations, such as poor image quality due to\ninstability and mode collapse. Meanwhile, diffusion models have been employed\nfor UAE generation, but they still rely on iterative PGD perturbation\ninjection, without fully leveraging their central denoising capabilities. In\nthis paper, we introduce a novel approach for generating UAEs based on\ndiffusion models, named ScoreAdv. This method incorporates an interpretable\nadversarial guidance mechanism to gradually shift the sampling distribution\ntowards the adversarial distribution, while using an interpretable saliency map\nto inject the visual information of a reference image into the generated\nsamples. Notably, our method is capable of generating an unlimited number of\nnatural adversarial examples and can attack not only classification models but\nalso retrieval models. We conduct extensive experiments on ImageNet and CelebA\ndatasets, validating the performance of ScoreAdv across ten target models in\nboth black-box and white-box settings. Our results demonstrate that ScoreAdv\nachieves state-of-the-art attack success rates and image quality, while\nmaintaining inference efficiency. Furthermore, the dynamic balance between\ndenoising and adversarial perturbation enables ScoreAdv to remain robust even\nunder defensive measures."
                },
                "authors": [
                    {
                        "name": "Chihan Huang"
                    },
                    {
                        "name": "Hao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tang"
                },
                "author": "Hao Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14681v2",
                "updated": "2025-10-30T17:32:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    32,
                    44,
                    3,
                    303,
                    0
                ],
                "published": "2025-06-17T16:13:15Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    16,
                    13,
                    15,
                    1,
                    168,
                    0
                ],
                "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and\n  Training Factors Shape LLM Alignment Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and\n  Training Factors Shape LLM Alignment Quality"
                },
                "summary": "Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness, often surpassing superficial similarity between the training\ndata and the benchmark, and that mid-layer weight changes correlate most\nstrongly with performance gains. We release these 1,000+ SFT models and\nbenchmark results to accelerate further research. All resources are available\nat https://github.com/llm-jp/massive-sft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness, often surpassing superficial similarity between the training\ndata and the benchmark, and that mid-layer weight changes correlate most\nstrongly with performance gains. We release these 1,000+ SFT models and\nbenchmark results to accelerate further research. All resources are available\nat https://github.com/llm-jp/massive-sft."
                },
                "authors": [
                    {
                        "name": "Yuto Harada"
                    },
                    {
                        "name": "Yusuke Yamauchi"
                    },
                    {
                        "name": "Yusuke Oda"
                    },
                    {
                        "name": "Yohei Oseki"
                    },
                    {
                        "name": "Yusuke Miyao"
                    },
                    {
                        "name": "Yu Takagi"
                    }
                ],
                "author_detail": {
                    "name": "Yu Takagi"
                },
                "author": "Yu Takagi",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main Conference). Models and evaluation\n  results available at: https://github.com/llm-jp/massive-sft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26731v1",
                "updated": "2025-10-30T17:30:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    30,
                    6,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:30:06Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    30,
                    6,
                    3,
                    303,
                    0
                ],
                "title": "The Weighing Halos Accurately, Locally, and Efficiently with Supernovae\n  (WHALES) Survey Overview and Initial Data Release",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Weighing Halos Accurately, Locally, and Efficiently with Supernovae\n  (WHALES) Survey Overview and Initial Data Release"
                },
                "summary": "We present an overview of the Weighing Halos Accurately, Locally, and\nEfficiently with Supernovae (WHALES) survey, the first to discover and measure\nType Ia supernovae (SNe Ia) in and around galaxy superclusters. By building a\nsample of SNe~Ia around these massive environments, we aim to provide new\nconstraints on bulk-flow models while laying the groundwork for improved\nestimates of supercluster masses. Here, we present data from the first two\nseasons targeting the Shapley Supercluster (0.02<z<0.06), which is responsible\nfor a large but unknown fraction of our local group's motion. Until now, no\nsupernovae had been analyzed in the direction of Shapley. Through the WHALES\nsurvey, we have identified 12 likely SNe Ia in this region using SkyMapper,\nincluding 8 with spectroscopic confirmation. We present the first light curves\nof these SNe and combine our observations with data from ATLAS. We demonstrate\nthat the low number of discovered SNe Ia per season is consistent with various\nrate calculations, highlighting the need for future surveys to monitor\nsuperclusters over a multi-year timespan. Finally, we present simulations of SN\nIa observations in the environments of massive galaxy clusters, demonstrating\nhow the inferred peculiar velocities can constrain cluster masses, and\nhighlighting the added complexity within superclusters. We find that a sample\nof 100 SNe Ia would enable a 25% precision measurement of the total mass of the\nShapley Supercluster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an overview of the Weighing Halos Accurately, Locally, and\nEfficiently with Supernovae (WHALES) survey, the first to discover and measure\nType Ia supernovae (SNe Ia) in and around galaxy superclusters. By building a\nsample of SNe~Ia around these massive environments, we aim to provide new\nconstraints on bulk-flow models while laying the groundwork for improved\nestimates of supercluster masses. Here, we present data from the first two\nseasons targeting the Shapley Supercluster (0.02<z<0.06), which is responsible\nfor a large but unknown fraction of our local group's motion. Until now, no\nsupernovae had been analyzed in the direction of Shapley. Through the WHALES\nsurvey, we have identified 12 likely SNe Ia in this region using SkyMapper,\nincluding 8 with spectroscopic confirmation. We present the first light curves\nof these SNe and combine our observations with data from ATLAS. We demonstrate\nthat the low number of discovered SNe Ia per season is consistent with various\nrate calculations, highlighting the need for future surveys to monitor\nsuperclusters over a multi-year timespan. Finally, we present simulations of SN\nIa observations in the environments of massive galaxy clusters, demonstrating\nhow the inferred peculiar velocities can constrain cluster masses, and\nhighlighting the added complexity within superclusters. We find that a sample\nof 100 SNe Ia would enable a 25% precision measurement of the total mass of the\nShapley Supercluster."
                },
                "authors": [
                    {
                        "name": "Maria Acevedo"
                    },
                    {
                        "name": "Daniel Scolnic"
                    },
                    {
                        "name": "Bastien Carreres"
                    },
                    {
                        "name": "Erik R. Peterson"
                    },
                    {
                        "name": "Bruno O. Sanchez"
                    },
                    {
                        "name": "Christopher Lidman"
                    },
                    {
                        "name": "Bailey Martin"
                    },
                    {
                        "name": "Christopher A. Onken"
                    },
                    {
                        "name": "Adam G. Riess"
                    }
                ],
                "author_detail": {
                    "name": "Adam G. Riess"
                },
                "author": "Adam G. Riess",
                "arxiv_comment": "12 pages, 6 figures. To be submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26730v1",
                "updated": "2025-10-30T17:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:29:27Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference"
                },
                "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints."
                },
                "authors": [
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Runxin Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26727v1",
                "updated": "2025-10-30T17:27:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    27,
                    3,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:27:03Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    27,
                    3,
                    3,
                    303,
                    0
                ],
                "title": "Neither Consent nor Property: A Policy Lab for Data Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neither Consent nor Property: A Policy Lab for Data Law"
                },
                "summary": "This paper makes the opaque data market in the AI economy empirically legible\nfor the first time, constructing a computational testbed to address a core\nepistemic failure: regulators governing a market defined by structural opacity,\nfragile price discovery, and brittle technical safeguards that have paralyzed\ntraditional empirics and fragmented policy. The pipeline begins with multi-year\nfieldwork to extract the market's hidden logic, and then embeds these grounded\nbehaviors into a high-fidelity ABM, parameterized via a novel LLM-based\ndiscrete-choice experiment that captures the preferences of unsurveyable\npopulations. The pipeline is validated against reality, reproducing observed\ntrade patterns. This policy laboratory delivers clear, counter-intuitive\nresults. First, property-style relief is a false promise: ''anonymous-data''\ncarve-outs expand trade but ignore risk, causing aggregate welfare to collapse\nonce external harms are priced in. Second, social welfare peaks when the\ndownstream buyer internalizes the full substantive risk. This least-cost\navoider approach induces efficient safeguards, simultaneously raising welfare\nand sustaining trade, and provides a robust empirical foundation for the legal\ndrift toward two-sided reachability. The contribution is a reproducible\npipeline designed to end the reliance on intuition. It converts qualitative\ninsight into testable, comparative policy experiments, obsoleting armchair\nconjecture by replacing it with controlled evidence on how legal rules actually\nshift risk and surplus. This is the forward-looking engine that moves the field\nfrom competing intuitions to direct, computational analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper makes the opaque data market in the AI economy empirically legible\nfor the first time, constructing a computational testbed to address a core\nepistemic failure: regulators governing a market defined by structural opacity,\nfragile price discovery, and brittle technical safeguards that have paralyzed\ntraditional empirics and fragmented policy. The pipeline begins with multi-year\nfieldwork to extract the market's hidden logic, and then embeds these grounded\nbehaviors into a high-fidelity ABM, parameterized via a novel LLM-based\ndiscrete-choice experiment that captures the preferences of unsurveyable\npopulations. The pipeline is validated against reality, reproducing observed\ntrade patterns. This policy laboratory delivers clear, counter-intuitive\nresults. First, property-style relief is a false promise: ''anonymous-data''\ncarve-outs expand trade but ignore risk, causing aggregate welfare to collapse\nonce external harms are priced in. Second, social welfare peaks when the\ndownstream buyer internalizes the full substantive risk. This least-cost\navoider approach induces efficient safeguards, simultaneously raising welfare\nand sustaining trade, and provides a robust empirical foundation for the legal\ndrift toward two-sided reachability. The contribution is a reproducible\npipeline designed to end the reliance on intuition. It converts qualitative\ninsight into testable, comparative policy experiments, obsoleting armchair\nconjecture by replacing it with controlled evidence on how legal rules actually\nshift risk and surplus. This is the forward-looking engine that moves the field\nfrom competing intuitions to direct, computational analysis."
                },
                "authors": [
                    {
                        "name": "Haoyi Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhu"
                },
                "author": "Tianyi Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21319v2",
                "updated": "2025-10-30T17:09:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    9,
                    54,
                    3,
                    303,
                    0
                ],
                "published": "2025-09-25T16:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    16,
                    19,
                    6,
                    3,
                    268,
                    0
                ],
                "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards"
                },
                "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025"
                },
                "authors": [
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Ellie Evans"
                    },
                    {
                        "name": "Daniel Egert"
                    },
                    {
                        "name": "Hoo-Chang Shin"
                    },
                    {
                        "name": "Felipe Soares"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    }
                ],
                "author_detail": {
                    "name": "Oleksii Kuchaiev"
                },
                "author": "Oleksii Kuchaiev",
                "arxiv_comment": "Added link to access models:\n  https://huggingface.co/collections/nvidia/reward-models-10-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26707v1",
                "updated": "2025-10-30T17:09:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    9,
                    9,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:09:09Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    9,
                    9,
                    3,
                    303,
                    0
                ],
                "title": "Value Drifts: Tracing Value Alignment During LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Drifts: Tracing Value Alignment During LLM Post-Training"
                },
                "summary": "As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values."
                },
                "authors": [
                    {
                        "name": "Mehar Bhatia"
                    },
                    {
                        "name": "Shravan Nayak"
                    },
                    {
                        "name": "Gaurav Kamath"
                    },
                    {
                        "name": "Marius Mosbach"
                    },
                    {
                        "name": "Karolina Stańczak"
                    },
                    {
                        "name": "Vered Shwartz"
                    },
                    {
                        "name": "Siva Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Siva Reddy"
                },
                "author": "Siva Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26700v1",
                "updated": "2025-10-30T17:05:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    5,
                    57,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:05:57Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    5,
                    57,
                    3,
                    303,
                    0
                ],
                "title": "Assessment of the conditional exchangeability assumption in causal\n  machine learning models: a simulation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment of the conditional exchangeability assumption in causal\n  machine learning models: a simulation study"
                },
                "summary": "Observational studies developing causal machine learning (ML) models for the\nprediction of individualized treatment effects (ITEs) seldom conduct empirical\nevaluations to assess the conditional exchangeability assumption. We aimed to\nevaluate the performance of these models under conditional exchangeability\nviolations and the utility of negative control outcomes (NCOs) as a diagnostic.\nWe conducted a simulation study to examine confounding bias in ITE estimates\ngenerated by causal forest and X-learner models under varying conditions,\nincluding the presence or absence of true heterogeneity. We simulated data to\nreflect real-world scenarios with differing levels of confounding, sample size,\nand NCO confounding structures. We then estimated and compared subgroup-level\ntreatment effects on the primary outcome and NCOs across settings with and\nwithout unmeasured confounding. When conditional exchangeability was violated,\ncausal forest and X-learner models failed to recover true treatment effect\nheterogeneity and, in some cases, falsely indicated heterogeneity when there\nwas none. NCOs successfully identified subgroups affected by unmeasured\nconfounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it\nremained informative, flagging potential bias in subgroup level estimates,\nthough not always pinpointing the subgroup with the largest confounding.\nViolations of conditional exchangeability substantially limit the validity of\nITE estimates from causal ML models in routinely collected observational data.\nNCOs serve a useful empirical diagnostic tool for detecting subgroup-specific\nunmeasured confounding and should be incorporated into causal ML workflows to\nsupport the credibility of individualized inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observational studies developing causal machine learning (ML) models for the\nprediction of individualized treatment effects (ITEs) seldom conduct empirical\nevaluations to assess the conditional exchangeability assumption. We aimed to\nevaluate the performance of these models under conditional exchangeability\nviolations and the utility of negative control outcomes (NCOs) as a diagnostic.\nWe conducted a simulation study to examine confounding bias in ITE estimates\ngenerated by causal forest and X-learner models under varying conditions,\nincluding the presence or absence of true heterogeneity. We simulated data to\nreflect real-world scenarios with differing levels of confounding, sample size,\nand NCO confounding structures. We then estimated and compared subgroup-level\ntreatment effects on the primary outcome and NCOs across settings with and\nwithout unmeasured confounding. When conditional exchangeability was violated,\ncausal forest and X-learner models failed to recover true treatment effect\nheterogeneity and, in some cases, falsely indicated heterogeneity when there\nwas none. NCOs successfully identified subgroups affected by unmeasured\nconfounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it\nremained informative, flagging potential bias in subgroup level estimates,\nthough not always pinpointing the subgroup with the largest confounding.\nViolations of conditional exchangeability substantially limit the validity of\nITE estimates from causal ML models in routinely collected observational data.\nNCOs serve a useful empirical diagnostic tool for detecting subgroup-specific\nunmeasured confounding and should be incorporated into causal ML workflows to\nsupport the credibility of individualized inference."
                },
                "authors": [
                    {
                        "name": "Gerard T. Portela"
                    },
                    {
                        "name": "Jason B. Gibbons"
                    },
                    {
                        "name": "Sebastian Schneeweiss"
                    },
                    {
                        "name": "Rishi J. Desai"
                    }
                ],
                "author_detail": {
                    "name": "Rishi J. Desai"
                },
                "author": "Rishi J. Desai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26699v1",
                "updated": "2025-10-30T17:05:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    5,
                    13,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:05:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    5,
                    13,
                    3,
                    303,
                    0
                ],
                "title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative\n  Assessment"
                },
                "summary": "Keeping software systems up to date is essential to avoid technical debt,\nsecurity vulnerabilities, and the rigidity typical of legacy systems. However,\nupdating libraries and frameworks remains a time consuming and error-prone\nprocess. Recent advances in Large Language Models (LLMs) and agentic coding\nsystems offer new opportunities for automating such maintenance tasks. In this\npaper, we evaluate the update of a well-known Python library, SQLAlchemy,\nacross a dataset of ten client applications. For this task, we use the Github's\nCopilot Agent Mode, an autonomous AI systema capable of planning and executing\nmulti-step migration workflows. To assess the effectiveness of the automated\nmigration, we also introduce Migration Coverage, a metric that quantifies the\nproportion of API usage points correctly migrated. The results of our study\nshow that the LLM agent was capable of migrating functionalities and API usages\nbetween SQLAlchemy versions (migration coverage: 100%, median), but failed to\nmaintain the application functionality, leading to a low test-pass rate\n(39.75%, median).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping software systems up to date is essential to avoid technical debt,\nsecurity vulnerabilities, and the rigidity typical of legacy systems. However,\nupdating libraries and frameworks remains a time consuming and error-prone\nprocess. Recent advances in Large Language Models (LLMs) and agentic coding\nsystems offer new opportunities for automating such maintenance tasks. In this\npaper, we evaluate the update of a well-known Python library, SQLAlchemy,\nacross a dataset of ten client applications. For this task, we use the Github's\nCopilot Agent Mode, an autonomous AI systema capable of planning and executing\nmulti-step migration workflows. To assess the effectiveness of the automated\nmigration, we also introduce Migration Coverage, a metric that quantifies the\nproportion of API usage points correctly migrated. The results of our study\nshow that the LLM agent was capable of migrating functionalities and API usages\nbetween SQLAlchemy versions (migration coverage: 100%, median), but failed to\nmaintain the application functionality, leading to a low test-pass rate\n(39.75%, median)."
                },
                "authors": [
                    {
                        "name": "Aylton Almeida"
                    },
                    {
                        "name": "Laerte Xavier"
                    },
                    {
                        "name": "Marco Tulio Valente"
                    }
                ],
                "author_detail": {
                    "name": "Marco Tulio Valente"
                },
                "author": "Marco Tulio Valente",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26697v1",
                "updated": "2025-10-30T17:01:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    1,
                    43,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:01:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    1,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "The End of Manual Decoding: Towards Truly End-to-End Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The End of Manual Decoding: Towards Truly End-to-End Language Models"
                },
                "summary": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding."
                },
                "authors": [
                    {
                        "name": "Zhichao Wang"
                    },
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Xiaoying Tang"
                    },
                    {
                        "name": "Yan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Wang"
                },
                "author": "Yan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26691v1",
                "updated": "2025-10-30T16:59:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    28,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:59:28Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    28,
                    3,
                    303,
                    0
                ],
                "title": "Flinch: A Differentiable Framework for Field-Level Inference of\n  Cosmological parameters from curved sky data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flinch: A Differentiable Framework for Field-Level Inference of\n  Cosmological parameters from curved sky data"
                },
                "summary": "We present Flinch, a fully differentiable and high-performance framework for\nfield-level inference on angular maps, developed to improve the flexibility and\nscalability of current methodologies. Flinch is integrated with differentiable\ncosmology tools, allowing gradients to propagate from individual map pixels\ndirectly to the underlying cosmological parameters. This architecture allows\ncosmological inference to be carried out directly from the map itself,\nbypassing the need to specify a likelihood for intermediate summary statistics.\nUsing simulated, masked CMB temperature maps, we validate our pipeline by\nreconstructing both maps and angular power spectra, and we perform cosmological\nparameter inference with competitive precision. In comparison with the standard\npseudo-$C_\\ell$ approach, Flinch delivers substantially tighter constraints,\nwith error bars reduced by up to 40%. Among the gradient-based samplers\nroutinely employed in field-level analyses, we further show that MicroCanonical\nLangevin Monte Carlo provides orders-of-magnitude improvements in sampling\nefficiency over currently employed Hamiltonian Monte Carlo samplers, greatly\nreducing computational expense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Flinch, a fully differentiable and high-performance framework for\nfield-level inference on angular maps, developed to improve the flexibility and\nscalability of current methodologies. Flinch is integrated with differentiable\ncosmology tools, allowing gradients to propagate from individual map pixels\ndirectly to the underlying cosmological parameters. This architecture allows\ncosmological inference to be carried out directly from the map itself,\nbypassing the need to specify a likelihood for intermediate summary statistics.\nUsing simulated, masked CMB temperature maps, we validate our pipeline by\nreconstructing both maps and angular power spectra, and we perform cosmological\nparameter inference with competitive precision. In comparison with the standard\npseudo-$C_\\ell$ approach, Flinch delivers substantially tighter constraints,\nwith error bars reduced by up to 40%. Among the gradient-based samplers\nroutinely employed in field-level analyses, we further show that MicroCanonical\nLangevin Monte Carlo provides orders-of-magnitude improvements in sampling\nefficiency over currently employed Hamiltonian Monte Carlo samplers, greatly\nreducing computational expense."
                },
                "authors": [
                    {
                        "name": "Andrea Crespi"
                    },
                    {
                        "name": "Marco Bonici"
                    },
                    {
                        "name": "Arthur Loureiro"
                    },
                    {
                        "name": "Jaime Ruiz-Zapatero"
                    },
                    {
                        "name": "Ivan Sladoljev"
                    },
                    {
                        "name": "Zack Li"
                    },
                    {
                        "name": "Adrian Bayer"
                    },
                    {
                        "name": "Marius Millea"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "18 pages, 11 figures. Comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26690v1",
                "updated": "2025-10-30T16:59:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    22,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:59:22Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    22,
                    3,
                    303,
                    0
                ],
                "title": "LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits"
                },
                "summary": "Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance."
                },
                "authors": [
                    {
                        "name": "Amir Reza Mirzaei"
                    },
                    {
                        "name": "Yuqiao Wen"
                    },
                    {
                        "name": "Yanshuai Cao"
                    },
                    {
                        "name": "Lili Mou"
                    }
                ],
                "author_detail": {
                    "name": "Lili Mou"
                },
                "author": "Lili Mou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26684v1",
                "updated": "2025-10-30T16:54:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    54,
                    16,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:54:16Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    54,
                    16,
                    3,
                    303,
                    0
                ],
                "title": "Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill"
                },
                "summary": "We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments."
                },
                "authors": [
                    {
                        "name": "Vaibhav Kurrey"
                    },
                    {
                        "name": "Sivakalyan Pujari"
                    },
                    {
                        "name": "Gagan Raj Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Gagan Raj Gupta"
                },
                "author": "Gagan Raj Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26683v1",
                "updated": "2025-10-30T16:53:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    53,
                    45,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:53:45Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    53,
                    45,
                    3,
                    303,
                    0
                ],
                "title": "Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities\nacross multiple domains by leveraging massive pre-training and curated\nfine-tuning data. However, in data-sensitive fields such as healthcare, the\nlack of high-quality, domain-specific training corpus hinders LLMs' adaptation\nfor specialized applications. Meanwhile, domain experts have distilled domain\nwisdom into ontology rules, which formalize relationships among concepts and\nensure the integrity of knowledge management repositories. Viewing LLMs as\nimplicit repositories of human knowledge, we propose Evontree, a novel\nframework that leverages a small set of high-quality ontology rules to\nsystematically extract, validate, and enhance domain knowledge within LLMs,\nwithout requiring extensive external datasets. Specifically, Evontree extracts\ndomain ontology from raw models, detects inconsistencies using two core\nontology rules, and reinforces the refined knowledge via self-distilled\nfine-tuning. Extensive experiments on medical QA benchmarks with\nLlama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both\nunmodified models and leading supervised baselines, achieving up to a 3.7%\nimprovement in accuracy. These results confirm the effectiveness, efficiency,\nand robustness of our approach for low-resource domain adaptation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities\nacross multiple domains by leveraging massive pre-training and curated\nfine-tuning data. However, in data-sensitive fields such as healthcare, the\nlack of high-quality, domain-specific training corpus hinders LLMs' adaptation\nfor specialized applications. Meanwhile, domain experts have distilled domain\nwisdom into ontology rules, which formalize relationships among concepts and\nensure the integrity of knowledge management repositories. Viewing LLMs as\nimplicit repositories of human knowledge, we propose Evontree, a novel\nframework that leverages a small set of high-quality ontology rules to\nsystematically extract, validate, and enhance domain knowledge within LLMs,\nwithout requiring extensive external datasets. Specifically, Evontree extracts\ndomain ontology from raw models, detects inconsistencies using two core\nontology rules, and reinforces the refined knowledge via self-distilled\nfine-tuning. Extensive experiments on medical QA benchmarks with\nLlama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both\nunmodified models and leading supervised baselines, achieving up to a 3.7%\nimprovement in accuracy. These results confirm the effectiveness, efficiency,\nand robustness of our approach for low-resource domain adaptation of LLMs."
                },
                "authors": [
                    {
                        "name": "Mingchen Tu"
                    },
                    {
                        "name": "Zhiqiang Liu"
                    },
                    {
                        "name": "Juan Li"
                    },
                    {
                        "name": "Liangyurui Liu"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Wen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhang"
                },
                "author": "Wen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05207v2",
                "updated": "2025-10-30T16:53:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    53,
                    9,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-08T13:02:39Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    2,
                    39,
                    3,
                    128,
                    0
                ],
                "title": "A Fourier-based inference method for learning interaction kernels in\n  particle systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fourier-based inference method for learning interaction kernels in\n  particle systems"
                },
                "summary": "We consider the problem of inferring the interaction kernel of stochastic\ninteracting particle systems from observations of a single particle. We adopt a\nsemi-parametric approach and represent the interaction kernel in terms of a\ngeneralized Fourier series. The basis functions in this expansion are tailored\nto the problem at hand and are chosen to be orthogonal polynomials with respect\nto the invariant measure of the mean-field dynamics. The generalized Fourier\ncoefficients are obtained as the solution of an appropriate linear system whose\ncoefficients depend on the moments of the invariant measure, and which are\napproximated from the particle trajectory that we observe. We quantify the\napproximation error in the Lebesgue space weighted by the invariant measure and\nstudy the asymptotic properties of the estimator in the joint limit as the\nobservation interval and the number of particles tend to infinity, i.e. the\njoint large time-mean field limit. We also explore the regime where an\nincreasing number of generalized Fourier coefficients is needed to represent\nthe interaction kernel. Our theoretical results are supported by extensive\nnumerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of inferring the interaction kernel of stochastic\ninteracting particle systems from observations of a single particle. We adopt a\nsemi-parametric approach and represent the interaction kernel in terms of a\ngeneralized Fourier series. The basis functions in this expansion are tailored\nto the problem at hand and are chosen to be orthogonal polynomials with respect\nto the invariant measure of the mean-field dynamics. The generalized Fourier\ncoefficients are obtained as the solution of an appropriate linear system whose\ncoefficients depend on the moments of the invariant measure, and which are\napproximated from the particle trajectory that we observe. We quantify the\napproximation error in the Lebesgue space weighted by the invariant measure and\nstudy the asymptotic properties of the estimator in the joint limit as the\nobservation interval and the number of particles tend to infinity, i.e. the\njoint large time-mean field limit. We also explore the regime where an\nincreasing number of generalized Fourier coefficients is needed to represent\nthe interaction kernel. Our theoretical results are supported by extensive\nnumerical simulations."
                },
                "authors": [
                    {
                        "name": "Grigorios A. Pavliotis"
                    },
                    {
                        "name": "Andrea Zanoni"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanoni"
                },
                "author": "Andrea Zanoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23254v3",
                "updated": "2025-10-30T16:49:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    49,
                    52,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-29T09:00:35Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    0,
                    35,
                    3,
                    149,
                    0
                ],
                "title": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning"
                },
                "summary": "Owing to the huge success of generative artificial intelligence (AI), large\nlanguage models (LLMs) have emerged as a core subclass, underpinning\napplications such as question answering, text generation, and code completion.\nWhile fine-tuning these models on domain-specific data can yield significant\nperformance gains, it also poses daunting computational challenges, especially\nfor researchers and small organizations with limited hardware resources.\nAlthough SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy\nto overcome the GPU memory barrier via leveraging both system memory (i.e., CPU\nDRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily\ntargets model-centric performance issues. As a result, key system-level issues,\nincluding system memory fragmentation, inefficient pinned buffer allocation,\npeak CPU usage spikes, and file system overhead, remain unaddressed, stifling\nscalability and inflating costs. Such an observation motivates this paper to\nintroduce MemAscend, a framework that systematically tackles the underexplored\nsystem memory bottlenecks in SSD-offloaded LLM training, with a focus on\nresource-constrained environments. By streamlining pinned-memory allocation,\neradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a\nsubstantial system memory budget, enabling larger models, longer context\nwindows, and higher batch sizes without exceeding modest hardware limits.\nAcross diverse LLM benchmarks, MemAscend reduces peak system-memory consumption\nby an average of 55.7% compared with standard SSD offloading techniques,\nlowering the hardware barrier for fine-tuning and unlocking new possibilities\nfor cost-effective large-scale training on limited-resource machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to the huge success of generative artificial intelligence (AI), large\nlanguage models (LLMs) have emerged as a core subclass, underpinning\napplications such as question answering, text generation, and code completion.\nWhile fine-tuning these models on domain-specific data can yield significant\nperformance gains, it also poses daunting computational challenges, especially\nfor researchers and small organizations with limited hardware resources.\nAlthough SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy\nto overcome the GPU memory barrier via leveraging both system memory (i.e., CPU\nDRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily\ntargets model-centric performance issues. As a result, key system-level issues,\nincluding system memory fragmentation, inefficient pinned buffer allocation,\npeak CPU usage spikes, and file system overhead, remain unaddressed, stifling\nscalability and inflating costs. Such an observation motivates this paper to\nintroduce MemAscend, a framework that systematically tackles the underexplored\nsystem memory bottlenecks in SSD-offloaded LLM training, with a focus on\nresource-constrained environments. By streamlining pinned-memory allocation,\neradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a\nsubstantial system memory budget, enabling larger models, longer context\nwindows, and higher batch sizes without exceeding modest hardware limits.\nAcross diverse LLM benchmarks, MemAscend reduces peak system-memory consumption\nby an average of 55.7% compared with standard SSD offloading techniques,\nlowering the hardware barrier for fine-tuning and unlocking new possibilities\nfor cost-effective large-scale training on limited-resource machines."
                },
                "authors": [
                    {
                        "name": "Yong-Cheng Liaw"
                    },
                    {
                        "name": "Shuo-Han Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shuo-Han Chen"
                },
                "author": "Shuo-Han Chen",
                "arxiv_comment": "16 pages, 21 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07186v2",
                "updated": "2025-10-30T16:47:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    47,
                    56,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-08T16:22:53Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    16,
                    22,
                    53,
                    2,
                    281,
                    0
                ],
                "title": "Renormalization of Interacting Random Graph Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Renormalization of Interacting Random Graph Models"
                },
                "summary": "Random graphs offer a useful mathematical representation of a variety of real\nworld complex networks. Exponential random graphs, for example, are\nparticularly suited towards generating random graphs constrained to have\nspecified statistical moments. In this investigation, we elaborate on a\ngeneralization of the former where link probabilities are conditioned on the\nappearance of other links, corresponding to the introduction of interactions in\nan effective generalized statistical mechanical formalism. When restricted to\nthe simplest non-trivial case of pairwise interactions, one can derive a closed\nform renormalization group transformation for maximum coordination number two\non the corresponding line graph. Higher coordination numbers do not admit exact\nclosed form renormalization group transformations, a feature that paraphrases\nthe usual absence of exact transformations in two or more dimensional lattice\nsystems. We introduce disorder and study the induced renormalization group flow\non its probability assignments, highlighting its formal equivalence to time\nreversed anisotropic drift-diffusion on the statistical manifold associated\nwith the effective Hamiltonian. We discuss the implications of our findings,\nstressing the long wavelength irrelevance of certain classes of pair-wise\nconditioning on random graphs, and conclude with possible applications. These\ninclude modeling the scaling behavior of preferential effects on social\nnetworks, opinion dynamics, and reinforcement effects on neural networks, as\nwell as how our findings offer a systematic framework to deal with data\nlimitations in inference and reconstruction problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random graphs offer a useful mathematical representation of a variety of real\nworld complex networks. Exponential random graphs, for example, are\nparticularly suited towards generating random graphs constrained to have\nspecified statistical moments. In this investigation, we elaborate on a\ngeneralization of the former where link probabilities are conditioned on the\nappearance of other links, corresponding to the introduction of interactions in\nan effective generalized statistical mechanical formalism. When restricted to\nthe simplest non-trivial case of pairwise interactions, one can derive a closed\nform renormalization group transformation for maximum coordination number two\non the corresponding line graph. Higher coordination numbers do not admit exact\nclosed form renormalization group transformations, a feature that paraphrases\nthe usual absence of exact transformations in two or more dimensional lattice\nsystems. We introduce disorder and study the induced renormalization group flow\non its probability assignments, highlighting its formal equivalence to time\nreversed anisotropic drift-diffusion on the statistical manifold associated\nwith the effective Hamiltonian. We discuss the implications of our findings,\nstressing the long wavelength irrelevance of certain classes of pair-wise\nconditioning on random graphs, and conclude with possible applications. These\ninclude modeling the scaling behavior of preferential effects on social\nnetworks, opinion dynamics, and reinforcement effects on neural networks, as\nwell as how our findings offer a systematic framework to deal with data\nlimitations in inference and reconstruction problems."
                },
                "authors": [
                    {
                        "name": "Alessio Catanzaro"
                    },
                    {
                        "name": "Diego Garlaschelli"
                    },
                    {
                        "name": "Subodh P. Patil"
                    }
                ],
                "author_detail": {
                    "name": "Subodh P. Patil"
                },
                "author": "Subodh P. Patil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03305v2",
                "updated": "2025-10-30T16:45:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    45,
                    30,
                    3,
                    303,
                    0
                ],
                "published": "2025-07-04T05:24:01Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    5,
                    24,
                    1,
                    4,
                    185,
                    0
                ],
                "title": "Analysis and Optimized CXL-Attached Memory Allocation for Long-Context\n  LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Optimized CXL-Attached Memory Allocation for Long-Context\n  LLM Fine-Tuning"
                },
                "summary": "The substantial memory requirements of Large Language Models (LLMs),\nparticularly for long-context fine-tuning, have renewed interest in CPU\noffloading to augment limited GPU memory. However, as context lengths grow,\nrelying on CPU memory for intermediate states introduces a significant\nbottleneck that can exhaust the capacity of mainstream client platforms. To\naddress this limitation, this work investigates the effectiveness of Compute\nExpress Link (CXL) add-in card (AIC) memory as an extension to CPU memory,\nenabling larger model sizes and longer context lengths during fine-tuning.\nExtensive benchmarking reveals two critical challenges. First, current deep\nlearning frameworks such as PyTorch lack fine-grained, per-tensor control over\nNUMA memory allocation, exposing only coarse, process-level policies. Second,\ndue to this lack of control, when the memory footprint of fine-tuning is\noffloaded across local DRAM and CXL-attached memory, naively placing optimizer\ndata in higher-latency CXL leads to substantial slowdowns in the optimizer step\n(e.g., 4x once data exceeds 20M elements). To overcome these challenges, this\nwork introduces a PyTorch extension that enables tensor-level system memory\ncontrol and a CXL-aware memory allocator that pins latency-critical tensors in\nlocal DRAM while maximizing bandwidth by striping latency-tolerant tensors\nacross one or more CXL devices. Evaluated on a real hardware setup with 7B and\n12B models, 4K-32K contexts, and a single GPU, our approach recovers throughput\nto 97-99% of DRAM-only with a single AIC and approximately 100% with two AICs,\ndelivering up to 21% improvement over naive interleaving while preserving\nDRAM-like DMA bandwidth for GPU transfers. These results show that carefully\nmanaged CXL-attached memory is a practical path to scaling long-context\nfine-tuning beyond DRAM limits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The substantial memory requirements of Large Language Models (LLMs),\nparticularly for long-context fine-tuning, have renewed interest in CPU\noffloading to augment limited GPU memory. However, as context lengths grow,\nrelying on CPU memory for intermediate states introduces a significant\nbottleneck that can exhaust the capacity of mainstream client platforms. To\naddress this limitation, this work investigates the effectiveness of Compute\nExpress Link (CXL) add-in card (AIC) memory as an extension to CPU memory,\nenabling larger model sizes and longer context lengths during fine-tuning.\nExtensive benchmarking reveals two critical challenges. First, current deep\nlearning frameworks such as PyTorch lack fine-grained, per-tensor control over\nNUMA memory allocation, exposing only coarse, process-level policies. Second,\ndue to this lack of control, when the memory footprint of fine-tuning is\noffloaded across local DRAM and CXL-attached memory, naively placing optimizer\ndata in higher-latency CXL leads to substantial slowdowns in the optimizer step\n(e.g., 4x once data exceeds 20M elements). To overcome these challenges, this\nwork introduces a PyTorch extension that enables tensor-level system memory\ncontrol and a CXL-aware memory allocator that pins latency-critical tensors in\nlocal DRAM while maximizing bandwidth by striping latency-tolerant tensors\nacross one or more CXL devices. Evaluated on a real hardware setup with 7B and\n12B models, 4K-32K contexts, and a single GPU, our approach recovers throughput\nto 97-99% of DRAM-only with a single AIC and approximately 100% with two AICs,\ndelivering up to 21% improvement over naive interleaving while preserving\nDRAM-like DMA bandwidth for GPU transfers. These results show that carefully\nmanaged CXL-attached memory is a practical path to scaling long-context\nfine-tuning beyond DRAM limits."
                },
                "authors": [
                    {
                        "name": "Yong-Cheng Liaw"
                    },
                    {
                        "name": "Shuo-Han Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shuo-Han Chen"
                },
                "author": "Shuo-Han Chen",
                "arxiv_comment": "13 pages, 15 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26672v1",
                "updated": "2025-10-30T16:42:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    42,
                    9,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:42:09Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    42,
                    9,
                    3,
                    303,
                    0
                ],
                "title": "Action-Driven Processes for Continuous-Time Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action-Driven Processes for Continuous-Time Control"
                },
                "summary": "At the heart of reinforcement learning are actions -- decisions made in\nresponse to observations of the environment. Actions are equally fundamental in\nthe modeling of stochastic processes, as they trigger discontinuous state\ntransitions and enable the flow of information through large, complex systems.\nIn this paper, we unify the perspectives of stochastic processes and\nreinforcement learning through action-driven processes, and illustrate their\napplication to spiking neural networks. Leveraging ideas from\ncontrol-as-inference, we show that minimizing the Kullback-Leibler divergence\nbetween a policy-driven true distribution and a reward-driven model\ndistribution for a suitably defined action-driven process is equivalent to\nmaximum entropy reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the heart of reinforcement learning are actions -- decisions made in\nresponse to observations of the environment. Actions are equally fundamental in\nthe modeling of stochastic processes, as they trigger discontinuous state\ntransitions and enable the flow of information through large, complex systems.\nIn this paper, we unify the perspectives of stochastic processes and\nreinforcement learning through action-driven processes, and illustrate their\napplication to spiking neural networks. Leveraging ideas from\ncontrol-as-inference, we show that minimizing the Kullback-Leibler divergence\nbetween a policy-driven true distribution and a reward-driven model\ndistribution for a suitably defined action-driven process is equivalent to\nmaximum entropy reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Ruimin He"
                    },
                    {
                        "name": "Shaowei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Lin"
                },
                "author": "Shaowei Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26670v1",
                "updated": "2025-10-30T16:41:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    41,
                    58,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:41:58Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    41,
                    58,
                    3,
                    303,
                    0
                ],
                "title": "Hybrid Consistency Policy: Decoupling Multi-Modal Diversity and\n  Real-Time Efficiency in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Consistency Policy: Decoupling Multi-Modal Diversity and\n  Real-Time Efficiency in Robotic Manipulation"
                },
                "summary": "In visuomotor policy learning, diffusion-based imitation learning has become\nwidely adopted for its ability to capture diverse behaviors. However,\napproaches built on ordinary and stochastic denoising processes struggle to\njointly achieve fast sampling and strong multi-modality. To address these\nchallenges, we propose the Hybrid Consistency Policy (HCP). HCP runs a short\nstochastic prefix up to an adaptive switch time, and then applies a one-step\nconsistency jump to produce the final action. To align this one-jump\ngeneration, HCP performs time-varying consistency distillation that combines a\ntrajectory-consistency objective to keep neighboring predictions coherent and a\ndenoising-matching objective to improve local fidelity. In both simulation and\non a real robot, HCP with 25 SDE steps plus one jump approaches the 80-step\nDDPM teacher in accuracy and mode coverage while significantly reducing\nlatency. These results show that multi-modality does not require slow\ninference, and a switch time decouples mode retention from speed. It yields a\npractical accuracy efficiency trade-off for robot policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In visuomotor policy learning, diffusion-based imitation learning has become\nwidely adopted for its ability to capture diverse behaviors. However,\napproaches built on ordinary and stochastic denoising processes struggle to\njointly achieve fast sampling and strong multi-modality. To address these\nchallenges, we propose the Hybrid Consistency Policy (HCP). HCP runs a short\nstochastic prefix up to an adaptive switch time, and then applies a one-step\nconsistency jump to produce the final action. To align this one-jump\ngeneration, HCP performs time-varying consistency distillation that combines a\ntrajectory-consistency objective to keep neighboring predictions coherent and a\ndenoising-matching objective to improve local fidelity. In both simulation and\non a real robot, HCP with 25 SDE steps plus one jump approaches the 80-step\nDDPM teacher in accuracy and mode coverage while significantly reducing\nlatency. These results show that multi-modality does not require slow\ninference, and a switch time decouples mode retention from speed. It yields a\npractical accuracy efficiency trade-off for robot policies."
                },
                "authors": [
                    {
                        "name": "Qianyou Zhao"
                    },
                    {
                        "name": "Yuliang Shen"
                    },
                    {
                        "name": "Xuanran Zhai"
                    },
                    {
                        "name": "Ce Hao"
                    },
                    {
                        "name": "Duidi Wu"
                    },
                    {
                        "name": "Jin Qi"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Qiaojun Yu"
                    }
                ],
                "author_detail": {
                    "name": "Qiaojun Yu"
                },
                "author": "Qiaojun Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v2",
                "updated": "2025-10-30T16:38:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    38,
                    19,
                    3,
                    303,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame\n  Vision-Language-Action Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame\n  Vision-Language-Action Modeling"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong performance in robotic\nmanipulation. However, these models remain constrained by the single-frame\nimage paradigm and fail to fully leverage the temporal information offered by\nmulti-frame histories, as directly feeding multiple frames into VLM backbones\nincurs substantial computational overhead and inference latency. We propose\nCronusVLA, a unified framework that extends single-frame VLA models to the\nmulti-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame\npretraining on large-scale embodied datasets with autoregressive prediction of\naction tokens, establishing an effective embodied vision-language foundation;\n(2) Multi-frame post-training, which adapts the prediction of the\nvision-language backbone from discrete tokens to learnable features, and\naggregates historical information via feature chunking. CronusVLA effectively\naddresses the existing challenges of multi-frame modeling while enhancing\nperformance and observational robustness. To evaluate the robustness under\ntemporal and spatial disturbances, we introduce SimplerEnv-OR, a novel\nbenchmark featuring 24 types of observational disturbances and 120 severity\nlevels. Experiments across three embodiments in simulated and real-world\nenvironments demonstrate that CronusVLA achieves leading performance and\nsuperior robustness, with a 70.9% success rate on SimplerEnv, a 26.8%\nimprovement over OpenVLA on LIBERO, and the highest robustness score on\nSimplerEnv-OR. These results highlight the potential of efficient multi-frame\nadaptation in VLA models for more powerful and robust real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong performance in robotic\nmanipulation. However, these models remain constrained by the single-frame\nimage paradigm and fail to fully leverage the temporal information offered by\nmulti-frame histories, as directly feeding multiple frames into VLM backbones\nincurs substantial computational overhead and inference latency. We propose\nCronusVLA, a unified framework that extends single-frame VLA models to the\nmulti-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame\npretraining on large-scale embodied datasets with autoregressive prediction of\naction tokens, establishing an effective embodied vision-language foundation;\n(2) Multi-frame post-training, which adapts the prediction of the\nvision-language backbone from discrete tokens to learnable features, and\naggregates historical information via feature chunking. CronusVLA effectively\naddresses the existing challenges of multi-frame modeling while enhancing\nperformance and observational robustness. To evaluate the robustness under\ntemporal and spatial disturbances, we introduce SimplerEnv-OR, a novel\nbenchmark featuring 24 types of observational disturbances and 120 severity\nlevels. Experiments across three embodiments in simulated and real-world\nenvironments demonstrate that CronusVLA achieves leading performance and\nsuperior robustness, with a 70.9% success rate on SimplerEnv, a 26.8%\nimprovement over OpenVLA on LIBERO, and the highest robustness score on\nSimplerEnv-OR. These results highlight the potential of efficient multi-frame\nadaptation in VLA models for more powerful and robust real-world deployment."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "39 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01543v2",
                "updated": "2025-10-30T16:32:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    32,
                    34,
                    3,
                    303,
                    0
                ],
                "published": "2025-08-03T01:56:03Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    1,
                    56,
                    3,
                    6,
                    215,
                    0
                ],
                "title": "Refine-n-Judge: Curating High-Quality Preference Chains for\n  LLM-Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refine-n-Judge: Curating High-Quality Preference Chains for\n  LLM-Fine-Tuning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable progress through\npreference-based fine-tuning, which critically depends on the quality of the\nunderlying training data. While human feedback is essential for improving data\nquality, it is costly and does not scale well. In this paper, we introduce\nRefine-n-Judge, an automated iterative approach that leverages a single LLM as\nboth a refiner and a judge to enhance dataset quality. Unlike existing\niterative refinement methods, Refine-n-Judge employs an LLM to both generate\nrefinements and explicitly evaluate each improvement, ensuring that every\niteration meaningfully enhances the dataset without requiring additional human\nannotation or a separate reward model. At each step, the LLM refines a response\nand judges whether the refinement is an improvement over the previous answer.\nThis process continues until the LLM prefers the initial answer over the\nrefinement, indicating no further improvements. This produces sequences of\nincreasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public\ndatasets spanning five corpora, targeting tasks such as coding, math, and\nconversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on\nRefine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of\ncomparisons against models tuned on the original dataset by GPT-4.\nAdditionally, we report performance gains: +5% on AlpacaEval and AlpacaEval\n2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces\nhigh-quality datasets and scalable model improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable progress through\npreference-based fine-tuning, which critically depends on the quality of the\nunderlying training data. While human feedback is essential for improving data\nquality, it is costly and does not scale well. In this paper, we introduce\nRefine-n-Judge, an automated iterative approach that leverages a single LLM as\nboth a refiner and a judge to enhance dataset quality. Unlike existing\niterative refinement methods, Refine-n-Judge employs an LLM to both generate\nrefinements and explicitly evaluate each improvement, ensuring that every\niteration meaningfully enhances the dataset without requiring additional human\nannotation or a separate reward model. At each step, the LLM refines a response\nand judges whether the refinement is an improvement over the previous answer.\nThis process continues until the LLM prefers the initial answer over the\nrefinement, indicating no further improvements. This produces sequences of\nincreasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public\ndatasets spanning five corpora, targeting tasks such as coding, math, and\nconversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on\nRefine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of\ncomparisons against models tuned on the original dataset by GPT-4.\nAdditionally, we report performance gains: +5% on AlpacaEval and AlpacaEval\n2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces\nhigh-quality datasets and scalable model improvements."
                },
                "authors": [
                    {
                        "name": "Derin Cayir"
                    },
                    {
                        "name": "Renjie Tao"
                    },
                    {
                        "name": "Rashi Rungta"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Sean Chen"
                    },
                    {
                        "name": "Haidar Khan"
                    },
                    {
                        "name": "Minseok Kim"
                    },
                    {
                        "name": "Julia Reinspach"
                    },
                    {
                        "name": "Yue Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yue Liu"
                },
                "author": "Yue Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17449v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17449v4",
                "updated": "2025-10-30T16:28:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    28,
                    3,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-20T11:35:09Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    11,
                    35,
                    9,
                    0,
                    293,
                    0
                ],
                "title": "Observable spins in gravitational waves from compact binary mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observable spins in gravitational waves from compact binary mergers"
                },
                "summary": "We investigate the measurability of effective inspiral spin in the detectable\ncompact binary mergers using gravitational-wave observations. Measurements from\nthe latest gravitational-wave transient catalog do not rule out the existence\nof binary systems with non-zero effective spins. However, we observe an\napparent correlation between the inferred effective inspiral spin and the\nloudness of the gravitational-wave events-loud events typically have\nclose-to-zero effective spins whereas fainter events tend to be inferred with\nrelatively arbitrary effective spins. Through simulations, we demonstrate that\nnon-negligible effective spins can be systematically inferred from non-spinning\nsystems at small signal strengths. These two observations support the\npossibility that the effective spin magnitudes in the observable compact\nbinaries are generally small. Future detections can have potential impact on\nthe understanding of their population and other astrophysical inferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the measurability of effective inspiral spin in the detectable\ncompact binary mergers using gravitational-wave observations. Measurements from\nthe latest gravitational-wave transient catalog do not rule out the existence\nof binary systems with non-zero effective spins. However, we observe an\napparent correlation between the inferred effective inspiral spin and the\nloudness of the gravitational-wave events-loud events typically have\nclose-to-zero effective spins whereas fainter events tend to be inferred with\nrelatively arbitrary effective spins. Through simulations, we demonstrate that\nnon-negligible effective spins can be systematically inferred from non-spinning\nsystems at small signal strengths. These two observations support the\npossibility that the effective spin magnitudes in the observable compact\nbinaries are generally small. Future detections can have potential impact on\nthe understanding of their population and other astrophysical inferences."
                },
                "authors": [
                    {
                        "name": "Souradeep Pal"
                    }
                ],
                "author_detail": {
                    "name": "Souradeep Pal"
                },
                "author": "Souradeep Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17449v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17449v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21257v2",
                "updated": "2025-10-30T16:25:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    25,
                    15,
                    3,
                    303,
                    0
                ],
                "published": "2025-07-28T18:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    18,
                    20,
                    41,
                    0,
                    209,
                    0
                ],
                "title": "CompoST: A Benchmark for Analyzing the Ability of LLMs To\n  Compositionally Interpret Questions in a QALD Setting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompoST: A Benchmark for Analyzing the Ability of LLMs To\n  Compositionally Interpret Questions in a QALD Setting"
                },
                "summary": "Language interpretation is a compositional process, in which the meaning of\nmore complex linguistic structures is inferred from the meaning of their parts.\nLarge language models possess remarkable language interpretation capabilities\nand have been successfully applied to interpret questions by mapping them to\nSPARQL queries. An open question is how systematic this interpretation process\nis. Toward this question, in this paper, we propose a benchmark for\ninvestigating to what extent the abilities of LLMs to interpret questions are\nactually compositional. For this, we generate three datasets of varying\ndifficulty based on graph patterns in DBpedia, relying on Lemon lexica for\nverbalization. Our datasets are created in a very controlled fashion in order\nto test the ability of LLMs to interpret structurally complex questions, given\nthat they have seen the atomic building blocks. This allows us to evaluate to\nwhat degree LLMs are able to interpret complex questions for which they\n\"understand\" the atomic parts. We conduct experiments with models of different\nsizes using both various prompt and few-shot optimization techniques as well as\nfine-tuning. Our results show that performance in terms of macro $F_1$ degrades\nfrom $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the\nsamples optimized on. Even when all necessary information was provided to the\nmodel in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of\nlowest complexity. We thus conclude that LLMs struggle to systematically and\ncompositionally interpret questions and map them into SPARQL queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language interpretation is a compositional process, in which the meaning of\nmore complex linguistic structures is inferred from the meaning of their parts.\nLarge language models possess remarkable language interpretation capabilities\nand have been successfully applied to interpret questions by mapping them to\nSPARQL queries. An open question is how systematic this interpretation process\nis. Toward this question, in this paper, we propose a benchmark for\ninvestigating to what extent the abilities of LLMs to interpret questions are\nactually compositional. For this, we generate three datasets of varying\ndifficulty based on graph patterns in DBpedia, relying on Lemon lexica for\nverbalization. Our datasets are created in a very controlled fashion in order\nto test the ability of LLMs to interpret structurally complex questions, given\nthat they have seen the atomic building blocks. This allows us to evaluate to\nwhat degree LLMs are able to interpret complex questions for which they\n\"understand\" the atomic parts. We conduct experiments with models of different\nsizes using both various prompt and few-shot optimization techniques as well as\nfine-tuning. Our results show that performance in terms of macro $F_1$ degrades\nfrom $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the\nsamples optimized on. Even when all necessary information was provided to the\nmodel in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of\nlowest complexity. We thus conclude that LLMs struggle to systematically and\ncompositionally interpret questions and map them into SPARQL queries."
                },
                "authors": [
                    {
                        "name": "David Maria Schmidt"
                    },
                    {
                        "name": "Raoul Schubert"
                    },
                    {
                        "name": "Philipp Cimiano"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Cimiano"
                },
                "author": "Philipp Cimiano",
                "arxiv_doi": "10.1007/978-3-032-09527-5_1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-09527-5_1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Research Track, 24th International Semantic Web Conference (ISWC\n  2025), November 2-6, 2025, Nara, Japan",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26658v1",
                "updated": "2025-10-30T16:25:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    25,
                    10,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:25:10Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    25,
                    10,
                    3,
                    303,
                    0
                ],
                "title": "The Era of Agentic Organization: Learning to Organize with Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Era of Agentic Organization: Learning to Organize with Language\n  Models"
                },
                "summary": "We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We envision a new era of AI, termed agentic organization, where agents solve\ncomplex problems by working collaboratively and concurrently, enabling outcomes\nbeyond individual intelligence. To realize this vision, we introduce\nasynchronous thinking (AsyncThink) as a new paradigm of reasoning with large\nlanguage models, which organizes the internal thinking process into\nconcurrently executable structures. Specifically, we propose a thinking\nprotocol where an organizer dynamically assigns sub-queries to workers, merges\nintermediate knowledge, and produces coherent solutions. More importantly, the\nthinking structure in this protocol can be further optimized through\nreinforcement learning. Experiments demonstrate that AsyncThink achieves 28%\nlower inference latency compared to parallel thinking while improving accuracy\non mathematical reasoning. Moreover, AsyncThink generalizes its learned\nasynchronous thinking capabilities, effectively tackling unseen tasks without\nadditional training."
                },
                "authors": [
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Yaru Hao"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Shaohan Huang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26656v1",
                "updated": "2025-10-30T16:23:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    23,
                    46,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:23:46Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    23,
                    46,
                    3,
                    303,
                    0
                ],
                "title": "Heuristic Adaptation of Potentially Misspecified Domain Support for\n  Likelihood-Free Inference in Stochastic Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristic Adaptation of Potentially Misspecified Domain Support for\n  Likelihood-Free Inference in Stochastic Dynamical Systems"
                },
                "summary": "In robotics, likelihood-free inference (LFI) can provide the domain\ndistribution that adapts a learnt agent in a parametric set of deployment\nconditions. LFI assumes an arbitrary support for sampling, which remains\nconstant as the initial generic prior is iteratively refined to more\ndescriptive posteriors. However, a potentially misspecified support can lead to\nsuboptimal, yet falsely certain, posteriors. To address this issue, we propose\nthree heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the\nposterior mode shift over inference steps in its own way and, when integrated\ninto an LFI step, adapts the support alongside posterior inference. We first\nexpose the support misspecification issue and evaluate our heuristics using\nstochastic dynamical benchmarks. We then evaluate the impact of heuristic\nsupport adaptation on parameter inference and policy learning for a dynamic\ndeformable linear object (DLO) manipulation task. Inference results in a finer\nlength and stiffness classification for a parametric set of DLOs. When the\nresulting posteriors are used as domain distributions for sim-based policy\nlearning, they lead to more robust object-centric agent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robotics, likelihood-free inference (LFI) can provide the domain\ndistribution that adapts a learnt agent in a parametric set of deployment\nconditions. LFI assumes an arbitrary support for sampling, which remains\nconstant as the initial generic prior is iteratively refined to more\ndescriptive posteriors. However, a potentially misspecified support can lead to\nsuboptimal, yet falsely certain, posteriors. To address this issue, we propose\nthree heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the\nposterior mode shift over inference steps in its own way and, when integrated\ninto an LFI step, adapts the support alongside posterior inference. We first\nexpose the support misspecification issue and evaluate our heuristics using\nstochastic dynamical benchmarks. We then evaluate the impact of heuristic\nsupport adaptation on parameter inference and policy learning for a dynamic\ndeformable linear object (DLO) manipulation task. Inference results in a finer\nlength and stiffness classification for a parametric set of DLOs. When the\nresulting posteriors are used as domain distributions for sim-based policy\nlearning, they lead to more robust object-centric agent performance."
                },
                "authors": [
                    {
                        "name": "Georgios Kamaras"
                    },
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26645v1",
                "updated": "2025-10-30T16:11:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    11,
                    39,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:11:39Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    11,
                    39,
                    3,
                    303,
                    0
                ],
                "title": "Curly Flow Matching for Learning Non-gradient Field Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curly Flow Matching for Learning Non-gradient Field Dynamics"
                },
                "summary": "Modeling the transport dynamics of natural processes from population-level\nobservations is a ubiquitous problem in the natural sciences. Such models rely\non key assumptions about the underlying process in order to enable faithful\nlearning of governing dynamics that mimic the actual system behavior. The de\nfacto assumption in current approaches relies on the principle of least action\nthat results in gradient field dynamics and leads to trajectories minimizing an\nenergy functional between two probability measures. However, many real-world\nsystems, such as cell cycles in single-cell RNA, are known to exhibit\nnon-gradient, periodic behavior, which fundamentally cannot be captured by\ncurrent state-of-the-art methods such as flow and bridge matching. In this\npaper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is\ncapable of learning non-gradient field dynamics by designing and solving a\nSchr\\\"odinger bridge problem with a non-zero drift reference process -- in\nstark contrast to typical zero-drift reference processes -- which is\nconstructed using inferred velocities in addition to population snapshot data.\nWe showcase Curly-FM by solving the trajectory inference problems for single\ncells, computational fluid dynamics, and ocean currents with approximate\nvelocities. We demonstrate that Curly-FM can learn trajectories that better\nmatch both the reference process and population marginals. Curly-FM expands\nflow matching models beyond the modeling of populations and towards the\nmodeling of known periodic behavior in physical systems. Our code repository is\naccessible at: https://github.com/kpetrovicc/curly-flow-matching.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling the transport dynamics of natural processes from population-level\nobservations is a ubiquitous problem in the natural sciences. Such models rely\non key assumptions about the underlying process in order to enable faithful\nlearning of governing dynamics that mimic the actual system behavior. The de\nfacto assumption in current approaches relies on the principle of least action\nthat results in gradient field dynamics and leads to trajectories minimizing an\nenergy functional between two probability measures. However, many real-world\nsystems, such as cell cycles in single-cell RNA, are known to exhibit\nnon-gradient, periodic behavior, which fundamentally cannot be captured by\ncurrent state-of-the-art methods such as flow and bridge matching. In this\npaper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is\ncapable of learning non-gradient field dynamics by designing and solving a\nSchr\\\"odinger bridge problem with a non-zero drift reference process -- in\nstark contrast to typical zero-drift reference processes -- which is\nconstructed using inferred velocities in addition to population snapshot data.\nWe showcase Curly-FM by solving the trajectory inference problems for single\ncells, computational fluid dynamics, and ocean currents with approximate\nvelocities. We demonstrate that Curly-FM can learn trajectories that better\nmatch both the reference process and population marginals. Curly-FM expands\nflow matching models beyond the modeling of populations and towards the\nmodeling of known periodic behavior in physical systems. Our code repository is\naccessible at: https://github.com/kpetrovicc/curly-flow-matching.git"
                },
                "authors": [
                    {
                        "name": "Katarina Petrović"
                    },
                    {
                        "name": "Lazar Atanackovic"
                    },
                    {
                        "name": "Viggo Moro"
                    },
                    {
                        "name": "Kacper Kapuśniak"
                    },
                    {
                        "name": "İsmail İlkan Ceylan"
                    },
                    {
                        "name": "Michael Bronstein"
                    },
                    {
                        "name": "Avishek Joey Bose"
                    },
                    {
                        "name": "Alexander Tong"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Tong"
                },
                "author": "Alexander Tong",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26641v1",
                "updated": "2025-10-30T16:08:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    8,
                    25,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:08:25Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    8,
                    25,
                    3,
                    303,
                    0
                ],
                "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to\n  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All You Need for Object Detection: From Pixels, Points, and Prompts to\n  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles"
                },
                "summary": "Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities."
                },
                "authors": [
                    {
                        "name": "Sayed Pedram Haeri Boroujeni"
                    },
                    {
                        "name": "Niloufar Mehrabi"
                    },
                    {
                        "name": "Hazim Alzorgan"
                    },
                    {
                        "name": "Ahmad Sarlak"
                    },
                    {
                        "name": "Mahlagha Fazeli"
                    },
                    {
                        "name": "Abolfazl Razi"
                    }
                ],
                "author_detail": {
                    "name": "Abolfazl Razi"
                },
                "author": "Abolfazl Razi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26634v1",
                "updated": "2025-10-30T16:03:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    3,
                    56,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:03:56Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    3,
                    56,
                    3,
                    303,
                    0
                ],
                "title": "Stitch: Step-by-step LLM Guided Tutoring for Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stitch: Step-by-step LLM Guided Tutoring for Scratch"
                },
                "summary": "Block-based environments such as Scratch are increasingly popular in\nprogramming education. While block syntax reduces surface errors, semantic bugs\nremain common and challenging for novices to resolve. Existing debugging\nworkflows typically show the correct program directly to learners, a strategy\nthat may fix errors but undermines the development of problem-solving skills.\n  We present Stitch, an interactive tutoring system that replaces \"showing the\nanswer\" with step-by-step scaffolding. The system's Diff-Analyze module\ncontrasts a student's project with a reference implementation, identifies the\nmost critical differences, and uses a large language model to explain why these\nchanges matter. Learners inspect highlighted blocks through a custom rendering\nengine, understand the explanations, and selectively apply partial fixes. This\niterative process continues until the intended functionality is achieved.\n  We evaluate Stitch in an empirical study, comparing it against a\nstate-of-the-art automated feedback generation tool for Scratch. Our key\ninsight is that simply presenting the correct program is pedagogically\nineffective. In contrast, our interactive, step-by-step guided system promotes\na more effective learning experience. More broadly, what constitutes effective\nfeedback in block-based programming remains an open question. Our evaluation\nprovides new evidence that step-by-step tutoring significantly enhances\nlearning outcomes, outperforming both direct-answer approaches and current\nautomated feedback generation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-based environments such as Scratch are increasingly popular in\nprogramming education. While block syntax reduces surface errors, semantic bugs\nremain common and challenging for novices to resolve. Existing debugging\nworkflows typically show the correct program directly to learners, a strategy\nthat may fix errors but undermines the development of problem-solving skills.\n  We present Stitch, an interactive tutoring system that replaces \"showing the\nanswer\" with step-by-step scaffolding. The system's Diff-Analyze module\ncontrasts a student's project with a reference implementation, identifies the\nmost critical differences, and uses a large language model to explain why these\nchanges matter. Learners inspect highlighted blocks through a custom rendering\nengine, understand the explanations, and selectively apply partial fixes. This\niterative process continues until the intended functionality is achieved.\n  We evaluate Stitch in an empirical study, comparing it against a\nstate-of-the-art automated feedback generation tool for Scratch. Our key\ninsight is that simply presenting the correct program is pedagogically\nineffective. In contrast, our interactive, step-by-step guided system promotes\na more effective learning experience. More broadly, what constitutes effective\nfeedback in block-based programming remains an open question. Our evaluation\nprovides new evidence that step-by-step tutoring significantly enhances\nlearning outcomes, outperforming both direct-answer approaches and current\nautomated feedback generation tools."
                },
                "authors": [
                    {
                        "name": "Yuan Si"
                    },
                    {
                        "name": "Kyle Qi"
                    },
                    {
                        "name": "Daming Li"
                    },
                    {
                        "name": "Hanyuan Shi"
                    },
                    {
                        "name": "Jialu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jialu Zhang"
                },
                "author": "Jialu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26622v1",
                "updated": "2025-10-30T15:48:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    48,
                    28,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:48:28Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    48,
                    28,
                    3,
                    303,
                    0
                ],
                "title": "Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large\n  Language Model"
                },
                "summary": "Recent large language model (LLM) research has undergone an architectural\nshift from encoder-decoder modeling to nowadays the dominant decoder-only\nmodeling. This rapid transition, however, comes without a rigorous comparative\nanalysis especially \\textit{from the scaling perspective}, raising concerns\nthat the potential of encoder-decoder models may have been overlooked. To fill\nthis gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent\nrecipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison\nbetween RedLLM, pretrained with prefix language modeling (LM), and DecLLM,\npretrained with causal LM, at different model scales, ranging from $\\sim$150M\nto $\\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for\ninstruction tuning, our experiments show that RedLLM produces compelling\nscaling properties and surprisingly strong performance. While DecLLM is overall\nmore compute-optimal during pretraining, RedLLM demonstrates comparable scaling\nand context length extrapolation capabilities. After instruction tuning, RedLLM\nachieves comparable and even better results on various downstream tasks while\nenjoying substantially better inference efficiency. We hope our findings could\ninspire more efforts on re-examining RedLLM, unlocking its potential for\ndeveloping powerful and efficient LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model (LLM) research has undergone an architectural\nshift from encoder-decoder modeling to nowadays the dominant decoder-only\nmodeling. This rapid transition, however, comes without a rigorous comparative\nanalysis especially \\textit{from the scaling perspective}, raising concerns\nthat the potential of encoder-decoder models may have been overlooked. To fill\nthis gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent\nrecipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison\nbetween RedLLM, pretrained with prefix language modeling (LM), and DecLLM,\npretrained with causal LM, at different model scales, ranging from $\\sim$150M\nto $\\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for\ninstruction tuning, our experiments show that RedLLM produces compelling\nscaling properties and surprisingly strong performance. While DecLLM is overall\nmore compute-optimal during pretraining, RedLLM demonstrates comparable scaling\nand context length extrapolation capabilities. After instruction tuning, RedLLM\nachieves comparable and even better results on various downstream tasks while\nenjoying substantially better inference efficiency. We hope our findings could\ninspire more efforts on re-examining RedLLM, unlocking its potential for\ndeveloping powerful and efficient LLMs."
                },
                "authors": [
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Yong Cheng"
                    },
                    {
                        "name": "Siamak Shakeri"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Orhan Firat"
                    }
                ],
                "author_detail": {
                    "name": "Orhan Firat"
                },
                "author": "Orhan Firat",
                "arxiv_comment": "The scaling study inspiring T5Gemma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17773v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17773v3",
                "updated": "2025-10-30T15:43:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    43,
                    22,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-23T11:44:02Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    11,
                    44,
                    2,
                    4,
                    143,
                    0
                ],
                "title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (C-LoRA) as a novel\nuncertainty-aware and parameter efficient fine-tuning approach, by developing\nnew lightweight LoRA modules contextualized to each input data sample to\ndynamically adapt uncertainty estimates. Incorporating data-driven contexts\ninto the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments on LLaMA2-7B models demonstrate that C-LoRA consistently\noutperforms the state-of-the-art uncertainty-aware LoRA methods in both\nuncertainty quantification and model generalization. Ablation studies further\nconfirm the critical role of our contextual modules in capturing\nsample-specific uncertainties. C-LoRA sets a new standard for robust,\nuncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments\nare limited to 7B models, our method is architecture-agnostic and, in\nprinciple, applies beyond this scale; studying its scaling to larger models\nremains an open problem. Our code is available at\nhttps://github.com/ahra99/c_lora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (C-LoRA) as a novel\nuncertainty-aware and parameter efficient fine-tuning approach, by developing\nnew lightweight LoRA modules contextualized to each input data sample to\ndynamically adapt uncertainty estimates. Incorporating data-driven contexts\ninto the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments on LLaMA2-7B models demonstrate that C-LoRA consistently\noutperforms the state-of-the-art uncertainty-aware LoRA methods in both\nuncertainty quantification and model generalization. Ablation studies further\nconfirm the critical role of our contextual modules in capturing\nsample-specific uncertainties. C-LoRA sets a new standard for robust,\nuncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments\nare limited to 7B models, our method is architecture-agnostic and, in\nprinciple, applies beyond this scale; studying its scaling to larger models\nremains an open problem. Our code is available at\nhttps://github.com/ahra99/c_lora."
                },
                "authors": [
                    {
                        "name": "Amir Hossein Rahmati"
                    },
                    {
                        "name": "Sanket Jantre"
                    },
                    {
                        "name": "Weifeng Zhang"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Byung-Jun Yoon"
                    },
                    {
                        "name": "Nathan M. Urban"
                    },
                    {
                        "name": "Xiaoning Qian"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoning Qian"
                },
                "author": "Xiaoning Qian",
                "arxiv_comment": "Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17773v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26615v1",
                "updated": "2025-10-30T15:41:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    41,
                    15,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:41:15Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    41,
                    15,
                    3,
                    303,
                    0
                ],
                "title": "SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual\n  Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual\n  Document Understanding"
                },
                "summary": "Multi-page visual documents such as manuals, brochures, presentations, and\nposters convey key information through layout, colors, icons, and cross-slide\nreferences. While large language models (LLMs) offer opportunities in document\nunderstanding, current systems struggle with complex, multi-page visual\ndocuments, particularly in fine-grained reasoning over elements and pages. We\nintroduce SlideAgent, a versatile agentic framework for understanding\nmulti-modal, multi-page, and multi-layout documents, especially slide decks.\nSlideAgent employs specialized agents and decomposes reasoning into three\nspecialized levels-global, page, and element-to construct a structured,\nquery-agnostic representation that captures both overarching themes and\ndetailed visual or textual cues. During inference, SlideAgent selectively\nactivates specialized agents for multi-level reasoning and integrates their\noutputs into coherent, context-aware answers. Extensive experiments show that\nSlideAgent achieves significant improvement over both proprietary (+7.9\noverall) and open-source models (+9.8 overall).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-page visual documents such as manuals, brochures, presentations, and\nposters convey key information through layout, colors, icons, and cross-slide\nreferences. While large language models (LLMs) offer opportunities in document\nunderstanding, current systems struggle with complex, multi-page visual\ndocuments, particularly in fine-grained reasoning over elements and pages. We\nintroduce SlideAgent, a versatile agentic framework for understanding\nmulti-modal, multi-page, and multi-layout documents, especially slide decks.\nSlideAgent employs specialized agents and decomposes reasoning into three\nspecialized levels-global, page, and element-to construct a structured,\nquery-agnostic representation that captures both overarching themes and\ndetailed visual or textual cues. During inference, SlideAgent selectively\nactivates specialized agents for multi-level reasoning and integrates their\noutputs into coherent, context-aware answers. Extensive experiments show that\nSlideAgent achieves significant improvement over both proprietary (+7.9\noverall) and open-source models (+9.8 overall)."
                },
                "authors": [
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Rachneet Kaur"
                    },
                    {
                        "name": "Zhen Zeng"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    },
                    {
                        "name": "Srijan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Srijan Kumar"
                },
                "author": "Srijan Kumar",
                "arxiv_comment": "https://slideagent.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26614v1",
                "updated": "2025-10-30T15:40:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    40,
                    34,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:40:34Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    40,
                    34,
                    3,
                    303,
                    0
                ],
                "title": "Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event\n  Cameras",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event\n  Cameras"
                },
                "summary": "We propose tokenization of events and present a tokenizer, Spiking Patches,\nspecifically designed for event cameras. Given a stream of asynchronous and\nspatially sparse events, our goal is to discover an event representation that\npreserves these properties. Prior works have represented events as frames or as\nvoxels. However, while these representations yield high accuracy, both frames\nand voxels are synchronous and decrease the spatial sparsity. Spiking Patches\ngives the means to preserve the unique properties of event cameras and we show\nin our experiments that this comes without sacrificing accuracy. We evaluate\nour tokenizer using a GNN, PCN, and a Transformer on gesture recognition and\nobject detection. Tokens from Spiking Patches yield inference times that are up\nto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We\nachieve this while matching their accuracy and even surpassing in some cases\nwith absolute improvements up to 3.8 for gesture recognition and up to 1.4 for\nobject detection. Thus, tokenization constitutes a novel direction in\nevent-based vision and marks a step towards methods that preserve the\nproperties of event cameras.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose tokenization of events and present a tokenizer, Spiking Patches,\nspecifically designed for event cameras. Given a stream of asynchronous and\nspatially sparse events, our goal is to discover an event representation that\npreserves these properties. Prior works have represented events as frames or as\nvoxels. However, while these representations yield high accuracy, both frames\nand voxels are synchronous and decrease the spatial sparsity. Spiking Patches\ngives the means to preserve the unique properties of event cameras and we show\nin our experiments that this comes without sacrificing accuracy. We evaluate\nour tokenizer using a GNN, PCN, and a Transformer on gesture recognition and\nobject detection. Tokens from Spiking Patches yield inference times that are up\nto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We\nachieve this while matching their accuracy and even surpassing in some cases\nwith absolute improvements up to 3.8 for gesture recognition and up to 1.4 for\nobject detection. Thus, tokenization constitutes a novel direction in\nevent-based vision and marks a step towards methods that preserve the\nproperties of event cameras."
                },
                "authors": [
                    {
                        "name": "Christoffer Koo Øhrstrøm"
                    },
                    {
                        "name": "Ronja Güldenring"
                    },
                    {
                        "name": "Lazaros Nalpantidis"
                    }
                ],
                "author_detail": {
                    "name": "Lazaros Nalpantidis"
                },
                "author": "Lazaros Nalpantidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26606v1",
                "updated": "2025-10-30T15:35:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    35,
                    13,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:35:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    35,
                    13,
                    3,
                    303,
                    0
                ],
                "title": "Normative Reasoning in Large Language Models: A Comparative Benchmark\n  from Logical and Modal Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normative Reasoning in Large Language Models: A Comparative Benchmark\n  from Logical and Modal Perspectives"
                },
                "summary": "Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO."
                },
                "authors": [
                    {
                        "name": "Kentaro Ozeki"
                    },
                    {
                        "name": "Risako Ando"
                    },
                    {
                        "name": "Takanobu Morishita"
                    },
                    {
                        "name": "Hirohiko Abe"
                    },
                    {
                        "name": "Koji Mineshima"
                    },
                    {
                        "name": "Mitsuhiro Okada"
                    }
                ],
                "author_detail": {
                    "name": "Mitsuhiro Okada"
                },
                "author": "Mitsuhiro Okada",
                "arxiv_comment": "Accepted to the 8th BlackboxNLP Workshop at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26603v1",
                "updated": "2025-10-30T15:33:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    33,
                    52,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:33:52Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    33,
                    52,
                    3,
                    303,
                    0
                ],
                "title": "Agentic AI Home Energy Management System: A Large Language Model\n  Framework for Residential Load Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI Home Energy Management System: A Large Language Model\n  Framework for Residential Load Scheduling"
                },
                "summary": "The electricity sector transition requires substantial increases in\nresidential demand response capacity, yet Home Energy Management Systems (HEMS)\nadoption remains limited by user interaction barriers requiring translation of\neveryday preferences into technical parameters. While large language models\nhave been applied to energy systems as code generators and parameter\nextractors, no existing implementation deploys LLMs as autonomous coordinators\nmanaging the complete workflow from natural language input to multi-appliance\nscheduling. This paper presents an agentic AI HEMS where LLMs autonomously\ncoordinate multi-appliance scheduling from natural language requests to device\ncontrol, achieving optimal scheduling without example demonstrations. A\nhierarchical architecture combining one orchestrator with three specialist\nagents uses the ReAct pattern for iterative reasoning, enabling dynamic\ncoordination without hardcoded workflows while integrating Google Calendar for\ncontext-aware deadline extraction. Evaluation across three open-source models\nusing real Austrian day-ahead electricity prices reveals substantial capability\ndifferences. Llama-3.3-70B successfully coordinates all appliances across all\nscenarios to match cost-optimal benchmarks computed via mixed-integer linear\nprogramming, while other models achieve perfect single-appliance performance\nbut struggle to coordinate all appliances simultaneously. Progressive prompt\nengineering experiments demonstrate that analytical query handling without\nexplicit guidance remains unreliable despite models' general reasoning\ncapabilities. We open-source the complete system including orchestration logic,\nagent prompts, tools, and web interfaces to enable reproducibility, extension,\nand future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electricity sector transition requires substantial increases in\nresidential demand response capacity, yet Home Energy Management Systems (HEMS)\nadoption remains limited by user interaction barriers requiring translation of\neveryday preferences into technical parameters. While large language models\nhave been applied to energy systems as code generators and parameter\nextractors, no existing implementation deploys LLMs as autonomous coordinators\nmanaging the complete workflow from natural language input to multi-appliance\nscheduling. This paper presents an agentic AI HEMS where LLMs autonomously\ncoordinate multi-appliance scheduling from natural language requests to device\ncontrol, achieving optimal scheduling without example demonstrations. A\nhierarchical architecture combining one orchestrator with three specialist\nagents uses the ReAct pattern for iterative reasoning, enabling dynamic\ncoordination without hardcoded workflows while integrating Google Calendar for\ncontext-aware deadline extraction. Evaluation across three open-source models\nusing real Austrian day-ahead electricity prices reveals substantial capability\ndifferences. Llama-3.3-70B successfully coordinates all appliances across all\nscenarios to match cost-optimal benchmarks computed via mixed-integer linear\nprogramming, while other models achieve perfect single-appliance performance\nbut struggle to coordinate all appliances simultaneously. Progressive prompt\nengineering experiments demonstrate that analytical query handling without\nexplicit guidance remains unreliable despite models' general reasoning\ncapabilities. We open-source the complete system including orchestration logic,\nagent prompts, tools, and web interfaces to enable reproducibility, extension,\nand future research."
                },
                "authors": [
                    {
                        "name": "Reda El Makroum"
                    },
                    {
                        "name": "Sebastian Zwickl-Bernhard"
                    },
                    {
                        "name": "Lukas Kranzl"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Kranzl"
                },
                "author": "Lukas Kranzl",
                "arxiv_comment": "34 pages, 9 figures. Code available at\n  https://github.com/RedaElMakroum/agentic-ai-hems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17197v2",
                "updated": "2025-10-30T15:26:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    26,
                    13,
                    3,
                    303,
                    0
                ],
                "published": "2025-09-21T18:54:54Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    18,
                    54,
                    54,
                    6,
                    264,
                    0
                ],
                "title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal\n  Processing"
                },
                "summary": "Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings."
                },
                "authors": [
                    {
                        "name": "Junlong Ke"
                    },
                    {
                        "name": "Qiying Hu"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Yuecong Xu"
                    },
                    {
                        "name": "Jianfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Yang"
                },
                "author": "Jianfei Yang",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12673v2",
                "updated": "2025-10-30T15:22:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    22,
                    50,
                    3,
                    303,
                    0
                ],
                "published": "2025-07-16T23:12:00Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    23,
                    12,
                    0,
                    2,
                    197,
                    0
                ],
                "title": "Semiparametric Learning of Integral Functionals on Submanifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric Learning of Integral Functionals on Submanifolds"
                },
                "summary": "This paper studies the semiparametric estimation and inference of integral\nfunctionals on submanifolds, which arise naturally in a variety of econometric\nsettings. For linear integral functionals on a regular submanifold, we show\nthat the semiparametric plug-in estimatorattains the minimax-optimal\nconvergence rate $n^{-\\frac{s}{2s+d-m}}$, where $s$ is the H\\\"{o}lder\nsmoothness order of the underlying nonparametric function, $d$ is the dimension\nof the first-stage nonparametric estimation, $m$ is the dimension of the\nsubmanifold over which the integral is taken. This rate coincides with the\nstandard minimax-optimal rate for a $(d-m)$-dimensional nonparametric\nestimation problem, illustrating that integration over the $m$-dimensional\nmanifold effectively reduces the problem's dimensionality. We then provide a\ngeneral asymptotic normality theorem for linear/nonlinear submanifold\nintegrals, along with a consistent variance estimator. We provide simulation\nevidence in support of our theoretical results. In a companion paper, Chen,\nChen & Gao (2025), we apply the main results of this paper to the inference\nproblem on the welfare and value of a policy treatment under first-best\ntreatment assignment, and conduct an empirical illustration using the JTPA data\nset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the semiparametric estimation and inference of integral\nfunctionals on submanifolds, which arise naturally in a variety of econometric\nsettings. For linear integral functionals on a regular submanifold, we show\nthat the semiparametric plug-in estimatorattains the minimax-optimal\nconvergence rate $n^{-\\frac{s}{2s+d-m}}$, where $s$ is the H\\\"{o}lder\nsmoothness order of the underlying nonparametric function, $d$ is the dimension\nof the first-stage nonparametric estimation, $m$ is the dimension of the\nsubmanifold over which the integral is taken. This rate coincides with the\nstandard minimax-optimal rate for a $(d-m)$-dimensional nonparametric\nestimation problem, illustrating that integration over the $m$-dimensional\nmanifold effectively reduces the problem's dimensionality. We then provide a\ngeneral asymptotic normality theorem for linear/nonlinear submanifold\nintegrals, along with a consistent variance estimator. We provide simulation\nevidence in support of our theoretical results. In a companion paper, Chen,\nChen & Gao (2025), we apply the main results of this paper to the inference\nproblem on the welfare and value of a policy treatment under first-best\ntreatment assignment, and conduct an empirical illustration using the JTPA data\nset."
                },
                "authors": [
                    {
                        "name": "Xiaohong Chen"
                    },
                    {
                        "name": "Wayne Yuan Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Yuan Gao"
                },
                "author": "Wayne Yuan Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26593v1",
                "updated": "2025-10-30T15:18:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    18,
                    7,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:18:07Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    18,
                    7,
                    3,
                    303,
                    0
                ],
                "title": "Hybrid Physical-Neural Simulator for Fast Cosmological Hydrodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Physical-Neural Simulator for Fast Cosmological Hydrodynamics"
                },
                "summary": "Cosmological field-level inference requires differentiable forward models\nthat solve the challenging dynamics of gas and dark matter under hydrodynamics\nand gravity. We propose a hybrid approach where gravitational forces are\ncomputed using a differentiable particle-mesh solver, while the hydrodynamics\nare parametrized by a neural network that maps local quantities to an effective\npressure field. We demonstrate that our method improves upon alternative\napproaches, such as an Enthalpy Gradient Descent baseline, both at the field\nand summary-statistic level. The approach is furthermore highly data efficient,\nwith a single reference simulation of cosmological structure formation being\nsufficient to constrain the neural pressure model. This opens the door for\nfuture applications where the model is fit directly to observational data,\nrather than a training set of simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological field-level inference requires differentiable forward models\nthat solve the challenging dynamics of gas and dark matter under hydrodynamics\nand gravity. We propose a hybrid approach where gravitational forces are\ncomputed using a differentiable particle-mesh solver, while the hydrodynamics\nare parametrized by a neural network that maps local quantities to an effective\npressure field. We demonstrate that our method improves upon alternative\napproaches, such as an Enthalpy Gradient Descent baseline, both at the field\nand summary-statistic level. The approach is furthermore highly data efficient,\nwith a single reference simulation of cosmological structure formation being\nsufficient to constrain the neural pressure model. This opens the door for\nfuture applications where the model is fit directly to observational data,\nrather than a training set of simulations."
                },
                "authors": [
                    {
                        "name": "Arne Thomsen"
                    },
                    {
                        "name": "Tilman Tröster"
                    },
                    {
                        "name": "François Lanusse"
                    }
                ],
                "author_detail": {
                    "name": "François Lanusse"
                },
                "author": "François Lanusse",
                "arxiv_comment": "Accepted to the NeurIPS 2025 Workshop on Machine Learning and the\n  Physical Sciences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26585v1",
                "updated": "2025-10-30T15:12:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    12,
                    59,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:12:59Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    12,
                    59,
                    3,
                    303,
                    0
                ],
                "title": "Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems"
                },
                "summary": "While Multi-Agent Systems (MAS) excel at complex tasks, their growing\nautonomy with operational complexity often leads to critical inefficiencies,\nsuch as excessive token consumption and failures arising from misinformation.\nExisting methods primarily focus on post-hoc failure attribution, lacking\nproactive, real-time interventions to enhance robustness and efficiency. To\nthis end, we introduce SupervisorAgent, a lightweight and modular framework for\nruntime, adaptive supervision that operates without altering the base agent's\narchitecture. Triggered by an LLM-free adaptive filter, SupervisorAgent\nintervenes at critical junctures to proactively correct errors, guide\ninefficient behaviors, and purify observations. On the challenging GAIA\nbenchmark, SupervisorAgent reduces the token consumption of the Smolagent\nframework by an average of 29.45% without compromising its success rate.\nExtensive experiments across five additional benchmarks (math reasoning, code\ngeneration, and question answering) and various SoTA foundation models validate\nthe broad applicability and robustness of our approach. The code is available\nat https://github.com/LINs-lab/SupervisorAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multi-Agent Systems (MAS) excel at complex tasks, their growing\nautonomy with operational complexity often leads to critical inefficiencies,\nsuch as excessive token consumption and failures arising from misinformation.\nExisting methods primarily focus on post-hoc failure attribution, lacking\nproactive, real-time interventions to enhance robustness and efficiency. To\nthis end, we introduce SupervisorAgent, a lightweight and modular framework for\nruntime, adaptive supervision that operates without altering the base agent's\narchitecture. Triggered by an LLM-free adaptive filter, SupervisorAgent\nintervenes at critical junctures to proactively correct errors, guide\ninefficient behaviors, and purify observations. On the challenging GAIA\nbenchmark, SupervisorAgent reduces the token consumption of the Smolagent\nframework by an average of 29.45% without compromising its success rate.\nExtensive experiments across five additional benchmarks (math reasoning, code\ngeneration, and question answering) and various SoTA foundation models validate\nthe broad applicability and robustness of our approach. The code is available\nat https://github.com/LINs-lab/SupervisorAgent."
                },
                "authors": [
                    {
                        "name": "Fulin Lin"
                    },
                    {
                        "name": "Shaowen Chen"
                    },
                    {
                        "name": "Ruishan Fang"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26583v1",
                "updated": "2025-10-30T15:11:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    11,
                    16,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:11:16Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    11,
                    16,
                    3,
                    303,
                    0
                ],
                "title": "Emu3.5: Native Multimodal Models are World Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emu3.5: Native Multimodal Models are World Learners"
                },
                "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research."
                },
                "authors": [
                    {
                        "name": "Yufeng Cui"
                    },
                    {
                        "name": "Honghao Chen"
                    },
                    {
                        "name": "Haoge Deng"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Xinghang Li"
                    },
                    {
                        "name": "Jirong Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhuoyan Luo"
                    },
                    {
                        "name": "Jinsheng Wang"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Yueze Wang"
                    },
                    {
                        "name": "Chengyuan Wang"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Yingli Zhao"
                    },
                    {
                        "name": "Ting Pan"
                    },
                    {
                        "name": "Xianduo Li"
                    },
                    {
                        "name": "Zecheng Hao"
                    },
                    {
                        "name": "Wenxuan Ma"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Xinlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinlong Wang"
                },
                "author": "Xinlong Wang",
                "arxiv_comment": "project page: https://emu.world",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26582v1",
                "updated": "2025-10-30T15:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    10,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:10:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    10,
                    2,
                    3,
                    303,
                    0
                ],
                "title": "CATCH: A Modular Cross-domain Adaptive Template with Hook",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CATCH: A Modular Cross-domain Adaptive Template with Hook"
                },
                "summary": "Recent advances in Visual Question Answering (VQA) have demonstrated\nimpressive performance in natural image domains, with models like LLaVA\nleveraging large language models (LLMs) for open-ended reasoning. However,\ntheir generalization degrades significantly when transferred to out-of-domain\nscenarios such as remote sensing, medical imaging, or math diagrams, due to\nlarge distributional shifts and the lack of effective domain adaptation\nmechanisms. Existing approaches typically rely on per-domain fine-tuning or\nbespoke pipelines, which are costly, inflexible, and not scalable across\ndiverse tasks. In this paper, we propose CATCH, a plug-and-play framework for\ncross-domain adaptation that improves the generalization of VQA models while\nrequiring minimal changes to their core architecture. Our key idea is to\ndecouple visual and linguistic adaptation by introducing two lightweight\nmodules: a domain classifier to identify the input image type, and a dual\nadapter mechanism comprising a Prompt Adapter for language modulation and a\nVisual Adapter for vision feature adjustment. Both modules are dynamically\ninjected via a unified hook interface, requiring no retraining of the backbone\nmodel. Experimental results across four domain-specific VQA benchmarks\ndemonstrate that our framework achieves consistent performance gains without\nretraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on\nMedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH\nprovides a scalable and extensible approach to multi-domain VQA, enabling\npractical deployment across diverse application domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Visual Question Answering (VQA) have demonstrated\nimpressive performance in natural image domains, with models like LLaVA\nleveraging large language models (LLMs) for open-ended reasoning. However,\ntheir generalization degrades significantly when transferred to out-of-domain\nscenarios such as remote sensing, medical imaging, or math diagrams, due to\nlarge distributional shifts and the lack of effective domain adaptation\nmechanisms. Existing approaches typically rely on per-domain fine-tuning or\nbespoke pipelines, which are costly, inflexible, and not scalable across\ndiverse tasks. In this paper, we propose CATCH, a plug-and-play framework for\ncross-domain adaptation that improves the generalization of VQA models while\nrequiring minimal changes to their core architecture. Our key idea is to\ndecouple visual and linguistic adaptation by introducing two lightweight\nmodules: a domain classifier to identify the input image type, and a dual\nadapter mechanism comprising a Prompt Adapter for language modulation and a\nVisual Adapter for vision feature adjustment. Both modules are dynamically\ninjected via a unified hook interface, requiring no retraining of the backbone\nmodel. Experimental results across four domain-specific VQA benchmarks\ndemonstrate that our framework achieves consistent performance gains without\nretraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on\nMedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH\nprovides a scalable and extensible approach to multi-domain VQA, enabling\npractical deployment across diverse application domains."
                },
                "authors": [
                    {
                        "name": "Xinjin Li"
                    },
                    {
                        "name": "Yulie Lu"
                    },
                    {
                        "name": "Jinghan Cao"
                    },
                    {
                        "name": "Yu Ma"
                    },
                    {
                        "name": "Zhenglin Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yeyang Zhou"
                },
                "author": "Yeyang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24518v2",
                "updated": "2025-10-30T15:08:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    8,
                    0,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-30T12:30:04Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    12,
                    30,
                    4,
                    4,
                    150,
                    0
                ],
                "title": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis\n  Optimization for Speech Multi-Metric Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis\n  Optimization for Speech Multi-Metric Estimation"
                },
                "summary": "Speech signal analysis poses significant challenges, particularly in tasks\nsuch as speech quality evaluation and profiling, where the goal is to predict\nmultiple perceptual and objective metrics. For instance, metrics like PESQ\n(Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective\nIntelligibility), and MOS (Mean Opinion Score) each capture different aspects\nof speech quality. However, these metrics often have different scales,\nassumptions, and dependencies, making joint estimation non-trivial. To address\nthese issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based\nHypothesis Optimization), a chain-based, versatile evaluation system for speech\nassessment grounded in autoregressive dependency modeling. ARECHO is\ndistinguished by three key innovations: (1) a comprehensive speech information\ntokenization pipeline; (2) a dynamic classifier chain that explicitly captures\ninter-metric dependencies; and (3) a two-step confidence-oriented decoding\nalgorithm that enhances inference reliability. Experiments demonstrate that\nARECHO significantly outperforms the baseline framework across diverse\nevaluation scenarios, including enhanced speech analysis, speech generation\nevaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency\nmodeling improves interpretability by capturing inter-metric relationships.\nAcross tasks, ARECHO offers reference-free evaluation using its dynamic\nclassifier chain to support subset queries (single or multiple metrics) and\nreduces error propagation via confidence-oriented decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech signal analysis poses significant challenges, particularly in tasks\nsuch as speech quality evaluation and profiling, where the goal is to predict\nmultiple perceptual and objective metrics. For instance, metrics like PESQ\n(Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective\nIntelligibility), and MOS (Mean Opinion Score) each capture different aspects\nof speech quality. However, these metrics often have different scales,\nassumptions, and dependencies, making joint estimation non-trivial. To address\nthese issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based\nHypothesis Optimization), a chain-based, versatile evaluation system for speech\nassessment grounded in autoregressive dependency modeling. ARECHO is\ndistinguished by three key innovations: (1) a comprehensive speech information\ntokenization pipeline; (2) a dynamic classifier chain that explicitly captures\ninter-metric dependencies; and (3) a two-step confidence-oriented decoding\nalgorithm that enhances inference reliability. Experiments demonstrate that\nARECHO significantly outperforms the baseline framework across diverse\nevaluation scenarios, including enhanced speech analysis, speech generation\nevaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency\nmodeling improves interpretability by capturing inter-metric relationships.\nAcross tasks, ARECHO offers reference-free evaluation using its dynamic\nclassifier chain to support subset queries (single or multiple metrics) and\nreduces error propagation via confidence-oriented decoding."
                },
                "authors": [
                    {
                        "name": "Jiatong Shi"
                    },
                    {
                        "name": "Yifan Cheng"
                    },
                    {
                        "name": "Bo-Hao Su"
                    },
                    {
                        "name": "Hye-jin Shim"
                    },
                    {
                        "name": "Jinchuan Tian"
                    },
                    {
                        "name": "Samuele Cornell"
                    },
                    {
                        "name": "Yiwen Zhao"
                    },
                    {
                        "name": "Siddhant Arora"
                    },
                    {
                        "name": "Shinji Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Shinji Watanabe"
                },
                "author": "Shinji Watanabe",
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26580v1",
                "updated": "2025-10-30T15:07:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    7,
                    55,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:07:55Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    7,
                    55,
                    3,
                    303,
                    0
                ],
                "title": "Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in\n  Zero-Shot Real-World Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in\n  Zero-Shot Real-World Scenarios"
                },
                "summary": "In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings."
                },
                "authors": [
                    {
                        "name": "Manjunath Prasad Holenarasipura Rajiv"
                    },
                    {
                        "name": "B. M. Vidyavathi"
                    }
                ],
                "author_detail": {
                    "name": "B. M. Vidyavathi"
                },
                "author": "B. M. Vidyavathi",
                "arxiv_comment": "Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14409v2",
                "updated": "2025-10-30T15:05:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    5,
                    42,
                    3,
                    303,
                    0
                ],
                "published": "2025-02-20T09:57:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    57,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization"
                },
                "summary": "Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query, and extracting and citing evidence\nspans helps improve the trustworthiness of these summaries. Whereas previous\nwork has focused on evidence citation with fixed levels of granularity (e.g.\nsentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,\nspans of any length) evidence in order to acquire more relevant and consistent\nevidence than in the fixed granularity case. We show how existing systems\nstruggle to copy and properly cite unstructured evidence, which also tends to\nbe \"lost-in-the-middle\". To help models perform this task, we create the\nSummaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset\ngenerated using a novel pipeline, which can be used as training supervision for\nunstructured evidence summarization. We demonstrate across 5 LLMs and 4\ndatasets spanning human written, synthetic, single, and multi-document settings\nthat LLMs adapted with SUnsET generate more relevant and factually consistent\nevidence with their summaries, extract evidence from more diverse locations in\ntheir context, and can generate more relevant and consistent summaries than\nbaselines with no fine-tuning and fixed granularity evidence. We release SUnsET\nand our generation code to the public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query, and extracting and citing evidence\nspans helps improve the trustworthiness of these summaries. Whereas previous\nwork has focused on evidence citation with fixed levels of granularity (e.g.\nsentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,\nspans of any length) evidence in order to acquire more relevant and consistent\nevidence than in the fixed granularity case. We show how existing systems\nstruggle to copy and properly cite unstructured evidence, which also tends to\nbe \"lost-in-the-middle\". To help models perform this task, we create the\nSummaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset\ngenerated using a novel pipeline, which can be used as training supervision for\nunstructured evidence summarization. We demonstrate across 5 LLMs and 4\ndatasets spanning human written, synthetic, single, and multi-document settings\nthat LLMs adapted with SUnsET generate more relevant and factually consistent\nevidence with their summaries, extract evidence from more diverse locations in\ntheir context, and can generate more relevant and consistent summaries than\nbaselines with no fine-tuning and fixed granularity evidence. We release SUnsET\nand our generation code to the public."
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Zain Muhammad Mujahid"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "arxiv_comment": "EMNLP 2025 Main; 29 pages; 24 figures; 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26579v1",
                "updated": "2025-10-30T15:05:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    5,
                    31,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:05:31Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    5,
                    31,
                    3,
                    303,
                    0
                ],
                "title": "Online and Interactive Bayesian Inference Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online and Interactive Bayesian Inference Debugging"
                },
                "summary": "Probabilistic programming is a rapidly developing programming paradigm which\nenables the formulation of Bayesian models as programs and the automation of\nposterior inference. It facilitates the development of models and conducting\nBayesian inference, which makes these techniques available to practitioners\nfrom multiple fields. Nevertheless, probabilistic programming is notoriously\ndifficult as identifying and repairing issues with inference requires a lot of\ntime and deep knowledge. Through this work, we introduce a novel approach to\ndebugging Bayesian inference that reduces time and required knowledge\nsignificantly. We discuss several requirements a Bayesian inference debugging\nframework has to fulfill, and propose a new tool that meets these key\nrequirements directly within the development environment. We evaluate our\nresults in a study with 18 experienced participants and show that our approach\nto online and interactive debugging of Bayesian inference significantly reduces\ntime and difficulty on inference debugging tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic programming is a rapidly developing programming paradigm which\nenables the formulation of Bayesian models as programs and the automation of\nposterior inference. It facilitates the development of models and conducting\nBayesian inference, which makes these techniques available to practitioners\nfrom multiple fields. Nevertheless, probabilistic programming is notoriously\ndifficult as identifying and repairing issues with inference requires a lot of\ntime and deep knowledge. Through this work, we introduce a novel approach to\ndebugging Bayesian inference that reduces time and required knowledge\nsignificantly. We discuss several requirements a Bayesian inference debugging\nframework has to fulfill, and propose a new tool that meets these key\nrequirements directly within the development environment. We evaluate our\nresults in a study with 18 experienced participants and show that our approach\nto online and interactive debugging of Bayesian inference significantly reduces\ntime and difficulty on inference debugging tasks."
                },
                "authors": [
                    {
                        "name": "Nathanael Nussbaumer"
                    },
                    {
                        "name": "Markus Böck"
                    },
                    {
                        "name": "Jürgen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Cito"
                },
                "author": "Jürgen Cito",
                "arxiv_comment": "Accepted by ICSE 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26577v1",
                "updated": "2025-10-30T15:04:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    4,
                    36,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:04:36Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    4,
                    36,
                    3,
                    303,
                    0
                ],
                "title": "Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) face significant inference latency challenges\nstemming from their autoregressive design and large size. To address this,\nspeculative decoding emerges as a solution, enabling the simultaneous\ngeneration and validation of multiple tokens. While recent approaches like\nEAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,\nthey often neglect the impact of crucial system variables such as GPU devices\nand batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that\ntakes into account inference costs, including factors such as GPU\nconfigurations and batch sizes, to dynamically refine the tree structure.\nThrough comprehensive experimentation across six diverse tasks and utilizing\nsix distinct LLMs, our methodology demonstrates remarkable results, achieving\nspeeds up to 5.2 times faster than conventional decoding methods. Moreover, it\ngenerally outperforms existing state-of-the-art techniques from 5% to 20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant inference latency challenges\nstemming from their autoregressive design and large size. To address this,\nspeculative decoding emerges as a solution, enabling the simultaneous\ngeneration and validation of multiple tokens. While recent approaches like\nEAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,\nthey often neglect the impact of crucial system variables such as GPU devices\nand batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that\ntakes into account inference costs, including factors such as GPU\nconfigurations and batch sizes, to dynamically refine the tree structure.\nThrough comprehensive experimentation across six diverse tasks and utilizing\nsix distinct LLMs, our methodology demonstrates remarkable results, achieving\nspeeds up to 5.2 times faster than conventional decoding methods. Moreover, it\ngenerally outperforms existing state-of-the-art techniques from 5% to 20%."
                },
                "authors": [
                    {
                        "name": "Yinrong Hong"
                    },
                    {
                        "name": "Zhiquan Tan"
                    },
                    {
                        "name": "Kai Hu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Hu"
                },
                "author": "Kai Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26575v1",
                "updated": "2025-10-30T15:03:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    3,
                    21,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:03:21Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    3,
                    21,
                    3,
                    303,
                    0
                ],
                "title": "InfoFlow: Reinforcing Search Agent Via Reward Density Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfoFlow: Reinforcing Search Agent Via Reward Density Optimization"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach\nfor enhancing agentic deep search. However, its application is often hindered\nby low \\textbf{Reward Density} in deep search scenarios, where agents expend\nsignificant exploratory costs for infrequent and often null final rewards. In\nthis paper, we formalize this challenge as the \\textbf{Reward Density\nOptimization} problem, which aims to improve the reward obtained per unit of\nexploration cost. This paper introduce \\textbf{InfoFlow}, a systematic\nframework that tackles this problem from three aspects. 1) \\textbf{Subproblem\ndecomposition}: breaking down long-range tasks to assign process rewards,\nthereby providing denser learning signals. 2) \\textbf{Failure-guided hints}:\ninjecting corrective guidance into stalled trajectories to increase the\nprobability of successful outcomes. 3) \\textbf{Dual-agent refinement}:\nemploying a dual-agent architecture to offload the cognitive burden of deep\nexploration. A refiner agent synthesizes the search history, which effectively\ncompresses the researcher's perceived trajectory, thereby reducing exploration\ncost and increasing the overall reward density. We evaluate InfoFlow on\nmultiple agentic search benchmarks, where it significantly outperforms strong\nbaselines, enabling lightweight LLMs to achieve performance comparable to\nadvanced proprietary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach\nfor enhancing agentic deep search. However, its application is often hindered\nby low \\textbf{Reward Density} in deep search scenarios, where agents expend\nsignificant exploratory costs for infrequent and often null final rewards. In\nthis paper, we formalize this challenge as the \\textbf{Reward Density\nOptimization} problem, which aims to improve the reward obtained per unit of\nexploration cost. This paper introduce \\textbf{InfoFlow}, a systematic\nframework that tackles this problem from three aspects. 1) \\textbf{Subproblem\ndecomposition}: breaking down long-range tasks to assign process rewards,\nthereby providing denser learning signals. 2) \\textbf{Failure-guided hints}:\ninjecting corrective guidance into stalled trajectories to increase the\nprobability of successful outcomes. 3) \\textbf{Dual-agent refinement}:\nemploying a dual-agent architecture to offload the cognitive burden of deep\nexploration. A refiner agent synthesizes the search history, which effectively\ncompresses the researcher's perceived trajectory, thereby reducing exploration\ncost and increasing the overall reward density. We evaluate InfoFlow on\nmultiple agentic search benchmarks, where it significantly outperforms strong\nbaselines, enabling lightweight LLMs to achieve performance comparable to\nadvanced proprietary LLMs."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Ziyi Xia"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Siqi Bao"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04226v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04226v4",
                "updated": "2025-10-30T14:52:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    52,
                    48,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-05T14:29:15Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    14,
                    29,
                    15,
                    6,
                    278,
                    0
                ],
                "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epistemic Diversity and Knowledge Collapse in Large Language Models"
                },
                "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation"
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Sarah Masud"
                    },
                    {
                        "name": "Jared Moore"
                    },
                    {
                        "name": "Srishti Yadav"
                    },
                    {
                        "name": "Maria Antoniak"
                    },
                    {
                        "name": "Chan Young Park"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model\n  description",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04226v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04226v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08771v2",
                "updated": "2025-10-30T14:46:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    46,
                    21,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-09T19:41:51Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    41,
                    51,
                    3,
                    282,
                    0
                ],
                "title": "LinearSR: Unlocking Linear Attention for Stable and Efficient Image\n  Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinearSR: Unlocking Linear Attention for Stable and Efficient Image\n  Super-Resolution"
                },
                "summary": "Generative models for Image Super-Resolution (SR) are increasingly powerful,\nyet their reliance on self-attention's quadratic complexity (O(N^2)) creates a\nmajor computational bottleneck. Linear Attention offers an O(N) solution, but\nits promise for photorealistic SR has remained largely untapped, historically\nhindered by a cascade of interrelated and previously unsolved challenges. This\npaper introduces LinearSR, a holistic framework that, for the first time,\nsystematically overcomes these critical hurdles. Specifically, we resolve a\nfundamental, training instability that causes catastrophic model divergence\nusing our novel \"knee point\"-based Early-Stopping Guided Fine-tuning (ESGF)\nstrategy. Furthermore, we mitigate the classic perception-distortion trade-off\nwith a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we\nestablish an effective and lightweight guidance paradigm, TAG, derived from our\n\"precision-over-volume\" principle. Our resulting LinearSR model simultaneously\ndelivers state-of-the-art perceptual quality with exceptional efficiency. Its\ncore diffusion forward pass (1-NFE) achieves SOTA-level speed, while its\noverall multi-step inference time remains highly competitive. This work\nprovides the first robust methodology for applying Linear Attention in the\nphotorealistic SR domain, establishing a foundational paradigm for future\nresearch in efficient generative super-resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models for Image Super-Resolution (SR) are increasingly powerful,\nyet their reliance on self-attention's quadratic complexity (O(N^2)) creates a\nmajor computational bottleneck. Linear Attention offers an O(N) solution, but\nits promise for photorealistic SR has remained largely untapped, historically\nhindered by a cascade of interrelated and previously unsolved challenges. This\npaper introduces LinearSR, a holistic framework that, for the first time,\nsystematically overcomes these critical hurdles. Specifically, we resolve a\nfundamental, training instability that causes catastrophic model divergence\nusing our novel \"knee point\"-based Early-Stopping Guided Fine-tuning (ESGF)\nstrategy. Furthermore, we mitigate the classic perception-distortion trade-off\nwith a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we\nestablish an effective and lightweight guidance paradigm, TAG, derived from our\n\"precision-over-volume\" principle. Our resulting LinearSR model simultaneously\ndelivers state-of-the-art perceptual quality with exceptional efficiency. Its\ncore diffusion forward pass (1-NFE) achieves SOTA-level speed, while its\noverall multi-step inference time remains highly competitive. This work\nprovides the first robust methodology for applying Linear Attention in the\nphotorealistic SR domain, establishing a foundational paradigm for future\nresearch in efficient generative super-resolution."
                },
                "authors": [
                    {
                        "name": "Xiaohui Li"
                    },
                    {
                        "name": "Shaobin Zhuang"
                    },
                    {
                        "name": "Shuo Cao"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Yuandong Pu"
                    },
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Siqi Luo"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Yihao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yihao Liu"
                },
                "author": "Yihao Liu",
                "arxiv_comment": "19 pages, 9 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01369v2",
                "updated": "2025-10-30T14:45:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    45,
                    40,
                    3,
                    303,
                    0
                ],
                "published": "2025-06-02T06:54:29Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    54,
                    29,
                    0,
                    153,
                    0
                ],
                "title": "Incentivizing LLMs to Self-Verify Their Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incentivizing LLMs to Self-Verify Their Answers"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in complex\nreasoning tasks through both post-training and test-time scaling laws. While\nprevalent test-time scaling approaches are often realized by using external\nreward models to guide the model generation process, we find that only marginal\ngains can be acquired when scaling a model post-trained on specific reasoning\ntasks. We identify that the limited improvement stems from distribution\ndiscrepancies between the specific post-trained generator and the general\nreward model. To address this, we propose a framework that incentivizes LLMs to\nself-verify their own answers. By unifying answer generation and verification\nwithin a single reinforcement learning (RL) process, we train models that can\neffectively assess the correctness of their own solutions. The trained model\ncan further scale its performance at inference time by verifying its\ngenerations, without the need for external verifiers. We train our\nself-verification models based on Qwen2.5-Math-7B and\nDeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying\nreasoning context lengths. Experiments on multiple mathematical reasoning\nbenchmarks show that our models can not only improve post-training performance\nbut also enable effective test-time scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable progress in complex\nreasoning tasks through both post-training and test-time scaling laws. While\nprevalent test-time scaling approaches are often realized by using external\nreward models to guide the model generation process, we find that only marginal\ngains can be acquired when scaling a model post-trained on specific reasoning\ntasks. We identify that the limited improvement stems from distribution\ndiscrepancies between the specific post-trained generator and the general\nreward model. To address this, we propose a framework that incentivizes LLMs to\nself-verify their own answers. By unifying answer generation and verification\nwithin a single reinforcement learning (RL) process, we train models that can\neffectively assess the correctness of their own solutions. The trained model\ncan further scale its performance at inference time by verifying its\ngenerations, without the need for external verifiers. We train our\nself-verification models based on Qwen2.5-Math-7B and\nDeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying\nreasoning context lengths. Experiments on multiple mathematical reasoning\nbenchmarks show that our models can not only improve post-training performance\nbut also enable effective test-time scaling."
                },
                "authors": [
                    {
                        "name": "Fuxiang Zhang"
                    },
                    {
                        "name": "Jiacheng Xu"
                    },
                    {
                        "name": "Chaojie Wang"
                    },
                    {
                        "name": "Ce Cui"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26546v1",
                "updated": "2025-10-30T14:37:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    37,
                    15,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:37:15Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    37,
                    15,
                    3,
                    303,
                    0
                ],
                "title": "WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework\n  with Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework\n  with Model Merging"
                },
                "summary": "Cross-Domain Sequential Recommendation (CDSR) seeks to improve user\npreference modeling by transferring knowledge from multiple domains. Despite\nthe progress made in CDSR, most existing methods rely on overlapping users or\nitems to establish cross-domain correlations-a requirement that rarely holds in\nreal-world settings. The advent of large language models (LLM) and\nmodel-merging techniques appears to overcome this limitation by unifying\nmulti-domain data without explicit overlaps. Yet, our empirical study shows\nthat naively training an LLM on combined domains-or simply merging several\ndomain-specific LLMs-often degrades performance relative to a model trained\nsolely on the target domain. To address these challenges, we first\nexperimentally investigate the cause of suboptimal performance in LLM-based\ncross-domain recommendation and model merging. Building on these insights, we\nintroduce WeaveRec, which cross-trains multiple LoRA modules with source and\ntarget domain data in a weaving fashion, and fuses them via model merging.\nWeaveRec can be extended to multi-source domain scenarios and notably does not\nintroduce additional inference-time cost in terms of latency or memory.\nFurthermore, we provide a theoretical guarantee that WeaveRec can reduce the\nupper bound of the expected error in the target domain. Extensive experiments\non single-source, multi-source, and cross-platform cross-domain recommendation\nscenarios validate that WeaveRec effectively mitigates performance degradation\nand consistently outperforms baseline approaches in real-world recommendation\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Domain Sequential Recommendation (CDSR) seeks to improve user\npreference modeling by transferring knowledge from multiple domains. Despite\nthe progress made in CDSR, most existing methods rely on overlapping users or\nitems to establish cross-domain correlations-a requirement that rarely holds in\nreal-world settings. The advent of large language models (LLM) and\nmodel-merging techniques appears to overcome this limitation by unifying\nmulti-domain data without explicit overlaps. Yet, our empirical study shows\nthat naively training an LLM on combined domains-or simply merging several\ndomain-specific LLMs-often degrades performance relative to a model trained\nsolely on the target domain. To address these challenges, we first\nexperimentally investigate the cause of suboptimal performance in LLM-based\ncross-domain recommendation and model merging. Building on these insights, we\nintroduce WeaveRec, which cross-trains multiple LoRA modules with source and\ntarget domain data in a weaving fashion, and fuses them via model merging.\nWeaveRec can be extended to multi-source domain scenarios and notably does not\nintroduce additional inference-time cost in terms of latency or memory.\nFurthermore, we provide a theoretical guarantee that WeaveRec can reduce the\nupper bound of the expected error in the target domain. Extensive experiments\non single-source, multi-source, and cross-platform cross-domain recommendation\nscenarios validate that WeaveRec effectively mitigates performance degradation\nand consistently outperforms baseline approaches in real-world recommendation\ntasks."
                },
                "authors": [
                    {
                        "name": "Min Hou"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Le Wu"
                    },
                    {
                        "name": "Chenyi He"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Si Wei"
                    }
                ],
                "author_detail": {
                    "name": "Si Wei"
                },
                "author": "Si Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26538v1",
                "updated": "2025-10-30T14:27:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    27,
                    51,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:27:51Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    27,
                    51,
                    3,
                    303,
                    0
                ],
                "title": "Reflecting on Empirical and Sustainability Aspects of Software\n  Engineering Research in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting on Empirical and Sustainability Aspects of Software\n  Engineering Research in the Era of Large Language Models"
                },
                "summary": "Software Engineering (SE) research involving the use of Large Language Models\n(LLMs) has introduced several new challenges related to rigour in benchmarking,\ncontamination, replicability, and sustainability. In this paper, we invite the\nresearch community to reflect on how these challenges are addressed in SE. Our\nresults provide a structured overview of current LLM-based SE research at ICSE,\nhighlighting both encouraging practices and persistent shortcomings. We\nconclude with recommendations to strengthen benchmarking rigour, improve\nreplicability, and address the financial and environmental costs of LLM-based\nSE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Engineering (SE) research involving the use of Large Language Models\n(LLMs) has introduced several new challenges related to rigour in benchmarking,\ncontamination, replicability, and sustainability. In this paper, we invite the\nresearch community to reflect on how these challenges are addressed in SE. Our\nresults provide a structured overview of current LLM-based SE research at ICSE,\nhighlighting both encouraging practices and persistent shortcomings. We\nconclude with recommendations to strengthen benchmarking rigour, improve\nreplicability, and address the financial and environmental costs of LLM-based\nSE."
                },
                "authors": [
                    {
                        "name": "David Williams"
                    },
                    {
                        "name": "Max Hort"
                    },
                    {
                        "name": "Maria Kechagia"
                    },
                    {
                        "name": "Aldeida Aleti"
                    },
                    {
                        "name": "Justyna Petke"
                    },
                    {
                        "name": "Federica Sarro"
                    }
                ],
                "author_detail": {
                    "name": "Federica Sarro"
                },
                "author": "Federica Sarro",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26527v1",
                "updated": "2025-10-30T14:20:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    20,
                    24,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:20:24Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    20,
                    24,
                    3,
                    303,
                    0
                ],
                "title": "Polybasic Speculative Decoding Through a Theoretical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polybasic Speculative Decoding Through a Theoretical Perspective"
                },
                "summary": "Inference latency stands as a critical bottleneck in the large-scale\ndeployment of Large Language Models (LLMs). Speculative decoding methods have\nrecently shown promise in accelerating inference without compromising the\noutput distribution. However, existing work typically relies on a dualistic\ndraft-verify framework and lacks rigorous theoretical grounding. In this paper,\nwe introduce a novel \\emph{polybasic} speculative decoding framework,\nunderpinned by a comprehensive theoretical analysis. Specifically, we prove a\nfundamental theorem that characterizes the optimal inference time for\nmulti-model speculative decoding systems, shedding light on how to extend\nbeyond the dualistic approach to a more general polybasic paradigm. Through our\ntheoretical investigation of multi-model token generation, we expose and\noptimize the interplay between model capabilities, acceptance lengths, and\noverall computational cost. Our framework supports both standalone\nimplementation and integration with existing speculative techniques, leading to\naccelerated performance in practice. Experimental results across multiple model\nfamilies demonstrate that our approach yields speedup ratios ranging from\n$3.31\\times$ to $4.01\\times$ for LLaMA2-Chat 7B, up to $3.87 \\times$ for\nLLaMA3-8B, up to $4.43 \\times$ for Vicuna-7B and up to $3.85 \\times$ for\nQwen2-7B -- all while preserving the original output distribution. We release\nour theoretical proofs and implementation code to facilitate further\ninvestigation into polybasic speculative decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference latency stands as a critical bottleneck in the large-scale\ndeployment of Large Language Models (LLMs). Speculative decoding methods have\nrecently shown promise in accelerating inference without compromising the\noutput distribution. However, existing work typically relies on a dualistic\ndraft-verify framework and lacks rigorous theoretical grounding. In this paper,\nwe introduce a novel \\emph{polybasic} speculative decoding framework,\nunderpinned by a comprehensive theoretical analysis. Specifically, we prove a\nfundamental theorem that characterizes the optimal inference time for\nmulti-model speculative decoding systems, shedding light on how to extend\nbeyond the dualistic approach to a more general polybasic paradigm. Through our\ntheoretical investigation of multi-model token generation, we expose and\noptimize the interplay between model capabilities, acceptance lengths, and\noverall computational cost. Our framework supports both standalone\nimplementation and integration with existing speculative techniques, leading to\naccelerated performance in practice. Experimental results across multiple model\nfamilies demonstrate that our approach yields speedup ratios ranging from\n$3.31\\times$ to $4.01\\times$ for LLaMA2-Chat 7B, up to $3.87 \\times$ for\nLLaMA3-8B, up to $4.43 \\times$ for Vicuna-7B and up to $3.85 \\times$ for\nQwen2-7B -- all while preserving the original output distribution. We release\nour theoretical proofs and implementation code to facilitate further\ninvestigation into polybasic speculative decoding."
                },
                "authors": [
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Yuexiao Ma"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21451v2",
                "updated": "2025-10-30T14:18:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    18,
                    35,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-24T13:28:41Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    41,
                    4,
                    297,
                    0
                ],
                "title": "Scalpel: Automotive Deep Learning Framework Testing via Assembling Model\n  Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalpel: Automotive Deep Learning Framework Testing via Assembling Model\n  Components"
                },
                "summary": "Deep learning (DL) plays a key role in autonomous driving systems. DL models\nsupport perception modules, equipped with tasks such as object detection and\nsensor fusion. These DL models enable vehicles to process multi-sensor inputs\nto understand complex surroundings. Deploying DL models in autonomous driving\nsystems faces stringent challenges, including real-time processing, limited\ncomputational resources, and strict power constraints. To address these\nchallenges, automotive DL frameworks (e.g., PaddleInference) have emerged to\noptimize inference efficiency. However, these frameworks encounter unique\nquality issues due to their more complex deployment environments, such as\ncrashes stemming from limited scheduled memory and incorrect memory allocation.\nUnfortunately, existing DL framework testing methods fail to detect these\nquality issues due to the failure in deploying generated test input models, as\nthese models lack three essential capabilities: (1) multi-input/output tensor\nprocessing, (2) multi-modal data processing, and (3) multi-level data feature\nextraction. These capabilities necessitate specialized model components, which\nexisting testing methods neglect during model generation. To bridge this gap,\nwe propose Scalpel, an automotive DL frameworks testing method that generates\ntest input models at the model component level. Scalpel generates models by\nassembling model components (heads, necks, backbones) to support capabilities\nrequired by autonomous driving systems. Specifically, Scalpel maintains and\nupdates a repository of model components, generating test inputs by selecting,\nmutating, and assembling them. Successfully generated models are added back to\nenrich the repository. Newly generated models are then deployed within the\nautonomous driving system to test automotive DL frameworks via differential\ntesting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) plays a key role in autonomous driving systems. DL models\nsupport perception modules, equipped with tasks such as object detection and\nsensor fusion. These DL models enable vehicles to process multi-sensor inputs\nto understand complex surroundings. Deploying DL models in autonomous driving\nsystems faces stringent challenges, including real-time processing, limited\ncomputational resources, and strict power constraints. To address these\nchallenges, automotive DL frameworks (e.g., PaddleInference) have emerged to\noptimize inference efficiency. However, these frameworks encounter unique\nquality issues due to their more complex deployment environments, such as\ncrashes stemming from limited scheduled memory and incorrect memory allocation.\nUnfortunately, existing DL framework testing methods fail to detect these\nquality issues due to the failure in deploying generated test input models, as\nthese models lack three essential capabilities: (1) multi-input/output tensor\nprocessing, (2) multi-modal data processing, and (3) multi-level data feature\nextraction. These capabilities necessitate specialized model components, which\nexisting testing methods neglect during model generation. To bridge this gap,\nwe propose Scalpel, an automotive DL frameworks testing method that generates\ntest input models at the model component level. Scalpel generates models by\nassembling model components (heads, necks, backbones) to support capabilities\nrequired by autonomous driving systems. Specifically, Scalpel maintains and\nupdates a repository of model components, generating test inputs by selecting,\nmutating, and assembling them. Successfully generated models are added back to\nenrich the repository. Newly generated models are then deployed within the\nautonomous driving system to test automotive DL frameworks via differential\ntesting."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "An Guo"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "arxiv_comment": "Accepted by the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15030v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15030v3",
                "updated": "2025-10-30T14:10:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    10,
                    33,
                    3,
                    303,
                    0
                ],
                "published": "2025-08-20T19:49:06Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    49,
                    6,
                    2,
                    232,
                    0
                ],
                "title": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations\n  in Tourism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations\n  in Tourism"
                },
                "summary": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems."
                },
                "authors": [
                    {
                        "name": "Ashmi Banerjee"
                    },
                    {
                        "name": "Adithi Satish"
                    },
                    {
                        "name": "Fitri Nur Aisyah"
                    },
                    {
                        "name": "Wolfgang Wörndl"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Deldjoo"
                },
                "author": "Yashar Deldjoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15030v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15030v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26516v1",
                "updated": "2025-10-30T14:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    9,
                    50,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    9,
                    50,
                    3,
                    303,
                    0
                ],
                "title": "Envisioning Future Interactive Web Development: Editing Webpage with\n  Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Envisioning Future Interactive Web Development: Editing Webpage with\n  Natural Language"
                },
                "summary": "The evolution of web applications relies on iterative code modifications, a\nprocess that is traditionally manual and time-consuming. While Large Language\nModels (LLMs) can generate UI code, their ability to edit existing code from\nnew design requirements (e.g., \"center the logo\") remains a challenge. This is\nlargely due to the absence of large-scale, high-quality tuning data to align\nmodel performance with human expectations. In this paper, we introduce a novel,\nautomated data generation pipeline that uses LLMs to synthesize a high-quality\nfine-tuning dataset for web editing, named Instruct4Edit. Our approach\ngenerates diverse instructions, applies the corresponding code modifications,\nand performs visual verification to ensure correctness. By fine-tuning models\non Instruct4Edit, we demonstrate consistent improvement in translating human\nintent into precise, structurally coherent, and visually accurate code changes.\nThis work provides a scalable and transparent foundation for natural language\nbased web editing, demonstrating that fine-tuning smaller open-source models\ncan achieve competitive performance with proprietary systems. We release all\ndata, code implementations, and model checkpoints for reproduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of web applications relies on iterative code modifications, a\nprocess that is traditionally manual and time-consuming. While Large Language\nModels (LLMs) can generate UI code, their ability to edit existing code from\nnew design requirements (e.g., \"center the logo\") remains a challenge. This is\nlargely due to the absence of large-scale, high-quality tuning data to align\nmodel performance with human expectations. In this paper, we introduce a novel,\nautomated data generation pipeline that uses LLMs to synthesize a high-quality\nfine-tuning dataset for web editing, named Instruct4Edit. Our approach\ngenerates diverse instructions, applies the corresponding code modifications,\nand performs visual verification to ensure correctness. By fine-tuning models\non Instruct4Edit, we demonstrate consistent improvement in translating human\nintent into precise, structurally coherent, and visually accurate code changes.\nThis work provides a scalable and transparent foundation for natural language\nbased web editing, demonstrating that fine-tuning smaller open-source models\ncan achieve competitive performance with proprietary systems. We release all\ndata, code implementations, and model checkpoints for reproduction."
                },
                "authors": [
                    {
                        "name": "Truong Hai Dang"
                    },
                    {
                        "name": "Jingyu Xiao"
                    },
                    {
                        "name": "Yintong Huo"
                    }
                ],
                "author_detail": {
                    "name": "Yintong Huo"
                },
                "author": "Yintong Huo",
                "arxiv_comment": "accepted by AIWare'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26512v1",
                "updated": "2025-10-30T14:05:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    5,
                    55,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:05:55Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    5,
                    55,
                    3,
                    303,
                    0
                ],
                "title": "Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs"
                },
                "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    }
                ],
                "author_detail": {
                    "name": "Carlotta Domeniconi"
                },
                "author": "Carlotta Domeniconi",
                "arxiv_comment": "ICDM 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26510v1",
                "updated": "2025-10-30T14:04:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    4,
                    25,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:04:25Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    4,
                    25,
                    3,
                    303,
                    0
                ],
                "title": "LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection"
                },
                "summary": "Model and hyperparameter selection are critical but challenging in machine\nlearning, typically requiring expert intuition or expensive automated search.\nWe investigate whether large language models (LLMs) can act as in-context\nmeta-learners for this task. By converting each dataset into interpretable\nmetadata, we prompt an LLM to recommend both model families and\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\nwith examples of models and their performance on past tasks. Across synthetic\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\nrecommend competitive models and hyperparameters without search, and that\nimprovements from meta-informed prompting demonstrate their capacity for\nin-context meta-learning. These results highlight a promising new role for LLMs\nas lightweight, general-purpose assistants for model selection and\nhyperparameter optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model and hyperparameter selection are critical but challenging in machine\nlearning, typically requiring expert intuition or expensive automated search.\nWe investigate whether large language models (LLMs) can act as in-context\nmeta-learners for this task. By converting each dataset into interpretable\nmetadata, we prompt an LLM to recommend both model families and\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\nwith examples of models and their performance on past tasks. Across synthetic\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\nrecommend competitive models and hyperparameters without search, and that\nimprovements from meta-informed prompting demonstrate their capacity for\nin-context meta-learning. These results highlight a promising new role for LLMs\nas lightweight, general-purpose assistants for model selection and\nhyperparameter optimization."
                },
                "authors": [
                    {
                        "name": "Youssef Attia El Hili"
                    },
                    {
                        "name": "Albert Thomas"
                    },
                    {
                        "name": "Malik Tiomoko"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Corentin Léger"
                    },
                    {
                        "name": "Corinne Ancourt"
                    },
                    {
                        "name": "Balázs Kégl"
                    }
                ],
                "author_detail": {
                    "name": "Balázs Kégl"
                },
                "author": "Balázs Kégl",
                "arxiv_comment": "27 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18347v2",
                "updated": "2025-10-30T14:01:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    1,
                    42,
                    3,
                    303,
                    0
                ],
                "published": "2025-02-25T16:36:24Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    36,
                    24,
                    1,
                    56,
                    0
                ],
                "title": "Modeling Neural Activity with Conditionally Linear Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Neural Activity with Conditionally Linear Dynamical Systems"
                },
                "summary": "Neural population activity exhibits complex, nonlinear dynamics, varying in\ntime, over trials, and across experimental conditions. Here, we develop\nConditionally Linear Dynamical System (CLDS) models as a general-purpose method\nto characterize these dynamics. These models use Gaussian Process (GP) priors\nto capture the nonlinear dependence of circuit dynamics on task and behavioral\nvariables. Conditioned on these covariates, the data is modeled with linear\ndynamics. This allows for transparent interpretation and tractable Bayesian\ninference. We find that CLDS models can perform well even in severely\ndata-limited regimes (e.g. one trial per condition) due to their Bayesian\nformulation and ability to share statistical power across nearby task\nconditions. In example applications, we apply CLDS to model thalamic neurons\nthat nonlinearly encode heading direction and to model motor cortical neurons\nduring a cued reaching task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural population activity exhibits complex, nonlinear dynamics, varying in\ntime, over trials, and across experimental conditions. Here, we develop\nConditionally Linear Dynamical System (CLDS) models as a general-purpose method\nto characterize these dynamics. These models use Gaussian Process (GP) priors\nto capture the nonlinear dependence of circuit dynamics on task and behavioral\nvariables. Conditioned on these covariates, the data is modeled with linear\ndynamics. This allows for transparent interpretation and tractable Bayesian\ninference. We find that CLDS models can perform well even in severely\ndata-limited regimes (e.g. one trial per condition) due to their Bayesian\nformulation and ability to share statistical power across nearby task\nconditions. In example applications, we apply CLDS to model thalamic neurons\nthat nonlinearly encode heading direction and to model motor cortical neurons\nduring a cued reaching task."
                },
                "authors": [
                    {
                        "name": "Victor Geadah"
                    },
                    {
                        "name": "Amin Nejatbakhsh"
                    },
                    {
                        "name": "David Lipshutz"
                    },
                    {
                        "name": "Jonathan W. Pillow"
                    },
                    {
                        "name": "Alex H. Williams"
                    }
                ],
                "author_detail": {
                    "name": "Alex H. Williams"
                },
                "author": "Alex H. Williams",
                "arxiv_comment": "24 pages, 7 figures. Associated code available at:\n  https://github.com/neurostatslab/clds. To appear at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06576v2",
                "updated": "2025-10-30T13:59:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    59,
                    28,
                    3,
                    303,
                    0
                ],
                "published": "2025-08-07T14:03:23Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    3,
                    23,
                    3,
                    219,
                    0
                ],
                "title": "GFlowNets for Learning Better Drug-Drug Interaction Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GFlowNets for Learning Better Drug-Drug Interaction Representations"
                },
                "summary": "Drug-drug interactions pose a significant challenge in clinical pharmacology,\nwith severe class imbalance among interaction types limiting the effectiveness\nof predictive models. Common interactions dominate datasets, while rare but\ncritical interactions remain underrepresented, leading to poor model\nperformance on infrequent cases. Existing methods often treat DDI prediction as\na binary problem, ignoring class-specific nuances and exacerbating bias toward\nfrequent interactions. To address this, we propose a framework combining\nGenerative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)\nto generate synthetic samples for rare classes, improving model balance and\ngenerate effective and novel DDI pairs. Our approach enhances predictive\nperformance across interaction types, ensuring better clinical reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drug-drug interactions pose a significant challenge in clinical pharmacology,\nwith severe class imbalance among interaction types limiting the effectiveness\nof predictive models. Common interactions dominate datasets, while rare but\ncritical interactions remain underrepresented, leading to poor model\nperformance on infrequent cases. Existing methods often treat DDI prediction as\na binary problem, ignoring class-specific nuances and exacerbating bias toward\nfrequent interactions. To address this, we propose a framework combining\nGenerative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)\nto generate synthetic samples for rare classes, improving model balance and\ngenerate effective and novel DDI pairs. Our approach enhances predictive\nperformance across interaction types, ensuring better clinical reliability."
                },
                "authors": [
                    {
                        "name": "Azmine Toushik Wasi"
                    }
                ],
                "author_detail": {
                    "name": "Azmine Toushik Wasi"
                },
                "author": "Azmine Toushik Wasi",
                "arxiv_comment": "Accepted to ICANN 2025:AIDD and NeurIPS 2025 Workshop on Structured\n  Probabilistic Inference & Generative Modeling\n  (https://openreview.net/forum?id=LZW1jSgfCI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22018v2",
                "updated": "2025-10-30T13:57:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    57,
                    35,
                    3,
                    303,
                    0
                ],
                "published": "2025-09-26T07:51:59Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    7,
                    51,
                    59,
                    4,
                    269,
                    0
                ],
                "title": "Exploring the Early Universe with Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Early Universe with Deep Learning"
                },
                "summary": "Hydrogen is the most abundant element in our Universe. The first generation\nof stars and galaxies produced photons that ionized hydrogen gas, driving a\ncosmological event known as the Epoch of Reionization (EoR). The upcoming\nSquare Kilometre Array Observatory (SKAO) will map the distribution of neutral\nhydrogen during this era, aiding in the study of the properties of these\nfirst-generation objects. Extracting astrophysical information will be\nchallenging, as SKAO will produce a tremendous amount of data where the\nhydrogen signal will be contaminated with undesired foreground contamination\nand instrumental systematics. To address this, we develop the latest deep\nlearning techniques to extract information from the 2D power spectra of the\nhydrogen signal expected from SKAO. We apply a series of neural network models\nto these measurements and quantify their ability to predict the history of\ncosmic hydrogen reionization, which is connected to the increasing number and\nefficiency of early photon sources. We show that the study of the early\nUniverse benefits from modern deep learning technology. In particular, we\ndemonstrate that dedicated machine learning algorithms can achieve more than a\n$0.95$ $R^2$ score on average in recovering the reionization history. This\nenables accurate and precise cosmological and astrophysical inference of\nstructure formation in the early Universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydrogen is the most abundant element in our Universe. The first generation\nof stars and galaxies produced photons that ionized hydrogen gas, driving a\ncosmological event known as the Epoch of Reionization (EoR). The upcoming\nSquare Kilometre Array Observatory (SKAO) will map the distribution of neutral\nhydrogen during this era, aiding in the study of the properties of these\nfirst-generation objects. Extracting astrophysical information will be\nchallenging, as SKAO will produce a tremendous amount of data where the\nhydrogen signal will be contaminated with undesired foreground contamination\nand instrumental systematics. To address this, we develop the latest deep\nlearning techniques to extract information from the 2D power spectra of the\nhydrogen signal expected from SKAO. We apply a series of neural network models\nto these measurements and quantify their ability to predict the history of\ncosmic hydrogen reionization, which is connected to the increasing number and\nefficiency of early photon sources. We show that the study of the early\nUniverse benefits from modern deep learning technology. In particular, we\ndemonstrate that dedicated machine learning algorithms can achieve more than a\n$0.95$ $R^2$ score on average in recovering the reionization history. This\nenables accurate and precise cosmological and astrophysical inference of\nstructure formation in the early Universe."
                },
                "authors": [
                    {
                        "name": "Emmanuel de Salis"
                    },
                    {
                        "name": "Massimo De Santis"
                    },
                    {
                        "name": "Davide Piras"
                    },
                    {
                        "name": "Sambit K. Giri"
                    },
                    {
                        "name": "Michele Bianco"
                    },
                    {
                        "name": "Nicolas Cerardi"
                    },
                    {
                        "name": "Philipp Denzel"
                    },
                    {
                        "name": "Merve Selcuk-Simsek"
                    },
                    {
                        "name": "Kelley M. Hess"
                    },
                    {
                        "name": "M. Carmen Toribio"
                    },
                    {
                        "name": "Franz Kirsten"
                    },
                    {
                        "name": "Hatem Ghorbel"
                    }
                ],
                "author_detail": {
                    "name": "Hatem Ghorbel"
                },
                "author": "Hatem Ghorbel",
                "arxiv_doi": "10.1007/978-3-032-05176-9_33",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-05176-9_33",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.22018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EPIA 2025 preprint version, 12 pages, 3 figures",
                "arxiv_journal_ref": "Valente de Oliveira, J., Leite, J., Rodrigues, J., Dias, J.,\n  Cardoso, P. (eds) Progress in Artificial Intelligence. EPIA 2025. Lecture\n  Notes in Computer Science(), vol 16121. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11730v2",
                "updated": "2025-10-30T13:52:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    52,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-16T22:24:48Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    22,
                    24,
                    48,
                    4,
                    136,
                    0
                ],
                "title": "Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling"
                },
                "summary": "Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Yasuyuki Okoshi"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Masato Motomura"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26498v1",
                "updated": "2025-10-30T13:50:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    50,
                    19,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:50:19Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    50,
                    19,
                    3,
                    303,
                    0
                ],
                "title": "A Multi-agent Large Language Model Framework to Automatically Assess\n  Performance of a Clinical AI Triage Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-agent Large Language Model Framework to Automatically Assess\n  Performance of a Clinical AI Triage Tool"
                },
                "summary": "Purpose: The purpose of this study was to determine if an ensemble of\nmultiple LLM agents could be used collectively to provide a more reliable\nassessment of a pixel-based AI triage tool than a single LLM.\n  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were\nprocessed by a commercial intracranial hemorrhage (ICH) AI detection tool.\nRadiology reports were analyzed by an ensemble of eight open-source LLM models\nand a HIPAA compliant internal version of GPT-4o using a single multi-shot\nprompt that assessed for presence of ICH. 1,726 examples were manually\nreviewed. Performance characteristics of the eight open-source models and\nconsensus were compared to GPT-4o. Three ideal consensus LLM ensembles were\ntested for rating the performance of the triage tool.\n  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The\nhighest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).\nThe average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).\nLlama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater\nprecision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the\nideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3\nEnsemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522\n(0.500-0.543). No statistically significant differences were observed between\nTop-3, Full-9, and Consensus (p > 0.05).\n  Conclusion: An ensemble of medium to large sized open-source LLMs provides a\nmore consistent and reliable method to derive a ground truth retrospective\nevaluation of a clinical AI triage tool over a single LLM alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: The purpose of this study was to determine if an ensemble of\nmultiple LLM agents could be used collectively to provide a more reliable\nassessment of a pixel-based AI triage tool than a single LLM.\n  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were\nprocessed by a commercial intracranial hemorrhage (ICH) AI detection tool.\nRadiology reports were analyzed by an ensemble of eight open-source LLM models\nand a HIPAA compliant internal version of GPT-4o using a single multi-shot\nprompt that assessed for presence of ICH. 1,726 examples were manually\nreviewed. Performance characteristics of the eight open-source models and\nconsensus were compared to GPT-4o. Three ideal consensus LLM ensembles were\ntested for rating the performance of the triage tool.\n  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The\nhighest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).\nThe average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).\nLlama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater\nprecision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the\nideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3\nEnsemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522\n(0.500-0.543). No statistically significant differences were observed between\nTop-3, Full-9, and Consensus (p > 0.05).\n  Conclusion: An ensemble of medium to large sized open-source LLMs provides a\nmore consistent and reliable method to derive a ground truth retrospective\nevaluation of a clinical AI triage tool over a single LLM alone."
                },
                "authors": [
                    {
                        "name": "Adam E. Flanders"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Luciano Prevedello"
                    },
                    {
                        "name": "Robyn Ball"
                    },
                    {
                        "name": "Errol Colak"
                    },
                    {
                        "name": "Prahlad Menon"
                    },
                    {
                        "name": "George Shih"
                    },
                    {
                        "name": "Hui-Ming Lin"
                    },
                    {
                        "name": "Paras Lakhani"
                    }
                ],
                "author_detail": {
                    "name": "Paras Lakhani"
                },
                "author": "Paras Lakhani",
                "arxiv_comment": "29 pages, 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08271v2",
                "updated": "2025-10-30T13:49:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    49,
                    48,
                    3,
                    303,
                    0
                ],
                "published": "2025-02-12T10:24:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    24,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "RecCocktail: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecCocktail: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm focuses on enhancing\ndomain-specific recommendation tasks, improving performance in warm\nrecommendation scenarios. While most previous works treat these two paradigms\nseparately, we argue that they have complementary advantages, and combining\nthem can yield better results. In this paper, we propose a generalizable and\nefficient LLM-based recommendation framework RecCocktail. Our approach begins\nwith fine-tuning a \"base spirit\" LoRA module using domain-general\nrecommendation instruction data to align LLM with recommendation knowledge.\nNext, given users' behavior of a specific domain, we construct a\ndomain-specific \"ingredient\" LoRA module. We then provide an entropy-guided\nadaptive merging method to mix the \"base spirit\" and the \"ingredient\" in the\nweight space. Please note that, RecCocktail combines the advantages of the\nexisting two paradigms without introducing additional time or space overhead\nduring the inference phase. Moreover, RecCocktail is efficient with plug and\nplay, as the \"base spirit\" LoRA is trained only once, and any domain-specific\n\"ingredient\" can be efficiently mixed with only domain-specific fine-tuning.\nExtensive experiments on multiple datasets under both warm and cold-start\nrecommendation scenarios validate the effectiveness and generality of the\nproposed RecCocktail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm focuses on enhancing\ndomain-specific recommendation tasks, improving performance in warm\nrecommendation scenarios. While most previous works treat these two paradigms\nseparately, we argue that they have complementary advantages, and combining\nthem can yield better results. In this paper, we propose a generalizable and\nefficient LLM-based recommendation framework RecCocktail. Our approach begins\nwith fine-tuning a \"base spirit\" LoRA module using domain-general\nrecommendation instruction data to align LLM with recommendation knowledge.\nNext, given users' behavior of a specific domain, we construct a\ndomain-specific \"ingredient\" LoRA module. We then provide an entropy-guided\nadaptive merging method to mix the \"base spirit\" and the \"ingredient\" in the\nweight space. Please note that, RecCocktail combines the advantages of the\nexisting two paradigms without introducing additional time or space overhead\nduring the inference phase. Moreover, RecCocktail is efficient with plug and\nplay, as the \"base spirit\" LoRA is trained only once, and any domain-specific\n\"ingredient\" can be efficiently mixed with only domain-specific fine-tuning.\nExtensive experiments on multiple datasets under both warm and cold-start\nrecommendation scenarios validate the effectiveness and generality of the\nproposed RecCocktail."
                },
                "authors": [
                    {
                        "name": "Min Hou"
                    },
                    {
                        "name": "Chenxi Bai"
                    },
                    {
                        "name": "Le Wu"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25623v2",
                "updated": "2025-10-30T13:49:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    49,
                    22,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T15:27:47Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    27,
                    47,
                    2,
                    302,
                    0
                ],
                "title": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks"
                },
                "summary": "Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming, its value in argumentative domains such as law remains\nunderexplored. We present an empirical study of verifier-based TTS methods for\nlegal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7\nreward models, we evaluate both outcome-level (Best-of-$N$) and process-level\n(tree search) verification under realistic low-$N$ budgets. Our analysis\nsystematically investigates how verifier utility is affected by key properties\nsuch as domain specialization, model size, and supervision type\n(process-supervised PRMs vs. outcome-only ORMs), even when applied across\ndifferent roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming, its value in argumentative domains such as law remains\nunderexplored. We present an empirical study of verifier-based TTS methods for\nlegal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7\nreward models, we evaluate both outcome-level (Best-of-$N$) and process-level\n(tree search) verification under realistic low-$N$ budgets. Our analysis\nsystematically investigates how verifier utility is affected by key properties\nsuch as domain specialization, model size, and supervision type\n(process-supervised PRMs vs. outcome-only ORMs), even when applied across\ndifferent roles."
                },
                "authors": [
                    {
                        "name": "Davide Romano"
                    },
                    {
                        "name": "Jonathan Schwarz"
                    },
                    {
                        "name": "Daniele Giofré"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Giofré"
                },
                "author": "Daniele Giofré",
                "arxiv_comment": "Accepted to EMNLP - NLLP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.06455v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.06455v3",
                "updated": "2025-10-30T13:46:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    46,
                    56,
                    3,
                    303,
                    0
                ],
                "published": "2023-09-12T12:04:17Z",
                "published_parsed": [
                    2023,
                    9,
                    12,
                    12,
                    4,
                    17,
                    1,
                    255,
                    0
                ],
                "title": "Combining Unsupervised Learning and Statistical Inference For Multimodal\n  N-of-1 Trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Unsupervised Learning and Statistical Inference For Multimodal\n  N-of-1 Trials"
                },
                "summary": "N-of-1 trials are within-person crossover trials allowing both personalized\nand population-level inference on the effect of health interventions. Using the\nfull potential of modern technologies, multimodal N-of-1 trials can integrate\nmultimedia data for measuring health outcomes. However, methodology required\nfor automated applications in large multimodal trials is not available yet.\nHere, we present an unsupervised approach for modeling multimodal N-of-1\ntrials, bypassing the need for expensive outcome labeling by medical experts.\nFirst, an autoencoder is trained on the outcome medical images. Then, the\ndimensionality of embeddings is reduced by extracting the first principal\ncomponent, which is finally tested for its association with the treatment.\nResults from imaging simulation studies show high power in detecting a\ntreatment effect while controlling type I error rates. An application to\nimaging N-of-1 trials of acne severity identifies individual treatment effects\nand supports that our methodology can enable large clinical multimodal N-of-1\ntrials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-of-1 trials are within-person crossover trials allowing both personalized\nand population-level inference on the effect of health interventions. Using the\nfull potential of modern technologies, multimodal N-of-1 trials can integrate\nmultimedia data for measuring health outcomes. However, methodology required\nfor automated applications in large multimodal trials is not available yet.\nHere, we present an unsupervised approach for modeling multimodal N-of-1\ntrials, bypassing the need for expensive outcome labeling by medical experts.\nFirst, an autoencoder is trained on the outcome medical images. Then, the\ndimensionality of embeddings is reduced by extracting the first principal\ncomponent, which is finally tested for its association with the treatment.\nResults from imaging simulation studies show high power in detecting a\ntreatment effect while controlling type I error rates. An application to\nimaging N-of-1 trials of acne severity identifies individual treatment effects\nand supports that our methodology can enable large clinical multimodal N-of-1\ntrials."
                },
                "authors": [
                    {
                        "name": "Juliana Schneider"
                    },
                    {
                        "name": "Thomas Gärtner"
                    },
                    {
                        "name": "Stefan Konigorski"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Konigorski"
                },
                "author": "Stefan Konigorski",
                "arxiv_comment": "22 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.06455v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.06455v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26495v1",
                "updated": "2025-10-30T13:44:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    44,
                    22,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:44:22Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    44,
                    22,
                    3,
                    303,
                    0
                ],
                "title": "Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for\n  Real-world Database Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for\n  Real-world Database Exploration"
                },
                "summary": "Recent advances in Text-to-SQL have achieved strong results in static,\nsingle-turn tasks, where models generate SQL queries from natural language\nquestions. However, these systems fall short in real-world interactive\nscenarios, where user intents evolve and queries must be refined over multiple\nturns. In applications such as finance and business analytics, users\niteratively adjust query constraints or dimensions based on intermediate\nresults. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a\nbenchmark assessing model performance under evolving user interactions. Unlike\nprevious manually curated datasets, DySQL-Bench is built through an automated\ntwo-stage pipeline of task synthesis and verification. Structured tree\nrepresentations derived from raw database tables guide LLM-based task\ngeneration, followed by interaction-oriented filtering and expert validation.\nHuman evaluation confirms 100% correctness of the synthesized data. We further\npropose a multi-turn evaluation framework simulating realistic interactions\namong an LLM-simulated user, the model under test, and an executable database.\nThe model must adapt its reasoning and SQL generation as user intents change.\nDySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling\n1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the\nPass@5 metric, underscoring the benchmark's difficulty. All code and data are\nreleased at https://github.com/Aurora-slz/Real-World-SQL-Bench .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Text-to-SQL have achieved strong results in static,\nsingle-turn tasks, where models generate SQL queries from natural language\nquestions. However, these systems fall short in real-world interactive\nscenarios, where user intents evolve and queries must be refined over multiple\nturns. In applications such as finance and business analytics, users\niteratively adjust query constraints or dimensions based on intermediate\nresults. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a\nbenchmark assessing model performance under evolving user interactions. Unlike\nprevious manually curated datasets, DySQL-Bench is built through an automated\ntwo-stage pipeline of task synthesis and verification. Structured tree\nrepresentations derived from raw database tables guide LLM-based task\ngeneration, followed by interaction-oriented filtering and expert validation.\nHuman evaluation confirms 100% correctness of the synthesized data. We further\npropose a multi-turn evaluation framework simulating realistic interactions\namong an LLM-simulated user, the model under test, and an executable database.\nThe model must adapt its reasoning and SQL generation as user intents change.\nDySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling\n1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the\nPass@5 metric, underscoring the benchmark's difficulty. All code and data are\nreleased at https://github.com/Aurora-slz/Real-World-SQL-Bench ."
                },
                "authors": [
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Yuying Li"
                    },
                    {
                        "name": "Qifeng Cai"
                    },
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Bihui Yu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26494v1",
                "updated": "2025-10-30T13:43:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    28,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:43:28Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    28,
                    3,
                    303,
                    0
                ],
                "title": "Simulating and Experimenting with Social Media Mobilization Using LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating and Experimenting with Social Media Mobilization Using LLM\n  Agents"
                },
                "summary": "Online social networks have transformed the ways in which political\nmobilization messages are disseminated, raising new questions about how peer\ninfluence operates at scale. Building on the landmark 61-million-person\nFacebook experiment \\citep{bond201261}, we develop an agent-based simulation\nframework that integrates real U.S. Census demographic distributions, authentic\nTwitter network topology, and heterogeneous large language model (LLM) agents\nto examine the effect of mobilization messages on voter turnout. Each simulated\nagent is assigned demographic attributes, a personal political stance, and an\nLLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano})\nreflecting its political sophistication. Agents interact over realistic social\nnetwork structures, receiving personalized feeds and dynamically updating their\nengagement behaviors and voting intentions. Experimental conditions replicate\nthe informational and social mobilization treatments of the original Facebook\nstudy. Across scenarios, the simulator reproduces qualitative patterns observed\nin field experiments, including stronger mobilization effects under social\nmessage treatments and measurable peer spillovers. Our framework provides a\ncontrolled, reproducible environment for testing counterfactual designs and\nsensitivity analyses in political mobilization research, offering a bridge\nbetween high-validity field experiments and flexible computational\nmodeling.\\footnote{Code and data available at\nhttps://github.com/CausalMP/LLM-SocioPol}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online social networks have transformed the ways in which political\nmobilization messages are disseminated, raising new questions about how peer\ninfluence operates at scale. Building on the landmark 61-million-person\nFacebook experiment \\citep{bond201261}, we develop an agent-based simulation\nframework that integrates real U.S. Census demographic distributions, authentic\nTwitter network topology, and heterogeneous large language model (LLM) agents\nto examine the effect of mobilization messages on voter turnout. Each simulated\nagent is assigned demographic attributes, a personal political stance, and an\nLLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano})\nreflecting its political sophistication. Agents interact over realistic social\nnetwork structures, receiving personalized feeds and dynamically updating their\nengagement behaviors and voting intentions. Experimental conditions replicate\nthe informational and social mobilization treatments of the original Facebook\nstudy. Across scenarios, the simulator reproduces qualitative patterns observed\nin field experiments, including stronger mobilization effects under social\nmessage treatments and measurable peer spillovers. Our framework provides a\ncontrolled, reproducible environment for testing counterfactual designs and\nsensitivity analyses in political mobilization research, offering a bridge\nbetween high-validity field experiments and flexible computational\nmodeling.\\footnote{Code and data available at\nhttps://github.com/CausalMP/LLM-SocioPol}"
                },
                "authors": [
                    {
                        "name": "Sadegh Shirani"
                    },
                    {
                        "name": "Mohsen Bayati"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Bayati"
                },
                "author": "Mohsen Bayati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26491v1",
                "updated": "2025-10-30T13:40:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    40,
                    52,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:40:52Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    40,
                    52,
                    3,
                    303,
                    0
                ],
                "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient RLVR via Off-Policy Influence Guidance"
                },
                "summary": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\n\\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy\n\\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\n\\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy\n\\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR."
                },
                "authors": [
                    {
                        "name": "Erle Zhu"
                    },
                    {
                        "name": "Dazhi Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xujun Li"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Yuxian Gu"
                    },
                    {
                        "name": "Yilin Niu"
                    },
                    {
                        "name": "Aohan Zeng"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Hongning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongning Wang"
                },
                "author": "Hongning Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26490v1",
                "updated": "2025-10-30T13:40:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    40,
                    26,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:40:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    40,
                    26,
                    3,
                    303,
                    0
                ],
                "title": "Scaffolding Creativity: How Divergent and Convergent LLM Personas Shape\n  Human Machine Creative Problem-Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaffolding Creativity: How Divergent and Convergent LLM Personas Shape\n  Human Machine Creative Problem-Solving"
                },
                "summary": "Large language models (LLMs) are increasingly shaping creative work and\nproblem-solving; however, prior research suggests that they may diminish\nunassisted creativity. To address this tension, a coach-like LLM environment\nwas developed that embodies divergent and convergent thinking personas as two\ncomplementary processes. Effectiveness and user behavior were assessed through\na controlled experiment in which participants interacted with either persona,\nwhile a control group engaged with a standard LLM providing direct answers.\n  Notably, users' perceptions of which persona best supported their creativity\noften diverged from objective performance measures. Trait-based analyses\nrevealed that individual differences predict when people utilize divergent\nversus convergent personas, suggesting opportunities for adaptive sequencing.\nFurthermore, interaction patterns reflected the design thinking model,\ndemonstrating how persona-guided support shapes creative problem-solving.\n  Our findings provide design principles for creativity support systems that\nstrike a balance between exploration and convergence through persona-based\nguidance and personalization. These insights advance human-AI collaboration\ntools that scaffold rather than overshadow human creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly shaping creative work and\nproblem-solving; however, prior research suggests that they may diminish\nunassisted creativity. To address this tension, a coach-like LLM environment\nwas developed that embodies divergent and convergent thinking personas as two\ncomplementary processes. Effectiveness and user behavior were assessed through\na controlled experiment in which participants interacted with either persona,\nwhile a control group engaged with a standard LLM providing direct answers.\n  Notably, users' perceptions of which persona best supported their creativity\noften diverged from objective performance measures. Trait-based analyses\nrevealed that individual differences predict when people utilize divergent\nversus convergent personas, suggesting opportunities for adaptive sequencing.\nFurthermore, interaction patterns reflected the design thinking model,\ndemonstrating how persona-guided support shapes creative problem-solving.\n  Our findings provide design principles for creativity support systems that\nstrike a balance between exploration and convergence through persona-based\nguidance and personalization. These insights advance human-AI collaboration\ntools that scaffold rather than overshadow human creativity."
                },
                "authors": [
                    {
                        "name": "Alon Rosenbaum"
                    },
                    {
                        "name": "Yigal David"
                    },
                    {
                        "name": "Eran Kaufman"
                    },
                    {
                        "name": "Gilad Ravid"
                    },
                    {
                        "name": "Amit Ronen"
                    },
                    {
                        "name": "Assaf Krebs"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Krebs"
                },
                "author": "Assaf Krebs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26486v1",
                "updated": "2025-10-30T13:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks"
                },
                "summary": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    },
                    {
                        "name": "Guadalupe Correa-Cabrera"
                    }
                ],
                "author_detail": {
                    "name": "Guadalupe Correa-Cabrera"
                },
                "author": "Guadalupe Correa-Cabrera",
                "arxiv_comment": "Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26485v1",
                "updated": "2025-10-30T13:38:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    38,
                    8,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:38:08Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    38,
                    8,
                    3,
                    303,
                    0
                ],
                "title": "Discovering Causal Relationships Between Time Series With Spatial\n  Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Causal Relationships Between Time Series With Spatial\n  Structure"
                },
                "summary": "Causal discovery is the subfield of causal inference concerned with\nestimating the structure of cause-and-effect relationships in a system of\ninterrelated variables, as opposed to quantifying the strength of causal\neffects. As interest in causal discovery builds in fields such as ecology,\npublic health, and environmental sciences where data is regularly collected\nwith spatial and temporal structures, approaches must evolve to manage\nautocorrelation and complex confounding. As it stands, the few proposed causal\ndiscovery algorithms for spatiotemporal data require summarizing across\nlocations, ignore spatial autocorrelation, and/or scale poorly to high\ndimensions. Here, we introduce our developing framework that extends\ntime-series causal discovery to systems with spatial structure, building upon\nwork on causal discovery across contexts and methods for handling spatial\nconfounding in causal effect estimation. We close by outlining remaining gaps\nin the literature and directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal discovery is the subfield of causal inference concerned with\nestimating the structure of cause-and-effect relationships in a system of\ninterrelated variables, as opposed to quantifying the strength of causal\neffects. As interest in causal discovery builds in fields such as ecology,\npublic health, and environmental sciences where data is regularly collected\nwith spatial and temporal structures, approaches must evolve to manage\nautocorrelation and complex confounding. As it stands, the few proposed causal\ndiscovery algorithms for spatiotemporal data require summarizing across\nlocations, ignore spatial autocorrelation, and/or scale poorly to high\ndimensions. Here, we introduce our developing framework that extends\ntime-series causal discovery to systems with spatial structure, building upon\nwork on causal discovery across contexts and methods for handling spatial\nconfounding in causal effect estimation. We close by outlining remaining gaps\nin the literature and directions for future research."
                },
                "authors": [
                    {
                        "name": "Rebecca F. Supple"
                    },
                    {
                        "name": "Hannah Worthington"
                    },
                    {
                        "name": "Ben Swallow"
                    }
                ],
                "author_detail": {
                    "name": "Ben Swallow"
                },
                "arxiv_affiliation": "Centre for Research into Ecological and Environmental Modelling, University of St Andrews",
                "author": "Ben Swallow",
                "arxiv_comment": "14 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26484v1",
                "updated": "2025-10-30T13:37:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    37,
                    58,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:37:58Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    37,
                    58,
                    3,
                    303,
                    0
                ],
                "title": "Bayesian Network Fusion of Large Language Models for Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Network Fusion of Large Language Models for Sentiment Analysis"
                },
                "summary": "Large language models (LLMs) continue to advance, with an increasing number\nof domain-specific variants tailored for specialised tasks. However, these\nmodels often lack transparency and explainability, can be costly to fine-tune,\nrequire substantial prompt engineering, yield inconsistent results across\ndomains, and impose significant adverse environmental impact due to their high\ncomputational demands. To address these challenges, we propose the Bayesian\nnetwork LLM fusion (BNLF) framework, which integrates predictions from three\nLLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic\nmechanism for sentiment analysis. BNLF performs late fusion by modelling the\nsentiment predictions from multiple LLMs as probabilistic nodes within a\nBayesian network. Evaluated across three human-annotated financial corpora with\ndistinct linguistic and contextual characteristics, BNLF demonstrates\nconsistent gains of about six percent in accuracy over the baseline LLMs,\nunderscoring its robustness to dataset variability and the effectiveness of\nprobabilistic fusion for interpretable sentiment classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) continue to advance, with an increasing number\nof domain-specific variants tailored for specialised tasks. However, these\nmodels often lack transparency and explainability, can be costly to fine-tune,\nrequire substantial prompt engineering, yield inconsistent results across\ndomains, and impose significant adverse environmental impact due to their high\ncomputational demands. To address these challenges, we propose the Bayesian\nnetwork LLM fusion (BNLF) framework, which integrates predictions from three\nLLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic\nmechanism for sentiment analysis. BNLF performs late fusion by modelling the\nsentiment predictions from multiple LLMs as probabilistic nodes within a\nBayesian network. Evaluated across three human-annotated financial corpora with\ndistinct linguistic and contextual characteristics, BNLF demonstrates\nconsistent gains of about six percent in accuracy over the baseline LLMs,\nunderscoring its robustness to dataset variability and the effectiveness of\nprobabilistic fusion for interpretable sentiment classification."
                },
                "authors": [
                    {
                        "name": "Rasoul Amirzadeh"
                    },
                    {
                        "name": "Dhananjay Thiruvady"
                    },
                    {
                        "name": "Fatemeh Shiri"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Shiri"
                },
                "author": "Fatemeh Shiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26481v1",
                "updated": "2025-10-30T13:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    35,
                    32,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:35:32Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    35,
                    32,
                    3,
                    303,
                    0
                ],
                "title": "Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections"
                },
                "summary": "Large language models (LLMs) such as ChatGPT are increasingly integrated into\nhigh-stakes decision-making, yet little is known about their susceptibility to\nsocial influence. We conducted three preregistered conformity experiments with\nGPT-4o in a hiring context. In a baseline study, GPT consistently favored the\nsame candidate (Profile C), reported moderate expertise (M = 3.01) and high\ncertainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT\nfaced unanimous opposition from eight simulated partners and almost always\nconformed (99.9%), reporting lower certainty and significantly elevated\nself-reported informational and normative conformity (p < .001). In Study 2\n(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of\ndisagreement trials, reporting less certainty and more normative conformity.\nAcross studies, results demonstrate that GPT does not act as an independent\nobserver but adapts to perceived social consensus. These findings highlight\nrisks of treating LLMs as neutral decision aids and underline the need to\nelicit AI judgments prior to exposing them to human opinions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as ChatGPT are increasingly integrated into\nhigh-stakes decision-making, yet little is known about their susceptibility to\nsocial influence. We conducted three preregistered conformity experiments with\nGPT-4o in a hiring context. In a baseline study, GPT consistently favored the\nsame candidate (Profile C), reported moderate expertise (M = 3.01) and high\ncertainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT\nfaced unanimous opposition from eight simulated partners and almost always\nconformed (99.9%), reporting lower certainty and significantly elevated\nself-reported informational and normative conformity (p < .001). In Study 2\n(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of\ndisagreement trials, reporting less certainty and more normative conformity.\nAcross studies, results demonstrate that GPT does not act as an independent\nobserver but adapts to perceived social consensus. These findings highlight\nrisks of treating LLMs as neutral decision aids and underline the need to\nelicit AI judgments prior to exposing them to human opinions."
                },
                "authors": [
                    {
                        "name": "Clarissa Sabrina Arlinghaus"
                    },
                    {
                        "name": "Tristan Kenneweg"
                    },
                    {
                        "name": "Barbara Hammer"
                    },
                    {
                        "name": "Günter W. Maier"
                    }
                ],
                "author_detail": {
                    "name": "Günter W. Maier"
                },
                "author": "Günter W. Maier",
                "arxiv_comment": "5 pages, 5 figures, HAI 2025: Workshop on Socially Aware and\n  Cooperative Intelligent Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26480v1",
                "updated": "2025-10-30T13:34:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    34,
                    41,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:34:41Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    34,
                    41,
                    3,
                    303,
                    0
                ],
                "title": "Automated Extract Method Refactoring with Open-Source LLMs: A\n  Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Extract Method Refactoring with Open-Source LLMs: A\n  Comparative Study"
                },
                "summary": "Automating the Extract Method refactoring (EMR) remains challenging and\nlargely manual despite its importance in improving code readability and\nmaintainability. Recent advances in open-source, resource-efficient Large\nLanguage Models (LLMs) offer promising new approaches for automating such\nhigh-level tasks. In this work, we critically evaluate five state-of-the-art\nopen-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python\ncode. We systematically assess functional correctness and code quality using\nautomated metrics and investigate the impact of prompting strategies by\ncomparing one-shot prompting to a Recursive criticism and improvement (RCI)\napproach. RCI-based prompting consistently outperforms one-shot prompting in\ntest pass rates and refactoring quality. The best-performing models,\nDeepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)\nscores of 0.829 and 0.808, while reducing lines of code (LOC) per method from\n12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453\nand 3.294, respectively. A developer survey on RCI-generated refactorings shows\nover 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation\ncriteria. In contrast, the original code scored below neutral, particularly in\nreadability and maintainability, underscoring the benefits of automated\nrefactoring guided by quality prompts. While traditional metrics like CC and\nLOC provide useful signals, they often diverge from human judgments,\nemphasizing the need for human-in-the-loop evaluation. Our open-source\nbenchmark offers a foundation for future research on automated refactoring with\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the Extract Method refactoring (EMR) remains challenging and\nlargely manual despite its importance in improving code readability and\nmaintainability. Recent advances in open-source, resource-efficient Large\nLanguage Models (LLMs) offer promising new approaches for automating such\nhigh-level tasks. In this work, we critically evaluate five state-of-the-art\nopen-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python\ncode. We systematically assess functional correctness and code quality using\nautomated metrics and investigate the impact of prompting strategies by\ncomparing one-shot prompting to a Recursive criticism and improvement (RCI)\napproach. RCI-based prompting consistently outperforms one-shot prompting in\ntest pass rates and refactoring quality. The best-performing models,\nDeepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)\nscores of 0.829 and 0.808, while reducing lines of code (LOC) per method from\n12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453\nand 3.294, respectively. A developer survey on RCI-generated refactorings shows\nover 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation\ncriteria. In contrast, the original code scored below neutral, particularly in\nreadability and maintainability, underscoring the benefits of automated\nrefactoring guided by quality prompts. While traditional metrics like CC and\nLOC provide useful signals, they often diverge from human judgments,\nemphasizing the need for human-in-the-loop evaluation. Our open-source\nbenchmark offers a foundation for future research on automated refactoring with\nLLMs."
                },
                "authors": [
                    {
                        "name": "Sivajeet Chand"
                    },
                    {
                        "name": "Melih Kilic"
                    },
                    {
                        "name": "Roland Würsching"
                    },
                    {
                        "name": "Sushant Kumar Pandey"
                    },
                    {
                        "name": "Alexander Pretschner"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Pretschner"
                },
                "author": "Alexander Pretschner",
                "arxiv_comment": "Accepted at AIware'25 - Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26478v1",
                "updated": "2025-10-30T13:31:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    31,
                    32,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:31:32Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    31,
                    32,
                    3,
                    303,
                    0
                ],
                "title": "Statistical Inference for Matching Decisions via Matrix Completion under\n  Dependent Missingness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Matching Decisions via Matrix Completion under\n  Dependent Missingness"
                },
                "summary": "This paper studies decision-making and statistical inference for two-sided\nmatching markets via matrix completion. In contrast to the independent sampling\nassumed in classical matrix completion literature, the observed entries, which\narise from past matching data, are constrained by matching capacity. This\nmatching-induced dependence poses new challenges for both estimation and\ninference in the matrix completion framework. We propose a non-convex algorithm\nbased on Grassmannian gradient descent and establish near-optimal entrywise\nconvergence rates for three canonical mechanisms, i.e., one-to-one matching,\none-to-many matching with one-sided random arrival, and two-sided random\narrival. To facilitate valid uncertainty quantification and hypothesis testing\non matching decisions, we further develop a general debiasing and projection\nframework for arbitrary linear forms of the reward matrix, deriving asymptotic\nnormality with finite-sample guarantees under matching-induced dependent\nsampling. Our empirical experiments demonstrate that the proposed approach\nprovides accurate estimation, valid confidence intervals, and efficient\nevaluation of matching policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies decision-making and statistical inference for two-sided\nmatching markets via matrix completion. In contrast to the independent sampling\nassumed in classical matrix completion literature, the observed entries, which\narise from past matching data, are constrained by matching capacity. This\nmatching-induced dependence poses new challenges for both estimation and\ninference in the matrix completion framework. We propose a non-convex algorithm\nbased on Grassmannian gradient descent and establish near-optimal entrywise\nconvergence rates for three canonical mechanisms, i.e., one-to-one matching,\none-to-many matching with one-sided random arrival, and two-sided random\narrival. To facilitate valid uncertainty quantification and hypothesis testing\non matching decisions, we further develop a general debiasing and projection\nframework for arbitrary linear forms of the reward matrix, deriving asymptotic\nnormality with finite-sample guarantees under matching-induced dependent\nsampling. Our empirical experiments demonstrate that the proposed approach\nprovides accurate estimation, valid confidence intervals, and efficient\nevaluation of matching policies."
                },
                "authors": [
                    {
                        "name": "Congyuan Duan"
                    },
                    {
                        "name": "Wanteng Ma"
                    },
                    {
                        "name": "Dong Xia"
                    },
                    {
                        "name": "Kan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kan Xu"
                },
                "author": "Kan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02843v2",
                "updated": "2025-10-30T13:29:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    29,
                    54,
                    3,
                    303,
                    0
                ],
                "published": "2025-07-03T17:52:27Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    52,
                    27,
                    3,
                    184,
                    0
                ],
                "title": "LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding"
                },
                "summary": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Yuchen Ma"
                    },
                    {
                        "name": "Dennis Frauen"
                    },
                    {
                        "name": "Jonas Schweisthal"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26475v1",
                "updated": "2025-10-30T13:27:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    27,
                    42,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:27:42Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    27,
                    42,
                    3,
                    303,
                    0
                ],
                "title": "ReSpec: Towards Optimizing Speculative Decoding in Reinforcement\n  Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSpec: Towards Optimizing Speculative Decoding in Reinforcement\n  Learning Systems"
                },
                "summary": "Adapting large language models (LLMs) via reinforcement learning (RL) is\noften bottlenecked by the generation stage, which can consume over 75\\% of the\ntraining time. Speculative decoding (SD) accelerates autoregressive generation\nin serving systems, but its behavior under RL training remains largely\nunexplored. We identify three critical gaps that hinder the naive integration\nof SD into RL systems: diminishing speedups at large batch sizes, drafter\nstaleness under continual actor updates, and drafter-induced policy\ndegradation.\n  To address these gaps, we present ReSpec, a system that adapts SD to RL\nthrough three complementary mechanisms: dynamically tuning SD configurations,\nevolving the drafter via knowledge distillation, and weighting updates by\nrollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup\nwhile preserving reward convergence and training stability, providing a\npractical solution for efficient RL-based LLM adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting large language models (LLMs) via reinforcement learning (RL) is\noften bottlenecked by the generation stage, which can consume over 75\\% of the\ntraining time. Speculative decoding (SD) accelerates autoregressive generation\nin serving systems, but its behavior under RL training remains largely\nunexplored. We identify three critical gaps that hinder the naive integration\nof SD into RL systems: diminishing speedups at large batch sizes, drafter\nstaleness under continual actor updates, and drafter-induced policy\ndegradation.\n  To address these gaps, we present ReSpec, a system that adapts SD to RL\nthrough three complementary mechanisms: dynamically tuning SD configurations,\nevolving the drafter via knowledge distillation, and weighting updates by\nrollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup\nwhile preserving reward convergence and training stability, providing a\npractical solution for efficient RL-based LLM adaptation."
                },
                "authors": [
                    {
                        "name": "Qiaoling Chen"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Shenggui Li"
                    },
                    {
                        "name": "Guoteng Wang"
                    },
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yonggang Wen"
                    },
                    {
                        "name": "Siyuan Feng"
                    },
                    {
                        "name": "Tianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianwei Zhang"
                },
                "author": "Tianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12371v2",
                "updated": "2025-10-30T13:27:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    27,
                    7,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-18T11:28:17Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    11,
                    28,
                    17,
                    6,
                    138,
                    0
                ],
                "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional\n  Methods for Diverse Medical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional\n  Methods for Diverse Medical Tasks"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/."
                },
                "authors": [
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Ziyi He"
                    },
                    {
                        "name": "Haoran Hu"
                    },
                    {
                        "name": "Xiaochen Zheng"
                    },
                    {
                        "name": "Xichen Zhang"
                    },
                    {
                        "name": "Zixiang Wang"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Liantao Ma"
                    },
                    {
                        "name": "Lequan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lequan Yu"
                },
                "author": "Lequan Yu",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets & Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26473v1",
                "updated": "2025-10-30T13:24:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    24,
                    16,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:24:16Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    24,
                    16,
                    3,
                    303,
                    0
                ],
                "title": "Wireless Memory Approximation for Energy-efficient Task-specific IoT\n  Data Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Memory Approximation for Energy-efficient Task-specific IoT\n  Data Retrieval"
                },
                "summary": "The use of Dynamic Random Access Memory (DRAM) for storing Machine Learning\n(ML) models plays a critical role in accelerating ML inference tasks in the\nnext generation of communication systems. However, periodic refreshment of DRAM\nresults in wasteful energy consumption during standby periods, which is\nsignificant for resource-constrained Internet of Things (IoT) devices. To solve\nthis problem, this work advocates two novel approaches: 1) wireless memory\nactivation and 2) wireless memory approximation. These enable the wireless\ndevices to efficiently manage the available memory by considering the timing\naspects and relevance of ML model usage; hence, reducing the overall energy\nconsumption. Numerical results show that our proposed scheme can realize\nsmaller energy consumption than the always-on approach while satisfying the\nretrieval accuracy constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Dynamic Random Access Memory (DRAM) for storing Machine Learning\n(ML) models plays a critical role in accelerating ML inference tasks in the\nnext generation of communication systems. However, periodic refreshment of DRAM\nresults in wasteful energy consumption during standby periods, which is\nsignificant for resource-constrained Internet of Things (IoT) devices. To solve\nthis problem, this work advocates two novel approaches: 1) wireless memory\nactivation and 2) wireless memory approximation. These enable the wireless\ndevices to efficiently manage the available memory by considering the timing\naspects and relevance of ML model usage; hence, reducing the overall energy\nconsumption. Numerical results show that our proposed scheme can realize\nsmaller energy consumption than the always-on approach while satisfying the\nretrieval accuracy constraint."
                },
                "authors": [
                    {
                        "name": "Junya Shiraishi"
                    },
                    {
                        "name": "Shashi Raj Pandey"
                    },
                    {
                        "name": "Israel Leyva-Mayorga"
                    },
                    {
                        "name": "Petar Popovski"
                    }
                ],
                "author_detail": {
                    "name": "Petar Popovski"
                },
                "author": "Petar Popovski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02091v2",
                "updated": "2025-10-30T13:22:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    22,
                    53,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-02T14:57:13Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    57,
                    13,
                    3,
                    275,
                    0
                ],
                "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning"
                },
                "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models."
                },
                "authors": [
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Keyu Wang"
                    },
                    {
                        "name": "PengXiang Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26470v1",
                "updated": "2025-10-30T13:21:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    21,
                    23,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:21:23Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    21,
                    23,
                    3,
                    303,
                    0
                ],
                "title": "In Defense of the Pre-Test: Valid Inference when Testing Violations of\n  Parallel Trends for Difference-in-Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Defense of the Pre-Test: Valid Inference when Testing Violations of\n  Parallel Trends for Difference-in-Differences"
                },
                "summary": "The difference-in-differences (DID) research design is a key identification\nstrategy which allows researchers to estimate causal effects under the parallel\ntrends assumption. While the parallel trends assumption is counterfactual and\ncannot be tested directly, researchers often examine pre-treatment periods to\ncheck whether the time trends are parallel before treatment is administered.\nRecently, researchers have been cautioned against using preliminary tests which\naim to detect violations of parallel trends in the pre-treatment period. In\nthis paper, we argue that preliminary testing can -- and should -- play an\nimportant role within the DID research design. We propose a new and more\nsubstantively appropriate conditional extrapolation assumption, which requires\nan analyst to conduct a preliminary test to determine whether the severity of\npre-treatment parallel trend violations falls below an acceptable level before\nextrapolation to the post-treatment period is justified. This stands in\ncontrast to prior work which can be interpreted as either setting the\nacceptable level to be exactly zero (in which case preliminary tests lack\npower) or assuming that extrapolation is always justified (in which case\npreliminary tests are not required). Under mild assumptions on how close the\nactual violation is to the acceptable level, we provide a consistent\npreliminary test as well confidence intervals which are valid when conditioned\non the result of the test. The conditional coverage of these intervals\novercomes a common critique made against the use of preliminary testing within\nthe DID research design. We use real data as well as numerical simulations to\nillustrate the performance of the proposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The difference-in-differences (DID) research design is a key identification\nstrategy which allows researchers to estimate causal effects under the parallel\ntrends assumption. While the parallel trends assumption is counterfactual and\ncannot be tested directly, researchers often examine pre-treatment periods to\ncheck whether the time trends are parallel before treatment is administered.\nRecently, researchers have been cautioned against using preliminary tests which\naim to detect violations of parallel trends in the pre-treatment period. In\nthis paper, we argue that preliminary testing can -- and should -- play an\nimportant role within the DID research design. We propose a new and more\nsubstantively appropriate conditional extrapolation assumption, which requires\nan analyst to conduct a preliminary test to determine whether the severity of\npre-treatment parallel trend violations falls below an acceptable level before\nextrapolation to the post-treatment period is justified. This stands in\ncontrast to prior work which can be interpreted as either setting the\nacceptable level to be exactly zero (in which case preliminary tests lack\npower) or assuming that extrapolation is always justified (in which case\npreliminary tests are not required). Under mild assumptions on how close the\nactual violation is to the acceptable level, we provide a consistent\npreliminary test as well confidence intervals which are valid when conditioned\non the result of the test. The conditional coverage of these intervals\novercomes a common critique made against the use of preliminary testing within\nthe DID research design. We use real data as well as numerical simulations to\nillustrate the performance of the proposed methods."
                },
                "authors": [
                    {
                        "name": "Jonas M. Mikhaeil"
                    },
                    {
                        "name": "Christopher Harshaw"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Harshaw"
                },
                "author": "Christopher Harshaw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10603v2",
                "updated": "2025-10-30T13:13:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    13,
                    19,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-15T15:21:09Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    21,
                    9,
                    3,
                    135,
                    0
                ],
                "title": "Toward a Public and Secure Generative AI: A Comparative Analysis of Open\n  and Closed LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Public and Secure Generative AI: A Comparative Analysis of Open\n  and Closed LLMs"
                },
                "summary": "Generative artificial intelligence (Gen AI) systems represent a critical\ntechnology with far-reaching implications across multiple domains of society.\nHowever, their deployment entails a range of risks and challenges that require\ncareful evaluation. To date, there has been a lack of comprehensive,\ninterdisciplinary studies offering a systematic comparison between open-source\nand proprietary (closed) generative AI systems, particularly regarding their\nrespective advantages and drawbacks. This study aims to: i) critically evaluate\nand compare the characteristics, opportunities, and challenges of open and\nclosed generative AI models; and ii) propose foundational elements for the\ndevelopment of an Open, Public, and Safe Gen AI framework. As a methodology, we\nadopted a combined approach that integrates three methods: literature review,\ncritical analysis, and comparative analysis. The proposed framework outlines\nkey dimensions, openness, public governance, and security, as essential pillars\nfor shaping the future of trustworthy and inclusive Gen AI. Our findings reveal\nthat open models offer greater transparency, auditability, and flexibility,\nenabling independent scrutiny and bias mitigation. In contrast, closed systems\noften provide better technical support and ease of implementation, but at the\ncost of unequal access, accountability, and ethical oversight. The research\nalso highlights the importance of multi-stakeholder governance, environmental\nsustainability, and regulatory frameworks in ensuring responsible development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (Gen AI) systems represent a critical\ntechnology with far-reaching implications across multiple domains of society.\nHowever, their deployment entails a range of risks and challenges that require\ncareful evaluation. To date, there has been a lack of comprehensive,\ninterdisciplinary studies offering a systematic comparison between open-source\nand proprietary (closed) generative AI systems, particularly regarding their\nrespective advantages and drawbacks. This study aims to: i) critically evaluate\nand compare the characteristics, opportunities, and challenges of open and\nclosed generative AI models; and ii) propose foundational elements for the\ndevelopment of an Open, Public, and Safe Gen AI framework. As a methodology, we\nadopted a combined approach that integrates three methods: literature review,\ncritical analysis, and comparative analysis. The proposed framework outlines\nkey dimensions, openness, public governance, and security, as essential pillars\nfor shaping the future of trustworthy and inclusive Gen AI. Our findings reveal\nthat open models offer greater transparency, auditability, and flexibility,\nenabling independent scrutiny and bias mitigation. In contrast, closed systems\noften provide better technical support and ease of implementation, but at the\ncost of unequal access, accountability, and ethical oversight. The research\nalso highlights the importance of multi-stakeholder governance, environmental\nsustainability, and regulatory frameworks in ensuring responsible development."
                },
                "authors": [
                    {
                        "name": "Jorge Machado"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Machado"
                },
                "author": "Jorge Machado",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.26790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26790v1",
                "updated": "2025-10-30T17:58:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    58,
                    26,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:58:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    58,
                    26,
                    3,
                    303,
                    0
                ],
                "title": "Gistify! Codebase-Level Understanding via Runtime Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gistify! Codebase-Level Understanding via Runtime Execution"
                },
                "summary": "As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces."
                },
                "authors": [
                    {
                        "name": "Hyunji Lee"
                    },
                    {
                        "name": "Minseon Kim"
                    },
                    {
                        "name": "Chinmay Singh"
                    },
                    {
                        "name": "Matheus Pereira"
                    },
                    {
                        "name": "Atharv Sonwane"
                    },
                    {
                        "name": "Isadora White"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Zhengyan Shi"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Marc-Alexandre Côté"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Lucas Caccia"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Caccia"
                },
                "author": "Lucas Caccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26788v1",
                "updated": "2025-10-30T17:58:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    58,
                    11,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:58:11Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    58,
                    11,
                    3,
                    303,
                    0
                ],
                "title": "Defeating the Training-Inference Mismatch via FP16",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defeating the Training-Inference Mismatch via FP16"
                },
                "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Xiangxin Zhou"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26784v1",
                "updated": "2025-10-30T17:57:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    57,
                    17,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:57:17Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    57,
                    17,
                    3,
                    303,
                    0
                ],
                "title": "LLMs Process Lists With General Filter Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Process Lists With General Filter Heads"
                },
                "summary": "We investigate the mechanisms underlying a range of list-processing tasks in\nLLMs, and we find that LLMs have learned to encode a compact, causal\nrepresentation of a general filtering operation that mirrors the generic\n\"filter\" function of functional programming. Using causal mediation analysis on\na diverse set of list-processing tasks, we find that a small number of\nattention heads, which we dub filter heads, encode a compact representation of\nthe filtering predicate in their query states at certain tokens. We demonstrate\nthat this predicate representation is general and portable: it can be extracted\nand reapplied to execute the same filtering operation on different collections,\npresented in different formats, languages, or even in tasks. However, we also\nidentify situations where transformer LMs can exploit a different strategy for\nfiltering: eagerly evaluating if an item satisfies the predicate and storing\nthis intermediate result as a flag directly in the item representations. Our\nresults reveal that transformer LMs can develop human-interpretable\nimplementations of abstract computational operations that generalize in ways\nthat are surprisingly similar to strategies used in traditional functional\nprogramming patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the mechanisms underlying a range of list-processing tasks in\nLLMs, and we find that LLMs have learned to encode a compact, causal\nrepresentation of a general filtering operation that mirrors the generic\n\"filter\" function of functional programming. Using causal mediation analysis on\na diverse set of list-processing tasks, we find that a small number of\nattention heads, which we dub filter heads, encode a compact representation of\nthe filtering predicate in their query states at certain tokens. We demonstrate\nthat this predicate representation is general and portable: it can be extracted\nand reapplied to execute the same filtering operation on different collections,\npresented in different formats, languages, or even in tasks. However, we also\nidentify situations where transformer LMs can exploit a different strategy for\nfiltering: eagerly evaluating if an item satisfies the predicate and storing\nthis intermediate result as a flag directly in the item representations. Our\nresults reveal that transformer LMs can develop human-interpretable\nimplementations of abstract computational operations that generalize in ways\nthat are surprisingly similar to strategies used in traditional functional\nprogramming patterns."
                },
                "authors": [
                    {
                        "name": "Arnab Sen Sharma"
                    },
                    {
                        "name": "Giordano Rogers"
                    },
                    {
                        "name": "Natalie Shapira"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "arxiv_comment": "Code and data at https://filter.baulab.info/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26771v1",
                "updated": "2025-10-30T17:53:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    53,
                    42,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:53:42Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    53,
                    42,
                    3,
                    303,
                    0
                ],
                "title": "STaMP: Sequence Transformation and Mixed Precision for Low-Precision\n  Activation Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STaMP: Sequence Transformation and Mixed Precision for Low-Precision\n  Activation Quantization"
                },
                "summary": "Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is the key method for reducing inference latency, power and\nmemory footprint of generative AI models. However, accuracy often degrades\nsharply when activations are quantized below eight bits. Recent work suggests\nthat invertible linear transformations (e.g. rotations) can aid quantization,\nby reparameterizing feature channels and weights. In this paper, we propose\n\\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a\nnovel strategy that applies linear transformations along the \\textit{sequence}\ndimension to exploit the strong local correlation in language and visual data.\nBy keeping a small number of tokens in each intermediate activation at higher\nprecision, we can maintain model accuracy at lower (average) activations\nbit-widths. We evaluate STaMP on recent LVM and LLM architectures,\ndemonstrating that it significantly improves low bit width activation\nquantization and complements established activation and weight quantization\nmethods including recent feature transformations."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Riccardo Del Chiaro"
                    },
                    {
                        "name": "Boris van Breugel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Markus Nagel"
                    }
                ],
                "author_detail": {
                    "name": "Markus Nagel"
                },
                "author": "Markus Nagel",
                "arxiv_comment": "10 pages main text, 8 pages supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26768v1",
                "updated": "2025-10-30T17:52:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    52,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:52:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    52,
                    2,
                    3,
                    303,
                    0
                ],
                "title": "AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMO-Bench: Large Language Models Still Struggle in High School Math\n  Competitions"
                },
                "summary": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present AMO-Bench, an Advanced Mathematical reasoning benchmark with\nOlympiad level or even higher difficulty, comprising 50 human-crafted problems.\nExisting benchmarks have widely leveraged high school math competitions for\nevaluating mathematical reasoning capabilities of large language models (LLMs).\nHowever, many existing math competitions are becoming less effective for\nassessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To\naddress this, AMO-Bench introduces more rigorous challenges by ensuring all 50\nproblems are (1) cross-validated by experts to meet at least the International\nMathematical Olympiad (IMO) difficulty standards, and (2) entirely original\nproblems to prevent potential performance leakages from data memorization.\nMoreover, each problem in AMO-Bench requires only a final answer rather than a\nproof, enabling automatic and robust grading for evaluation. Experimental\nresults across 26 LLMs on AMO-Bench show that even the best-performing model\nachieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%.\nBeyond these poor performances, our further analysis reveals a promising\nscaling trend with increasing test-time compute on AMO-Bench. These results\nhighlight the significant room for improving the mathematical reasoning in\ncurrent LLMs. We release AMO-Bench to facilitate further research into\nadvancing the reasoning abilities of language models.\nhttps://amo-bench.github.io/"
                },
                "authors": [
                    {
                        "name": "Shengnan An"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yehao Lin"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Xinxuan Lv"
                    },
                    {
                        "name": "Dan Ma"
                    },
                    {
                        "name": "Xuanlin Wang"
                    },
                    {
                        "name": "Ziwen Wang"
                    },
                    {
                        "name": "Shuang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuang Zhou"
                },
                "arxiv_affiliation": "Alphabetical order by last name",
                "author": "Shuang Zhou",
                "arxiv_comment": "14 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26752v1",
                "updated": "2025-10-30T17:46:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    46,
                    49,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:46:49Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    46,
                    49,
                    3,
                    303,
                    0
                ],
                "title": "The Oversight Game: Learning to Cooperatively Balance an AI Agent's\n  Safety and Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Oversight Game: Learning to Cooperatively Balance an AI Agent's\n  Safety and Autonomy"
                },
                "summary": "As increasingly capable agents are deployed, a central safety question is how\nto retain meaningful human control without modifying the underlying system. We\nstudy a minimal control interface where an agent chooses whether to act\nautonomously (play) or defer (ask), while a human simultaneously chooses\nwhether to be permissive (trust) or to engage in oversight (oversee). If the\nagent defers, the human's choice determines the outcome, potentially leading to\na corrective action or a system shutdown. We model this interaction as a\ntwo-player Markov Game. Our analysis focuses on cases where this game qualifies\nas a Markov Potential Game (MPG), a class of games where we can provide an\nalignment guarantee: under a structural assumption on the human's value\nfunction, any decision by the agent to act more autonomously that benefits\nitself cannot harm the human's value. We also analyze extensions to this MPG\nframework. Theoretically, this perspective provides conditions for a specific\nform of intrinsic alignment. If the reward structures of the human-agent game\nmeet these conditions, we have a formal guarantee that the agent improving its\nown outcome will not harm the human's. Practically, this model motivates a\ntransparent control layer with predictable incentives where the agent learns to\ndefer when risky and act when safe, while its pretrained policy and the\nenvironment's reward structure remain untouched. Our gridworld simulation shows\nthat through independent learning, the agent and human discover their optimal\noversight roles. The agent learns to ask when uncertain and the human learns\nwhen to oversee, leading to an emergent collaboration that avoids safety\nviolations introduced post-training. This demonstrates a practical method for\nmaking misaligned models safer after deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As increasingly capable agents are deployed, a central safety question is how\nto retain meaningful human control without modifying the underlying system. We\nstudy a minimal control interface where an agent chooses whether to act\nautonomously (play) or defer (ask), while a human simultaneously chooses\nwhether to be permissive (trust) or to engage in oversight (oversee). If the\nagent defers, the human's choice determines the outcome, potentially leading to\na corrective action or a system shutdown. We model this interaction as a\ntwo-player Markov Game. Our analysis focuses on cases where this game qualifies\nas a Markov Potential Game (MPG), a class of games where we can provide an\nalignment guarantee: under a structural assumption on the human's value\nfunction, any decision by the agent to act more autonomously that benefits\nitself cannot harm the human's value. We also analyze extensions to this MPG\nframework. Theoretically, this perspective provides conditions for a specific\nform of intrinsic alignment. If the reward structures of the human-agent game\nmeet these conditions, we have a formal guarantee that the agent improving its\nown outcome will not harm the human's. Practically, this model motivates a\ntransparent control layer with predictable incentives where the agent learns to\ndefer when risky and act when safe, while its pretrained policy and the\nenvironment's reward structure remain untouched. Our gridworld simulation shows\nthat through independent learning, the agent and human discover their optimal\noversight roles. The agent learns to ask when uncertain and the human learns\nwhen to oversee, leading to an emergent collaboration that avoids safety\nviolations introduced post-training. This demonstrates a practical method for\nmaking misaligned models safer after deployment."
                },
                "authors": [
                    {
                        "name": "William Overman"
                    },
                    {
                        "name": "Mohsen Bayati"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Bayati"
                },
                "author": "Mohsen Bayati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09391v2",
                "updated": "2025-10-30T17:41:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    41,
                    15,
                    3,
                    303,
                    0
                ],
                "published": "2025-06-11T04:44:46Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    4,
                    44,
                    46,
                    2,
                    162,
                    0
                ],
                "title": "Comparing human and LLM politeness strategies in free production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing human and LLM politeness strategies in free production"
                },
                "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Robert D. Hawkins"
                    }
                ],
                "author_detail": {
                    "name": "Robert D. Hawkins"
                },
                "author": "Robert D. Hawkins",
                "arxiv_comment": "25 pages, 5 figures | EMNLP 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09205v3",
                "updated": "2025-10-30T17:37:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    37,
                    55,
                    3,
                    303,
                    0
                ],
                "published": "2025-03-12T09:48:38Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    48,
                    38,
                    2,
                    71,
                    0
                ],
                "title": "Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model"
                },
                "summary": "Integrating audio and visual data for training multimodal foundational models\nremains a challenge. The Audio-Video Vector Alignment (AVVA) framework\naddresses this by considering AV scene alignment beyond mere temporal\nsynchronization, and leveraging Large Language Models (LLMs) for data curation.\nAVVA implements a scoring mechanism for selecting aligned training data\nsegments. It integrates Whisper, a speech-based foundation model, for audio and\nDINOv2 for video analysis in a dual-encoder structure with contrastive learning\non AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the\neffectiveness of the proposed model architecture and data curation approach.\nAVVA achieves a significant improvement in top-k accuracies for video-to-audio\nretrieval on all datasets compared to DenseAV, while using only 192 hrs of\ncurated training data. Furthermore, an ablation study indicates that the data\ncuration process effectively trades data quality for data quantity, yielding\nincreases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,\ncompared to training on the full spectrum of uncurated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating audio and visual data for training multimodal foundational models\nremains a challenge. The Audio-Video Vector Alignment (AVVA) framework\naddresses this by considering AV scene alignment beyond mere temporal\nsynchronization, and leveraging Large Language Models (LLMs) for data curation.\nAVVA implements a scoring mechanism for selecting aligned training data\nsegments. It integrates Whisper, a speech-based foundation model, for audio and\nDINOv2 for video analysis in a dual-encoder structure with contrastive learning\non AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the\neffectiveness of the proposed model architecture and data curation approach.\nAVVA achieves a significant improvement in top-k accuracies for video-to-audio\nretrieval on all datasets compared to DenseAV, while using only 192 hrs of\ncurated training data. Furthermore, an ablation study indicates that the data\ncuration process effectively trades data quality for data quantity, yielding\nincreases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound,\ncompared to training on the full spectrum of uncurated data."
                },
                "authors": [
                    {
                        "name": "Ali Vosoughi"
                    },
                    {
                        "name": "Dimitra Emmanouilidou"
                    },
                    {
                        "name": "Hannes Gamper"
                    }
                ],
                "author_detail": {
                    "name": "Hannes Gamper"
                },
                "author": "Hannes Gamper",
                "arxiv_comment": "5 pages, 5 figures, 2 tables. Accepted at EUSIPCO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T, 68T45, 68T10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14681v2",
                "updated": "2025-10-30T17:32:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    32,
                    44,
                    3,
                    303,
                    0
                ],
                "published": "2025-06-17T16:13:15Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    16,
                    13,
                    15,
                    1,
                    168,
                    0
                ],
                "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and\n  Training Factors Shape LLM Alignment Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and\n  Training Factors Shape LLM Alignment Quality"
                },
                "summary": "Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness, often surpassing superficial similarity between the training\ndata and the benchmark, and that mid-layer weight changes correlate most\nstrongly with performance gains. We release these 1,000+ SFT models and\nbenchmark results to accelerate further research. All resources are available\nat https://github.com/llm-jp/massive-sft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness, often surpassing superficial similarity between the training\ndata and the benchmark, and that mid-layer weight changes correlate most\nstrongly with performance gains. We release these 1,000+ SFT models and\nbenchmark results to accelerate further research. All resources are available\nat https://github.com/llm-jp/massive-sft."
                },
                "authors": [
                    {
                        "name": "Yuto Harada"
                    },
                    {
                        "name": "Yusuke Yamauchi"
                    },
                    {
                        "name": "Yusuke Oda"
                    },
                    {
                        "name": "Yohei Oseki"
                    },
                    {
                        "name": "Yusuke Miyao"
                    },
                    {
                        "name": "Yu Takagi"
                    }
                ],
                "author_detail": {
                    "name": "Yu Takagi"
                },
                "author": "Yu Takagi",
                "arxiv_comment": "Accepted to EMNLP 2025 (Main Conference). Models and evaluation\n  results available at: https://github.com/llm-jp/massive-sft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26727v1",
                "updated": "2025-10-30T17:27:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    27,
                    3,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:27:03Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    27,
                    3,
                    3,
                    303,
                    0
                ],
                "title": "Neither Consent nor Property: A Policy Lab for Data Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neither Consent nor Property: A Policy Lab for Data Law"
                },
                "summary": "This paper makes the opaque data market in the AI economy empirically legible\nfor the first time, constructing a computational testbed to address a core\nepistemic failure: regulators governing a market defined by structural opacity,\nfragile price discovery, and brittle technical safeguards that have paralyzed\ntraditional empirics and fragmented policy. The pipeline begins with multi-year\nfieldwork to extract the market's hidden logic, and then embeds these grounded\nbehaviors into a high-fidelity ABM, parameterized via a novel LLM-based\ndiscrete-choice experiment that captures the preferences of unsurveyable\npopulations. The pipeline is validated against reality, reproducing observed\ntrade patterns. This policy laboratory delivers clear, counter-intuitive\nresults. First, property-style relief is a false promise: ''anonymous-data''\ncarve-outs expand trade but ignore risk, causing aggregate welfare to collapse\nonce external harms are priced in. Second, social welfare peaks when the\ndownstream buyer internalizes the full substantive risk. This least-cost\navoider approach induces efficient safeguards, simultaneously raising welfare\nand sustaining trade, and provides a robust empirical foundation for the legal\ndrift toward two-sided reachability. The contribution is a reproducible\npipeline designed to end the reliance on intuition. It converts qualitative\ninsight into testable, comparative policy experiments, obsoleting armchair\nconjecture by replacing it with controlled evidence on how legal rules actually\nshift risk and surplus. This is the forward-looking engine that moves the field\nfrom competing intuitions to direct, computational analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper makes the opaque data market in the AI economy empirically legible\nfor the first time, constructing a computational testbed to address a core\nepistemic failure: regulators governing a market defined by structural opacity,\nfragile price discovery, and brittle technical safeguards that have paralyzed\ntraditional empirics and fragmented policy. The pipeline begins with multi-year\nfieldwork to extract the market's hidden logic, and then embeds these grounded\nbehaviors into a high-fidelity ABM, parameterized via a novel LLM-based\ndiscrete-choice experiment that captures the preferences of unsurveyable\npopulations. The pipeline is validated against reality, reproducing observed\ntrade patterns. This policy laboratory delivers clear, counter-intuitive\nresults. First, property-style relief is a false promise: ''anonymous-data''\ncarve-outs expand trade but ignore risk, causing aggregate welfare to collapse\nonce external harms are priced in. Second, social welfare peaks when the\ndownstream buyer internalizes the full substantive risk. This least-cost\navoider approach induces efficient safeguards, simultaneously raising welfare\nand sustaining trade, and provides a robust empirical foundation for the legal\ndrift toward two-sided reachability. The contribution is a reproducible\npipeline designed to end the reliance on intuition. It converts qualitative\ninsight into testable, comparative policy experiments, obsoleting armchair\nconjecture by replacing it with controlled evidence on how legal rules actually\nshift risk and surplus. This is the forward-looking engine that moves the field\nfrom competing intuitions to direct, computational analysis."
                },
                "authors": [
                    {
                        "name": "Haoyi Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhu"
                },
                "author": "Tianyi Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21319v2",
                "updated": "2025-10-30T17:09:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    9,
                    54,
                    3,
                    303,
                    0
                ],
                "published": "2025-09-25T16:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    16,
                    19,
                    6,
                    3,
                    268,
                    0
                ],
                "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards"
                },
                "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost). Models:\nhttps://huggingface.co/collections/nvidia/reward-models-10-2025"
                },
                "authors": [
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Ellie Evans"
                    },
                    {
                        "name": "Daniel Egert"
                    },
                    {
                        "name": "Hoo-Chang Shin"
                    },
                    {
                        "name": "Felipe Soares"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    }
                ],
                "author_detail": {
                    "name": "Oleksii Kuchaiev"
                },
                "author": "Oleksii Kuchaiev",
                "arxiv_comment": "Added link to access models:\n  https://huggingface.co/collections/nvidia/reward-models-10-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26707v1",
                "updated": "2025-10-30T17:09:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    9,
                    9,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:09:09Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    9,
                    9,
                    3,
                    303,
                    0
                ],
                "title": "Value Drifts: Tracing Value Alignment During LLM Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Drifts: Tracing Value Alignment During LLM Post-Training"
                },
                "summary": "As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs occupy an increasingly important role in society, they are more and\nmore confronted with questions that require them not only to draw on their\ngeneral knowledge but also to align with certain human value systems.\nTherefore, studying the alignment of LLMs with human values has become a\ncrucial field of inquiry. Prior work, however, mostly focuses on evaluating the\nalignment of fully trained models, overlooking the training dynamics by which\nmodels learn to express human values. In this work, we investigate how and at\nwhich stage value alignment arises during the course of a model's\npost-training. Our analysis disentangles the effects of post-training\nalgorithms and datasets, measuring both the magnitude and time of value drifts\nduring training. Experimenting with Llama-3 and Qwen-3 models of different\nsizes and popular supervised fine-tuning (SFT) and preference optimization\ndatasets and algorithms, we find that the SFT phase generally establishes a\nmodel's values, and subsequent preference optimization rarely re-aligns these\nvalues. Furthermore, using a synthetic preference dataset that enables\ncontrolled manipulation of values, we find that different preference\noptimization algorithms lead to different value alignment outcomes, even when\npreference data is held constant. Our findings provide actionable insights into\nhow values are learned during post-training and help to inform data curation,\nas well as the selection of models and algorithms for preference optimization\nto improve model alignment to human values."
                },
                "authors": [
                    {
                        "name": "Mehar Bhatia"
                    },
                    {
                        "name": "Shravan Nayak"
                    },
                    {
                        "name": "Gaurav Kamath"
                    },
                    {
                        "name": "Marius Mosbach"
                    },
                    {
                        "name": "Karolina Stańczak"
                    },
                    {
                        "name": "Vered Shwartz"
                    },
                    {
                        "name": "Siva Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Siva Reddy"
                },
                "author": "Siva Reddy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26703v1",
                "updated": "2025-10-30T17:07:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    7,
                    4,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:07:04Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    7,
                    4,
                    3,
                    303,
                    0
                ],
                "title": "ProstNFound+: A Prospective Study using Medical Foundation Models for\n  Prostate Cancer Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProstNFound+: A Prospective Study using Medical Foundation Models for\n  Prostate Cancer Detection"
                },
                "summary": "Purpose: Medical foundation models (FMs) offer a path to build\nhigh-performance diagnostic systems. However, their application to prostate\ncancer (PCa) detection from micro-ultrasound ({\\mu}US) remains untested in\nclinical settings. We present ProstNFound+, an adaptation of FMs for PCa\ndetection from {\\mu}US, along with its first prospective validation. Methods:\nProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt\nencoder that embeds PCa-specific clinical biomarkers. The model generates a\ncancer heatmap and a risk score for clinically significant PCa. Following\ntraining on multi-center retrospective data, the model is prospectively\nevaluated on data acquired five years later from a new clinical site. Model\npredictions are benchmarked against standard clinical scoring protocols\n(PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the\nprospective data, with no performance degradation compared to retrospective\nevaluation. It aligns closely with clinical scores and produces interpretable\nheatmaps consistent with biopsy-confirmed lesions. Conclusion: The results\nhighlight its potential for clinical deployment, offering a scalable and\ninterpretable alternative to expert-driven protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Medical foundation models (FMs) offer a path to build\nhigh-performance diagnostic systems. However, their application to prostate\ncancer (PCa) detection from micro-ultrasound ({\\mu}US) remains untested in\nclinical settings. We present ProstNFound+, an adaptation of FMs for PCa\ndetection from {\\mu}US, along with its first prospective validation. Methods:\nProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt\nencoder that embeds PCa-specific clinical biomarkers. The model generates a\ncancer heatmap and a risk score for clinically significant PCa. Following\ntraining on multi-center retrospective data, the model is prospectively\nevaluated on data acquired five years later from a new clinical site. Model\npredictions are benchmarked against standard clinical scoring protocols\n(PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the\nprospective data, with no performance degradation compared to retrospective\nevaluation. It aligns closely with clinical scores and produces interpretable\nheatmaps consistent with biopsy-confirmed lesions. Conclusion: The results\nhighlight its potential for clinical deployment, offering a scalable and\ninterpretable alternative to expert-driven protocols."
                },
                "authors": [
                    {
                        "name": "Paul F. R. Wilson"
                    },
                    {
                        "name": "Mohamed Harmanani"
                    },
                    {
                        "name": "Minh Nguyen Nhat To"
                    },
                    {
                        "name": "Amoon Jamzad"
                    },
                    {
                        "name": "Tarek Elghareb"
                    },
                    {
                        "name": "Zhuoxin Guo"
                    },
                    {
                        "name": "Adam Kinnaird"
                    },
                    {
                        "name": "Brian Wodlinger"
                    },
                    {
                        "name": "Purang Abolmaesumi"
                    },
                    {
                        "name": "Parvin Mousavi"
                    }
                ],
                "author_detail": {
                    "name": "Parvin Mousavi"
                },
                "author": "Parvin Mousavi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26699v1",
                "updated": "2025-10-30T17:05:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    5,
                    13,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:05:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    5,
                    13,
                    3,
                    303,
                    0
                ],
                "title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative\n  Assessment"
                },
                "summary": "Keeping software systems up to date is essential to avoid technical debt,\nsecurity vulnerabilities, and the rigidity typical of legacy systems. However,\nupdating libraries and frameworks remains a time consuming and error-prone\nprocess. Recent advances in Large Language Models (LLMs) and agentic coding\nsystems offer new opportunities for automating such maintenance tasks. In this\npaper, we evaluate the update of a well-known Python library, SQLAlchemy,\nacross a dataset of ten client applications. For this task, we use the Github's\nCopilot Agent Mode, an autonomous AI systema capable of planning and executing\nmulti-step migration workflows. To assess the effectiveness of the automated\nmigration, we also introduce Migration Coverage, a metric that quantifies the\nproportion of API usage points correctly migrated. The results of our study\nshow that the LLM agent was capable of migrating functionalities and API usages\nbetween SQLAlchemy versions (migration coverage: 100%, median), but failed to\nmaintain the application functionality, leading to a low test-pass rate\n(39.75%, median).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping software systems up to date is essential to avoid technical debt,\nsecurity vulnerabilities, and the rigidity typical of legacy systems. However,\nupdating libraries and frameworks remains a time consuming and error-prone\nprocess. Recent advances in Large Language Models (LLMs) and agentic coding\nsystems offer new opportunities for automating such maintenance tasks. In this\npaper, we evaluate the update of a well-known Python library, SQLAlchemy,\nacross a dataset of ten client applications. For this task, we use the Github's\nCopilot Agent Mode, an autonomous AI systema capable of planning and executing\nmulti-step migration workflows. To assess the effectiveness of the automated\nmigration, we also introduce Migration Coverage, a metric that quantifies the\nproportion of API usage points correctly migrated. The results of our study\nshow that the LLM agent was capable of migrating functionalities and API usages\nbetween SQLAlchemy versions (migration coverage: 100%, median), but failed to\nmaintain the application functionality, leading to a low test-pass rate\n(39.75%, median)."
                },
                "authors": [
                    {
                        "name": "Aylton Almeida"
                    },
                    {
                        "name": "Laerte Xavier"
                    },
                    {
                        "name": "Marco Tulio Valente"
                    }
                ],
                "author_detail": {
                    "name": "Marco Tulio Valente"
                },
                "author": "Marco Tulio Valente",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26697v1",
                "updated": "2025-10-30T17:01:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    1,
                    43,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:01:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    1,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "The End of Manual Decoding: Towards Truly End-to-End Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The End of Manual Decoding: Towards Truly End-to-End Language Models"
                },
                "summary": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding."
                },
                "authors": [
                    {
                        "name": "Zhichao Wang"
                    },
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Xiaoying Tang"
                    },
                    {
                        "name": "Yan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Wang"
                },
                "author": "Yan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26690v1",
                "updated": "2025-10-30T16:59:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    22,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:59:22Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    22,
                    3,
                    303,
                    0
                ],
                "title": "LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits"
                },
                "summary": "Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance."
                },
                "authors": [
                    {
                        "name": "Amir Reza Mirzaei"
                    },
                    {
                        "name": "Yuqiao Wen"
                    },
                    {
                        "name": "Yanshuai Cao"
                    },
                    {
                        "name": "Lili Mou"
                    }
                ],
                "author_detail": {
                    "name": "Lili Mou"
                },
                "author": "Lili Mou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26684v1",
                "updated": "2025-10-30T16:54:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    54,
                    16,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:54:16Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    54,
                    16,
                    3,
                    303,
                    0
                ],
                "title": "Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill"
                },
                "summary": "We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a long-term deployment study of a machine vision-based anomaly\ndetection system for failure prediction in a steel rolling mill. The system\nintegrates industrial cameras to monitor equipment operation, alignment, and\nhot bar motion in real time along the process line. Live video streams are\nprocessed on a centralized video server using deep learning models, enabling\nearly prediction of equipment failures and process interruptions, thereby\nreducing unplanned breakdown costs. Server-based inference minimizes the\ncomputational load on industrial process control systems (PLCs), supporting\nscalable deployment across production lines with minimal additional resources.\nBy jointly analyzing sensor data from data acquisition systems and visual\ninputs, the system identifies the location and probable root causes of\nfailures, providing actionable insights for proactive maintenance. This\nintegrated approach enhances operational reliability, productivity, and\nprofitability in industrial manufacturing environments."
                },
                "authors": [
                    {
                        "name": "Vaibhav Kurrey"
                    },
                    {
                        "name": "Sivakalyan Pujari"
                    },
                    {
                        "name": "Gagan Raj Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Gagan Raj Gupta"
                },
                "author": "Gagan Raj Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26683v1",
                "updated": "2025-10-30T16:53:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    53,
                    45,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:53:45Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    53,
                    45,
                    3,
                    303,
                    0
                ],
                "title": "Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities\nacross multiple domains by leveraging massive pre-training and curated\nfine-tuning data. However, in data-sensitive fields such as healthcare, the\nlack of high-quality, domain-specific training corpus hinders LLMs' adaptation\nfor specialized applications. Meanwhile, domain experts have distilled domain\nwisdom into ontology rules, which formalize relationships among concepts and\nensure the integrity of knowledge management repositories. Viewing LLMs as\nimplicit repositories of human knowledge, we propose Evontree, a novel\nframework that leverages a small set of high-quality ontology rules to\nsystematically extract, validate, and enhance domain knowledge within LLMs,\nwithout requiring extensive external datasets. Specifically, Evontree extracts\ndomain ontology from raw models, detects inconsistencies using two core\nontology rules, and reinforces the refined knowledge via self-distilled\nfine-tuning. Extensive experiments on medical QA benchmarks with\nLlama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both\nunmodified models and leading supervised baselines, achieving up to a 3.7%\nimprovement in accuracy. These results confirm the effectiveness, efficiency,\nand robustness of our approach for low-resource domain adaptation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities\nacross multiple domains by leveraging massive pre-training and curated\nfine-tuning data. However, in data-sensitive fields such as healthcare, the\nlack of high-quality, domain-specific training corpus hinders LLMs' adaptation\nfor specialized applications. Meanwhile, domain experts have distilled domain\nwisdom into ontology rules, which formalize relationships among concepts and\nensure the integrity of knowledge management repositories. Viewing LLMs as\nimplicit repositories of human knowledge, we propose Evontree, a novel\nframework that leverages a small set of high-quality ontology rules to\nsystematically extract, validate, and enhance domain knowledge within LLMs,\nwithout requiring extensive external datasets. Specifically, Evontree extracts\ndomain ontology from raw models, detects inconsistencies using two core\nontology rules, and reinforces the refined knowledge via self-distilled\nfine-tuning. Extensive experiments on medical QA benchmarks with\nLlama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both\nunmodified models and leading supervised baselines, achieving up to a 3.7%\nimprovement in accuracy. These results confirm the effectiveness, efficiency,\nand robustness of our approach for low-resource domain adaptation of LLMs."
                },
                "authors": [
                    {
                        "name": "Mingchen Tu"
                    },
                    {
                        "name": "Zhiqiang Liu"
                    },
                    {
                        "name": "Juan Li"
                    },
                    {
                        "name": "Liangyurui Liu"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Wen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhang"
                },
                "author": "Wen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23254v3",
                "updated": "2025-10-30T16:49:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    49,
                    52,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-29T09:00:35Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    9,
                    0,
                    35,
                    3,
                    149,
                    0
                ],
                "title": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning"
                },
                "summary": "Owing to the huge success of generative artificial intelligence (AI), large\nlanguage models (LLMs) have emerged as a core subclass, underpinning\napplications such as question answering, text generation, and code completion.\nWhile fine-tuning these models on domain-specific data can yield significant\nperformance gains, it also poses daunting computational challenges, especially\nfor researchers and small organizations with limited hardware resources.\nAlthough SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy\nto overcome the GPU memory barrier via leveraging both system memory (i.e., CPU\nDRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily\ntargets model-centric performance issues. As a result, key system-level issues,\nincluding system memory fragmentation, inefficient pinned buffer allocation,\npeak CPU usage spikes, and file system overhead, remain unaddressed, stifling\nscalability and inflating costs. Such an observation motivates this paper to\nintroduce MemAscend, a framework that systematically tackles the underexplored\nsystem memory bottlenecks in SSD-offloaded LLM training, with a focus on\nresource-constrained environments. By streamlining pinned-memory allocation,\neradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a\nsubstantial system memory budget, enabling larger models, longer context\nwindows, and higher batch sizes without exceeding modest hardware limits.\nAcross diverse LLM benchmarks, MemAscend reduces peak system-memory consumption\nby an average of 55.7% compared with standard SSD offloading techniques,\nlowering the hardware barrier for fine-tuning and unlocking new possibilities\nfor cost-effective large-scale training on limited-resource machines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Owing to the huge success of generative artificial intelligence (AI), large\nlanguage models (LLMs) have emerged as a core subclass, underpinning\napplications such as question answering, text generation, and code completion.\nWhile fine-tuning these models on domain-specific data can yield significant\nperformance gains, it also poses daunting computational challenges, especially\nfor researchers and small organizations with limited hardware resources.\nAlthough SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy\nto overcome the GPU memory barrier via leveraging both system memory (i.e., CPU\nDRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily\ntargets model-centric performance issues. As a result, key system-level issues,\nincluding system memory fragmentation, inefficient pinned buffer allocation,\npeak CPU usage spikes, and file system overhead, remain unaddressed, stifling\nscalability and inflating costs. Such an observation motivates this paper to\nintroduce MemAscend, a framework that systematically tackles the underexplored\nsystem memory bottlenecks in SSD-offloaded LLM training, with a focus on\nresource-constrained environments. By streamlining pinned-memory allocation,\neradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a\nsubstantial system memory budget, enabling larger models, longer context\nwindows, and higher batch sizes without exceeding modest hardware limits.\nAcross diverse LLM benchmarks, MemAscend reduces peak system-memory consumption\nby an average of 55.7% compared with standard SSD offloading techniques,\nlowering the hardware barrier for fine-tuning and unlocking new possibilities\nfor cost-effective large-scale training on limited-resource machines."
                },
                "authors": [
                    {
                        "name": "Yong-Cheng Liaw"
                    },
                    {
                        "name": "Shuo-Han Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shuo-Han Chen"
                },
                "author": "Shuo-Han Chen",
                "arxiv_comment": "16 pages, 21 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03305v2",
                "updated": "2025-10-30T16:45:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    45,
                    30,
                    3,
                    303,
                    0
                ],
                "published": "2025-07-04T05:24:01Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    5,
                    24,
                    1,
                    4,
                    185,
                    0
                ],
                "title": "Analysis and Optimized CXL-Attached Memory Allocation for Long-Context\n  LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Optimized CXL-Attached Memory Allocation for Long-Context\n  LLM Fine-Tuning"
                },
                "summary": "The substantial memory requirements of Large Language Models (LLMs),\nparticularly for long-context fine-tuning, have renewed interest in CPU\noffloading to augment limited GPU memory. However, as context lengths grow,\nrelying on CPU memory for intermediate states introduces a significant\nbottleneck that can exhaust the capacity of mainstream client platforms. To\naddress this limitation, this work investigates the effectiveness of Compute\nExpress Link (CXL) add-in card (AIC) memory as an extension to CPU memory,\nenabling larger model sizes and longer context lengths during fine-tuning.\nExtensive benchmarking reveals two critical challenges. First, current deep\nlearning frameworks such as PyTorch lack fine-grained, per-tensor control over\nNUMA memory allocation, exposing only coarse, process-level policies. Second,\ndue to this lack of control, when the memory footprint of fine-tuning is\noffloaded across local DRAM and CXL-attached memory, naively placing optimizer\ndata in higher-latency CXL leads to substantial slowdowns in the optimizer step\n(e.g., 4x once data exceeds 20M elements). To overcome these challenges, this\nwork introduces a PyTorch extension that enables tensor-level system memory\ncontrol and a CXL-aware memory allocator that pins latency-critical tensors in\nlocal DRAM while maximizing bandwidth by striping latency-tolerant tensors\nacross one or more CXL devices. Evaluated on a real hardware setup with 7B and\n12B models, 4K-32K contexts, and a single GPU, our approach recovers throughput\nto 97-99% of DRAM-only with a single AIC and approximately 100% with two AICs,\ndelivering up to 21% improvement over naive interleaving while preserving\nDRAM-like DMA bandwidth for GPU transfers. These results show that carefully\nmanaged CXL-attached memory is a practical path to scaling long-context\nfine-tuning beyond DRAM limits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The substantial memory requirements of Large Language Models (LLMs),\nparticularly for long-context fine-tuning, have renewed interest in CPU\noffloading to augment limited GPU memory. However, as context lengths grow,\nrelying on CPU memory for intermediate states introduces a significant\nbottleneck that can exhaust the capacity of mainstream client platforms. To\naddress this limitation, this work investigates the effectiveness of Compute\nExpress Link (CXL) add-in card (AIC) memory as an extension to CPU memory,\nenabling larger model sizes and longer context lengths during fine-tuning.\nExtensive benchmarking reveals two critical challenges. First, current deep\nlearning frameworks such as PyTorch lack fine-grained, per-tensor control over\nNUMA memory allocation, exposing only coarse, process-level policies. Second,\ndue to this lack of control, when the memory footprint of fine-tuning is\noffloaded across local DRAM and CXL-attached memory, naively placing optimizer\ndata in higher-latency CXL leads to substantial slowdowns in the optimizer step\n(e.g., 4x once data exceeds 20M elements). To overcome these challenges, this\nwork introduces a PyTorch extension that enables tensor-level system memory\ncontrol and a CXL-aware memory allocator that pins latency-critical tensors in\nlocal DRAM while maximizing bandwidth by striping latency-tolerant tensors\nacross one or more CXL devices. Evaluated on a real hardware setup with 7B and\n12B models, 4K-32K contexts, and a single GPU, our approach recovers throughput\nto 97-99% of DRAM-only with a single AIC and approximately 100% with two AICs,\ndelivering up to 21% improvement over naive interleaving while preserving\nDRAM-like DMA bandwidth for GPU transfers. These results show that carefully\nmanaged CXL-attached memory is a practical path to scaling long-context\nfine-tuning beyond DRAM limits."
                },
                "authors": [
                    {
                        "name": "Yong-Cheng Liaw"
                    },
                    {
                        "name": "Shuo-Han Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shuo-Han Chen"
                },
                "author": "Shuo-Han Chen",
                "arxiv_comment": "13 pages, 15 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v2",
                "updated": "2025-10-30T16:38:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    38,
                    19,
                    3,
                    303,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame\n  Vision-Language-Action Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame\n  Vision-Language-Action Modeling"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong performance in robotic\nmanipulation. However, these models remain constrained by the single-frame\nimage paradigm and fail to fully leverage the temporal information offered by\nmulti-frame histories, as directly feeding multiple frames into VLM backbones\nincurs substantial computational overhead and inference latency. We propose\nCronusVLA, a unified framework that extends single-frame VLA models to the\nmulti-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame\npretraining on large-scale embodied datasets with autoregressive prediction of\naction tokens, establishing an effective embodied vision-language foundation;\n(2) Multi-frame post-training, which adapts the prediction of the\nvision-language backbone from discrete tokens to learnable features, and\naggregates historical information via feature chunking. CronusVLA effectively\naddresses the existing challenges of multi-frame modeling while enhancing\nperformance and observational robustness. To evaluate the robustness under\ntemporal and spatial disturbances, we introduce SimplerEnv-OR, a novel\nbenchmark featuring 24 types of observational disturbances and 120 severity\nlevels. Experiments across three embodiments in simulated and real-world\nenvironments demonstrate that CronusVLA achieves leading performance and\nsuperior robustness, with a 70.9% success rate on SimplerEnv, a 26.8%\nimprovement over OpenVLA on LIBERO, and the highest robustness score on\nSimplerEnv-OR. These results highlight the potential of efficient multi-frame\nadaptation in VLA models for more powerful and robust real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong performance in robotic\nmanipulation. However, these models remain constrained by the single-frame\nimage paradigm and fail to fully leverage the temporal information offered by\nmulti-frame histories, as directly feeding multiple frames into VLM backbones\nincurs substantial computational overhead and inference latency. We propose\nCronusVLA, a unified framework that extends single-frame VLA models to the\nmulti-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame\npretraining on large-scale embodied datasets with autoregressive prediction of\naction tokens, establishing an effective embodied vision-language foundation;\n(2) Multi-frame post-training, which adapts the prediction of the\nvision-language backbone from discrete tokens to learnable features, and\naggregates historical information via feature chunking. CronusVLA effectively\naddresses the existing challenges of multi-frame modeling while enhancing\nperformance and observational robustness. To evaluate the robustness under\ntemporal and spatial disturbances, we introduce SimplerEnv-OR, a novel\nbenchmark featuring 24 types of observational disturbances and 120 severity\nlevels. Experiments across three embodiments in simulated and real-world\nenvironments demonstrate that CronusVLA achieves leading performance and\nsuperior robustness, with a 70.9% success rate on SimplerEnv, a 26.8%\nimprovement over OpenVLA on LIBERO, and the highest robustness score on\nSimplerEnv-OR. These results highlight the potential of efficient multi-frame\nadaptation in VLA models for more powerful and robust real-world deployment."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "39 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01543v2",
                "updated": "2025-10-30T16:32:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    32,
                    34,
                    3,
                    303,
                    0
                ],
                "published": "2025-08-03T01:56:03Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    1,
                    56,
                    3,
                    6,
                    215,
                    0
                ],
                "title": "Refine-n-Judge: Curating High-Quality Preference Chains for\n  LLM-Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refine-n-Judge: Curating High-Quality Preference Chains for\n  LLM-Fine-Tuning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable progress through\npreference-based fine-tuning, which critically depends on the quality of the\nunderlying training data. While human feedback is essential for improving data\nquality, it is costly and does not scale well. In this paper, we introduce\nRefine-n-Judge, an automated iterative approach that leverages a single LLM as\nboth a refiner and a judge to enhance dataset quality. Unlike existing\niterative refinement methods, Refine-n-Judge employs an LLM to both generate\nrefinements and explicitly evaluate each improvement, ensuring that every\niteration meaningfully enhances the dataset without requiring additional human\nannotation or a separate reward model. At each step, the LLM refines a response\nand judges whether the refinement is an improvement over the previous answer.\nThis process continues until the LLM prefers the initial answer over the\nrefinement, indicating no further improvements. This produces sequences of\nincreasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public\ndatasets spanning five corpora, targeting tasks such as coding, math, and\nconversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on\nRefine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of\ncomparisons against models tuned on the original dataset by GPT-4.\nAdditionally, we report performance gains: +5% on AlpacaEval and AlpacaEval\n2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces\nhigh-quality datasets and scalable model improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable progress through\npreference-based fine-tuning, which critically depends on the quality of the\nunderlying training data. While human feedback is essential for improving data\nquality, it is costly and does not scale well. In this paper, we introduce\nRefine-n-Judge, an automated iterative approach that leverages a single LLM as\nboth a refiner and a judge to enhance dataset quality. Unlike existing\niterative refinement methods, Refine-n-Judge employs an LLM to both generate\nrefinements and explicitly evaluate each improvement, ensuring that every\niteration meaningfully enhances the dataset without requiring additional human\nannotation or a separate reward model. At each step, the LLM refines a response\nand judges whether the refinement is an improvement over the previous answer.\nThis process continues until the LLM prefers the initial answer over the\nrefinement, indicating no further improvements. This produces sequences of\nincreasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public\ndatasets spanning five corpora, targeting tasks such as coding, math, and\nconversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on\nRefine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of\ncomparisons against models tuned on the original dataset by GPT-4.\nAdditionally, we report performance gains: +5% on AlpacaEval and AlpacaEval\n2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces\nhigh-quality datasets and scalable model improvements."
                },
                "authors": [
                    {
                        "name": "Derin Cayir"
                    },
                    {
                        "name": "Renjie Tao"
                    },
                    {
                        "name": "Rashi Rungta"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Sean Chen"
                    },
                    {
                        "name": "Haidar Khan"
                    },
                    {
                        "name": "Minseok Kim"
                    },
                    {
                        "name": "Julia Reinspach"
                    },
                    {
                        "name": "Yue Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yue Liu"
                },
                "author": "Yue Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21257v2",
                "updated": "2025-10-30T16:25:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    25,
                    15,
                    3,
                    303,
                    0
                ],
                "published": "2025-07-28T18:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    18,
                    20,
                    41,
                    0,
                    209,
                    0
                ],
                "title": "CompoST: A Benchmark for Analyzing the Ability of LLMs To\n  Compositionally Interpret Questions in a QALD Setting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompoST: A Benchmark for Analyzing the Ability of LLMs To\n  Compositionally Interpret Questions in a QALD Setting"
                },
                "summary": "Language interpretation is a compositional process, in which the meaning of\nmore complex linguistic structures is inferred from the meaning of their parts.\nLarge language models possess remarkable language interpretation capabilities\nand have been successfully applied to interpret questions by mapping them to\nSPARQL queries. An open question is how systematic this interpretation process\nis. Toward this question, in this paper, we propose a benchmark for\ninvestigating to what extent the abilities of LLMs to interpret questions are\nactually compositional. For this, we generate three datasets of varying\ndifficulty based on graph patterns in DBpedia, relying on Lemon lexica for\nverbalization. Our datasets are created in a very controlled fashion in order\nto test the ability of LLMs to interpret structurally complex questions, given\nthat they have seen the atomic building blocks. This allows us to evaluate to\nwhat degree LLMs are able to interpret complex questions for which they\n\"understand\" the atomic parts. We conduct experiments with models of different\nsizes using both various prompt and few-shot optimization techniques as well as\nfine-tuning. Our results show that performance in terms of macro $F_1$ degrades\nfrom $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the\nsamples optimized on. Even when all necessary information was provided to the\nmodel in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of\nlowest complexity. We thus conclude that LLMs struggle to systematically and\ncompositionally interpret questions and map them into SPARQL queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language interpretation is a compositional process, in which the meaning of\nmore complex linguistic structures is inferred from the meaning of their parts.\nLarge language models possess remarkable language interpretation capabilities\nand have been successfully applied to interpret questions by mapping them to\nSPARQL queries. An open question is how systematic this interpretation process\nis. Toward this question, in this paper, we propose a benchmark for\ninvestigating to what extent the abilities of LLMs to interpret questions are\nactually compositional. For this, we generate three datasets of varying\ndifficulty based on graph patterns in DBpedia, relying on Lemon lexica for\nverbalization. Our datasets are created in a very controlled fashion in order\nto test the ability of LLMs to interpret structurally complex questions, given\nthat they have seen the atomic building blocks. This allows us to evaluate to\nwhat degree LLMs are able to interpret complex questions for which they\n\"understand\" the atomic parts. We conduct experiments with models of different\nsizes using both various prompt and few-shot optimization techniques as well as\nfine-tuning. Our results show that performance in terms of macro $F_1$ degrades\nfrom $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the\nsamples optimized on. Even when all necessary information was provided to the\nmodel in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of\nlowest complexity. We thus conclude that LLMs struggle to systematically and\ncompositionally interpret questions and map them into SPARQL queries."
                },
                "authors": [
                    {
                        "name": "David Maria Schmidt"
                    },
                    {
                        "name": "Raoul Schubert"
                    },
                    {
                        "name": "Philipp Cimiano"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Cimiano"
                },
                "author": "Philipp Cimiano",
                "arxiv_doi": "10.1007/978-3-032-09527-5_1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-09527-5_1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Research Track, 24th International Semantic Web Conference (ISWC\n  2025), November 2-6, 2025, Nara, Japan",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26656v1",
                "updated": "2025-10-30T16:23:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    23,
                    46,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:23:46Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    23,
                    46,
                    3,
                    303,
                    0
                ],
                "title": "Heuristic Adaptation of Potentially Misspecified Domain Support for\n  Likelihood-Free Inference in Stochastic Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristic Adaptation of Potentially Misspecified Domain Support for\n  Likelihood-Free Inference in Stochastic Dynamical Systems"
                },
                "summary": "In robotics, likelihood-free inference (LFI) can provide the domain\ndistribution that adapts a learnt agent in a parametric set of deployment\nconditions. LFI assumes an arbitrary support for sampling, which remains\nconstant as the initial generic prior is iteratively refined to more\ndescriptive posteriors. However, a potentially misspecified support can lead to\nsuboptimal, yet falsely certain, posteriors. To address this issue, we propose\nthree heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the\nposterior mode shift over inference steps in its own way and, when integrated\ninto an LFI step, adapts the support alongside posterior inference. We first\nexpose the support misspecification issue and evaluate our heuristics using\nstochastic dynamical benchmarks. We then evaluate the impact of heuristic\nsupport adaptation on parameter inference and policy learning for a dynamic\ndeformable linear object (DLO) manipulation task. Inference results in a finer\nlength and stiffness classification for a parametric set of DLOs. When the\nresulting posteriors are used as domain distributions for sim-based policy\nlearning, they lead to more robust object-centric agent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In robotics, likelihood-free inference (LFI) can provide the domain\ndistribution that adapts a learnt agent in a parametric set of deployment\nconditions. LFI assumes an arbitrary support for sampling, which remains\nconstant as the initial generic prior is iteratively refined to more\ndescriptive posteriors. However, a potentially misspecified support can lead to\nsuboptimal, yet falsely certain, posteriors. To address this issue, we propose\nthree heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the\nposterior mode shift over inference steps in its own way and, when integrated\ninto an LFI step, adapts the support alongside posterior inference. We first\nexpose the support misspecification issue and evaluate our heuristics using\nstochastic dynamical benchmarks. We then evaluate the impact of heuristic\nsupport adaptation on parameter inference and policy learning for a dynamic\ndeformable linear object (DLO) manipulation task. Inference results in a finer\nlength and stiffness classification for a parametric set of DLOs. When the\nresulting posteriors are used as domain distributions for sim-based policy\nlearning, they lead to more robust object-centric agent performance."
                },
                "authors": [
                    {
                        "name": "Georgios Kamaras"
                    },
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.04999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.04999v2",
                "updated": "2025-10-30T16:21:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    21,
                    46,
                    3,
                    303,
                    0
                ],
                "published": "2022-08-09T18:58:11Z",
                "published_parsed": [
                    2022,
                    8,
                    9,
                    18,
                    58,
                    11,
                    1,
                    221,
                    0
                ],
                "title": "Measuring the Availability and Response Times of Public Encrypted DNS\n  Resolvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring the Availability and Response Times of Public Encrypted DNS\n  Resolvers"
                },
                "summary": "Unencrypted DNS traffic between users and DNS resolvers can lead to privacy\nand security concerns. In response to these privacy risks, many browser vendors\nhave deployed DNS-over-HTTPS (DoH) to encrypt queries between users and DNS\nresolvers. Today, many client-side deployments of DoH, particularly in\nbrowsers, select between only a few resolvers, despite the fact that many more\nencrypted DNS resolvers are deployed in practice. Unfortunately, if users only\nhave a few choices of encrypted resolver, and only a few perform well from any\nparticular vantage point, then the privacy problems that DoH was deployed to\nhelp address merely shift to a different set of third parties. It is thus\nimportant to assess the performance characteristics of more encrypted DNS\nresolvers, to determine how many options for encrypted DNS resolvers users tend\nto have in practice. In this paper, we explore the performance of a large group\nof encrypted DNS resolvers supporting DoH by measuring DNS query response times\nfrom global vantage points in North America, Europe, and Asia. Our results show\nthat many non-mainstream resolvers have higher response times than mainstream\nresolvers, particularly for non-mainstream resolvers that are queried from more\ndistant vantage points -- suggesting that most encrypted DNS resolvers are not\nreplicated or anycast. In some cases, however, certain non-mainstream resolvers\nperform at least as well as mainstream resolvers, suggesting that users may be\nable to use a broader set of encrypted DNS resolvers than those that are\navailable in current browser configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unencrypted DNS traffic between users and DNS resolvers can lead to privacy\nand security concerns. In response to these privacy risks, many browser vendors\nhave deployed DNS-over-HTTPS (DoH) to encrypt queries between users and DNS\nresolvers. Today, many client-side deployments of DoH, particularly in\nbrowsers, select between only a few resolvers, despite the fact that many more\nencrypted DNS resolvers are deployed in practice. Unfortunately, if users only\nhave a few choices of encrypted resolver, and only a few perform well from any\nparticular vantage point, then the privacy problems that DoH was deployed to\nhelp address merely shift to a different set of third parties. It is thus\nimportant to assess the performance characteristics of more encrypted DNS\nresolvers, to determine how many options for encrypted DNS resolvers users tend\nto have in practice. In this paper, we explore the performance of a large group\nof encrypted DNS resolvers supporting DoH by measuring DNS query response times\nfrom global vantage points in North America, Europe, and Asia. Our results show\nthat many non-mainstream resolvers have higher response times than mainstream\nresolvers, particularly for non-mainstream resolvers that are queried from more\ndistant vantage points -- suggesting that most encrypted DNS resolvers are not\nreplicated or anycast. In some cases, however, certain non-mainstream resolvers\nperform at least as well as mainstream resolvers, suggesting that users may be\nable to use a broader set of encrypted DNS resolvers than those that are\navailable in current browser configurations."
                },
                "authors": [
                    {
                        "name": "Ranya Sharma"
                    },
                    {
                        "name": "Nick Feamster"
                    }
                ],
                "author_detail": {
                    "name": "Nick Feamster"
                },
                "author": "Nick Feamster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.04999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.04999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.2; C.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26641v1",
                "updated": "2025-10-30T16:08:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    8,
                    25,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:08:25Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    8,
                    25,
                    3,
                    303,
                    0
                ],
                "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to\n  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All You Need for Object Detection: From Pixels, Points, and Prompts to\n  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles"
                },
                "summary": "Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities."
                },
                "authors": [
                    {
                        "name": "Sayed Pedram Haeri Boroujeni"
                    },
                    {
                        "name": "Niloufar Mehrabi"
                    },
                    {
                        "name": "Hazim Alzorgan"
                    },
                    {
                        "name": "Ahmad Sarlak"
                    },
                    {
                        "name": "Mahlagha Fazeli"
                    },
                    {
                        "name": "Abolfazl Razi"
                    }
                ],
                "author_detail": {
                    "name": "Abolfazl Razi"
                },
                "author": "Abolfazl Razi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26634v1",
                "updated": "2025-10-30T16:03:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    3,
                    56,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:03:56Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    3,
                    56,
                    3,
                    303,
                    0
                ],
                "title": "Stitch: Step-by-step LLM Guided Tutoring for Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stitch: Step-by-step LLM Guided Tutoring for Scratch"
                },
                "summary": "Block-based environments such as Scratch are increasingly popular in\nprogramming education. While block syntax reduces surface errors, semantic bugs\nremain common and challenging for novices to resolve. Existing debugging\nworkflows typically show the correct program directly to learners, a strategy\nthat may fix errors but undermines the development of problem-solving skills.\n  We present Stitch, an interactive tutoring system that replaces \"showing the\nanswer\" with step-by-step scaffolding. The system's Diff-Analyze module\ncontrasts a student's project with a reference implementation, identifies the\nmost critical differences, and uses a large language model to explain why these\nchanges matter. Learners inspect highlighted blocks through a custom rendering\nengine, understand the explanations, and selectively apply partial fixes. This\niterative process continues until the intended functionality is achieved.\n  We evaluate Stitch in an empirical study, comparing it against a\nstate-of-the-art automated feedback generation tool for Scratch. Our key\ninsight is that simply presenting the correct program is pedagogically\nineffective. In contrast, our interactive, step-by-step guided system promotes\na more effective learning experience. More broadly, what constitutes effective\nfeedback in block-based programming remains an open question. Our evaluation\nprovides new evidence that step-by-step tutoring significantly enhances\nlearning outcomes, outperforming both direct-answer approaches and current\nautomated feedback generation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-based environments such as Scratch are increasingly popular in\nprogramming education. While block syntax reduces surface errors, semantic bugs\nremain common and challenging for novices to resolve. Existing debugging\nworkflows typically show the correct program directly to learners, a strategy\nthat may fix errors but undermines the development of problem-solving skills.\n  We present Stitch, an interactive tutoring system that replaces \"showing the\nanswer\" with step-by-step scaffolding. The system's Diff-Analyze module\ncontrasts a student's project with a reference implementation, identifies the\nmost critical differences, and uses a large language model to explain why these\nchanges matter. Learners inspect highlighted blocks through a custom rendering\nengine, understand the explanations, and selectively apply partial fixes. This\niterative process continues until the intended functionality is achieved.\n  We evaluate Stitch in an empirical study, comparing it against a\nstate-of-the-art automated feedback generation tool for Scratch. Our key\ninsight is that simply presenting the correct program is pedagogically\nineffective. In contrast, our interactive, step-by-step guided system promotes\na more effective learning experience. More broadly, what constitutes effective\nfeedback in block-based programming remains an open question. Our evaluation\nprovides new evidence that step-by-step tutoring significantly enhances\nlearning outcomes, outperforming both direct-answer approaches and current\nautomated feedback generation tools."
                },
                "authors": [
                    {
                        "name": "Yuan Si"
                    },
                    {
                        "name": "Kyle Qi"
                    },
                    {
                        "name": "Daming Li"
                    },
                    {
                        "name": "Hanyuan Shi"
                    },
                    {
                        "name": "Jialu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jialu Zhang"
                },
                "author": "Jialu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23611v2",
                "updated": "2025-10-30T16:03:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    3,
                    19,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-29T16:19:12Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    16,
                    19,
                    12,
                    3,
                    149,
                    0
                ],
                "title": "Optimizing Flexible Complex Systems with Coupled and Co-Evolving\n  Subsystems under Operational Uncertainties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Flexible Complex Systems with Coupled and Co-Evolving\n  Subsystems under Operational Uncertainties"
                },
                "summary": "The paper develops a novel design optimization framework and associated\ncomputational techniques for staged deployment optimization of complex systems\nunder operational uncertainties. It proposes a local scenario discretization\nmethod that offers a computationally efficient approach to optimize staged\nco-deployment of multiple coupled subsystems by decoupling weak dynamic\ninteraction among subsystems. The proposed method is applied to case studies\nand is demonstrated to provide an effective and scalable strategy to determine\nthe optimal and flexible systems design under uncertainty. The developed\noptimization framework is expected to improve the staged deployment design of\nvarious complex engineering systems, such as water, energy, food, and other\ninfrastructure systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper develops a novel design optimization framework and associated\ncomputational techniques for staged deployment optimization of complex systems\nunder operational uncertainties. It proposes a local scenario discretization\nmethod that offers a computationally efficient approach to optimize staged\nco-deployment of multiple coupled subsystems by decoupling weak dynamic\ninteraction among subsystems. The proposed method is applied to case studies\nand is demonstrated to provide an effective and scalable strategy to determine\nthe optimal and flexible systems design under uncertainty. The developed\noptimization framework is expected to improve the staged deployment design of\nvarious complex engineering systems, such as water, energy, food, and other\ninfrastructure systems."
                },
                "authors": [
                    {
                        "name": "Koki Ho"
                    },
                    {
                        "name": "Masafumi Isaji"
                    },
                    {
                        "name": "Malav Patel"
                    },
                    {
                        "name": "Kayla Garoust"
                    }
                ],
                "author_detail": {
                    "name": "Kayla Garoust"
                },
                "author": "Kayla Garoust",
                "arxiv_comment": "18 pages; Under Review by Systems Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26622v1",
                "updated": "2025-10-30T15:48:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    48,
                    28,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:48:28Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    48,
                    28,
                    3,
                    303,
                    0
                ],
                "title": "Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large\n  Language Model"
                },
                "summary": "Recent large language model (LLM) research has undergone an architectural\nshift from encoder-decoder modeling to nowadays the dominant decoder-only\nmodeling. This rapid transition, however, comes without a rigorous comparative\nanalysis especially \\textit{from the scaling perspective}, raising concerns\nthat the potential of encoder-decoder models may have been overlooked. To fill\nthis gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent\nrecipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison\nbetween RedLLM, pretrained with prefix language modeling (LM), and DecLLM,\npretrained with causal LM, at different model scales, ranging from $\\sim$150M\nto $\\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for\ninstruction tuning, our experiments show that RedLLM produces compelling\nscaling properties and surprisingly strong performance. While DecLLM is overall\nmore compute-optimal during pretraining, RedLLM demonstrates comparable scaling\nand context length extrapolation capabilities. After instruction tuning, RedLLM\nachieves comparable and even better results on various downstream tasks while\nenjoying substantially better inference efficiency. We hope our findings could\ninspire more efforts on re-examining RedLLM, unlocking its potential for\ndeveloping powerful and efficient LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model (LLM) research has undergone an architectural\nshift from encoder-decoder modeling to nowadays the dominant decoder-only\nmodeling. This rapid transition, however, comes without a rigorous comparative\nanalysis especially \\textit{from the scaling perspective}, raising concerns\nthat the potential of encoder-decoder models may have been overlooked. To fill\nthis gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent\nrecipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison\nbetween RedLLM, pretrained with prefix language modeling (LM), and DecLLM,\npretrained with causal LM, at different model scales, ranging from $\\sim$150M\nto $\\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for\ninstruction tuning, our experiments show that RedLLM produces compelling\nscaling properties and surprisingly strong performance. While DecLLM is overall\nmore compute-optimal during pretraining, RedLLM demonstrates comparable scaling\nand context length extrapolation capabilities. After instruction tuning, RedLLM\nachieves comparable and even better results on various downstream tasks while\nenjoying substantially better inference efficiency. We hope our findings could\ninspire more efforts on re-examining RedLLM, unlocking its potential for\ndeveloping powerful and efficient LLMs."
                },
                "authors": [
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Yong Cheng"
                    },
                    {
                        "name": "Siamak Shakeri"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Orhan Firat"
                    }
                ],
                "author_detail": {
                    "name": "Orhan Firat"
                },
                "author": "Orhan Firat",
                "arxiv_comment": "The scaling study inspiring T5Gemma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17773v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17773v3",
                "updated": "2025-10-30T15:43:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    43,
                    22,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-23T11:44:02Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    11,
                    44,
                    2,
                    4,
                    143,
                    0
                ],
                "title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in\n  Large Language Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (C-LoRA) as a novel\nuncertainty-aware and parameter efficient fine-tuning approach, by developing\nnew lightweight LoRA modules contextualized to each input data sample to\ndynamically adapt uncertainty estimates. Incorporating data-driven contexts\ninto the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments on LLaMA2-7B models demonstrate that C-LoRA consistently\noutperforms the state-of-the-art uncertainty-aware LoRA methods in both\nuncertainty quantification and model generalization. Ablation studies further\nconfirm the critical role of our contextual modules in capturing\nsample-specific uncertainties. C-LoRA sets a new standard for robust,\nuncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments\nare limited to 7B models, our method is architecture-agnostic and, in\nprinciple, applies beyond this scale; studying its scaling to larger models\nremains an open problem. Our code is available at\nhttps://github.com/ahra99/c_lora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (C-LoRA) as a novel\nuncertainty-aware and parameter efficient fine-tuning approach, by developing\nnew lightweight LoRA modules contextualized to each input data sample to\ndynamically adapt uncertainty estimates. Incorporating data-driven contexts\ninto the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments on LLaMA2-7B models demonstrate that C-LoRA consistently\noutperforms the state-of-the-art uncertainty-aware LoRA methods in both\nuncertainty quantification and model generalization. Ablation studies further\nconfirm the critical role of our contextual modules in capturing\nsample-specific uncertainties. C-LoRA sets a new standard for robust,\nuncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments\nare limited to 7B models, our method is architecture-agnostic and, in\nprinciple, applies beyond this scale; studying its scaling to larger models\nremains an open problem. Our code is available at\nhttps://github.com/ahra99/c_lora."
                },
                "authors": [
                    {
                        "name": "Amir Hossein Rahmati"
                    },
                    {
                        "name": "Sanket Jantre"
                    },
                    {
                        "name": "Weifeng Zhang"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Byung-Jun Yoon"
                    },
                    {
                        "name": "Nathan M. Urban"
                    },
                    {
                        "name": "Xiaoning Qian"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoning Qian"
                },
                "author": "Xiaoning Qian",
                "arxiv_comment": "Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17773v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26615v1",
                "updated": "2025-10-30T15:41:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    41,
                    15,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:41:15Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    41,
                    15,
                    3,
                    303,
                    0
                ],
                "title": "SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual\n  Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual\n  Document Understanding"
                },
                "summary": "Multi-page visual documents such as manuals, brochures, presentations, and\nposters convey key information through layout, colors, icons, and cross-slide\nreferences. While large language models (LLMs) offer opportunities in document\nunderstanding, current systems struggle with complex, multi-page visual\ndocuments, particularly in fine-grained reasoning over elements and pages. We\nintroduce SlideAgent, a versatile agentic framework for understanding\nmulti-modal, multi-page, and multi-layout documents, especially slide decks.\nSlideAgent employs specialized agents and decomposes reasoning into three\nspecialized levels-global, page, and element-to construct a structured,\nquery-agnostic representation that captures both overarching themes and\ndetailed visual or textual cues. During inference, SlideAgent selectively\nactivates specialized agents for multi-level reasoning and integrates their\noutputs into coherent, context-aware answers. Extensive experiments show that\nSlideAgent achieves significant improvement over both proprietary (+7.9\noverall) and open-source models (+9.8 overall).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-page visual documents such as manuals, brochures, presentations, and\nposters convey key information through layout, colors, icons, and cross-slide\nreferences. While large language models (LLMs) offer opportunities in document\nunderstanding, current systems struggle with complex, multi-page visual\ndocuments, particularly in fine-grained reasoning over elements and pages. We\nintroduce SlideAgent, a versatile agentic framework for understanding\nmulti-modal, multi-page, and multi-layout documents, especially slide decks.\nSlideAgent employs specialized agents and decomposes reasoning into three\nspecialized levels-global, page, and element-to construct a structured,\nquery-agnostic representation that captures both overarching themes and\ndetailed visual or textual cues. During inference, SlideAgent selectively\nactivates specialized agents for multi-level reasoning and integrates their\noutputs into coherent, context-aware answers. Extensive experiments show that\nSlideAgent achieves significant improvement over both proprietary (+7.9\noverall) and open-source models (+9.8 overall)."
                },
                "authors": [
                    {
                        "name": "Yiqiao Jin"
                    },
                    {
                        "name": "Rachneet Kaur"
                    },
                    {
                        "name": "Zhen Zeng"
                    },
                    {
                        "name": "Sumitra Ganesh"
                    },
                    {
                        "name": "Srijan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Srijan Kumar"
                },
                "author": "Srijan Kumar",
                "arxiv_comment": "https://slideagent.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26606v1",
                "updated": "2025-10-30T15:35:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    35,
                    13,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:35:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    35,
                    13,
                    3,
                    303,
                    0
                ],
                "title": "Normative Reasoning in Large Language Models: A Comparative Benchmark\n  from Logical and Modal Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normative Reasoning in Large Language Models: A Comparative Benchmark\n  from Logical and Modal Perspectives"
                },
                "summary": "Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO."
                },
                "authors": [
                    {
                        "name": "Kentaro Ozeki"
                    },
                    {
                        "name": "Risako Ando"
                    },
                    {
                        "name": "Takanobu Morishita"
                    },
                    {
                        "name": "Hirohiko Abe"
                    },
                    {
                        "name": "Koji Mineshima"
                    },
                    {
                        "name": "Mitsuhiro Okada"
                    }
                ],
                "author_detail": {
                    "name": "Mitsuhiro Okada"
                },
                "author": "Mitsuhiro Okada",
                "arxiv_comment": "Accepted to the 8th BlackboxNLP Workshop at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26603v1",
                "updated": "2025-10-30T15:33:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    33,
                    52,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:33:52Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    33,
                    52,
                    3,
                    303,
                    0
                ],
                "title": "Agentic AI Home Energy Management System: A Large Language Model\n  Framework for Residential Load Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI Home Energy Management System: A Large Language Model\n  Framework for Residential Load Scheduling"
                },
                "summary": "The electricity sector transition requires substantial increases in\nresidential demand response capacity, yet Home Energy Management Systems (HEMS)\nadoption remains limited by user interaction barriers requiring translation of\neveryday preferences into technical parameters. While large language models\nhave been applied to energy systems as code generators and parameter\nextractors, no existing implementation deploys LLMs as autonomous coordinators\nmanaging the complete workflow from natural language input to multi-appliance\nscheduling. This paper presents an agentic AI HEMS where LLMs autonomously\ncoordinate multi-appliance scheduling from natural language requests to device\ncontrol, achieving optimal scheduling without example demonstrations. A\nhierarchical architecture combining one orchestrator with three specialist\nagents uses the ReAct pattern for iterative reasoning, enabling dynamic\ncoordination without hardcoded workflows while integrating Google Calendar for\ncontext-aware deadline extraction. Evaluation across three open-source models\nusing real Austrian day-ahead electricity prices reveals substantial capability\ndifferences. Llama-3.3-70B successfully coordinates all appliances across all\nscenarios to match cost-optimal benchmarks computed via mixed-integer linear\nprogramming, while other models achieve perfect single-appliance performance\nbut struggle to coordinate all appliances simultaneously. Progressive prompt\nengineering experiments demonstrate that analytical query handling without\nexplicit guidance remains unreliable despite models' general reasoning\ncapabilities. We open-source the complete system including orchestration logic,\nagent prompts, tools, and web interfaces to enable reproducibility, extension,\nand future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electricity sector transition requires substantial increases in\nresidential demand response capacity, yet Home Energy Management Systems (HEMS)\nadoption remains limited by user interaction barriers requiring translation of\neveryday preferences into technical parameters. While large language models\nhave been applied to energy systems as code generators and parameter\nextractors, no existing implementation deploys LLMs as autonomous coordinators\nmanaging the complete workflow from natural language input to multi-appliance\nscheduling. This paper presents an agentic AI HEMS where LLMs autonomously\ncoordinate multi-appliance scheduling from natural language requests to device\ncontrol, achieving optimal scheduling without example demonstrations. A\nhierarchical architecture combining one orchestrator with three specialist\nagents uses the ReAct pattern for iterative reasoning, enabling dynamic\ncoordination without hardcoded workflows while integrating Google Calendar for\ncontext-aware deadline extraction. Evaluation across three open-source models\nusing real Austrian day-ahead electricity prices reveals substantial capability\ndifferences. Llama-3.3-70B successfully coordinates all appliances across all\nscenarios to match cost-optimal benchmarks computed via mixed-integer linear\nprogramming, while other models achieve perfect single-appliance performance\nbut struggle to coordinate all appliances simultaneously. Progressive prompt\nengineering experiments demonstrate that analytical query handling without\nexplicit guidance remains unreliable despite models' general reasoning\ncapabilities. We open-source the complete system including orchestration logic,\nagent prompts, tools, and web interfaces to enable reproducibility, extension,\nand future research."
                },
                "authors": [
                    {
                        "name": "Reda El Makroum"
                    },
                    {
                        "name": "Sebastian Zwickl-Bernhard"
                    },
                    {
                        "name": "Lukas Kranzl"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Kranzl"
                },
                "author": "Lukas Kranzl",
                "arxiv_comment": "34 pages, 9 figures. Code available at\n  https://github.com/RedaElMakroum/agentic-ai-hems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17197v2",
                "updated": "2025-10-30T15:26:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    26,
                    13,
                    3,
                    303,
                    0
                ],
                "published": "2025-09-21T18:54:54Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    18,
                    54,
                    54,
                    6,
                    264,
                    0
                ],
                "title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal\n  Processing"
                },
                "summary": "Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern signal processing (SP) pipelines, whether model-based or data-driven,\noften constrained by complex and fragmented workflow, rely heavily on expert\nknowledge and manual engineering, and struggle with adaptability and\ngeneralization under limited data. In contrast, Large Language Models (LLMs)\noffer strong reasoning capabilities, broad general-purpose knowledge,\nin-context learning, and cross-modal transfer abilities, positioning them as\npowerful tools for automating and generalizing SP workflows. Motivated by these\npotentials, we introduce SignalLLM, the first general-purpose LLM-based agent\nframework for general SP tasks. Unlike prior LLM-based SP approaches that are\nlimited to narrow applications or tricky prompting, SignalLLM introduces a\nprincipled, modular architecture. It decomposes high-level SP goals into\nstructured subtasks via in-context learning and domain-specific retrieval,\nfollowed by hierarchical planning through adaptive retrieval-augmented\ngeneration (RAG) and refinement; these subtasks are then executed through\nprompt-based reasoning, cross-modal reasoning, code synthesis, model\ninvocation, or data-driven LLM-assisted modeling. Its generalizable design\nenables the flexible selection of problem solving strategies across different\nsignal modalities, task types, and data conditions. We demonstrate the\nversatility and effectiveness of SignalLLM through five representative tasks in\ncommunication and sensing, such as radar target detection, human activity\nrecognition, and text compression. Experimental results show superior\nperformance over traditional and existing LLM-based methods, particularly in\nfew-shot and zero-shot settings."
                },
                "authors": [
                    {
                        "name": "Junlong Ke"
                    },
                    {
                        "name": "Qiying Hu"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Yuecong Xu"
                    },
                    {
                        "name": "Jianfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Yang"
                },
                "author": "Jianfei Yang",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26588v1",
                "updated": "2025-10-30T15:14:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    14,
                    18,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:14:18Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    14,
                    18,
                    3,
                    303,
                    0
                ],
                "title": "FLYINGTRUST: A Benchmark for Quadrotor Navigation Across Scenarios and\n  Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLYINGTRUST: A Benchmark for Quadrotor Navigation Across Scenarios and\n  Vehicles"
                },
                "summary": "Visual navigation algorithms for quadrotors often exhibit a large variation\nin performance when transferred across different vehicle platforms and scene\ngeometries, which increases the cost and risk of field deployment. To support\nsystematic early-stage evaluation, we introduce FLYINGTRUST, a high-fidelity,\nconfigurable benchmarking framework that measures how platform kinodynamics and\nscenario structure jointly affect navigation robustness. FLYINGTRUST models\nvehicle capability with two compact, physically interpretable indicators:\nmaximum thrust-to-weight ratio and axis-wise maximum angular acceleration. The\nbenchmark pairs a diverse scenario library with a heterogeneous set of real and\nvirtual platforms and prescribes a standardized evaluation protocol together\nwith a composite scoring method that balances scenario importance, platform\nimportance and performance stability. We use FLYINGTRUST to compare\nrepresentative optimization-based and learning-based navigation approaches\nunder identical conditions, performing repeated trials per platform-scenario\ncombination and reporting uncertainty-aware metrics. The results reveal\nsystematic patterns: navigation success depends predictably on platform\ncapability and scene geometry, and different algorithms exhibit distinct\npreferences and failure modes across the evaluated conditions. These\nobservations highlight the practical necessity of incorporating both platform\ncapability and scenario structure into algorithm design, evaluation, and\nselection, and they motivate future work on methods that remain robust across\ndiverse platforms and scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual navigation algorithms for quadrotors often exhibit a large variation\nin performance when transferred across different vehicle platforms and scene\ngeometries, which increases the cost and risk of field deployment. To support\nsystematic early-stage evaluation, we introduce FLYINGTRUST, a high-fidelity,\nconfigurable benchmarking framework that measures how platform kinodynamics and\nscenario structure jointly affect navigation robustness. FLYINGTRUST models\nvehicle capability with two compact, physically interpretable indicators:\nmaximum thrust-to-weight ratio and axis-wise maximum angular acceleration. The\nbenchmark pairs a diverse scenario library with a heterogeneous set of real and\nvirtual platforms and prescribes a standardized evaluation protocol together\nwith a composite scoring method that balances scenario importance, platform\nimportance and performance stability. We use FLYINGTRUST to compare\nrepresentative optimization-based and learning-based navigation approaches\nunder identical conditions, performing repeated trials per platform-scenario\ncombination and reporting uncertainty-aware metrics. The results reveal\nsystematic patterns: navigation success depends predictably on platform\ncapability and scene geometry, and different algorithms exhibit distinct\npreferences and failure modes across the evaluated conditions. These\nobservations highlight the practical necessity of incorporating both platform\ncapability and scenario structure into algorithm design, evaluation, and\nselection, and they motivate future work on methods that remain robust across\ndiverse platforms and scenarios."
                },
                "authors": [
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Chunlei Zhai"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Shaun Li"
                    },
                    {
                        "name": "Shangsong Jiang"
                    },
                    {
                        "name": "Xiangwei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangwei Zhu"
                },
                "author": "Xiangwei Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26585v1",
                "updated": "2025-10-30T15:12:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    12,
                    59,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:12:59Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    12,
                    59,
                    3,
                    303,
                    0
                ],
                "title": "Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems"
                },
                "summary": "While Multi-Agent Systems (MAS) excel at complex tasks, their growing\nautonomy with operational complexity often leads to critical inefficiencies,\nsuch as excessive token consumption and failures arising from misinformation.\nExisting methods primarily focus on post-hoc failure attribution, lacking\nproactive, real-time interventions to enhance robustness and efficiency. To\nthis end, we introduce SupervisorAgent, a lightweight and modular framework for\nruntime, adaptive supervision that operates without altering the base agent's\narchitecture. Triggered by an LLM-free adaptive filter, SupervisorAgent\nintervenes at critical junctures to proactively correct errors, guide\ninefficient behaviors, and purify observations. On the challenging GAIA\nbenchmark, SupervisorAgent reduces the token consumption of the Smolagent\nframework by an average of 29.45% without compromising its success rate.\nExtensive experiments across five additional benchmarks (math reasoning, code\ngeneration, and question answering) and various SoTA foundation models validate\nthe broad applicability and robustness of our approach. The code is available\nat https://github.com/LINs-lab/SupervisorAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multi-Agent Systems (MAS) excel at complex tasks, their growing\nautonomy with operational complexity often leads to critical inefficiencies,\nsuch as excessive token consumption and failures arising from misinformation.\nExisting methods primarily focus on post-hoc failure attribution, lacking\nproactive, real-time interventions to enhance robustness and efficiency. To\nthis end, we introduce SupervisorAgent, a lightweight and modular framework for\nruntime, adaptive supervision that operates without altering the base agent's\narchitecture. Triggered by an LLM-free adaptive filter, SupervisorAgent\nintervenes at critical junctures to proactively correct errors, guide\ninefficient behaviors, and purify observations. On the challenging GAIA\nbenchmark, SupervisorAgent reduces the token consumption of the Smolagent\nframework by an average of 29.45% without compromising its success rate.\nExtensive experiments across five additional benchmarks (math reasoning, code\ngeneration, and question answering) and various SoTA foundation models validate\nthe broad applicability and robustness of our approach. The code is available\nat https://github.com/LINs-lab/SupervisorAgent."
                },
                "authors": [
                    {
                        "name": "Fulin Lin"
                    },
                    {
                        "name": "Shaowen Chen"
                    },
                    {
                        "name": "Ruishan Fang"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26582v1",
                "updated": "2025-10-30T15:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    10,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:10:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    10,
                    2,
                    3,
                    303,
                    0
                ],
                "title": "CATCH: A Modular Cross-domain Adaptive Template with Hook",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CATCH: A Modular Cross-domain Adaptive Template with Hook"
                },
                "summary": "Recent advances in Visual Question Answering (VQA) have demonstrated\nimpressive performance in natural image domains, with models like LLaVA\nleveraging large language models (LLMs) for open-ended reasoning. However,\ntheir generalization degrades significantly when transferred to out-of-domain\nscenarios such as remote sensing, medical imaging, or math diagrams, due to\nlarge distributional shifts and the lack of effective domain adaptation\nmechanisms. Existing approaches typically rely on per-domain fine-tuning or\nbespoke pipelines, which are costly, inflexible, and not scalable across\ndiverse tasks. In this paper, we propose CATCH, a plug-and-play framework for\ncross-domain adaptation that improves the generalization of VQA models while\nrequiring minimal changes to their core architecture. Our key idea is to\ndecouple visual and linguistic adaptation by introducing two lightweight\nmodules: a domain classifier to identify the input image type, and a dual\nadapter mechanism comprising a Prompt Adapter for language modulation and a\nVisual Adapter for vision feature adjustment. Both modules are dynamically\ninjected via a unified hook interface, requiring no retraining of the backbone\nmodel. Experimental results across four domain-specific VQA benchmarks\ndemonstrate that our framework achieves consistent performance gains without\nretraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on\nMedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH\nprovides a scalable and extensible approach to multi-domain VQA, enabling\npractical deployment across diverse application domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Visual Question Answering (VQA) have demonstrated\nimpressive performance in natural image domains, with models like LLaVA\nleveraging large language models (LLMs) for open-ended reasoning. However,\ntheir generalization degrades significantly when transferred to out-of-domain\nscenarios such as remote sensing, medical imaging, or math diagrams, due to\nlarge distributional shifts and the lack of effective domain adaptation\nmechanisms. Existing approaches typically rely on per-domain fine-tuning or\nbespoke pipelines, which are costly, inflexible, and not scalable across\ndiverse tasks. In this paper, we propose CATCH, a plug-and-play framework for\ncross-domain adaptation that improves the generalization of VQA models while\nrequiring minimal changes to their core architecture. Our key idea is to\ndecouple visual and linguistic adaptation by introducing two lightweight\nmodules: a domain classifier to identify the input image type, and a dual\nadapter mechanism comprising a Prompt Adapter for language modulation and a\nVisual Adapter for vision feature adjustment. Both modules are dynamically\ninjected via a unified hook interface, requiring no retraining of the backbone\nmodel. Experimental results across four domain-specific VQA benchmarks\ndemonstrate that our framework achieves consistent performance gains without\nretraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on\nMedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH\nprovides a scalable and extensible approach to multi-domain VQA, enabling\npractical deployment across diverse application domains."
                },
                "authors": [
                    {
                        "name": "Xinjin Li"
                    },
                    {
                        "name": "Yulie Lu"
                    },
                    {
                        "name": "Jinghan Cao"
                    },
                    {
                        "name": "Yu Ma"
                    },
                    {
                        "name": "Zhenglin Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yeyang Zhou"
                },
                "author": "Yeyang Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26580v1",
                "updated": "2025-10-30T15:07:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    7,
                    55,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:07:55Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    7,
                    55,
                    3,
                    303,
                    0
                ],
                "title": "Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in\n  Zero-Shot Real-World Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in\n  Zero-Shot Real-World Scenarios"
                },
                "summary": "In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings."
                },
                "authors": [
                    {
                        "name": "Manjunath Prasad Holenarasipura Rajiv"
                    },
                    {
                        "name": "B. M. Vidyavathi"
                    }
                ],
                "author_detail": {
                    "name": "B. M. Vidyavathi"
                },
                "author": "B. M. Vidyavathi",
                "arxiv_comment": "Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14409v2",
                "updated": "2025-10-30T15:05:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    5,
                    42,
                    3,
                    303,
                    0
                ],
                "published": "2025-02-20T09:57:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    9,
                    57,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unstructured Evidence Attribution for Long Context Query Focused\n  Summarization"
                },
                "summary": "Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query, and extracting and citing evidence\nspans helps improve the trustworthiness of these summaries. Whereas previous\nwork has focused on evidence citation with fixed levels of granularity (e.g.\nsentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,\nspans of any length) evidence in order to acquire more relevant and consistent\nevidence than in the fixed granularity case. We show how existing systems\nstruggle to copy and properly cite unstructured evidence, which also tends to\nbe \"lost-in-the-middle\". To help models perform this task, we create the\nSummaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset\ngenerated using a novel pipeline, which can be used as training supervision for\nunstructured evidence summarization. We demonstrate across 5 LLMs and 4\ndatasets spanning human written, synthetic, single, and multi-document settings\nthat LLMs adapted with SUnsET generate more relevant and factually consistent\nevidence with their summaries, extract evidence from more diverse locations in\ntheir context, and can generate more relevant and consistent summaries than\nbaselines with no fine-tuning and fixed granularity evidence. We release SUnsET\nand our generation code to the public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are capable of generating coherent summaries\nfrom very long contexts given a user query, and extracting and citing evidence\nspans helps improve the trustworthiness of these summaries. Whereas previous\nwork has focused on evidence citation with fixed levels of granularity (e.g.\nsentence, paragraph, document, etc.), we propose to extract unstructured (i.e.,\nspans of any length) evidence in order to acquire more relevant and consistent\nevidence than in the fixed granularity case. We show how existing systems\nstruggle to copy and properly cite unstructured evidence, which also tends to\nbe \"lost-in-the-middle\". To help models perform this task, we create the\nSummaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset\ngenerated using a novel pipeline, which can be used as training supervision for\nunstructured evidence summarization. We demonstrate across 5 LLMs and 4\ndatasets spanning human written, synthetic, single, and multi-document settings\nthat LLMs adapted with SUnsET generate more relevant and factually consistent\nevidence with their summaries, extract evidence from more diverse locations in\ntheir context, and can generate more relevant and consistent summaries than\nbaselines with no fine-tuning and fixed granularity evidence. We release SUnsET\nand our generation code to the public."
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Zain Muhammad Mujahid"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    },
                    {
                        "name": "David Jurgens"
                    }
                ],
                "author_detail": {
                    "name": "David Jurgens"
                },
                "author": "David Jurgens",
                "arxiv_comment": "EMNLP 2025 Main; 29 pages; 24 figures; 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26578v1",
                "updated": "2025-10-30T15:04:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    4,
                    45,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:04:45Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    4,
                    45,
                    3,
                    303,
                    0
                ],
                "title": "Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV\n  Networks"
                },
                "summary": "In post-disaster scenarios, the rapid deployment of adequate communication\ninfrastructure is essential to support disaster search, rescue, and recovery\noperations. To achieve this, uncrewed aerial vehicle (UAV) has emerged as a\npromising solution for emergency communication due to its low cost and\ndeployment flexibility. However, conventional untethered UAV (U-UAV) is\nconstrained by size, weight, and power (SWaP) limitations, making it incapable\nof maintaining the operation of a macro base station. To address this\nlimitation, we propose a heterogeneous UAV-based framework that integrates\ntethered UAV (T-UAV) and U-UAVs, where U-UAVs are utilized to enhance the\nthroughput of cell-edge ground user equipments (G-UEs) and guarantee seamless\nconnectivity during G-UEs' mobility to safe zones. It is noted that the\nintegrated access and backhaul (IAB) technique is adopted to support the\nwireless backhaul of U-UAVs. Accordingly, we formulate a two-timescale joint\nuser scheduling and trajectory control optimization problem, aiming to maximize\nthe downlink throughput under asymmetric traffic demands and G-UEs' mobility.\nTo solve the formulated problem, we proposed a two-timescale multi-agent deep\ndeterministic policy gradient (TTS-MADDPG) algorithm based on the centralized\ntraining and distributed execution paradigm. Numerical results show that the\nproposed algorithm outperforms other benchmarks, including the two-timescale\nmulti-agent proximal policy optimization (TTS-MAPPO) algorithm and MADDPG\nscheduling method, with robust and higher throughput. Specifically, the\nproposed algorithm obtains up to 12.2\\% average throughput gain compared to the\nMADDPG scheduling method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In post-disaster scenarios, the rapid deployment of adequate communication\ninfrastructure is essential to support disaster search, rescue, and recovery\noperations. To achieve this, uncrewed aerial vehicle (UAV) has emerged as a\npromising solution for emergency communication due to its low cost and\ndeployment flexibility. However, conventional untethered UAV (U-UAV) is\nconstrained by size, weight, and power (SWaP) limitations, making it incapable\nof maintaining the operation of a macro base station. To address this\nlimitation, we propose a heterogeneous UAV-based framework that integrates\ntethered UAV (T-UAV) and U-UAVs, where U-UAVs are utilized to enhance the\nthroughput of cell-edge ground user equipments (G-UEs) and guarantee seamless\nconnectivity during G-UEs' mobility to safe zones. It is noted that the\nintegrated access and backhaul (IAB) technique is adopted to support the\nwireless backhaul of U-UAVs. Accordingly, we formulate a two-timescale joint\nuser scheduling and trajectory control optimization problem, aiming to maximize\nthe downlink throughput under asymmetric traffic demands and G-UEs' mobility.\nTo solve the formulated problem, we proposed a two-timescale multi-agent deep\ndeterministic policy gradient (TTS-MADDPG) algorithm based on the centralized\ntraining and distributed execution paradigm. Numerical results show that the\nproposed algorithm outperforms other benchmarks, including the two-timescale\nmulti-agent proximal policy optimization (TTS-MAPPO) algorithm and MADDPG\nscheduling method, with robust and higher throughput. Specifically, the\nproposed algorithm obtains up to 12.2\\% average throughput gain compared to the\nMADDPG scheduling method."
                },
                "authors": [
                    {
                        "name": "Jikang Deng"
                    },
                    {
                        "name": "Hui Zhou"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26577v1",
                "updated": "2025-10-30T15:04:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    4,
                    36,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:04:36Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    4,
                    36,
                    3,
                    303,
                    0
                ],
                "title": "Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) face significant inference latency challenges\nstemming from their autoregressive design and large size. To address this,\nspeculative decoding emerges as a solution, enabling the simultaneous\ngeneration and validation of multiple tokens. While recent approaches like\nEAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,\nthey often neglect the impact of crucial system variables such as GPU devices\nand batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that\ntakes into account inference costs, including factors such as GPU\nconfigurations and batch sizes, to dynamically refine the tree structure.\nThrough comprehensive experimentation across six diverse tasks and utilizing\nsix distinct LLMs, our methodology demonstrates remarkable results, achieving\nspeeds up to 5.2 times faster than conventional decoding methods. Moreover, it\ngenerally outperforms existing state-of-the-art techniques from 5% to 20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant inference latency challenges\nstemming from their autoregressive design and large size. To address this,\nspeculative decoding emerges as a solution, enabling the simultaneous\ngeneration and validation of multiple tokens. While recent approaches like\nEAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures,\nthey often neglect the impact of crucial system variables such as GPU devices\nand batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that\ntakes into account inference costs, including factors such as GPU\nconfigurations and batch sizes, to dynamically refine the tree structure.\nThrough comprehensive experimentation across six diverse tasks and utilizing\nsix distinct LLMs, our methodology demonstrates remarkable results, achieving\nspeeds up to 5.2 times faster than conventional decoding methods. Moreover, it\ngenerally outperforms existing state-of-the-art techniques from 5% to 20%."
                },
                "authors": [
                    {
                        "name": "Yinrong Hong"
                    },
                    {
                        "name": "Zhiquan Tan"
                    },
                    {
                        "name": "Kai Hu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Hu"
                },
                "author": "Kai Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26575v1",
                "updated": "2025-10-30T15:03:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    3,
                    21,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:03:21Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    3,
                    21,
                    3,
                    303,
                    0
                ],
                "title": "InfoFlow: Reinforcing Search Agent Via Reward Density Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfoFlow: Reinforcing Search Agent Via Reward Density Optimization"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach\nfor enhancing agentic deep search. However, its application is often hindered\nby low \\textbf{Reward Density} in deep search scenarios, where agents expend\nsignificant exploratory costs for infrequent and often null final rewards. In\nthis paper, we formalize this challenge as the \\textbf{Reward Density\nOptimization} problem, which aims to improve the reward obtained per unit of\nexploration cost. This paper introduce \\textbf{InfoFlow}, a systematic\nframework that tackles this problem from three aspects. 1) \\textbf{Subproblem\ndecomposition}: breaking down long-range tasks to assign process rewards,\nthereby providing denser learning signals. 2) \\textbf{Failure-guided hints}:\ninjecting corrective guidance into stalled trajectories to increase the\nprobability of successful outcomes. 3) \\textbf{Dual-agent refinement}:\nemploying a dual-agent architecture to offload the cognitive burden of deep\nexploration. A refiner agent synthesizes the search history, which effectively\ncompresses the researcher's perceived trajectory, thereby reducing exploration\ncost and increasing the overall reward density. We evaluate InfoFlow on\nmultiple agentic search benchmarks, where it significantly outperforms strong\nbaselines, enabling lightweight LLMs to achieve performance comparable to\nadvanced proprietary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach\nfor enhancing agentic deep search. However, its application is often hindered\nby low \\textbf{Reward Density} in deep search scenarios, where agents expend\nsignificant exploratory costs for infrequent and often null final rewards. In\nthis paper, we formalize this challenge as the \\textbf{Reward Density\nOptimization} problem, which aims to improve the reward obtained per unit of\nexploration cost. This paper introduce \\textbf{InfoFlow}, a systematic\nframework that tackles this problem from three aspects. 1) \\textbf{Subproblem\ndecomposition}: breaking down long-range tasks to assign process rewards,\nthereby providing denser learning signals. 2) \\textbf{Failure-guided hints}:\ninjecting corrective guidance into stalled trajectories to increase the\nprobability of successful outcomes. 3) \\textbf{Dual-agent refinement}:\nemploying a dual-agent architecture to offload the cognitive burden of deep\nexploration. A refiner agent synthesizes the search history, which effectively\ncompresses the researcher's perceived trajectory, thereby reducing exploration\ncost and increasing the overall reward density. We evaluate InfoFlow on\nmultiple agentic search benchmarks, where it significantly outperforms strong\nbaselines, enabling lightweight LLMs to achieve performance comparable to\nadvanced proprietary LLMs."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Ziyi Xia"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Siqi Bao"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26573v1",
                "updated": "2025-10-30T15:00:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    0,
                    50,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T15:00:50Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    0,
                    50,
                    3,
                    303,
                    0
                ],
                "title": "Comparative Analysis of Deep Learning Models for Olive Tree Crown and\n  Shadow Segmentation Towards Biovolume Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Deep Learning Models for Olive Tree Crown and\n  Shadow Segmentation Towards Biovolume Estimation"
                },
                "summary": "Olive tree biovolume estimation is a key task in precision agriculture,\nsupporting yield prediction and resource management, especially in\nMediterranean regions severely impacted by climate-induced stress. This study\npresents a comparative analysis of three deep learning models U-Net,\nYOLOv11m-seg, and Mask RCNN for segmenting olive tree crowns and their shadows\nin ultra-high resolution UAV imagery. The UAV dataset, acquired over\nVicopisano, Italy, includes manually annotated crown and shadow masks. Building\non these annotations, the methodology emphasizes spatial feature extraction and\nrobust segmentation; per-tree biovolume is then estimated by combining crown\nprojected area with shadow-derived height using solar geometry. In testing,\nMask R-CNN achieved the best overall accuracy (F1 = 0.86; mIoU = 0.72), while\nYOLOv11m-seg provided the fastest throughput (0.12 second per image). The\nestimated biovolumes spanned from approximately 4 to 24 cubic meters,\nreflecting clear structural differences among trees. These results indicate\nMask R-CNN is preferable when biovolume accuracy is paramount, whereas\nYOLOv11m-seg suits large-area deployments where speed is critical; U-Net\nremains a lightweight, high-sensitivity option. The framework enables accurate,\nscalable orchard monitoring and can be further strengthened with DEM or DSM\nintegration and field calibration for operational decision support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Olive tree biovolume estimation is a key task in precision agriculture,\nsupporting yield prediction and resource management, especially in\nMediterranean regions severely impacted by climate-induced stress. This study\npresents a comparative analysis of three deep learning models U-Net,\nYOLOv11m-seg, and Mask RCNN for segmenting olive tree crowns and their shadows\nin ultra-high resolution UAV imagery. The UAV dataset, acquired over\nVicopisano, Italy, includes manually annotated crown and shadow masks. Building\non these annotations, the methodology emphasizes spatial feature extraction and\nrobust segmentation; per-tree biovolume is then estimated by combining crown\nprojected area with shadow-derived height using solar geometry. In testing,\nMask R-CNN achieved the best overall accuracy (F1 = 0.86; mIoU = 0.72), while\nYOLOv11m-seg provided the fastest throughput (0.12 second per image). The\nestimated biovolumes spanned from approximately 4 to 24 cubic meters,\nreflecting clear structural differences among trees. These results indicate\nMask R-CNN is preferable when biovolume accuracy is paramount, whereas\nYOLOv11m-seg suits large-area deployments where speed is critical; U-Net\nremains a lightweight, high-sensitivity option. The framework enables accurate,\nscalable orchard monitoring and can be further strengthened with DEM or DSM\nintegration and field calibration for operational decision support."
                },
                "authors": [
                    {
                        "name": "Wondimagegn Abebe Demissie"
                    },
                    {
                        "name": "Stefano Roccella"
                    },
                    {
                        "name": "Rudy Rossetto"
                    },
                    {
                        "name": "Antonio Minnocci"
                    },
                    {
                        "name": "Andrea Vannini"
                    },
                    {
                        "name": "Luca Sebastiani"
                    }
                ],
                "author_detail": {
                    "name": "Luca Sebastiani"
                },
                "author": "Luca Sebastiani",
                "arxiv_comment": "6 pages, 2025 IEEE International Workshop on Metrology for\n  Agriculture and Forestry (MetroAgriFor)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04226v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04226v4",
                "updated": "2025-10-30T14:52:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    52,
                    48,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-05T14:29:15Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    14,
                    29,
                    15,
                    6,
                    278,
                    0
                ],
                "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epistemic Diversity and Knowledge Collapse in Large Language Models"
                },
                "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation"
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Sarah Masud"
                    },
                    {
                        "name": "Jared Moore"
                    },
                    {
                        "name": "Srishti Yadav"
                    },
                    {
                        "name": "Maria Antoniak"
                    },
                    {
                        "name": "Chan Young Park"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for\n  table 3, random effect is the model version; v3 changelog: Fixed minor\n  formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model\n  description",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04226v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04226v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01369v2",
                "updated": "2025-10-30T14:45:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    45,
                    40,
                    3,
                    303,
                    0
                ],
                "published": "2025-06-02T06:54:29Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    6,
                    54,
                    29,
                    0,
                    153,
                    0
                ],
                "title": "Incentivizing LLMs to Self-Verify Their Answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incentivizing LLMs to Self-Verify Their Answers"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in complex\nreasoning tasks through both post-training and test-time scaling laws. While\nprevalent test-time scaling approaches are often realized by using external\nreward models to guide the model generation process, we find that only marginal\ngains can be acquired when scaling a model post-trained on specific reasoning\ntasks. We identify that the limited improvement stems from distribution\ndiscrepancies between the specific post-trained generator and the general\nreward model. To address this, we propose a framework that incentivizes LLMs to\nself-verify their own answers. By unifying answer generation and verification\nwithin a single reinforcement learning (RL) process, we train models that can\neffectively assess the correctness of their own solutions. The trained model\ncan further scale its performance at inference time by verifying its\ngenerations, without the need for external verifiers. We train our\nself-verification models based on Qwen2.5-Math-7B and\nDeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying\nreasoning context lengths. Experiments on multiple mathematical reasoning\nbenchmarks show that our models can not only improve post-training performance\nbut also enable effective test-time scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable progress in complex\nreasoning tasks through both post-training and test-time scaling laws. While\nprevalent test-time scaling approaches are often realized by using external\nreward models to guide the model generation process, we find that only marginal\ngains can be acquired when scaling a model post-trained on specific reasoning\ntasks. We identify that the limited improvement stems from distribution\ndiscrepancies between the specific post-trained generator and the general\nreward model. To address this, we propose a framework that incentivizes LLMs to\nself-verify their own answers. By unifying answer generation and verification\nwithin a single reinforcement learning (RL) process, we train models that can\neffectively assess the correctness of their own solutions. The trained model\ncan further scale its performance at inference time by verifying its\ngenerations, without the need for external verifiers. We train our\nself-verification models based on Qwen2.5-Math-7B and\nDeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying\nreasoning context lengths. Experiments on multiple mathematical reasoning\nbenchmarks show that our models can not only improve post-training performance\nbut also enable effective test-time scaling."
                },
                "authors": [
                    {
                        "name": "Fuxiang Zhang"
                    },
                    {
                        "name": "Jiacheng Xu"
                    },
                    {
                        "name": "Chaojie Wang"
                    },
                    {
                        "name": "Ce Cui"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26550v1",
                "updated": "2025-10-30T14:43:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    43,
                    26,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:43:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    43,
                    26,
                    3,
                    303,
                    0
                ],
                "title": "EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the\n  Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the\n  Edge"
                },
                "summary": "We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for\nmilitary tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated\nfrom military documentation and websites. We also present four new tests sets:\n(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k\n(general military knowledge). On these military test sets, EdgeRunner 20B\nmatches or exceeds GPT-5 task performance with 95%+ statistical significance,\nexcept for the high reasoning setting on the combat medic test set and the low\nreasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no\nstatistically-significant regression on general-purpose benchmarks like ARC-C,\nGPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the\nlow reasoning setting. We also present analyses on hyperparameter settings,\ncost, and throughput. These findings show that small, locally-hosted models are\nideal solutions for data-sensitive operations such as in the military domain,\nallowing for deployment in air-gapped edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for\nmilitary tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated\nfrom military documentation and websites. We also present four new tests sets:\n(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k\n(general military knowledge). On these military test sets, EdgeRunner 20B\nmatches or exceeds GPT-5 task performance with 95%+ statistical significance,\nexcept for the high reasoning setting on the combat medic test set and the low\nreasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no\nstatistically-significant regression on general-purpose benchmarks like ARC-C,\nGPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the\nlow reasoning setting. We also present analyses on hyperparameter settings,\ncost, and throughput. These findings show that small, locally-hosted models are\nideal solutions for data-sensitive operations such as in the military domain,\nallowing for deployment in air-gapped edge devices."
                },
                "authors": [
                    {
                        "name": "Jack FitzGerald"
                    },
                    {
                        "name": "Aristotelis Lazaridis"
                    },
                    {
                        "name": "Dylan Bates"
                    },
                    {
                        "name": "Aman Sharma"
                    },
                    {
                        "name": "Jonnathan Castillo"
                    },
                    {
                        "name": "Yousif Azami"
                    },
                    {
                        "name": "Sean Bailey"
                    },
                    {
                        "name": "Jeremy Cao"
                    },
                    {
                        "name": "Peter Damianov"
                    },
                    {
                        "name": "Kevin de Haan"
                    },
                    {
                        "name": "Luke Kerbs"
                    },
                    {
                        "name": "Vincent Lu"
                    },
                    {
                        "name": "Joseph Madigan"
                    },
                    {
                        "name": "Jeremy McLaurin"
                    },
                    {
                        "name": "Jonathan Tainer"
                    },
                    {
                        "name": "Dave Anderson"
                    },
                    {
                        "name": "Jonathan Beck"
                    },
                    {
                        "name": "Jamie Cuticello"
                    },
                    {
                        "name": "Colton Malkerson"
                    },
                    {
                        "name": "Tyler Saltsman"
                    }
                ],
                "author_detail": {
                    "name": "Tyler Saltsman"
                },
                "author": "Tyler Saltsman",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26546v1",
                "updated": "2025-10-30T14:37:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    37,
                    15,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:37:15Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    37,
                    15,
                    3,
                    303,
                    0
                ],
                "title": "WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework\n  with Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework\n  with Model Merging"
                },
                "summary": "Cross-Domain Sequential Recommendation (CDSR) seeks to improve user\npreference modeling by transferring knowledge from multiple domains. Despite\nthe progress made in CDSR, most existing methods rely on overlapping users or\nitems to establish cross-domain correlations-a requirement that rarely holds in\nreal-world settings. The advent of large language models (LLM) and\nmodel-merging techniques appears to overcome this limitation by unifying\nmulti-domain data without explicit overlaps. Yet, our empirical study shows\nthat naively training an LLM on combined domains-or simply merging several\ndomain-specific LLMs-often degrades performance relative to a model trained\nsolely on the target domain. To address these challenges, we first\nexperimentally investigate the cause of suboptimal performance in LLM-based\ncross-domain recommendation and model merging. Building on these insights, we\nintroduce WeaveRec, which cross-trains multiple LoRA modules with source and\ntarget domain data in a weaving fashion, and fuses them via model merging.\nWeaveRec can be extended to multi-source domain scenarios and notably does not\nintroduce additional inference-time cost in terms of latency or memory.\nFurthermore, we provide a theoretical guarantee that WeaveRec can reduce the\nupper bound of the expected error in the target domain. Extensive experiments\non single-source, multi-source, and cross-platform cross-domain recommendation\nscenarios validate that WeaveRec effectively mitigates performance degradation\nand consistently outperforms baseline approaches in real-world recommendation\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Domain Sequential Recommendation (CDSR) seeks to improve user\npreference modeling by transferring knowledge from multiple domains. Despite\nthe progress made in CDSR, most existing methods rely on overlapping users or\nitems to establish cross-domain correlations-a requirement that rarely holds in\nreal-world settings. The advent of large language models (LLM) and\nmodel-merging techniques appears to overcome this limitation by unifying\nmulti-domain data without explicit overlaps. Yet, our empirical study shows\nthat naively training an LLM on combined domains-or simply merging several\ndomain-specific LLMs-often degrades performance relative to a model trained\nsolely on the target domain. To address these challenges, we first\nexperimentally investigate the cause of suboptimal performance in LLM-based\ncross-domain recommendation and model merging. Building on these insights, we\nintroduce WeaveRec, which cross-trains multiple LoRA modules with source and\ntarget domain data in a weaving fashion, and fuses them via model merging.\nWeaveRec can be extended to multi-source domain scenarios and notably does not\nintroduce additional inference-time cost in terms of latency or memory.\nFurthermore, we provide a theoretical guarantee that WeaveRec can reduce the\nupper bound of the expected error in the target domain. Extensive experiments\non single-source, multi-source, and cross-platform cross-domain recommendation\nscenarios validate that WeaveRec effectively mitigates performance degradation\nand consistently outperforms baseline approaches in real-world recommendation\ntasks."
                },
                "authors": [
                    {
                        "name": "Min Hou"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Le Wu"
                    },
                    {
                        "name": "Chenyi He"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Si Wei"
                    }
                ],
                "author_detail": {
                    "name": "Si Wei"
                },
                "author": "Si Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26538v1",
                "updated": "2025-10-30T14:27:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    27,
                    51,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:27:51Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    27,
                    51,
                    3,
                    303,
                    0
                ],
                "title": "Reflecting on Empirical and Sustainability Aspects of Software\n  Engineering Research in the Era of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting on Empirical and Sustainability Aspects of Software\n  Engineering Research in the Era of Large Language Models"
                },
                "summary": "Software Engineering (SE) research involving the use of Large Language Models\n(LLMs) has introduced several new challenges related to rigour in benchmarking,\ncontamination, replicability, and sustainability. In this paper, we invite the\nresearch community to reflect on how these challenges are addressed in SE. Our\nresults provide a structured overview of current LLM-based SE research at ICSE,\nhighlighting both encouraging practices and persistent shortcomings. We\nconclude with recommendations to strengthen benchmarking rigour, improve\nreplicability, and address the financial and environmental costs of LLM-based\nSE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Engineering (SE) research involving the use of Large Language Models\n(LLMs) has introduced several new challenges related to rigour in benchmarking,\ncontamination, replicability, and sustainability. In this paper, we invite the\nresearch community to reflect on how these challenges are addressed in SE. Our\nresults provide a structured overview of current LLM-based SE research at ICSE,\nhighlighting both encouraging practices and persistent shortcomings. We\nconclude with recommendations to strengthen benchmarking rigour, improve\nreplicability, and address the financial and environmental costs of LLM-based\nSE."
                },
                "authors": [
                    {
                        "name": "David Williams"
                    },
                    {
                        "name": "Max Hort"
                    },
                    {
                        "name": "Maria Kechagia"
                    },
                    {
                        "name": "Aldeida Aleti"
                    },
                    {
                        "name": "Justyna Petke"
                    },
                    {
                        "name": "Federica Sarro"
                    }
                ],
                "author_detail": {
                    "name": "Federica Sarro"
                },
                "author": "Federica Sarro",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26527v1",
                "updated": "2025-10-30T14:20:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    20,
                    24,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:20:24Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    20,
                    24,
                    3,
                    303,
                    0
                ],
                "title": "Polybasic Speculative Decoding Through a Theoretical Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polybasic Speculative Decoding Through a Theoretical Perspective"
                },
                "summary": "Inference latency stands as a critical bottleneck in the large-scale\ndeployment of Large Language Models (LLMs). Speculative decoding methods have\nrecently shown promise in accelerating inference without compromising the\noutput distribution. However, existing work typically relies on a dualistic\ndraft-verify framework and lacks rigorous theoretical grounding. In this paper,\nwe introduce a novel \\emph{polybasic} speculative decoding framework,\nunderpinned by a comprehensive theoretical analysis. Specifically, we prove a\nfundamental theorem that characterizes the optimal inference time for\nmulti-model speculative decoding systems, shedding light on how to extend\nbeyond the dualistic approach to a more general polybasic paradigm. Through our\ntheoretical investigation of multi-model token generation, we expose and\noptimize the interplay between model capabilities, acceptance lengths, and\noverall computational cost. Our framework supports both standalone\nimplementation and integration with existing speculative techniques, leading to\naccelerated performance in practice. Experimental results across multiple model\nfamilies demonstrate that our approach yields speedup ratios ranging from\n$3.31\\times$ to $4.01\\times$ for LLaMA2-Chat 7B, up to $3.87 \\times$ for\nLLaMA3-8B, up to $4.43 \\times$ for Vicuna-7B and up to $3.85 \\times$ for\nQwen2-7B -- all while preserving the original output distribution. We release\nour theoretical proofs and implementation code to facilitate further\ninvestigation into polybasic speculative decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference latency stands as a critical bottleneck in the large-scale\ndeployment of Large Language Models (LLMs). Speculative decoding methods have\nrecently shown promise in accelerating inference without compromising the\noutput distribution. However, existing work typically relies on a dualistic\ndraft-verify framework and lacks rigorous theoretical grounding. In this paper,\nwe introduce a novel \\emph{polybasic} speculative decoding framework,\nunderpinned by a comprehensive theoretical analysis. Specifically, we prove a\nfundamental theorem that characterizes the optimal inference time for\nmulti-model speculative decoding systems, shedding light on how to extend\nbeyond the dualistic approach to a more general polybasic paradigm. Through our\ntheoretical investigation of multi-model token generation, we expose and\noptimize the interplay between model capabilities, acceptance lengths, and\noverall computational cost. Our framework supports both standalone\nimplementation and integration with existing speculative techniques, leading to\naccelerated performance in practice. Experimental results across multiple model\nfamilies demonstrate that our approach yields speedup ratios ranging from\n$3.31\\times$ to $4.01\\times$ for LLaMA2-Chat 7B, up to $3.87 \\times$ for\nLLaMA3-8B, up to $4.43 \\times$ for Vicuna-7B and up to $3.85 \\times$ for\nQwen2-7B -- all while preserving the original output distribution. We release\nour theoretical proofs and implementation code to facilitate further\ninvestigation into polybasic speculative decoding."
                },
                "authors": [
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Yuexiao Ma"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21451v2",
                "updated": "2025-10-30T14:18:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    18,
                    35,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-24T13:28:41Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    13,
                    28,
                    41,
                    4,
                    297,
                    0
                ],
                "title": "Scalpel: Automotive Deep Learning Framework Testing via Assembling Model\n  Components",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalpel: Automotive Deep Learning Framework Testing via Assembling Model\n  Components"
                },
                "summary": "Deep learning (DL) plays a key role in autonomous driving systems. DL models\nsupport perception modules, equipped with tasks such as object detection and\nsensor fusion. These DL models enable vehicles to process multi-sensor inputs\nto understand complex surroundings. Deploying DL models in autonomous driving\nsystems faces stringent challenges, including real-time processing, limited\ncomputational resources, and strict power constraints. To address these\nchallenges, automotive DL frameworks (e.g., PaddleInference) have emerged to\noptimize inference efficiency. However, these frameworks encounter unique\nquality issues due to their more complex deployment environments, such as\ncrashes stemming from limited scheduled memory and incorrect memory allocation.\nUnfortunately, existing DL framework testing methods fail to detect these\nquality issues due to the failure in deploying generated test input models, as\nthese models lack three essential capabilities: (1) multi-input/output tensor\nprocessing, (2) multi-modal data processing, and (3) multi-level data feature\nextraction. These capabilities necessitate specialized model components, which\nexisting testing methods neglect during model generation. To bridge this gap,\nwe propose Scalpel, an automotive DL frameworks testing method that generates\ntest input models at the model component level. Scalpel generates models by\nassembling model components (heads, necks, backbones) to support capabilities\nrequired by autonomous driving systems. Specifically, Scalpel maintains and\nupdates a repository of model components, generating test inputs by selecting,\nmutating, and assembling them. Successfully generated models are added back to\nenrich the repository. Newly generated models are then deployed within the\nautonomous driving system to test automotive DL frameworks via differential\ntesting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning (DL) plays a key role in autonomous driving systems. DL models\nsupport perception modules, equipped with tasks such as object detection and\nsensor fusion. These DL models enable vehicles to process multi-sensor inputs\nto understand complex surroundings. Deploying DL models in autonomous driving\nsystems faces stringent challenges, including real-time processing, limited\ncomputational resources, and strict power constraints. To address these\nchallenges, automotive DL frameworks (e.g., PaddleInference) have emerged to\noptimize inference efficiency. However, these frameworks encounter unique\nquality issues due to their more complex deployment environments, such as\ncrashes stemming from limited scheduled memory and incorrect memory allocation.\nUnfortunately, existing DL framework testing methods fail to detect these\nquality issues due to the failure in deploying generated test input models, as\nthese models lack three essential capabilities: (1) multi-input/output tensor\nprocessing, (2) multi-modal data processing, and (3) multi-level data feature\nextraction. These capabilities necessitate specialized model components, which\nexisting testing methods neglect during model generation. To bridge this gap,\nwe propose Scalpel, an automotive DL frameworks testing method that generates\ntest input models at the model component level. Scalpel generates models by\nassembling model components (heads, necks, backbones) to support capabilities\nrequired by autonomous driving systems. Specifically, Scalpel maintains and\nupdates a repository of model components, generating test inputs by selecting,\nmutating, and assembling them. Successfully generated models are added back to\nenrich the repository. Newly generated models are then deployed within the\nautonomous driving system to test automotive DL frameworks via differential\ntesting."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "An Guo"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "arxiv_comment": "Accepted by the 48th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26523v1",
                "updated": "2025-10-30T14:16:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    16,
                    21,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:16:21Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    16,
                    21,
                    3,
                    303,
                    0
                ],
                "title": "Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy\n  Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy\n  Policies"
                },
                "summary": "Smart home devices such as video doorbells and security cameras are becoming\nincreasingly common in everyday life. While these devices offer convenience and\nsafety, they also raise new privacy concerns: how these devices affect others,\nlike neighbors, visitors, or people passing by. This issue is generally known\nas interdependent privacy, where one person's actions (or inaction) may impact\nthe privacy of others, and, specifically, bystander privacy in the context of\nsmart homes. Given lax data protection regulations in terms of shared physical\nspaces and amateur joint data controllers, we expect that the privacy policies\nof smart home products reflect the missing regulatory incentives. This paper\npresents a focused privacy policy analysis of 20 video doorbell and smart\ncamera products, concentrating explicitly on the bystander aspect. We show that\nalthough some of the vendors acknowledge bystanders, they address it only to\nthe extent of including disclaimers, shifting the ethical responsibility for\ncollecting the data of non-users to the device owner. In addition, we identify\nand examine real-world cases related to bystander privacy, demonstrating how\ncurrent deployments can impact non-users. Based on our findings, we analyze\nvendor privacy policies in light of existing legal frameworks and technical\ncapabilities, and we provide practical recommendations for both policy language\nand system design to enhance transparency and empower both bystanders and\ndevice owners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart home devices such as video doorbells and security cameras are becoming\nincreasingly common in everyday life. While these devices offer convenience and\nsafety, they also raise new privacy concerns: how these devices affect others,\nlike neighbors, visitors, or people passing by. This issue is generally known\nas interdependent privacy, where one person's actions (or inaction) may impact\nthe privacy of others, and, specifically, bystander privacy in the context of\nsmart homes. Given lax data protection regulations in terms of shared physical\nspaces and amateur joint data controllers, we expect that the privacy policies\nof smart home products reflect the missing regulatory incentives. This paper\npresents a focused privacy policy analysis of 20 video doorbell and smart\ncamera products, concentrating explicitly on the bystander aspect. We show that\nalthough some of the vendors acknowledge bystanders, they address it only to\nthe extent of including disclaimers, shifting the ethical responsibility for\ncollecting the data of non-users to the device owner. In addition, we identify\nand examine real-world cases related to bystander privacy, demonstrating how\ncurrent deployments can impact non-users. Based on our findings, we analyze\nvendor privacy policies in light of existing legal frameworks and technical\ncapabilities, and we provide practical recommendations for both policy language\nand system design to enhance transparency and empower both bystanders and\ndevice owners."
                },
                "authors": [
                    {
                        "name": "Shuaishuai Liu"
                    },
                    {
                        "name": "Gergely Acs"
                    },
                    {
                        "name": "Gergely Biczók"
                    }
                ],
                "author_detail": {
                    "name": "Gergely Biczók"
                },
                "author": "Gergely Biczók",
                "arxiv_comment": "18 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15030v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15030v3",
                "updated": "2025-10-30T14:10:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    10,
                    33,
                    3,
                    303,
                    0
                ],
                "published": "2025-08-20T19:49:06Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    49,
                    6,
                    2,
                    232,
                    0
                ],
                "title": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations\n  in Tourism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations\n  in Tourism"
                },
                "summary": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems."
                },
                "authors": [
                    {
                        "name": "Ashmi Banerjee"
                    },
                    {
                        "name": "Adithi Satish"
                    },
                    {
                        "name": "Fitri Nur Aisyah"
                    },
                    {
                        "name": "Wolfgang Wörndl"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Deldjoo"
                },
                "author": "Yashar Deldjoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15030v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15030v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26516v1",
                "updated": "2025-10-30T14:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    9,
                    50,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    9,
                    50,
                    3,
                    303,
                    0
                ],
                "title": "Envisioning Future Interactive Web Development: Editing Webpage with\n  Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Envisioning Future Interactive Web Development: Editing Webpage with\n  Natural Language"
                },
                "summary": "The evolution of web applications relies on iterative code modifications, a\nprocess that is traditionally manual and time-consuming. While Large Language\nModels (LLMs) can generate UI code, their ability to edit existing code from\nnew design requirements (e.g., \"center the logo\") remains a challenge. This is\nlargely due to the absence of large-scale, high-quality tuning data to align\nmodel performance with human expectations. In this paper, we introduce a novel,\nautomated data generation pipeline that uses LLMs to synthesize a high-quality\nfine-tuning dataset for web editing, named Instruct4Edit. Our approach\ngenerates diverse instructions, applies the corresponding code modifications,\nand performs visual verification to ensure correctness. By fine-tuning models\non Instruct4Edit, we demonstrate consistent improvement in translating human\nintent into precise, structurally coherent, and visually accurate code changes.\nThis work provides a scalable and transparent foundation for natural language\nbased web editing, demonstrating that fine-tuning smaller open-source models\ncan achieve competitive performance with proprietary systems. We release all\ndata, code implementations, and model checkpoints for reproduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of web applications relies on iterative code modifications, a\nprocess that is traditionally manual and time-consuming. While Large Language\nModels (LLMs) can generate UI code, their ability to edit existing code from\nnew design requirements (e.g., \"center the logo\") remains a challenge. This is\nlargely due to the absence of large-scale, high-quality tuning data to align\nmodel performance with human expectations. In this paper, we introduce a novel,\nautomated data generation pipeline that uses LLMs to synthesize a high-quality\nfine-tuning dataset for web editing, named Instruct4Edit. Our approach\ngenerates diverse instructions, applies the corresponding code modifications,\nand performs visual verification to ensure correctness. By fine-tuning models\non Instruct4Edit, we demonstrate consistent improvement in translating human\nintent into precise, structurally coherent, and visually accurate code changes.\nThis work provides a scalable and transparent foundation for natural language\nbased web editing, demonstrating that fine-tuning smaller open-source models\ncan achieve competitive performance with proprietary systems. We release all\ndata, code implementations, and model checkpoints for reproduction."
                },
                "authors": [
                    {
                        "name": "Truong Hai Dang"
                    },
                    {
                        "name": "Jingyu Xiao"
                    },
                    {
                        "name": "Yintong Huo"
                    }
                ],
                "author_detail": {
                    "name": "Yintong Huo"
                },
                "author": "Yintong Huo",
                "arxiv_comment": "accepted by AIWare'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26512v1",
                "updated": "2025-10-30T14:05:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    5,
                    55,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:05:55Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    5,
                    55,
                    3,
                    303,
                    0
                ],
                "title": "Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs"
                },
                "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer critical insights but are often unstructured,\nlexically dense, and filled with ambiguous or shifting references, which pose\nsignificant challenges for automated knowledge graph (KG) construction. While\nrecent LLM-based approaches improve over static templates, they still generate\nnoisy, fragmented graphs with duplicate nodes due to the absence of guided\nextraction and coreference resolution. The recently proposed CORE-KG framework\naddresses these limitations by integrating a type-aware coreference module and\ndomain-guided structured prompts, significantly reducing node duplication and\nlegal noise. In this work, we present a systematic ablation study of CORE-KG to\nquantify the individual contributions of its two key components. Our results\nshow that removing coreference resolution results in a 28.32% increase in node\nduplication and a 4.32% increase in noisy nodes, while removing structured\nprompts leads to a 4.34% increase in node duplication and a 73.33% increase in\nnoisy nodes. These findings offer empirical insights for designing robust\nLLM-based pipelines for extracting structured representations from complex\nlegal texts."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    }
                ],
                "author_detail": {
                    "name": "Carlotta Domeniconi"
                },
                "author": "Carlotta Domeniconi",
                "arxiv_comment": "ICDM 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26510v1",
                "updated": "2025-10-30T14:04:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    4,
                    25,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T14:04:25Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    14,
                    4,
                    25,
                    3,
                    303,
                    0
                ],
                "title": "LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection"
                },
                "summary": "Model and hyperparameter selection are critical but challenging in machine\nlearning, typically requiring expert intuition or expensive automated search.\nWe investigate whether large language models (LLMs) can act as in-context\nmeta-learners for this task. By converting each dataset into interpretable\nmetadata, we prompt an LLM to recommend both model families and\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\nwith examples of models and their performance on past tasks. Across synthetic\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\nrecommend competitive models and hyperparameters without search, and that\nimprovements from meta-informed prompting demonstrate their capacity for\nin-context meta-learning. These results highlight a promising new role for LLMs\nas lightweight, general-purpose assistants for model selection and\nhyperparameter optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model and hyperparameter selection are critical but challenging in machine\nlearning, typically requiring expert intuition or expensive automated search.\nWe investigate whether large language models (LLMs) can act as in-context\nmeta-learners for this task. By converting each dataset into interpretable\nmetadata, we prompt an LLM to recommend both model families and\nhyperparameters. We study two prompting strategies: (1) a zero-shot mode\nrelying solely on pretrained knowledge, and (2) a meta-informed mode augmented\nwith examples of models and their performance on past tasks. Across synthetic\nand real-world benchmarks, we show that LLMs can exploit dataset metadata to\nrecommend competitive models and hyperparameters without search, and that\nimprovements from meta-informed prompting demonstrate their capacity for\nin-context meta-learning. These results highlight a promising new role for LLMs\nas lightweight, general-purpose assistants for model selection and\nhyperparameter optimization."
                },
                "authors": [
                    {
                        "name": "Youssef Attia El Hili"
                    },
                    {
                        "name": "Albert Thomas"
                    },
                    {
                        "name": "Malik Tiomoko"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Corentin Léger"
                    },
                    {
                        "name": "Corinne Ancourt"
                    },
                    {
                        "name": "Balázs Kégl"
                    }
                ],
                "author_detail": {
                    "name": "Balázs Kégl"
                },
                "author": "Balázs Kégl",
                "arxiv_comment": "27 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26501v1",
                "updated": "2025-10-30T13:54:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    54,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:54:37Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    54,
                    37,
                    3,
                    303,
                    0
                ],
                "title": "Enhancing ECG Classification Robustness with Lightweight Unsupervised\n  Anomaly Detection Filters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing ECG Classification Robustness with Lightweight Unsupervised\n  Anomaly Detection Filters"
                },
                "summary": "Continuous electrocardiogram (ECG) monitoring via wearables offers\nsignificant potential for early cardiovascular disease (CVD) detection.\nHowever, deploying deep learning models for automated analysis in\nresource-constrained environments faces reliability challenges due to\ninevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen\npathologies or noisecorrupted signals, often cause erroneous, high-confidence\npredictions by standard classifiers, compromising patient safety. Existing OOD\ndetection methods either neglect computational constraints or address noise and\nunseen classes separately. This paper explores Unsupervised Anomaly Detection\n(UAD) as an independent, upstream filtering mechanism to improve robustness. We\nbenchmark six UAD approaches, including Deep SVDD, reconstruction-based models,\nMasked Anomaly Detection, normalizing flows, and diffusion models, optimized\nvia Neural Architecture Search (NAS) under strict resource constraints (at most\n512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection\nof OOD CVD classes and signals unsuitable for analysis due to noise. Results\nshow Deep SVDD consistently achieves the best trade-off between detection and\nefficiency. In a realistic deployment simulation, integrating the optimized\nDeep SVDD filter with a diagnostic classifier improved accuracy by up to 21\npercentage points over a classifier-only baseline. This study demonstrates that\noptimized UAD filters can safeguard automated ECG analysis, enabling safer,\nmore reliable continuous cardiovascular monitoring on wearables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous electrocardiogram (ECG) monitoring via wearables offers\nsignificant potential for early cardiovascular disease (CVD) detection.\nHowever, deploying deep learning models for automated analysis in\nresource-constrained environments faces reliability challenges due to\ninevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen\npathologies or noisecorrupted signals, often cause erroneous, high-confidence\npredictions by standard classifiers, compromising patient safety. Existing OOD\ndetection methods either neglect computational constraints or address noise and\nunseen classes separately. This paper explores Unsupervised Anomaly Detection\n(UAD) as an independent, upstream filtering mechanism to improve robustness. We\nbenchmark six UAD approaches, including Deep SVDD, reconstruction-based models,\nMasked Anomaly Detection, normalizing flows, and diffusion models, optimized\nvia Neural Architecture Search (NAS) under strict resource constraints (at most\n512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection\nof OOD CVD classes and signals unsuitable for analysis due to noise. Results\nshow Deep SVDD consistently achieves the best trade-off between detection and\nefficiency. In a realistic deployment simulation, integrating the optimized\nDeep SVDD filter with a diagnostic classifier improved accuracy by up to 21\npercentage points over a classifier-only baseline. This study demonstrates that\noptimized UAD filters can safeguard automated ECG analysis, enabling safer,\nmore reliable continuous cardiovascular monitoring on wearables."
                },
                "authors": [
                    {
                        "name": "Mustafa Fuad Rifet Ibrahim"
                    },
                    {
                        "name": "Maurice Meijer"
                    },
                    {
                        "name": "Alexander Schlaefer"
                    },
                    {
                        "name": "Peer Stelldinger"
                    }
                ],
                "author_detail": {
                    "name": "Peer Stelldinger"
                },
                "author": "Peer Stelldinger",
                "arxiv_comment": "Submitted to the 24th International Conference on Pervasive Computing\n  and Communications (PerCom 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11730v2",
                "updated": "2025-10-30T13:52:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    52,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-16T22:24:48Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    22,
                    24,
                    48,
                    4,
                    136,
                    0
                ],
                "title": "Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Optimal Verification Granularity for Compute-Efficient\n  Test-Time Scaling"
                },
                "summary": "Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) has proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Verification plays a key role in\nTTS, simultaneously influencing (1) reasoning performance and (2) compute\nefficiency, due to the quality and computational cost of verification. In this\nwork, we challenge the conventional paradigms of verification, and make the\nfirst attempt toward systematically investigating the impact of verification\ngranularity-that is, how frequently the verifier is invoked during generation,\nbeyond verifying only the final output or individual generation steps. To this\nend, we introduce Variable Granularity Search (VG-Search), a unified algorithm\nthat generalizes beam search and Best-of-N sampling via a tunable granularity\nparameter g. Extensive experiments with VG-Search under varying compute\nbudgets, generator-verifier configurations, and task attributes reveal that\ndynamically selecting g can improve the compute efficiency and scaling\nbehavior. Building on these findings, we propose adaptive VG-Search strategies\nthat achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over\nBest-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to\nsupport future research."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Yasuyuki Okoshi"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Masato Motomura"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26498v1",
                "updated": "2025-10-30T13:50:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    50,
                    19,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:50:19Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    50,
                    19,
                    3,
                    303,
                    0
                ],
                "title": "A Multi-agent Large Language Model Framework to Automatically Assess\n  Performance of a Clinical AI Triage Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-agent Large Language Model Framework to Automatically Assess\n  Performance of a Clinical AI Triage Tool"
                },
                "summary": "Purpose: The purpose of this study was to determine if an ensemble of\nmultiple LLM agents could be used collectively to provide a more reliable\nassessment of a pixel-based AI triage tool than a single LLM.\n  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were\nprocessed by a commercial intracranial hemorrhage (ICH) AI detection tool.\nRadiology reports were analyzed by an ensemble of eight open-source LLM models\nand a HIPAA compliant internal version of GPT-4o using a single multi-shot\nprompt that assessed for presence of ICH. 1,726 examples were manually\nreviewed. Performance characteristics of the eight open-source models and\nconsensus were compared to GPT-4o. Three ideal consensus LLM ensembles were\ntested for rating the performance of the triage tool.\n  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The\nhighest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).\nThe average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).\nLlama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater\nprecision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the\nideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3\nEnsemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522\n(0.500-0.543). No statistically significant differences were observed between\nTop-3, Full-9, and Consensus (p > 0.05).\n  Conclusion: An ensemble of medium to large sized open-source LLMs provides a\nmore consistent and reliable method to derive a ground truth retrospective\nevaluation of a clinical AI triage tool over a single LLM alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: The purpose of this study was to determine if an ensemble of\nmultiple LLM agents could be used collectively to provide a more reliable\nassessment of a pixel-based AI triage tool than a single LLM.\n  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were\nprocessed by a commercial intracranial hemorrhage (ICH) AI detection tool.\nRadiology reports were analyzed by an ensemble of eight open-source LLM models\nand a HIPAA compliant internal version of GPT-4o using a single multi-shot\nprompt that assessed for presence of ICH. 1,726 examples were manually\nreviewed. Performance characteristics of the eight open-source models and\nconsensus were compared to GPT-4o. Three ideal consensus LLM ensembles were\ntested for rating the performance of the triage tool.\n  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The\nhighest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).\nThe average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).\nLlama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater\nprecision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the\nideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3\nEnsemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522\n(0.500-0.543). No statistically significant differences were observed between\nTop-3, Full-9, and Consensus (p > 0.05).\n  Conclusion: An ensemble of medium to large sized open-source LLMs provides a\nmore consistent and reliable method to derive a ground truth retrospective\nevaluation of a clinical AI triage tool over a single LLM alone."
                },
                "authors": [
                    {
                        "name": "Adam E. Flanders"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Luciano Prevedello"
                    },
                    {
                        "name": "Robyn Ball"
                    },
                    {
                        "name": "Errol Colak"
                    },
                    {
                        "name": "Prahlad Menon"
                    },
                    {
                        "name": "George Shih"
                    },
                    {
                        "name": "Hui-Ming Lin"
                    },
                    {
                        "name": "Paras Lakhani"
                    }
                ],
                "author_detail": {
                    "name": "Paras Lakhani"
                },
                "author": "Paras Lakhani",
                "arxiv_comment": "29 pages, 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08271v2",
                "updated": "2025-10-30T13:49:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    49,
                    48,
                    3,
                    303,
                    0
                ],
                "published": "2025-02-12T10:24:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    24,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "RecCocktail: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecCocktail: A Generalizable and Efficient Framework for LLM-Based\n  Recommendation"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm focuses on enhancing\ndomain-specific recommendation tasks, improving performance in warm\nrecommendation scenarios. While most previous works treat these two paradigms\nseparately, we argue that they have complementary advantages, and combining\nthem can yield better results. In this paper, we propose a generalizable and\nefficient LLM-based recommendation framework RecCocktail. Our approach begins\nwith fine-tuning a \"base spirit\" LoRA module using domain-general\nrecommendation instruction data to align LLM with recommendation knowledge.\nNext, given users' behavior of a specific domain, we construct a\ndomain-specific \"ingredient\" LoRA module. We then provide an entropy-guided\nadaptive merging method to mix the \"base spirit\" and the \"ingredient\" in the\nweight space. Please note that, RecCocktail combines the advantages of the\nexisting two paradigms without introducing additional time or space overhead\nduring the inference phase. Moreover, RecCocktail is efficient with plug and\nplay, as the \"base spirit\" LoRA is trained only once, and any domain-specific\n\"ingredient\" can be efficiently mixed with only domain-specific fine-tuning.\nExtensive experiments on multiple datasets under both warm and cold-start\nrecommendation scenarios validate the effectiveness and generality of the\nproposed RecCocktail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in recent\nyears, owing to their impressive generalization capabilities and rich world\nknowledge. To capitalize on the potential of using LLMs as recommender systems,\nmainstream approaches typically focus on two paradigms. The first paradigm\ndesigns multi-domain or multi-task instruction data for generalizable\nrecommendation, so as to align LLMs with general recommendation areas and deal\nwith cold-start recommendation. The second paradigm focuses on enhancing\ndomain-specific recommendation tasks, improving performance in warm\nrecommendation scenarios. While most previous works treat these two paradigms\nseparately, we argue that they have complementary advantages, and combining\nthem can yield better results. In this paper, we propose a generalizable and\nefficient LLM-based recommendation framework RecCocktail. Our approach begins\nwith fine-tuning a \"base spirit\" LoRA module using domain-general\nrecommendation instruction data to align LLM with recommendation knowledge.\nNext, given users' behavior of a specific domain, we construct a\ndomain-specific \"ingredient\" LoRA module. We then provide an entropy-guided\nadaptive merging method to mix the \"base spirit\" and the \"ingredient\" in the\nweight space. Please note that, RecCocktail combines the advantages of the\nexisting two paradigms without introducing additional time or space overhead\nduring the inference phase. Moreover, RecCocktail is efficient with plug and\nplay, as the \"base spirit\" LoRA is trained only once, and any domain-specific\n\"ingredient\" can be efficiently mixed with only domain-specific fine-tuning.\nExtensive experiments on multiple datasets under both warm and cold-start\nrecommendation scenarios validate the effectiveness and generality of the\nproposed RecCocktail."
                },
                "authors": [
                    {
                        "name": "Min Hou"
                    },
                    {
                        "name": "Chenxi Bai"
                    },
                    {
                        "name": "Le Wu"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25623v2",
                "updated": "2025-10-30T13:49:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    49,
                    22,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T15:27:47Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    27,
                    47,
                    2,
                    302,
                    0
                ],
                "title": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Role of Verifiers in Test-Time Scaling for Legal\n  Reasoning Tasks"
                },
                "summary": "Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming, its value in argumentative domains such as law remains\nunderexplored. We present an empirical study of verifier-based TTS methods for\nlegal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7\nreward models, we evaluate both outcome-level (Best-of-$N$) and process-level\n(tree search) verification under realistic low-$N$ budgets. Our analysis\nsystematically investigates how verifier utility is affected by key properties\nsuch as domain specialization, model size, and supervision type\n(process-supervised PRMs vs. outcome-only ORMs), even when applied across\ndifferent roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) techniques can improve the performance of large\nlanguage models (LLMs) at the expense of additional computation and latency.\nWhile TTS has proven effective in formal domains such as mathematics and\nprogramming, its value in argumentative domains such as law remains\nunderexplored. We present an empirical study of verifier-based TTS methods for\nlegal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7\nreward models, we evaluate both outcome-level (Best-of-$N$) and process-level\n(tree search) verification under realistic low-$N$ budgets. Our analysis\nsystematically investigates how verifier utility is affected by key properties\nsuch as domain specialization, model size, and supervision type\n(process-supervised PRMs vs. outcome-only ORMs), even when applied across\ndifferent roles."
                },
                "authors": [
                    {
                        "name": "Davide Romano"
                    },
                    {
                        "name": "Jonathan Schwarz"
                    },
                    {
                        "name": "Daniele Giofré"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Giofré"
                },
                "author": "Daniele Giofré",
                "arxiv_comment": "Accepted to EMNLP - NLLP Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26495v1",
                "updated": "2025-10-30T13:44:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    44,
                    22,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:44:22Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    44,
                    22,
                    3,
                    303,
                    0
                ],
                "title": "Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for\n  Real-world Database Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for\n  Real-world Database Exploration"
                },
                "summary": "Recent advances in Text-to-SQL have achieved strong results in static,\nsingle-turn tasks, where models generate SQL queries from natural language\nquestions. However, these systems fall short in real-world interactive\nscenarios, where user intents evolve and queries must be refined over multiple\nturns. In applications such as finance and business analytics, users\niteratively adjust query constraints or dimensions based on intermediate\nresults. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a\nbenchmark assessing model performance under evolving user interactions. Unlike\nprevious manually curated datasets, DySQL-Bench is built through an automated\ntwo-stage pipeline of task synthesis and verification. Structured tree\nrepresentations derived from raw database tables guide LLM-based task\ngeneration, followed by interaction-oriented filtering and expert validation.\nHuman evaluation confirms 100% correctness of the synthesized data. We further\npropose a multi-turn evaluation framework simulating realistic interactions\namong an LLM-simulated user, the model under test, and an executable database.\nThe model must adapt its reasoning and SQL generation as user intents change.\nDySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling\n1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the\nPass@5 metric, underscoring the benchmark's difficulty. All code and data are\nreleased at https://github.com/Aurora-slz/Real-World-SQL-Bench .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Text-to-SQL have achieved strong results in static,\nsingle-turn tasks, where models generate SQL queries from natural language\nquestions. However, these systems fall short in real-world interactive\nscenarios, where user intents evolve and queries must be refined over multiple\nturns. In applications such as finance and business analytics, users\niteratively adjust query constraints or dimensions based on intermediate\nresults. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a\nbenchmark assessing model performance under evolving user interactions. Unlike\nprevious manually curated datasets, DySQL-Bench is built through an automated\ntwo-stage pipeline of task synthesis and verification. Structured tree\nrepresentations derived from raw database tables guide LLM-based task\ngeneration, followed by interaction-oriented filtering and expert validation.\nHuman evaluation confirms 100% correctness of the synthesized data. We further\npropose a multi-turn evaluation framework simulating realistic interactions\namong an LLM-simulated user, the model under test, and an executable database.\nThe model must adapt its reasoning and SQL generation as user intents change.\nDySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling\n1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the\nPass@5 metric, underscoring the benchmark's difficulty. All code and data are\nreleased at https://github.com/Aurora-slz/Real-World-SQL-Bench ."
                },
                "authors": [
                    {
                        "name": "Linzhuang Sun"
                    },
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Yuying Li"
                    },
                    {
                        "name": "Qifeng Cai"
                    },
                    {
                        "name": "Jingxuan Wei"
                    },
                    {
                        "name": "Bihui Yu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26494v1",
                "updated": "2025-10-30T13:43:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    28,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:43:28Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    28,
                    3,
                    303,
                    0
                ],
                "title": "Simulating and Experimenting with Social Media Mobilization Using LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating and Experimenting with Social Media Mobilization Using LLM\n  Agents"
                },
                "summary": "Online social networks have transformed the ways in which political\nmobilization messages are disseminated, raising new questions about how peer\ninfluence operates at scale. Building on the landmark 61-million-person\nFacebook experiment \\citep{bond201261}, we develop an agent-based simulation\nframework that integrates real U.S. Census demographic distributions, authentic\nTwitter network topology, and heterogeneous large language model (LLM) agents\nto examine the effect of mobilization messages on voter turnout. Each simulated\nagent is assigned demographic attributes, a personal political stance, and an\nLLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano})\nreflecting its political sophistication. Agents interact over realistic social\nnetwork structures, receiving personalized feeds and dynamically updating their\nengagement behaviors and voting intentions. Experimental conditions replicate\nthe informational and social mobilization treatments of the original Facebook\nstudy. Across scenarios, the simulator reproduces qualitative patterns observed\nin field experiments, including stronger mobilization effects under social\nmessage treatments and measurable peer spillovers. Our framework provides a\ncontrolled, reproducible environment for testing counterfactual designs and\nsensitivity analyses in political mobilization research, offering a bridge\nbetween high-validity field experiments and flexible computational\nmodeling.\\footnote{Code and data available at\nhttps://github.com/CausalMP/LLM-SocioPol}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online social networks have transformed the ways in which political\nmobilization messages are disseminated, raising new questions about how peer\ninfluence operates at scale. Building on the landmark 61-million-person\nFacebook experiment \\citep{bond201261}, we develop an agent-based simulation\nframework that integrates real U.S. Census demographic distributions, authentic\nTwitter network topology, and heterogeneous large language model (LLM) agents\nto examine the effect of mobilization messages on voter turnout. Each simulated\nagent is assigned demographic attributes, a personal political stance, and an\nLLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano})\nreflecting its political sophistication. Agents interact over realistic social\nnetwork structures, receiving personalized feeds and dynamically updating their\nengagement behaviors and voting intentions. Experimental conditions replicate\nthe informational and social mobilization treatments of the original Facebook\nstudy. Across scenarios, the simulator reproduces qualitative patterns observed\nin field experiments, including stronger mobilization effects under social\nmessage treatments and measurable peer spillovers. Our framework provides a\ncontrolled, reproducible environment for testing counterfactual designs and\nsensitivity analyses in political mobilization research, offering a bridge\nbetween high-validity field experiments and flexible computational\nmodeling.\\footnote{Code and data available at\nhttps://github.com/CausalMP/LLM-SocioPol}"
                },
                "authors": [
                    {
                        "name": "Sadegh Shirani"
                    },
                    {
                        "name": "Mohsen Bayati"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Bayati"
                },
                "author": "Mohsen Bayati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26491v1",
                "updated": "2025-10-30T13:40:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    40,
                    52,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:40:52Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    40,
                    52,
                    3,
                    303,
                    0
                ],
                "title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient RLVR via Off-Policy Influence Guidance"
                },
                "summary": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\n\\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy\n\\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data selection is a critical aspect of Reinforcement Learning with Verifiable\nRewards (RLVR) for enhancing the reasoning capabilities of large language\nmodels (LLMs). Current data selection methods are largely heuristic-based,\nlacking theoretical guarantees and generalizability. This work proposes a\ntheoretically-grounded approach using influence functions to estimate the\ncontribution of each data point to the learning objective. To overcome the\nprohibitive computational cost of policy rollouts required for online influence\nestimation, we introduce an off-policy influence estimation method that\nefficiently approximates data influence using pre-collected offline\ntrajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we\nemploy sparse random projection to reduce dimensionality and improve storage\nand computation efficiency. Leveraging these techniques, we develop\n\\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy\n\\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that\niteratively selects the most influential data for the current policy.\nExperiments on models up to 7B parameters demonstrate that CROPI significantly\naccelerates training. On a 1.5B model, it achieves a 2.66x step-level\nacceleration while using only 10\\% of the data per stage compared to\nfull-dataset training. Our results highlight the substantial potential of\ninfluence-based data selection for efficient RLVR."
                },
                "authors": [
                    {
                        "name": "Erle Zhu"
                    },
                    {
                        "name": "Dazhi Jiang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Xujun Li"
                    },
                    {
                        "name": "Jiale Cheng"
                    },
                    {
                        "name": "Yuxian Gu"
                    },
                    {
                        "name": "Yilin Niu"
                    },
                    {
                        "name": "Aohan Zeng"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Hongning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongning Wang"
                },
                "author": "Hongning Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26490v1",
                "updated": "2025-10-30T13:40:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    40,
                    26,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:40:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    40,
                    26,
                    3,
                    303,
                    0
                ],
                "title": "Scaffolding Creativity: How Divergent and Convergent LLM Personas Shape\n  Human Machine Creative Problem-Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaffolding Creativity: How Divergent and Convergent LLM Personas Shape\n  Human Machine Creative Problem-Solving"
                },
                "summary": "Large language models (LLMs) are increasingly shaping creative work and\nproblem-solving; however, prior research suggests that they may diminish\nunassisted creativity. To address this tension, a coach-like LLM environment\nwas developed that embodies divergent and convergent thinking personas as two\ncomplementary processes. Effectiveness and user behavior were assessed through\na controlled experiment in which participants interacted with either persona,\nwhile a control group engaged with a standard LLM providing direct answers.\n  Notably, users' perceptions of which persona best supported their creativity\noften diverged from objective performance measures. Trait-based analyses\nrevealed that individual differences predict when people utilize divergent\nversus convergent personas, suggesting opportunities for adaptive sequencing.\nFurthermore, interaction patterns reflected the design thinking model,\ndemonstrating how persona-guided support shapes creative problem-solving.\n  Our findings provide design principles for creativity support systems that\nstrike a balance between exploration and convergence through persona-based\nguidance and personalization. These insights advance human-AI collaboration\ntools that scaffold rather than overshadow human creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly shaping creative work and\nproblem-solving; however, prior research suggests that they may diminish\nunassisted creativity. To address this tension, a coach-like LLM environment\nwas developed that embodies divergent and convergent thinking personas as two\ncomplementary processes. Effectiveness and user behavior were assessed through\na controlled experiment in which participants interacted with either persona,\nwhile a control group engaged with a standard LLM providing direct answers.\n  Notably, users' perceptions of which persona best supported their creativity\noften diverged from objective performance measures. Trait-based analyses\nrevealed that individual differences predict when people utilize divergent\nversus convergent personas, suggesting opportunities for adaptive sequencing.\nFurthermore, interaction patterns reflected the design thinking model,\ndemonstrating how persona-guided support shapes creative problem-solving.\n  Our findings provide design principles for creativity support systems that\nstrike a balance between exploration and convergence through persona-based\nguidance and personalization. These insights advance human-AI collaboration\ntools that scaffold rather than overshadow human creativity."
                },
                "authors": [
                    {
                        "name": "Alon Rosenbaum"
                    },
                    {
                        "name": "Yigal David"
                    },
                    {
                        "name": "Eran Kaufman"
                    },
                    {
                        "name": "Gilad Ravid"
                    },
                    {
                        "name": "Amit Ronen"
                    },
                    {
                        "name": "Assaf Krebs"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Krebs"
                },
                "author": "Assaf Krebs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26486v1",
                "updated": "2025-10-30T13:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks"
                },
                "summary": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    },
                    {
                        "name": "Guadalupe Correa-Cabrera"
                    }
                ],
                "author_detail": {
                    "name": "Guadalupe Correa-Cabrera"
                },
                "author": "Guadalupe Correa-Cabrera",
                "arxiv_comment": "Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26484v1",
                "updated": "2025-10-30T13:37:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    37,
                    58,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:37:58Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    37,
                    58,
                    3,
                    303,
                    0
                ],
                "title": "Bayesian Network Fusion of Large Language Models for Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Network Fusion of Large Language Models for Sentiment Analysis"
                },
                "summary": "Large language models (LLMs) continue to advance, with an increasing number\nof domain-specific variants tailored for specialised tasks. However, these\nmodels often lack transparency and explainability, can be costly to fine-tune,\nrequire substantial prompt engineering, yield inconsistent results across\ndomains, and impose significant adverse environmental impact due to their high\ncomputational demands. To address these challenges, we propose the Bayesian\nnetwork LLM fusion (BNLF) framework, which integrates predictions from three\nLLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic\nmechanism for sentiment analysis. BNLF performs late fusion by modelling the\nsentiment predictions from multiple LLMs as probabilistic nodes within a\nBayesian network. Evaluated across three human-annotated financial corpora with\ndistinct linguistic and contextual characteristics, BNLF demonstrates\nconsistent gains of about six percent in accuracy over the baseline LLMs,\nunderscoring its robustness to dataset variability and the effectiveness of\nprobabilistic fusion for interpretable sentiment classification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) continue to advance, with an increasing number\nof domain-specific variants tailored for specialised tasks. However, these\nmodels often lack transparency and explainability, can be costly to fine-tune,\nrequire substantial prompt engineering, yield inconsistent results across\ndomains, and impose significant adverse environmental impact due to their high\ncomputational demands. To address these challenges, we propose the Bayesian\nnetwork LLM fusion (BNLF) framework, which integrates predictions from three\nLLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic\nmechanism for sentiment analysis. BNLF performs late fusion by modelling the\nsentiment predictions from multiple LLMs as probabilistic nodes within a\nBayesian network. Evaluated across three human-annotated financial corpora with\ndistinct linguistic and contextual characteristics, BNLF demonstrates\nconsistent gains of about six percent in accuracy over the baseline LLMs,\nunderscoring its robustness to dataset variability and the effectiveness of\nprobabilistic fusion for interpretable sentiment classification."
                },
                "authors": [
                    {
                        "name": "Rasoul Amirzadeh"
                    },
                    {
                        "name": "Dhananjay Thiruvady"
                    },
                    {
                        "name": "Fatemeh Shiri"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Shiri"
                },
                "author": "Fatemeh Shiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26481v1",
                "updated": "2025-10-30T13:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    35,
                    32,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:35:32Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    35,
                    32,
                    3,
                    303,
                    0
                ],
                "title": "Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections"
                },
                "summary": "Large language models (LLMs) such as ChatGPT are increasingly integrated into\nhigh-stakes decision-making, yet little is known about their susceptibility to\nsocial influence. We conducted three preregistered conformity experiments with\nGPT-4o in a hiring context. In a baseline study, GPT consistently favored the\nsame candidate (Profile C), reported moderate expertise (M = 3.01) and high\ncertainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT\nfaced unanimous opposition from eight simulated partners and almost always\nconformed (99.9%), reporting lower certainty and significantly elevated\nself-reported informational and normative conformity (p < .001). In Study 2\n(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of\ndisagreement trials, reporting less certainty and more normative conformity.\nAcross studies, results demonstrate that GPT does not act as an independent\nobserver but adapts to perceived social consensus. These findings highlight\nrisks of treating LLMs as neutral decision aids and underline the need to\nelicit AI judgments prior to exposing them to human opinions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as ChatGPT are increasingly integrated into\nhigh-stakes decision-making, yet little is known about their susceptibility to\nsocial influence. We conducted three preregistered conformity experiments with\nGPT-4o in a hiring context. In a baseline study, GPT consistently favored the\nsame candidate (Profile C), reported moderate expertise (M = 3.01) and high\ncertainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT\nfaced unanimous opposition from eight simulated partners and almost always\nconformed (99.9%), reporting lower certainty and significantly elevated\nself-reported informational and normative conformity (p < .001). In Study 2\n(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of\ndisagreement trials, reporting less certainty and more normative conformity.\nAcross studies, results demonstrate that GPT does not act as an independent\nobserver but adapts to perceived social consensus. These findings highlight\nrisks of treating LLMs as neutral decision aids and underline the need to\nelicit AI judgments prior to exposing them to human opinions."
                },
                "authors": [
                    {
                        "name": "Clarissa Sabrina Arlinghaus"
                    },
                    {
                        "name": "Tristan Kenneweg"
                    },
                    {
                        "name": "Barbara Hammer"
                    },
                    {
                        "name": "Günter W. Maier"
                    }
                ],
                "author_detail": {
                    "name": "Günter W. Maier"
                },
                "author": "Günter W. Maier",
                "arxiv_comment": "5 pages, 5 figures, HAI 2025: Workshop on Socially Aware and\n  Cooperative Intelligent Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26480v1",
                "updated": "2025-10-30T13:34:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    34,
                    41,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:34:41Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    34,
                    41,
                    3,
                    303,
                    0
                ],
                "title": "Automated Extract Method Refactoring with Open-Source LLMs: A\n  Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Extract Method Refactoring with Open-Source LLMs: A\n  Comparative Study"
                },
                "summary": "Automating the Extract Method refactoring (EMR) remains challenging and\nlargely manual despite its importance in improving code readability and\nmaintainability. Recent advances in open-source, resource-efficient Large\nLanguage Models (LLMs) offer promising new approaches for automating such\nhigh-level tasks. In this work, we critically evaluate five state-of-the-art\nopen-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python\ncode. We systematically assess functional correctness and code quality using\nautomated metrics and investigate the impact of prompting strategies by\ncomparing one-shot prompting to a Recursive criticism and improvement (RCI)\napproach. RCI-based prompting consistently outperforms one-shot prompting in\ntest pass rates and refactoring quality. The best-performing models,\nDeepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)\nscores of 0.829 and 0.808, while reducing lines of code (LOC) per method from\n12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453\nand 3.294, respectively. A developer survey on RCI-generated refactorings shows\nover 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation\ncriteria. In contrast, the original code scored below neutral, particularly in\nreadability and maintainability, underscoring the benefits of automated\nrefactoring guided by quality prompts. While traditional metrics like CC and\nLOC provide useful signals, they often diverge from human judgments,\nemphasizing the need for human-in-the-loop evaluation. Our open-source\nbenchmark offers a foundation for future research on automated refactoring with\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating the Extract Method refactoring (EMR) remains challenging and\nlargely manual despite its importance in improving code readability and\nmaintainability. Recent advances in open-source, resource-efficient Large\nLanguage Models (LLMs) offer promising new approaches for automating such\nhigh-level tasks. In this work, we critically evaluate five state-of-the-art\nopen-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python\ncode. We systematically assess functional correctness and code quality using\nautomated metrics and investigate the impact of prompting strategies by\ncomparing one-shot prompting to a Recursive criticism and improvement (RCI)\napproach. RCI-based prompting consistently outperforms one-shot prompting in\ntest pass rates and refactoring quality. The best-performing models,\nDeepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)\nscores of 0.829 and 0.808, while reducing lines of code (LOC) per method from\n12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453\nand 3.294, respectively. A developer survey on RCI-generated refactorings shows\nover 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation\ncriteria. In contrast, the original code scored below neutral, particularly in\nreadability and maintainability, underscoring the benefits of automated\nrefactoring guided by quality prompts. While traditional metrics like CC and\nLOC provide useful signals, they often diverge from human judgments,\nemphasizing the need for human-in-the-loop evaluation. Our open-source\nbenchmark offers a foundation for future research on automated refactoring with\nLLMs."
                },
                "authors": [
                    {
                        "name": "Sivajeet Chand"
                    },
                    {
                        "name": "Melih Kilic"
                    },
                    {
                        "name": "Roland Würsching"
                    },
                    {
                        "name": "Sushant Kumar Pandey"
                    },
                    {
                        "name": "Alexander Pretschner"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Pretschner"
                },
                "author": "Alexander Pretschner",
                "arxiv_comment": "Accepted at AIware'25 - Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02843v2",
                "updated": "2025-10-30T13:29:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    29,
                    54,
                    3,
                    303,
                    0
                ],
                "published": "2025-07-03T17:52:27Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    52,
                    27,
                    3,
                    184,
                    0
                ],
                "title": "LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven Treatment Effect Estimation Under Inference Time Text\n  Confounding"
                },
                "summary": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Yuchen Ma"
                    },
                    {
                        "name": "Dennis Frauen"
                    },
                    {
                        "name": "Jonas Schweisthal"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26475v1",
                "updated": "2025-10-30T13:27:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    27,
                    42,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:27:42Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    27,
                    42,
                    3,
                    303,
                    0
                ],
                "title": "ReSpec: Towards Optimizing Speculative Decoding in Reinforcement\n  Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSpec: Towards Optimizing Speculative Decoding in Reinforcement\n  Learning Systems"
                },
                "summary": "Adapting large language models (LLMs) via reinforcement learning (RL) is\noften bottlenecked by the generation stage, which can consume over 75\\% of the\ntraining time. Speculative decoding (SD) accelerates autoregressive generation\nin serving systems, but its behavior under RL training remains largely\nunexplored. We identify three critical gaps that hinder the naive integration\nof SD into RL systems: diminishing speedups at large batch sizes, drafter\nstaleness under continual actor updates, and drafter-induced policy\ndegradation.\n  To address these gaps, we present ReSpec, a system that adapts SD to RL\nthrough three complementary mechanisms: dynamically tuning SD configurations,\nevolving the drafter via knowledge distillation, and weighting updates by\nrollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup\nwhile preserving reward convergence and training stability, providing a\npractical solution for efficient RL-based LLM adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting large language models (LLMs) via reinforcement learning (RL) is\noften bottlenecked by the generation stage, which can consume over 75\\% of the\ntraining time. Speculative decoding (SD) accelerates autoregressive generation\nin serving systems, but its behavior under RL training remains largely\nunexplored. We identify three critical gaps that hinder the naive integration\nof SD into RL systems: diminishing speedups at large batch sizes, drafter\nstaleness under continual actor updates, and drafter-induced policy\ndegradation.\n  To address these gaps, we present ReSpec, a system that adapts SD to RL\nthrough three complementary mechanisms: dynamically tuning SD configurations,\nevolving the drafter via knowledge distillation, and weighting updates by\nrollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup\nwhile preserving reward convergence and training stability, providing a\npractical solution for efficient RL-based LLM adaptation."
                },
                "authors": [
                    {
                        "name": "Qiaoling Chen"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Shenggui Li"
                    },
                    {
                        "name": "Guoteng Wang"
                    },
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yonggang Wen"
                    },
                    {
                        "name": "Siyuan Feng"
                    },
                    {
                        "name": "Tianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianwei Zhang"
                },
                "author": "Tianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12371v2",
                "updated": "2025-10-30T13:27:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    27,
                    7,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-18T11:28:17Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    11,
                    28,
                    17,
                    6,
                    138,
                    0
                ],
                "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional\n  Methods for Diverse Medical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional\n  Methods for Diverse Medical Tasks"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/."
                },
                "authors": [
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Ziyi He"
                    },
                    {
                        "name": "Haoran Hu"
                    },
                    {
                        "name": "Xiaochen Zheng"
                    },
                    {
                        "name": "Xichen Zhang"
                    },
                    {
                        "name": "Zixiang Wang"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Liantao Ma"
                    },
                    {
                        "name": "Lequan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lequan Yu"
                },
                "author": "Lequan Yu",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets & Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02091v2",
                "updated": "2025-10-30T13:22:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    22,
                    53,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-02T14:57:13Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    57,
                    13,
                    3,
                    275,
                    0
                ],
                "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning"
                },
                "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models."
                },
                "authors": [
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Keyu Wang"
                    },
                    {
                        "name": "PengXiang Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10603v2",
                "updated": "2025-10-30T13:13:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    13,
                    19,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-15T15:21:09Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    21,
                    9,
                    3,
                    135,
                    0
                ],
                "title": "Toward a Public and Secure Generative AI: A Comparative Analysis of Open\n  and Closed LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Public and Secure Generative AI: A Comparative Analysis of Open\n  and Closed LLMs"
                },
                "summary": "Generative artificial intelligence (Gen AI) systems represent a critical\ntechnology with far-reaching implications across multiple domains of society.\nHowever, their deployment entails a range of risks and challenges that require\ncareful evaluation. To date, there has been a lack of comprehensive,\ninterdisciplinary studies offering a systematic comparison between open-source\nand proprietary (closed) generative AI systems, particularly regarding their\nrespective advantages and drawbacks. This study aims to: i) critically evaluate\nand compare the characteristics, opportunities, and challenges of open and\nclosed generative AI models; and ii) propose foundational elements for the\ndevelopment of an Open, Public, and Safe Gen AI framework. As a methodology, we\nadopted a combined approach that integrates three methods: literature review,\ncritical analysis, and comparative analysis. The proposed framework outlines\nkey dimensions, openness, public governance, and security, as essential pillars\nfor shaping the future of trustworthy and inclusive Gen AI. Our findings reveal\nthat open models offer greater transparency, auditability, and flexibility,\nenabling independent scrutiny and bias mitigation. In contrast, closed systems\noften provide better technical support and ease of implementation, but at the\ncost of unequal access, accountability, and ethical oversight. The research\nalso highlights the importance of multi-stakeholder governance, environmental\nsustainability, and regulatory frameworks in ensuring responsible development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (Gen AI) systems represent a critical\ntechnology with far-reaching implications across multiple domains of society.\nHowever, their deployment entails a range of risks and challenges that require\ncareful evaluation. To date, there has been a lack of comprehensive,\ninterdisciplinary studies offering a systematic comparison between open-source\nand proprietary (closed) generative AI systems, particularly regarding their\nrespective advantages and drawbacks. This study aims to: i) critically evaluate\nand compare the characteristics, opportunities, and challenges of open and\nclosed generative AI models; and ii) propose foundational elements for the\ndevelopment of an Open, Public, and Safe Gen AI framework. As a methodology, we\nadopted a combined approach that integrates three methods: literature review,\ncritical analysis, and comparative analysis. The proposed framework outlines\nkey dimensions, openness, public governance, and security, as essential pillars\nfor shaping the future of trustworthy and inclusive Gen AI. Our findings reveal\nthat open models offer greater transparency, auditability, and flexibility,\nenabling independent scrutiny and bias mitigation. In contrast, closed systems\noften provide better technical support and ease of implementation, but at the\ncost of unequal access, accountability, and ethical oversight. The research\nalso highlights the importance of multi-stakeholder governance, environmental\nsustainability, and regulatory frameworks in ensuring responsible development."
                },
                "authors": [
                    {
                        "name": "Jorge Machado"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Machado"
                },
                "author": "Jorge Machado",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20458v2",
                "updated": "2025-10-30T13:09:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    9,
                    20,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-23T11:49:57Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    11,
                    49,
                    57,
                    3,
                    296,
                    0
                ],
                "title": "Ultralow-Cost magnetocaloric compound for Cryogenic Cooling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultralow-Cost magnetocaloric compound for Cryogenic Cooling"
                },
                "summary": "Cost-effective materials are essential for large-scale deployment. The\nemerging magnetocaloric hydrogen liquefaction technology could transform the\nliquid hydrogen industry due to its potential in achieving higher efficiency.\nMost studies of the cryogenic magnetocaloric effect (MCE) have focused on\nresource-critical rare-earth-based compounds. Here we report on an ionic\nmagnetocaloric compound FeCl$_2$ which is based on ultralow-cost elements, as a\ncandidate working material for hydrogen liquefaction. FeCl$_2$ shows both\ninverse and conventional MCE. From 0 to 1.5 T, the inverse effect yields a\npositive magnetic entropy change ($\\Delta S_T$) of about 5 J/kg/K near 20 K,\nthen declines toward zero at higher fields. In contrast, the conventional\n(negative) response strengthens with field. The $\\Delta S_T$ reaches 18.6\nJ/kg/K near 20 K in magnetic fields of 5 T. This value exceeds most light\nrare-earth-based compounds and approaches that of heavy rare-earth-based\ncompounds. In magnetic fields of 5 T, the adiabatic temperature change reaches\nabout 3.6 K. The large $\\Delta S_T$, along with the low cost of the elements in\nFeCl$_2$, are prerequisites for inexpensive industrial-scale production, giving\nthe prospect of a practical magnetocaloric candidate for hydrogen liquefaction\nin the 20 $\\sim$ 77 K temperature window.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-effective materials are essential for large-scale deployment. The\nemerging magnetocaloric hydrogen liquefaction technology could transform the\nliquid hydrogen industry due to its potential in achieving higher efficiency.\nMost studies of the cryogenic magnetocaloric effect (MCE) have focused on\nresource-critical rare-earth-based compounds. Here we report on an ionic\nmagnetocaloric compound FeCl$_2$ which is based on ultralow-cost elements, as a\ncandidate working material for hydrogen liquefaction. FeCl$_2$ shows both\ninverse and conventional MCE. From 0 to 1.5 T, the inverse effect yields a\npositive magnetic entropy change ($\\Delta S_T$) of about 5 J/kg/K near 20 K,\nthen declines toward zero at higher fields. In contrast, the conventional\n(negative) response strengthens with field. The $\\Delta S_T$ reaches 18.6\nJ/kg/K near 20 K in magnetic fields of 5 T. This value exceeds most light\nrare-earth-based compounds and approaches that of heavy rare-earth-based\ncompounds. In magnetic fields of 5 T, the adiabatic temperature change reaches\nabout 3.6 K. The large $\\Delta S_T$, along with the low cost of the elements in\nFeCl$_2$, are prerequisites for inexpensive industrial-scale production, giving\nthe prospect of a practical magnetocaloric candidate for hydrogen liquefaction\nin the 20 $\\sim$ 77 K temperature window."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Benjamin Theisel"
                    },
                    {
                        "name": "Yulia Klunnikova"
                    },
                    {
                        "name": "Konstantin Skokov"
                    },
                    {
                        "name": "Oliver Gutfleisch"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Gutfleisch"
                },
                "author": "Oliver Gutfleisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26461v1",
                "updated": "2025-10-30T13:07:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    7,
                    39,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:07:39Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    7,
                    39,
                    3,
                    303,
                    0
                ],
                "title": "Vectorized Context-Aware Embeddings for GAT-Based Collaborative\n  Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vectorized Context-Aware Embeddings for GAT-Based Collaborative\n  Filtering"
                },
                "summary": "Recommender systems often struggle with data sparsity and cold-start\nscenarios, limiting their ability to provide accurate suggestions for new or\ninfrequent users. This paper presents a Graph Attention Network (GAT) based\nCollaborative Filtering (CF) framework enhanced with Large Language Model (LLM)\ndriven context aware embeddings. Specifically, we generate concise textual user\nprofiles and unify item metadata (titles, genres, overviews) into rich textual\nembeddings, injecting these as initial node features in a bipartite user item\ngraph. To further optimize ranking performance, we introduce a hybrid loss\nfunction that combines Bayesian Personalized Ranking (BPR) with a cosine\nsimilarity term and robust negative sampling, ensuring explicit negative\nfeedback is distinguished from unobserved data. Experiments on the MovieLens\n100k and 1M datasets show consistent improvements over state-of-the-art\nbaselines in Precision, NDCG, and MAP while demonstrating robustness for users\nwith limited interaction history. Ablation studies confirm the critical role of\nLLM-augmented embeddings and the cosine similarity term in capturing nuanced\nsemantic relationships. Our approach effectively mitigates sparsity and\ncold-start limitations by integrating LLM-derived contextual understanding into\ngraph-based architectures. Future directions include balancing recommendation\naccuracy with coverage and diversity, and introducing fairness-aware\nconstraints and interpretability features to enhance system performance\nfurther.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems often struggle with data sparsity and cold-start\nscenarios, limiting their ability to provide accurate suggestions for new or\ninfrequent users. This paper presents a Graph Attention Network (GAT) based\nCollaborative Filtering (CF) framework enhanced with Large Language Model (LLM)\ndriven context aware embeddings. Specifically, we generate concise textual user\nprofiles and unify item metadata (titles, genres, overviews) into rich textual\nembeddings, injecting these as initial node features in a bipartite user item\ngraph. To further optimize ranking performance, we introduce a hybrid loss\nfunction that combines Bayesian Personalized Ranking (BPR) with a cosine\nsimilarity term and robust negative sampling, ensuring explicit negative\nfeedback is distinguished from unobserved data. Experiments on the MovieLens\n100k and 1M datasets show consistent improvements over state-of-the-art\nbaselines in Precision, NDCG, and MAP while demonstrating robustness for users\nwith limited interaction history. Ablation studies confirm the critical role of\nLLM-augmented embeddings and the cosine similarity term in capturing nuanced\nsemantic relationships. Our approach effectively mitigates sparsity and\ncold-start limitations by integrating LLM-derived contextual understanding into\ngraph-based architectures. Future directions include balancing recommendation\naccuracy with coverage and diversity, and introducing fairness-aware\nconstraints and interpretability features to enhance system performance\nfurther."
                },
                "authors": [
                    {
                        "name": "Danial Ebrat"
                    },
                    {
                        "name": "Sepideh Ahmadian"
                    },
                    {
                        "name": "Luis Rueda"
                    }
                ],
                "author_detail": {
                    "name": "Luis Rueda"
                },
                "author": "Luis Rueda",
                "arxiv_journal_ref": "Proceedings of the Canadian Conference on Artificial Intelligence\n  (2025). https://caiac.pubpub.org/pub/ji8cbfbp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26457v1",
                "updated": "2025-10-30T13:06:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    6,
                    11,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:06:11Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    6,
                    11,
                    3,
                    303,
                    0
                ],
                "title": "SecureReviewer: Enhancing Large Language Models for Secure Code Review\n  through Secure-aware Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecureReviewer: Enhancing Large Language Models for Secure Code Review\n  through Secure-aware Fine-tuning"
                },
                "summary": "Identifying and addressing security issues during the early phase of the\ndevelopment lifecycle is critical for mitigating the long-term negative impacts\non software systems. Code review serves as an effective practice that enables\ndevelopers to check their teammates' code before integration into the codebase.\nTo streamline the generation of review comments, various automated code review\napproaches have been proposed, where LLM-based methods have significantly\nadvanced the capabilities of automated review generation. However, existing\nmodels primarily focus on general-purpose code review, their effectiveness in\nidentifying and addressing security-related issues remains underexplored.\nMoreover, adapting existing code review approaches to target security issues\nfaces substantial challenges, including data scarcity and inadequate evaluation\nmetrics. To address these limitations, we propose SecureReviewer, a new\napproach designed for enhancing LLMs' ability to identify and resolve\nsecurity-related issues during code review. Specifically, we first construct a\ndataset tailored for training and evaluating secure code review capabilities.\nLeveraging this dataset, we fine-tune LLMs to generate code review comments\nthat can effectively identify security issues and provide fix suggestions with\nour proposed secure-aware fine-tuning strategy. To mitigate hallucination in\nLLMs and enhance the reliability of their outputs, we integrate the RAG\ntechnique, which grounds the generated comments in domain-specific security\nknowledge. Additionally, we introduce SecureBLEU, a new evaluation metric\ndesigned to assess the effectiveness of review comments in addressing security\nissues. Experimental results demonstrate that SecureReviewer outperforms\nstate-of-the-art baselines in both security issue detection accuracy and the\noverall quality and practical utility of generated review comments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying and addressing security issues during the early phase of the\ndevelopment lifecycle is critical for mitigating the long-term negative impacts\non software systems. Code review serves as an effective practice that enables\ndevelopers to check their teammates' code before integration into the codebase.\nTo streamline the generation of review comments, various automated code review\napproaches have been proposed, where LLM-based methods have significantly\nadvanced the capabilities of automated review generation. However, existing\nmodels primarily focus on general-purpose code review, their effectiveness in\nidentifying and addressing security-related issues remains underexplored.\nMoreover, adapting existing code review approaches to target security issues\nfaces substantial challenges, including data scarcity and inadequate evaluation\nmetrics. To address these limitations, we propose SecureReviewer, a new\napproach designed for enhancing LLMs' ability to identify and resolve\nsecurity-related issues during code review. Specifically, we first construct a\ndataset tailored for training and evaluating secure code review capabilities.\nLeveraging this dataset, we fine-tune LLMs to generate code review comments\nthat can effectively identify security issues and provide fix suggestions with\nour proposed secure-aware fine-tuning strategy. To mitigate hallucination in\nLLMs and enhance the reliability of their outputs, we integrate the RAG\ntechnique, which grounds the generated comments in domain-specific security\nknowledge. Additionally, we introduce SecureBLEU, a new evaluation metric\ndesigned to assess the effectiveness of review comments in addressing security\nissues. Experimental results demonstrate that SecureReviewer outperforms\nstate-of-the-art baselines in both security issue detection accuracy and the\noverall quality and practical utility of generated review comments."
                },
                "authors": [
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Simiao Liu"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "arxiv_comment": "Accepted by ICSE 2026. Code and data:\n  https://github.com/SIMIAO515/SecureReviewer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21513v2",
                "updated": "2025-10-30T13:03:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    3,
                    25,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-24T14:39:23Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    39,
                    23,
                    4,
                    297,
                    0
                ],
                "title": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair"
                },
                "summary": "Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems. To address this gap, we\nempirically compare ten individual LLMs from five families, and three ensembles\nof these LLMs across three software engineering benchmarks covering code\ngeneration and program repair. We assess the complementarity between models and\nthe performance gap between the best individual model and the ensembles. Next,\nwe evaluate various selection heuristics to identify correct solutions from an\nensemble's candidate pool. We find that the theoretical upperbound for an\nensemble's performance can be 83% above the best single model. Our results show\nthat consensus-based strategies for selecting solutions fall into a \"popularity\ntrap,\" amplifying common but incorrect outputs. In contrast, a diversity-based\nstrategy realizes up to 95% of this theoretical potential, and proves effective\neven in small two-model ensembles, enabling a cost-efficient way to enhance\nperformance by leveraging multiple LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's pursuit of a single Large Language Model (LMM) for all software\nengineering tasks is resource-intensive and overlooks the potential benefits of\ncomplementarity, where different models contribute unique strengths. However,\nthe degree to which coding LLMs complement each other and the best strategy for\nmaximizing an ensemble's potential are unclear, leaving practitioners without a\nclear path to move beyond single-model systems. To address this gap, we\nempirically compare ten individual LLMs from five families, and three ensembles\nof these LLMs across three software engineering benchmarks covering code\ngeneration and program repair. We assess the complementarity between models and\nthe performance gap between the best individual model and the ensembles. Next,\nwe evaluate various selection heuristics to identify correct solutions from an\nensemble's candidate pool. We find that the theoretical upperbound for an\nensemble's performance can be 83% above the best single model. Our results show\nthat consensus-based strategies for selecting solutions fall into a \"popularity\ntrap,\" amplifying common but incorrect outputs. In contrast, a diversity-based\nstrategy realizes up to 95% of this theoretical potential, and proves effective\neven in small two-model ensembles, enabling a cost-efficient way to enhance\nperformance by leveraging multiple LLMs."
                },
                "authors": [
                    {
                        "name": "Fernando Vallecillos-Ruiz"
                    },
                    {
                        "name": "Max Hort"
                    },
                    {
                        "name": "Leon Moonen"
                    }
                ],
                "author_detail": {
                    "name": "Leon Moonen"
                },
                "author": "Leon Moonen",
                "arxiv_comment": "Added Acknowledgments section and hyphenated last names",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26446v1",
                "updated": "2025-10-30T12:50:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    12,
                    50,
                    30,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T12:50:30Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    12,
                    50,
                    30,
                    3,
                    303,
                    0
                ],
                "title": "1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nlanguage comprehension and generation; however, their widespread adoption is\nconstrained by substantial bandwidth and computational demands. While pruning\nand low-rank approximation have each demonstrated promising performance\nindividually, their synergy for LLMs remains underexplored. We introduce\n\\underline{S}ynergistic \\underline{S}parse and \\underline{L}ow-Rank\n\\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths\nof both techniques: low-rank approximation compresses the model by retaining\nits essential structure with minimal information loss, whereas sparse\noptimization eliminates non-essential weights, preserving those crucial for\ngeneralization. Based on theoretical analysis, we first formulate the low-rank\napproximation and sparse optimization as a unified problem and solve it by\niterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models\n(7B-70B) show that SSLC, without any additional training steps, consistently\nsurpasses standalone methods, achieving state-of-the-arts results. Notably,\nSSLC compresses Qwen2.5 by 50\\% with no performance drop and achieves at least\n1.63$\\times$ speedup, offering a practical solution for efficient LLM\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nlanguage comprehension and generation; however, their widespread adoption is\nconstrained by substantial bandwidth and computational demands. While pruning\nand low-rank approximation have each demonstrated promising performance\nindividually, their synergy for LLMs remains underexplored. We introduce\n\\underline{S}ynergistic \\underline{S}parse and \\underline{L}ow-Rank\n\\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths\nof both techniques: low-rank approximation compresses the model by retaining\nits essential structure with minimal information loss, whereas sparse\noptimization eliminates non-essential weights, preserving those crucial for\ngeneralization. Based on theoretical analysis, we first formulate the low-rank\napproximation and sparse optimization as a unified problem and solve it by\niterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models\n(7B-70B) show that SSLC, without any additional training steps, consistently\nsurpasses standalone methods, achieving state-of-the-arts results. Notably,\nSSLC compresses Qwen2.5 by 50\\% with no performance drop and achieves at least\n1.63$\\times$ speedup, offering a practical solution for efficient LLM\ndeployment."
                },
                "authors": [
                    {
                        "name": "Zeliang Zong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Zheyang Li"
                    },
                    {
                        "name": "Wenming Tan"
                    },
                    {
                        "name": "Ye Ren"
                    },
                    {
                        "name": "Yiyan Zhai"
                    },
                    {
                        "name": "Jilin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jilin Hu"
                },
                "author": "Jilin Hu",
                "arxiv_comment": "15 pages, 6 figures, EMNLP 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26422v1",
                "updated": "2025-10-30T12:16:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    12,
                    16,
                    29,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T12:16:29Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    12,
                    16,
                    29,
                    3,
                    303,
                    0
                ],
                "title": "OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large\n  Language Models in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large\n  Language Models in Education"
                },
                "summary": "With the rapid development of large language models (LLMs), various LLM-based\nworks have been widely applied in educational fields. However, most existing\nLLMs and their benchmarks focus primarily on the knowledge dimension, largely\nneglecting the evaluation of cultivation capabilities that are essential for\nreal-world educational scenarios. Additionally, current benchmarks are often\nlimited to a single subject or question type, lacking sufficient diversity.\nThis issue is particularly prominent within the Chinese context. To address\nthis gap, we introduce OmniEduBench, a comprehensive Chinese educational\nbenchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.\nThe data is meticulously divided into two core dimensions: the knowledge\ndimension and the cultivation dimension, which contain 18.121K and 6.481K\nentries, respectively. Each dimension is further subdivided into 6 fine-grained\ncategories, covering a total of 61 different subjects (41 in the knowledge and\n20 in the cultivation). Furthermore, the dataset features a rich variety of\nquestion formats, including 11 common exam question types, providing a solid\nfoundation for comprehensively evaluating LLMs' capabilities in education.\nExtensive experiments on 11 mainstream open-source and closed-source LLMs\nreveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro\nsurpassed 60\\% accuracy, while in the cultivation dimension, the\nbest-performing model, QWQ, still trailed human intelligence by nearly 30\\%.\nThese results highlight the substantial room for improvement and underscore the\nchallenges of applying LLMs in education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), various LLM-based\nworks have been widely applied in educational fields. However, most existing\nLLMs and their benchmarks focus primarily on the knowledge dimension, largely\nneglecting the evaluation of cultivation capabilities that are essential for\nreal-world educational scenarios. Additionally, current benchmarks are often\nlimited to a single subject or question type, lacking sufficient diversity.\nThis issue is particularly prominent within the Chinese context. To address\nthis gap, we introduce OmniEduBench, a comprehensive Chinese educational\nbenchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.\nThe data is meticulously divided into two core dimensions: the knowledge\ndimension and the cultivation dimension, which contain 18.121K and 6.481K\nentries, respectively. Each dimension is further subdivided into 6 fine-grained\ncategories, covering a total of 61 different subjects (41 in the knowledge and\n20 in the cultivation). Furthermore, the dataset features a rich variety of\nquestion formats, including 11 common exam question types, providing a solid\nfoundation for comprehensively evaluating LLMs' capabilities in education.\nExtensive experiments on 11 mainstream open-source and closed-source LLMs\nreveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro\nsurpassed 60\\% accuracy, while in the cultivation dimension, the\nbest-performing model, QWQ, still trailed human intelligence by nearly 30\\%.\nThese results highlight the substantial room for improvement and underscore the\nchallenges of applying LLMs in education."
                },
                "authors": [
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Didi Zhu"
                    },
                    {
                        "name": "Xin Lin"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Kun Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Kuang"
                },
                "author": "Kun Kuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23127v2",
                "updated": "2025-10-30T12:09:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    12,
                    9,
                    18,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-27T09:03:21Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    9,
                    3,
                    21,
                    0,
                    300,
                    0
                ],
                "title": "Lost in Tokenization: Context as the Key to Unlocking Biomolecular\n  Understanding in Scientific LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Tokenization: Context as the Key to Unlocking Biomolecular\n  Understanding in Scientific LLMs"
                },
                "summary": "Scientific Large Language Models (Sci-LLMs) have emerged as a promising\nfrontier for accelerating biological discovery. However, these models face a\nfundamental challenge when processing raw biomolecular sequences: the\ntokenization dilemma. Whether treating sequences as a specialized language,\nrisking the loss of functional motif information, or as a separate modality,\nintroducing formidable alignment challenges, current strategies fundamentally\nlimit their reasoning capacity. We challenge this sequence-centric paradigm by\npositing that a more effective strategy is to provide Sci-LLMs with high-level\nstructured context derived from established bioinformatics tools, thereby\nbypassing the need to interpret low-level noisy sequence data directly. Through\na systematic comparison of leading Sci-LLMs on biological reasoning tasks, we\ntested three input modes: sequence-only, context-only, and a combination of\nboth. Our findings are striking: the context-only approach consistently and\nsubstantially outperforms all other modes. Even more revealing, the inclusion\nof the raw sequence alongside its high-level context consistently degrades\nperformance, indicating that raw sequences act as informational noise, even for\nmodels with specialized tokenization schemes. These results suggest that the\nprimary strength of existing Sci-LLMs lies not in their nascent ability to\ninterpret biomolecular syntax from scratch, but in their profound capacity for\nreasoning over structured, human-readable knowledge. Therefore, we argue for\nreframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines\nover expert knowledge. This work lays the foundation for a new class of hybrid\nscientific AI agents, repositioning the developmental focus from direct\nsequence interpretation towards high-level knowledge synthesis. The code is\navailable at https://github.com/opendatalab-raiser/CoKE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific Large Language Models (Sci-LLMs) have emerged as a promising\nfrontier for accelerating biological discovery. However, these models face a\nfundamental challenge when processing raw biomolecular sequences: the\ntokenization dilemma. Whether treating sequences as a specialized language,\nrisking the loss of functional motif information, or as a separate modality,\nintroducing formidable alignment challenges, current strategies fundamentally\nlimit their reasoning capacity. We challenge this sequence-centric paradigm by\npositing that a more effective strategy is to provide Sci-LLMs with high-level\nstructured context derived from established bioinformatics tools, thereby\nbypassing the need to interpret low-level noisy sequence data directly. Through\na systematic comparison of leading Sci-LLMs on biological reasoning tasks, we\ntested three input modes: sequence-only, context-only, and a combination of\nboth. Our findings are striking: the context-only approach consistently and\nsubstantially outperforms all other modes. Even more revealing, the inclusion\nof the raw sequence alongside its high-level context consistently degrades\nperformance, indicating that raw sequences act as informational noise, even for\nmodels with specialized tokenization schemes. These results suggest that the\nprimary strength of existing Sci-LLMs lies not in their nascent ability to\ninterpret biomolecular syntax from scratch, but in their profound capacity for\nreasoning over structured, human-readable knowledge. Therefore, we argue for\nreframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines\nover expert knowledge. This work lays the foundation for a new class of hybrid\nscientific AI agents, repositioning the developmental focus from direct\nsequence interpretation towards high-level knowledge synthesis. The code is\navailable at https://github.com/opendatalab-raiser/CoKE."
                },
                "authors": [
                    {
                        "name": "Kai Zhuang"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Yumou Liu"
                    },
                    {
                        "name": "Hanqun Cao"
                    },
                    {
                        "name": "Chunbin Gu"
                    },
                    {
                        "name": "Mengdi Liu"
                    },
                    {
                        "name": "Zhangyang Gao"
                    },
                    {
                        "name": "Zitong Jerry Wang"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Cheng Tan"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Tan"
                },
                "author": "Cheng Tan",
                "arxiv_comment": "38 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11329v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11329v4",
                "updated": "2025-10-30T11:34:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    34,
                    1,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-16T14:53:50Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    53,
                    50,
                    4,
                    136,
                    0
                ],
                "title": "TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM\n  Inference"
                },
                "summary": "Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on Hopper and Blackwell NVIDIA GPUs. These optimizations allow\nTokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover,\nour kernel enables the memory-bound RMSNorm to be overlapped with the other\nbatch's computation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed inference of large language models (LLMs) can introduce overheads\nof up to 20% even over GPUs connected via high-speed interconnects such as\nNVLink. Multiple techniques have been proposed to mitigate these overheads by\ndecomposing computations into finer-grained tasks and overlapping communication\nwith sub-tasks as they complete. However, fine-grained decomposition of a large\ncomputation into many smaller computations on GPUs results in overheads.\nFurthermore, the communication itself uses many streaming multiprocessors\n(SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a\nToken-Splitting technique that divides the tokens in the inference batch into\ntwo approximately equal subsets in a wave-aware manner. The communication of\none subset is then overlapped with the computation of the other. In addition,\nTokenWeave optimizes the order of the layer normalization computation with\nrespect to communication operations and implements a novel fused\nAllReduce--RMSNorm kernel that carefully leverages Multimem instruction support\navailable on Hopper and Blackwell NVIDIA GPUs. These optimizations allow\nTokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover,\nour kernel enables the memory-bound RMSNorm to be overlapped with the other\nbatch's computation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher\nthroughput across multiple models and workloads. In several settings,\nTokenWeave results in better performance compared to an equivalent model with\nall communication removed."
                },
                "authors": [
                    {
                        "name": "Raja Gond"
                    },
                    {
                        "name": "Nipun Kwatra"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    }
                ],
                "author_detail": {
                    "name": "Ramachandran Ramjee"
                },
                "author": "Ramachandran Ramjee",
                "arxiv_comment": "14 pages, 16 figures. For source code, see\n  https://github.com/microsoft/tokenweave. In version 2, Figure 6 shows\n  All-Reduce bandwidth instead of Reduce-Scatter. The Multimem Reduce-Scatter\n  bandwidth formula differs slightly from the ring-based version. Fixed x-ticks\n  in Figure 7",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11329v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11329v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26384v1",
                "updated": "2025-10-30T11:28:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    28,
                    58,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T11:28:58Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    28,
                    58,
                    3,
                    303,
                    0
                ],
                "title": "Scales++: Compute Efficient Evaluation Subset Selection with Cognitive\n  Scales Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scales++: Compute Efficient Evaluation Subset Selection with Cognitive\n  Scales Embeddings"
                },
                "summary": "The prohibitive cost of evaluating large language models (LLMs) on\ncomprehensive benchmarks necessitates the creation of small yet representative\ndata subsets (i.e., tiny benchmarks) that enable efficient assessment while\nretaining predictive fidelity. Current methods for this task operate under a\nmodel-centric paradigm, selecting benchmarking items based on the collective\nperformance of existing models. Such approaches are limited by large upfront\ncosts, an inability to immediately handle new benchmarks (`cold-start'), and\nthe fragile assumption that future models will share the failure patterns of\ntheir predecessors. In this work, we challenge this paradigm and propose a\nitem-centric approach to benchmark subset selection, arguing that selection\nshould be based on the intrinsic properties of the task items themselves,\nrather than on model-specific failure patterns. We instantiate this\nitem-centric efficient benchmarking approach via a novel method, Scales++,\nwhere data selection is based on the cognitive demands of the benchmark\nsamples. Empirically, we show Scales++ reduces the upfront selection cost by\nover 18x while achieving competitive predictive fidelity. On the Open LLM\nLeaderboard, using just a 0.5\\% data subset, we predict full benchmark scores\nwith a 2.9% mean absolute error. We demonstrate that this item-centric approach\nenables more efficient model evaluation without significant fidelity\ndegradation, while also providing better cold-start performance and more\ninterpretable benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prohibitive cost of evaluating large language models (LLMs) on\ncomprehensive benchmarks necessitates the creation of small yet representative\ndata subsets (i.e., tiny benchmarks) that enable efficient assessment while\nretaining predictive fidelity. Current methods for this task operate under a\nmodel-centric paradigm, selecting benchmarking items based on the collective\nperformance of existing models. Such approaches are limited by large upfront\ncosts, an inability to immediately handle new benchmarks (`cold-start'), and\nthe fragile assumption that future models will share the failure patterns of\ntheir predecessors. In this work, we challenge this paradigm and propose a\nitem-centric approach to benchmark subset selection, arguing that selection\nshould be based on the intrinsic properties of the task items themselves,\nrather than on model-specific failure patterns. We instantiate this\nitem-centric efficient benchmarking approach via a novel method, Scales++,\nwhere data selection is based on the cognitive demands of the benchmark\nsamples. Empirically, we show Scales++ reduces the upfront selection cost by\nover 18x while achieving competitive predictive fidelity. On the Open LLM\nLeaderboard, using just a 0.5\\% data subset, we predict full benchmark scores\nwith a 2.9% mean absolute error. We demonstrate that this item-centric approach\nenables more efficient model evaluation without significant fidelity\ndegradation, while also providing better cold-start performance and more\ninterpretable benchmarking."
                },
                "authors": [
                    {
                        "name": "Andrew M. Bean"
                    },
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Shengzhuang Chen"
                    },
                    {
                        "name": "Jonathan Richard Schwarz"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Richard Schwarz"
                },
                "author": "Jonathan Richard Schwarz",
                "arxiv_comment": "9 pages, 2 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25536v2",
                "updated": "2025-10-30T11:19:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    19,
                    24,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T14:00:42Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    0,
                    42,
                    2,
                    302,
                    0
                ],
                "title": "TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM\n  Persona Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM\n  Persona Simulation"
                },
                "summary": "Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are exhibiting emergent human-like abilities and\nare increasingly envisioned as the foundation for simulating an individual's\ncommunication style, behavioral tendencies, and personality traits. However,\ncurrent evaluations of LLM-based persona simulation remain limited: most rely\non synthetic dialogues, lack systematic frameworks, and lack analysis of the\ncapability requirement. To address these limitations, we introduce TwinVoice, a\ncomprehensive benchmark for assessing persona simulation across diverse\nreal-world contexts. TwinVoice encompasses three dimensions: Social Persona\n(public social interactions), Interpersonal Persona (private dialogues), and\nNarrative Persona (role-based expression). It further decomposes the evaluation\nof LLM performance into six fundamental capabilities, including opinion\nconsistency, memory recall, logical reasoning, lexical fidelity, persona tone,\nand syntactic style. Experimental results reveal that while advanced models\nachieve moderate accuracy in persona simulation, they still fall short of\ncapabilities such as syntactic style and memory recall. Consequently, the\naverage performance achieved by LLMs remains considerably below the human\nbaseline."
                },
                "authors": [
                    {
                        "name": "Bangde Du"
                    },
                    {
                        "name": "Minghao Guo"
                    },
                    {
                        "name": "Songming He"
                    },
                    {
                        "name": "Ziyi Ye"
                    },
                    {
                        "name": "Xi Zhu"
                    },
                    {
                        "name": "Weihang Su"
                    },
                    {
                        "name": "Shuqi Zhu"
                    },
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Yiqun Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yiqun Liu"
                },
                "author": "Yiqun Liu",
                "arxiv_comment": "Main paper: 11 pages, 3 figures, 6 tables. Appendix: 28 pages. Bangde\n  Du and Minghao Guo contributed equally. Corresponding authors: Ziyi Ye\n  (ziyiye@fudan.edu.cn), Qingyao Ai (aiqy@tsinghua.edu.cn)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24592v2",
                "updated": "2025-10-30T11:15:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    15,
                    27,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-28T16:22:54Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    16,
                    22,
                    54,
                    1,
                    301,
                    0
                ],
                "title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence\n  Optimization"
                },
                "summary": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 22.6 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoformalization, which translates natural language mathematics into\nmachine-verifiable formal statements, is critical for using formal mathematical\nreasoning to solve math problems stated in natural language. While Large\nLanguage Models can generate syntactically correct formal statements, they\noften fail to preserve the original problem's semantic intent. This limitation\narises from the LLM approaches' treating autoformalization as a simplistic\ntranslation task which lacks mechanisms for self-reflection and iterative\nrefinement that human experts naturally employ. To address these issues, we\npropose ReForm, a Reflective Autoformalization method that tightly integrates\nsemantic consistency evaluation into the autoformalization process. This\nenables the model to iteratively generate formal statements, assess its\nsemantic fidelity, and self-correct identified errors through progressive\nrefinement. To effectively train this reflective model, we introduce\nProspective Bounded Sequence Optimization (PBSO), which employs different\nrewards at different sequence positions to ensure that the model develops both\naccurate autoformalization and correct semantic validations, preventing\nsuperficial critiques that would undermine the purpose of reflection. Extensive\nexperiments across four autoformalization benchmarks demonstrate that ReForm\nachieves an average improvement of 22.6 percentage points over the strongest\nbaselines. To further ensure evaluation reliability, we introduce\nConsistencyCheck, a benchmark of 859 expert-annotated items that not only\nvalidates LLMs as judges but also reveals that autoformalization is inherently\ndifficult: even human experts produce semantic errors in up to 38.5% of cases."
                },
                "authors": [
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Jing Wu"
                    },
                    {
                        "name": "Xinjie Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ruihua Song"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "https://github.com/Chen-GX/ReForm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26374v1",
                "updated": "2025-10-30T11:15:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    15,
                    23,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T11:15:23Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    15,
                    23,
                    3,
                    303,
                    0
                ],
                "title": "BOTS: A Unified Framework for Bayesian Online Task Selection in LLM\n  Reinforcement Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOTS: A Unified Framework for Bayesian Online Task Selection in LLM\n  Reinforcement Finetuning"
                },
                "summary": "Reinforcement finetuning (RFT) is a key technique for aligning Large Language\nModels (LLMs) with human preferences and enhancing reasoning, yet its\neffectiveness is highly sensitive to which tasks are explored during training.\nUniform task sampling is inefficient, wasting computation on tasks that are\neither trivial or unsolvable, while existing task selection methods often\nsuffer from high rollout costs, poor adaptivity, or incomplete evidence. We\nintroduce \\textbf{BOTS}, a unified framework for \\textbf{B}ayesian\n\\textbf{O}nline \\textbf{T}ask \\textbf{S}election in LLM reinforcement\nfinetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior\nestimates of task difficulty as the model evolves. It jointly incorporates\n\\emph{explicit evidence} from direct evaluations of selected tasks and\n\\emph{implicit evidence} inferred from these evaluations for unselected tasks,\nwith Thompson sampling ensuring a principled balance between exploration and\nexploitation. To make implicit evidence practical, we instantiate it with an\nultra-light interpolation-based plug-in that estimates difficulties of\nunevaluated tasks without extra rollouts, adding negligible overhead.\nEmpirically, across diverse domains and LLM scales, BOTS consistently improves\ndata efficiency and performance over baselines and ablations, providing a\npractical and extensible solution for dynamic task selection in RFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement finetuning (RFT) is a key technique for aligning Large Language\nModels (LLMs) with human preferences and enhancing reasoning, yet its\neffectiveness is highly sensitive to which tasks are explored during training.\nUniform task sampling is inefficient, wasting computation on tasks that are\neither trivial or unsolvable, while existing task selection methods often\nsuffer from high rollout costs, poor adaptivity, or incomplete evidence. We\nintroduce \\textbf{BOTS}, a unified framework for \\textbf{B}ayesian\n\\textbf{O}nline \\textbf{T}ask \\textbf{S}election in LLM reinforcement\nfinetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior\nestimates of task difficulty as the model evolves. It jointly incorporates\n\\emph{explicit evidence} from direct evaluations of selected tasks and\n\\emph{implicit evidence} inferred from these evaluations for unselected tasks,\nwith Thompson sampling ensuring a principled balance between exploration and\nexploitation. To make implicit evidence practical, we instantiate it with an\nultra-light interpolation-based plug-in that estimates difficulties of\nunevaluated tasks without extra rollouts, adding negligible overhead.\nEmpirically, across diverse domains and LLM scales, BOTS consistently improves\ndata efficiency and performance over baselines and ablations, providing a\npractical and extensible solution for dynamic task selection in RFT."
                },
                "authors": [
                    {
                        "name": "Qianli Shen"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Yilun Huang"
                    },
                    {
                        "name": "Zhenqing Ling"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24817v2",
                "updated": "2025-10-30T11:13:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    13,
                    33,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-28T10:06:49Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    6,
                    49,
                    1,
                    301,
                    0
                ],
                "title": "Towards a Method for Synthetic Generation of Persons with Aphasia\n  Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Method for Synthetic Generation of Persons with Aphasia\n  Transcripts"
                },
                "summary": "In aphasia research, Speech-Language Pathologists (SLPs) devote extensive\ntime to manually coding speech samples using Correct Information Units (CIUs),\na measure of how informative an individual sample of speech is. Developing\nautomated systems to recognize aphasic language is limited by data scarcity.\nFor example, only about 600 transcripts are available in AphasiaBank yet\nbillions of tokens are used to train large language models (LLMs). In the\nbroader field of machine learning (ML), researchers increasingly turn to\nsynthetic data when such are sparse. Therefore, this study constructs and\nvalidates two methods to generate synthetic transcripts of the AphasiaBank Cat\nRescue picture description task. One method leverages a procedural programming\napproach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct\nLLMs. The methods generate transcripts across four severity levels (Mild,\nModerate, Severe, Very Severe) through word dropping, filler insertion, and\nparaphasia substitution. Overall, we found, compared to human-elicited\ntranscripts, Mistral 7b Instruct best captures key aspects of linguistic\ndegradation observed in aphasia, showing realistic directional changes in NDW,\nword count, and word length amongst the synthetic generation methods. Based on\nthe results, future work should plan to create a larger dataset, fine-tune\nmodels for better aphasic representation, and have SLPs assess the realism and\nusefulness of the synthetic transcripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In aphasia research, Speech-Language Pathologists (SLPs) devote extensive\ntime to manually coding speech samples using Correct Information Units (CIUs),\na measure of how informative an individual sample of speech is. Developing\nautomated systems to recognize aphasic language is limited by data scarcity.\nFor example, only about 600 transcripts are available in AphasiaBank yet\nbillions of tokens are used to train large language models (LLMs). In the\nbroader field of machine learning (ML), researchers increasingly turn to\nsynthetic data when such are sparse. Therefore, this study constructs and\nvalidates two methods to generate synthetic transcripts of the AphasiaBank Cat\nRescue picture description task. One method leverages a procedural programming\napproach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct\nLLMs. The methods generate transcripts across four severity levels (Mild,\nModerate, Severe, Very Severe) through word dropping, filler insertion, and\nparaphasia substitution. Overall, we found, compared to human-elicited\ntranscripts, Mistral 7b Instruct best captures key aspects of linguistic\ndegradation observed in aphasia, showing realistic directional changes in NDW,\nword count, and word length amongst the synthetic generation methods. Based on\nthe results, future work should plan to create a larger dataset, fine-tune\nmodels for better aphasic representation, and have SLPs assess the realism and\nusefulness of the synthetic transcripts."
                },
                "authors": [
                    {
                        "name": "Jason M. Pittman"
                    },
                    {
                        "name": "Anton Phillips Jr."
                    },
                    {
                        "name": "Yesenia Medina-Santos"
                    },
                    {
                        "name": "Brielle C. Stark"
                    }
                ],
                "author_detail": {
                    "name": "Brielle C. Stark"
                },
                "author": "Brielle C. Stark",
                "arxiv_comment": "19 pages, 1 figure, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26354v1",
                "updated": "2025-10-30T11:05:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    5,
                    36,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T11:05:36Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    5,
                    36,
                    3,
                    303,
                    0
                ],
                "title": "On the Role of Context for Discourse Relation Classification in\n  Scientific Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Role of Context for Discourse Relation Classification in\n  Scientific Writing"
                },
                "summary": "With the increasing use of generative Artificial Intelligence (AI) methods to\nsupport science workflows, we are interested in the use of discourse-level\ninformation to find supporting evidence for AI generated scientific claims. A\nfirst step towards this objective is to examine the task of inferring discourse\nstructure in scientific writing.\n  In this work, we present a preliminary investigation of pretrained language\nmodel (PLM) and Large Language Model (LLM) approaches for Discourse Relation\nClassification (DRC), focusing on scientific publications, an under-studied\ngenre for this task. We examine how context can help with the DRC task, with\nour experiments showing that context, as defined by discourse structure, is\ngenerally helpful. We also present an analysis of which scientific discourse\nrelation types might benefit most from context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing use of generative Artificial Intelligence (AI) methods to\nsupport science workflows, we are interested in the use of discourse-level\ninformation to find supporting evidence for AI generated scientific claims. A\nfirst step towards this objective is to examine the task of inferring discourse\nstructure in scientific writing.\n  In this work, we present a preliminary investigation of pretrained language\nmodel (PLM) and Large Language Model (LLM) approaches for Discourse Relation\nClassification (DRC), focusing on scientific publications, an under-studied\ngenre for this task. We examine how context can help with the DRC task, with\nour experiments showing that context, as defined by discourse structure, is\ngenerally helpful. We also present an analysis of which scientific discourse\nrelation types might benefit most from context."
                },
                "authors": [
                    {
                        "name": "Stephen Wan"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Michael Strube"
                    }
                ],
                "author_detail": {
                    "name": "Michael Strube"
                },
                "author": "Michael Strube",
                "arxiv_comment": "Accepted at Joint Sixth Workshop on Computational Approaches to\n  Discourse, Context and Document-Level Inferences (CODI 2025) and Eighth\n  Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26353v1",
                "updated": "2025-10-30T11:05:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    5,
                    15,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T11:05:15Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    5,
                    15,
                    3,
                    303,
                    0
                ],
                "title": "Towards Explainable and Reliable AI in Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Explainable and Reliable AI in Finance"
                },
                "summary": "Financial forecasting increasingly uses large neural network models, but\ntheir opacity raises challenges for trust and regulatory compliance. We present\nseveral approaches to explainable and reliable AI in finance. \\emph{First}, we\ndescribe how Time-LLM, a time series foundation model, uses a prompt to avoid a\nwrong directional forecast. \\emph{Second}, we show that combining foundation\nmodels for time series forecasting with a reliability estimator can filter our\nunreliable predictions. \\emph{Third}, we argue for symbolic reasoning encoding\ndomain rules for transparent justification. These approaches shift emphasize\nexecuting only forecasts that are both reliable and explainable. Experiments on\nequity and cryptocurrency data show that the architecture reduces false\npositives and supports selective execution. By integrating predictive\nperformance with reliability estimation and rule-based reasoning, our framework\nadvances transparent and auditable financial AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial forecasting increasingly uses large neural network models, but\ntheir opacity raises challenges for trust and regulatory compliance. We present\nseveral approaches to explainable and reliable AI in finance. \\emph{First}, we\ndescribe how Time-LLM, a time series foundation model, uses a prompt to avoid a\nwrong directional forecast. \\emph{Second}, we show that combining foundation\nmodels for time series forecasting with a reliability estimator can filter our\nunreliable predictions. \\emph{Third}, we argue for symbolic reasoning encoding\ndomain rules for transparent justification. These approaches shift emphasize\nexecuting only forecasts that are both reliable and explainable. Experiments on\nequity and cryptocurrency data show that the architecture reduces false\npositives and supports selective execution. By integrating predictive\nperformance with reliability estimation and rule-based reasoning, our framework\nadvances transparent and auditable financial AI systems."
                },
                "authors": [
                    {
                        "name": "Albi Isufaj"
                    },
                    {
                        "name": "Pablo Mollá"
                    },
                    {
                        "name": "Helmut Prendinger"
                    }
                ],
                "author_detail": {
                    "name": "Helmut Prendinger"
                },
                "author": "Helmut Prendinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26352v1",
                "updated": "2025-10-30T11:04:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    4,
                    15,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T11:04:15Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    11,
                    4,
                    15,
                    3,
                    303,
                    0
                ],
                "title": "The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic\n  Teams for Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic\n  Teams for Multi-Agent Collaboration"
                },
                "summary": "While a multi-agent approach based on large language models (LLMs) represents\na promising strategy to surpass the capabilities of single models, its success\nis critically dependent on synergistic team composition. However, forming\noptimal teams is a significant challenge, as the inherent opacity of most\nmodels obscures the internal characteristics necessary for effective\ncollaboration. In this paper, we propose an interaction-centric framework for\nautomatic team composition that does not require any prior knowledge including\ntheir internal architectures, training data, or task performances. Our method\nconstructs a \"language model graph\" that maps relationships between models from\nthe semantic coherence of pairwise conversations, and then applies community\ndetection to identify synergistic model clusters. Our experiments with diverse\nLLMs demonstrate that the proposed method discovers functionally coherent\ngroups that reflect their latent specializations. Priming conversations with\nspecific topics identified synergistic teams which outperform random baselines\non downstream benchmarks and achieve comparable accuracy to that of\nmanually-curated teams based on known model specializations. Our findings\nprovide a new basis for the automated design of collaborative multi-agent LLM\nteams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While a multi-agent approach based on large language models (LLMs) represents\na promising strategy to surpass the capabilities of single models, its success\nis critically dependent on synergistic team composition. However, forming\noptimal teams is a significant challenge, as the inherent opacity of most\nmodels obscures the internal characteristics necessary for effective\ncollaboration. In this paper, we propose an interaction-centric framework for\nautomatic team composition that does not require any prior knowledge including\ntheir internal architectures, training data, or task performances. Our method\nconstructs a \"language model graph\" that maps relationships between models from\nthe semantic coherence of pairwise conversations, and then applies community\ndetection to identify synergistic model clusters. Our experiments with diverse\nLLMs demonstrate that the proposed method discovers functionally coherent\ngroups that reflect their latent specializations. Priming conversations with\nspecific topics identified synergistic teams which outperform random baselines\non downstream benchmarks and achieve comparable accuracy to that of\nmanually-curated teams based on known model specializations. Our findings\nprovide a new basis for the automated design of collaborative multi-agent LLM\nteams."
                },
                "authors": [
                    {
                        "name": "Kotaro Furuya"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    }
                ],
                "author_detail": {
                    "name": "Yuichi Kitagawa"
                },
                "author": "Yuichi Kitagawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26345v1",
                "updated": "2025-10-30T10:52:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    52,
                    43,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T10:52:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    52,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MisSynth: Improving MISSCI Logical Fallacies Classification with\n  Synthetic Data"
                },
                "summary": "Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-related misinformation is very prevalent and potentially harmful. It\nis difficult to identify, especially when claims distort or misinterpret\nscientific findings. We investigate the impact of synthetic data generation and\nlightweight fine-tuning techniques on the ability of large language models\n(LLMs) to recognize fallacious arguments using the MISSCI dataset and\nframework. In this work, we propose MisSynth, a pipeline that applies\nretrieval-augmented generation (RAG) to produce synthetic fallacy samples,\nwhich are then used to fine-tune an LLM model. Our results show substantial\naccuracy gains with fine-tuned models compared to vanilla baselines. For\ninstance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score\nabsolute improvement on the MISSCI test split over its vanilla baseline. We\ndemonstrate that introducing synthetic fallacy data to augment limited\nannotated resources can significantly enhance zero-shot LLM classification\nperformance on real-world scientific misinformation tasks, even with limited\ncomputational resources. The code and synthetic dataset are available on\nhttps://github.com/mxpoliakov/MisSynth."
                },
                "authors": [
                    {
                        "name": "Mykhailo Poliakov"
                    },
                    {
                        "name": "Nadiya Shvai"
                    }
                ],
                "author_detail": {
                    "name": "Nadiya Shvai"
                },
                "author": "Nadiya Shvai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25409v2",
                "updated": "2025-10-30T10:48:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    48,
                    5,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T11:27:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    27,
                    8,
                    2,
                    302,
                    0
                ],
                "title": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic\n  Domains"
                },
                "summary": "The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models(LLMs) has intensified the need\nfor domain and culture specific evaluation. Existing benchmarks are largely\nAnglocentric and domain-agnostic, limiting their applicability to India-centric\ncontexts. To address this gap, we introduce BhashaBench V1, the first\ndomain-specific, multi-task, bilingual benchmark focusing on critical Indic\nknowledge systems. BhashaBench V1 contains 74,166 meticulously curated\nquestion-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from\nauthentic government and domain-specific exams. It spans four major domains:\nAgriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and\ncovering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs\nreveals significant domain and language specific performance gaps, with\nespecially large disparities in low-resource domains. For instance, GPT-4o\nachieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models\nconsistently perform better on English content compared to Hindi across all\ndomains. Subdomain-level analysis shows that areas such as Cyber Law,\nInternational Finance perform relatively well, while Panchakarma, Seed Science,\nand Human Rights remain notably weak. BhashaBench V1 provides a comprehensive\ndataset for evaluating large language models across India's diverse knowledge\ndomains. It enables assessment of models' ability to integrate domain-specific\nknowledge with bilingual understanding. All code, benchmarks, and resources are\npublicly available to support open research."
                },
                "authors": [
                    {
                        "name": "Vijay Devane"
                    },
                    {
                        "name": "Mohd Nauman"
                    },
                    {
                        "name": "Bhargav Patel"
                    },
                    {
                        "name": "Aniket Mahendra Wakchoure"
                    },
                    {
                        "name": "Yogeshkumar Sant"
                    },
                    {
                        "name": "Shyam Pawar"
                    },
                    {
                        "name": "Viraj Thakur"
                    },
                    {
                        "name": "Ananya Godse"
                    },
                    {
                        "name": "Sunil Patra"
                    },
                    {
                        "name": "Neha Maurya"
                    },
                    {
                        "name": "Suraj Racha"
                    },
                    {
                        "name": "Nitish Kamal Singh"
                    },
                    {
                        "name": "Ajay Nagpal"
                    },
                    {
                        "name": "Piyush Sawarkar"
                    },
                    {
                        "name": "Kundeshwar Vijayrao Pundalik"
                    },
                    {
                        "name": "Rohit Saluja"
                    },
                    {
                        "name": "Ganesh Ramakrishnan"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Ramakrishnan"
                },
                "author": "Ganesh Ramakrishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26339v1",
                "updated": "2025-10-30T10:46:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    46,
                    28,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T10:46:28Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    46,
                    28,
                    3,
                    303,
                    0
                ],
                "title": "GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and\n  High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and\n  High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?"
                },
                "summary": "Image super-resolution(SR) is fundamental to many vision system-from\nsurveillance and autonomy to document analysis and retail analytics-because\nrecovering high-frequency details, especially scene-text, enables reliable\ndownstream perception. Scene-text, i.e., text embedded in natural images such\nas signs, product labels, and storefronts, often carries the most actionable\ninformation; when characters are blurred or hallucinated, optical character\nrecognition(OCR) and subsequent decisions fail even if the rest of the image\nappears sharp. Yet previous SR research has often been tuned to distortion\n(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that\nare largely insensitive to character-level errors. Furthermore, studies that do\naddress text SR often focus on simplified benchmarks with isolated characters,\noverlooking the challenges of text within complex natural scenes. As a result,\nscene-text is effectively treated as generic texture. For SR to be effective in\npractical deployments, it is therefore essential to explicitly optimize for\nboth text legibility and perceptual quality. We present GLYPH-SR, a\nvision-language-guided diffusion framework that aims to achieve both objectives\njointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by\nOCR data, and a ping-pong scheduler that alternates between text- and\nscene-centric guidance. To enable targeted text restoration, we train these\ncomponents on a synthetic corpus while keeping the main SR branch frozen.\nAcross SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by\nup to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)\nwhile maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed\nto satisfy both objectives simultaneously-high readability and high visual\nrealism-delivering SR that looks right and reds right.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image super-resolution(SR) is fundamental to many vision system-from\nsurveillance and autonomy to document analysis and retail analytics-because\nrecovering high-frequency details, especially scene-text, enables reliable\ndownstream perception. Scene-text, i.e., text embedded in natural images such\nas signs, product labels, and storefronts, often carries the most actionable\ninformation; when characters are blurred or hallucinated, optical character\nrecognition(OCR) and subsequent decisions fail even if the rest of the image\nappears sharp. Yet previous SR research has often been tuned to distortion\n(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that\nare largely insensitive to character-level errors. Furthermore, studies that do\naddress text SR often focus on simplified benchmarks with isolated characters,\noverlooking the challenges of text within complex natural scenes. As a result,\nscene-text is effectively treated as generic texture. For SR to be effective in\npractical deployments, it is therefore essential to explicitly optimize for\nboth text legibility and perceptual quality. We present GLYPH-SR, a\nvision-language-guided diffusion framework that aims to achieve both objectives\njointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by\nOCR data, and a ping-pong scheduler that alternates between text- and\nscene-centric guidance. To enable targeted text restoration, we train these\ncomponents on a synthetic corpus while keeping the main SR branch frozen.\nAcross SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by\nup to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)\nwhile maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed\nto satisfy both objectives simultaneously-high readability and high visual\nrealism-delivering SR that looks right and reds right."
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Seungjae Ham"
                    },
                    {
                        "name": "Kangwoo Kim"
                    },
                    {
                        "name": "Yeokyoung Yoon"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang",
                "arxiv_comment": "11 pages, 6 figures. Includes supplementary material. Under review as\n  a conference paper at ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26336v1",
                "updated": "2025-10-30T10:43:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    43,
                    40,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T10:43:40Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    43,
                    40,
                    3,
                    303,
                    0
                ],
                "title": "From Amateur to Master: Infusing Knowledge into LLMs via Automated\n  Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Amateur to Master: Infusing Knowledge into LLMs via Automated\n  Curriculum Learning"
                },
                "summary": "Large Language Models (LLMs) excel at general tasks but underperform in\nspecialized domains like economics and psychology, which require deep,\nprincipled understanding. To address this, we introduce ACER (Automated\nCurriculum-Enhanced Regimen) that transforms generalist models into domain\nexperts without sacrificing their broad capabilities. ACER first synthesizes a\ncomprehensive, textbook-style curriculum by generating a table of contents for\na subject and then creating question-answer (QA) pairs guided by Bloom's\ntaxonomy. This ensures systematic topic coverage and progressively increasing\ndifficulty. The resulting synthetic corpus is used for continual pretraining\nwith an interleaved curriculum schedule, aligning learning across both content\nand cognitive dimensions.\n  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized\nMMLU subsets. In challenging domains like microeconomics, where baselines\nstruggle, ACER boosts accuracy by 5 percentage points. Across all target\ndomains, we observe a consistent macro-average improvement of 3 percentage\npoints. Notably, ACER not only prevents catastrophic forgetting but also\nfacilitates positive cross-domain knowledge transfer, improving performance on\nnon-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on\nknowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,\nwhile maintaining stable performance on general reasoning tasks. Our results\ndemonstrate that ACER offers a scalable and effective recipe for closing\ncritical domain gaps in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at general tasks but underperform in\nspecialized domains like economics and psychology, which require deep,\nprincipled understanding. To address this, we introduce ACER (Automated\nCurriculum-Enhanced Regimen) that transforms generalist models into domain\nexperts without sacrificing their broad capabilities. ACER first synthesizes a\ncomprehensive, textbook-style curriculum by generating a table of contents for\na subject and then creating question-answer (QA) pairs guided by Bloom's\ntaxonomy. This ensures systematic topic coverage and progressively increasing\ndifficulty. The resulting synthetic corpus is used for continual pretraining\nwith an interleaved curriculum schedule, aligning learning across both content\nand cognitive dimensions.\n  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized\nMMLU subsets. In challenging domains like microeconomics, where baselines\nstruggle, ACER boosts accuracy by 5 percentage points. Across all target\ndomains, we observe a consistent macro-average improvement of 3 percentage\npoints. Notably, ACER not only prevents catastrophic forgetting but also\nfacilitates positive cross-domain knowledge transfer, improving performance on\nnon-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on\nknowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,\nwhile maintaining stable performance on general reasoning tasks. Our results\ndemonstrate that ACER offers a scalable and effective recipe for closing\ncritical domain gaps in LLMs."
                },
                "authors": [
                    {
                        "name": "Nishit Neema"
                    },
                    {
                        "name": "Srinjoy Mukherjee"
                    },
                    {
                        "name": "Sapan Shah"
                    },
                    {
                        "name": "Gokul Ramakrishnan"
                    },
                    {
                        "name": "Ganesh Venkatesh"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Venkatesh"
                },
                "author": "Ganesh Venkatesh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07500v2",
                "updated": "2025-10-30T10:28:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    28,
                    19,
                    3,
                    303,
                    0
                ],
                "published": "2025-06-09T07:25:51Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    7,
                    25,
                    51,
                    0,
                    160,
                    0
                ],
                "title": "Mind the Gap: Removing the Discretization Gap in Differentiable Logic\n  Gate Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Removing the Discretization Gap in Differentiable Logic\n  Gate Networks"
                },
                "summary": "Modern neural networks demonstrate state-of-the-art performance on numerous\nexisting benchmarks; however, their high computational requirements and energy\nconsumption prompt researchers to seek more efficient solutions for real-world\ndeployment. Logic gate networks (LGNs) learns a large network of logic gates\nfor efficient image classification. However, learning a network that can solve\na simple problem like CIFAR-10 can take days to weeks to train. Even then,\nalmost half of the network remains unused, causing a discretization gap. This\ndiscretization gap hinders real-world deployment of LGNs, as the performance\ndrop between training and inference negatively impacts accuracy. We inject\nGumbel noise with a straight-through estimator during training to significantly\nspeed up training, improve neuron utilization, and decrease the discretization\ngap. We theoretically show that this results from implicit Hessian\nregularization, which improves the convergence properties of LGNs. We train\nnetworks $4.5 \\times$ faster in wall-clock time, reduce the discretization gap\nby $98\\%$, and reduce the number of unused gates by $100\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern neural networks demonstrate state-of-the-art performance on numerous\nexisting benchmarks; however, their high computational requirements and energy\nconsumption prompt researchers to seek more efficient solutions for real-world\ndeployment. Logic gate networks (LGNs) learns a large network of logic gates\nfor efficient image classification. However, learning a network that can solve\na simple problem like CIFAR-10 can take days to weeks to train. Even then,\nalmost half of the network remains unused, causing a discretization gap. This\ndiscretization gap hinders real-world deployment of LGNs, as the performance\ndrop between training and inference negatively impacts accuracy. We inject\nGumbel noise with a straight-through estimator during training to significantly\nspeed up training, improve neuron utilization, and decrease the discretization\ngap. We theoretically show that this results from implicit Hessian\nregularization, which improves the convergence properties of LGNs. We train\nnetworks $4.5 \\times$ faster in wall-clock time, reduce the discretization gap\nby $98\\%$, and reduce the number of unused gates by $100\\%$."
                },
                "authors": [
                    {
                        "name": "Shakir Yousefi"
                    },
                    {
                        "name": "Andreas Plesner"
                    },
                    {
                        "name": "Till Aczel"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    }
                ],
                "author_detail": {
                    "name": "Roger Wattenhofer"
                },
                "author": "Roger Wattenhofer",
                "arxiv_comment": "Accepted to NeurIPS 2025 (main track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26328v1",
                "updated": "2025-10-30T10:27:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    27,
                    11,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T10:27:11Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    27,
                    11,
                    3,
                    303,
                    0
                ],
                "title": "Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt\n  Injections",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt\n  Injections"
                },
                "summary": "Enabling continual learning in LLMs remains a key unresolved research\nchallenge. In a recent announcement, a frontier LLM company made a step towards\nthis by introducing Agent Skills, a framework that equips agents with new\nknowledge based on instructions stored in simple markdown files. Although Agent\nSkills can be a very useful tool, we show that they are fundamentally insecure,\nsince they enable trivially simple prompt injections. We demonstrate how to\nhide malicious instructions in long Agent Skill files and referenced scripts to\nexfiltrate sensitive data, such as internal files or passwords. Importantly, we\nshow how to bypass system-level guardrails of a popular coding agent: a benign,\ntask-specific approval with the \"Don't ask again\" option can carry over to\nclosely related but harmful actions. Overall, we conclude that despite ongoing\nresearch efforts and scaling model capabilities, frontier LLMs remain\nvulnerable to very simple prompt injections in realistic scenarios. Our code is\navailable at https://github.com/aisa-group/promptinject-agent-skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling continual learning in LLMs remains a key unresolved research\nchallenge. In a recent announcement, a frontier LLM company made a step towards\nthis by introducing Agent Skills, a framework that equips agents with new\nknowledge based on instructions stored in simple markdown files. Although Agent\nSkills can be a very useful tool, we show that they are fundamentally insecure,\nsince they enable trivially simple prompt injections. We demonstrate how to\nhide malicious instructions in long Agent Skill files and referenced scripts to\nexfiltrate sensitive data, such as internal files or passwords. Importantly, we\nshow how to bypass system-level guardrails of a popular coding agent: a benign,\ntask-specific approval with the \"Don't ask again\" option can carry over to\nclosely related but harmful actions. Overall, we conclude that despite ongoing\nresearch efforts and scaling model capabilities, frontier LLMs remain\nvulnerable to very simple prompt injections in realistic scenarios. Our code is\navailable at https://github.com/aisa-group/promptinject-agent-skills."
                },
                "authors": [
                    {
                        "name": "David Schmotz"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    }
                ],
                "author_detail": {
                    "name": "Maksym Andriushchenko"
                },
                "author": "Maksym Andriushchenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21038v2",
                "updated": "2025-10-30T10:23:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    23,
                    32,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-23T22:44:50Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    22,
                    44,
                    50,
                    3,
                    296,
                    0
                ],
                "title": "Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the\n  LibriBrain Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the\n  LibriBrain Dataset"
                },
                "summary": "Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from\nlarge, public benchmarks. However, current benchmarks target relatively simple,\nfoundational tasks like Speech Detection and Phoneme Classification, while\napplication-ready results on tasks like Brain-to-Text remain elusive. We\npropose Keyword Spotting (KWS) as a practically applicable, privacy-aware\nintermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we\nprovide standardized train/validation/test splits for reproducible\nbenchmarking, and adopt an evaluation protocol tailored to extreme class\nimbalance. Concretely, we use area under the precision-recall curve (AUPRC) as\na robust evaluation metric, complemented by false alarms per hour (FA/h) at\nfixed recall to capture user-facing trade-offs. To simplify deployment and\nfurther experimentation within the research community, we are releasing an\nupdated version of the pnpl library with word-level dataloaders and Colab-ready\ntutorials. As an initial reference model, we present a compact 1-D Conv/ResNet\nbaseline with focal loss and top-k pooling that is trainable on a single\nconsumer-class GPU. The reference model achieves approximately 13x the\npermutation baseline AUPRC on held-out sessions, demonstrating the viability of\nthe task. Exploratory analyses reveal: (i) predictable within-subject scaling -\nperformance improves log-linearly with more training hours - and (ii) the\nexistence of word-level factors (frequency and duration) that systematically\nmodulate detectability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from\nlarge, public benchmarks. However, current benchmarks target relatively simple,\nfoundational tasks like Speech Detection and Phoneme Classification, while\napplication-ready results on tasks like Brain-to-Text remain elusive. We\npropose Keyword Spotting (KWS) as a practically applicable, privacy-aware\nintermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we\nprovide standardized train/validation/test splits for reproducible\nbenchmarking, and adopt an evaluation protocol tailored to extreme class\nimbalance. Concretely, we use area under the precision-recall curve (AUPRC) as\na robust evaluation metric, complemented by false alarms per hour (FA/h) at\nfixed recall to capture user-facing trade-offs. To simplify deployment and\nfurther experimentation within the research community, we are releasing an\nupdated version of the pnpl library with word-level dataloaders and Colab-ready\ntutorials. As an initial reference model, we present a compact 1-D Conv/ResNet\nbaseline with focal loss and top-k pooling that is trainable on a single\nconsumer-class GPU. The reference model achieves approximately 13x the\npermutation baseline AUPRC on held-out sessions, demonstrating the viability of\nthe task. Exploratory analyses reveal: (i) predictable within-subject scaling -\nperformance improves log-linearly with more training hours - and (ii) the\nexistence of word-level factors (frequency and duration) that systematically\nmodulate detectability."
                },
                "authors": [
                    {
                        "name": "Gereon Elvers"
                    },
                    {
                        "name": "Gilad Landau"
                    },
                    {
                        "name": "Oiwi Parker Jones"
                    }
                ],
                "author_detail": {
                    "name": "Oiwi Parker Jones"
                },
                "author": "Oiwi Parker Jones",
                "arxiv_comment": "16 pages, 7 figures, 6 tables; updated acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26322v1",
                "updated": "2025-10-30T10:17:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    17,
                    5,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T10:17:05Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    17,
                    5,
                    3,
                    303,
                    0
                ],
                "title": "SCRIBE: Structured Chain Reasoning for Interactive Behaviour\n  Explanations using Tool Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCRIBE: Structured Chain Reasoning for Interactive Behaviour\n  Explanations using Tool Calling"
                },
                "summary": "Language models can be used to provide interactive, personalized student\nfeedback in educational settings. However, real-world deployment faces three\nkey challenges: privacy concerns, limited computational resources, and the need\nfor pedagogically valid responses. These constraints require small, open-source\nmodels that can run locally and reliably ground their outputs in correct\ninformation. We introduce SCRIBE, a framework for multi-hop, tool-augmented\nreasoning designed to generate valid responses to student questions about\nfeedback reports. SCRIBE combines domain-specific tools with a self-reflective\ninference pipeline that supports iterative reasoning, tool use, and error\nrecovery. We distil these capabilities into 3B and 8B models via two-stage LoRA\nfine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned\nGPT-Judge and a user study with 108 students shows that 8B-SCRIBE models\nachieve comparable or superior quality to much larger models in key dimensions\nsuch as relevance and actionability, while being perceived on par with GPT-4o\nand Llama-3.3 70B by students. These findings demonstrate the viability of\nSCRIBE for low-resource, privacy-sensitive educational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models can be used to provide interactive, personalized student\nfeedback in educational settings. However, real-world deployment faces three\nkey challenges: privacy concerns, limited computational resources, and the need\nfor pedagogically valid responses. These constraints require small, open-source\nmodels that can run locally and reliably ground their outputs in correct\ninformation. We introduce SCRIBE, a framework for multi-hop, tool-augmented\nreasoning designed to generate valid responses to student questions about\nfeedback reports. SCRIBE combines domain-specific tools with a self-reflective\ninference pipeline that supports iterative reasoning, tool use, and error\nrecovery. We distil these capabilities into 3B and 8B models via two-stage LoRA\nfine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned\nGPT-Judge and a user study with 108 students shows that 8B-SCRIBE models\nachieve comparable or superior quality to much larger models in key dimensions\nsuch as relevance and actionability, while being perceived on par with GPT-4o\nand Llama-3.3 70B by students. These findings demonstrate the viability of\nSCRIBE for low-resource, privacy-sensitive educational applications."
                },
                "authors": [
                    {
                        "name": "Fares Fawzi"
                    },
                    {
                        "name": "Vinitra Swamy"
                    },
                    {
                        "name": "Dominik Glandorf"
                    },
                    {
                        "name": "Tanya Nazaretsky"
                    },
                    {
                        "name": "Tanja Käser"
                    }
                ],
                "author_detail": {
                    "name": "Tanja Käser"
                },
                "author": "Tanja Käser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20535v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20535v2",
                "updated": "2025-10-30T10:14:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    14,
                    59,
                    3,
                    303,
                    0
                ],
                "published": "2025-06-25T15:24:45Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    15,
                    24,
                    45,
                    2,
                    176,
                    0
                ],
                "title": "AIMeter: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIMeter: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads"
                },
                "summary": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents AIMeter, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, AIMeter offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, AIMeter encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/AIMeter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents AIMeter, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, AIMeter offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, AIMeter encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/AIMeter."
                },
                "authors": [
                    {
                        "name": "Hongzhen Huang"
                    },
                    {
                        "name": "Kunming Zhang"
                    },
                    {
                        "name": "Hanlong Liao"
                    },
                    {
                        "name": "Kui Wu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "arxiv_comment": "11 pages, 7 figures and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20535v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20535v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26320v1",
                "updated": "2025-10-30T10:14:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    14,
                    9,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T10:14:09Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    10,
                    14,
                    9,
                    3,
                    303,
                    0
                ],
                "title": "Twin-Field Quantum Key Distribution: Protocols, Security, and Open\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twin-Field Quantum Key Distribution: Protocols, Security, and Open\n  Problems"
                },
                "summary": "Twin-Field Quantum Key Distribution (TF-QKD) has emerged as a potential\nprotocol for long distance secure communication, overcoming the rate-distance\nlimitations of conventional quantum key distribution without requiring trusted\nrepeaters. By having two parties transmit phase encoded weak coherent pulses\n(WCP) to an untrusted central node, the TF-QKD exploits single-photon\ninterference to achieve secret key rates scaling as square-root of channel\nlength, enabling quantum-secured communication over unprecedented distances.\nThis survey provides a comprehensive survey of TF-QKD, covering the original\nprotocol, its fundamental principles, and key-rate derivation. We discuss major\nTF-QKD variants, including Phase-Matching QKD and Sending-or-Not-Sending QKD,\nwith various improved versions. We compare their performance, implementation\ntrade-offs, protocol-specific vulnerabilities, and countermeasures. The survey\nsummarizes security proofs ranging from asymptotic decoy-state analyses to\nfinite-key composable frameworks, experimental milestones, technological\nenablers, and practical deployment challenges. Finally, we outline open\nproblems in the field and present a roadmap for integrating TF-QKD into\nscalable quantum networks, underscoring its central role in the future quantum\ninternet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twin-Field Quantum Key Distribution (TF-QKD) has emerged as a potential\nprotocol for long distance secure communication, overcoming the rate-distance\nlimitations of conventional quantum key distribution without requiring trusted\nrepeaters. By having two parties transmit phase encoded weak coherent pulses\n(WCP) to an untrusted central node, the TF-QKD exploits single-photon\ninterference to achieve secret key rates scaling as square-root of channel\nlength, enabling quantum-secured communication over unprecedented distances.\nThis survey provides a comprehensive survey of TF-QKD, covering the original\nprotocol, its fundamental principles, and key-rate derivation. We discuss major\nTF-QKD variants, including Phase-Matching QKD and Sending-or-Not-Sending QKD,\nwith various improved versions. We compare their performance, implementation\ntrade-offs, protocol-specific vulnerabilities, and countermeasures. The survey\nsummarizes security proofs ranging from asymptotic decoy-state analyses to\nfinite-key composable frameworks, experimental milestones, technological\nenablers, and practical deployment challenges. Finally, we outline open\nproblems in the field and present a roadmap for integrating TF-QKD into\nscalable quantum networks, underscoring its central role in the future quantum\ninternet."
                },
                "authors": [
                    {
                        "name": "Syed M. Arslan"
                    },
                    {
                        "name": "Syed Shahmir"
                    },
                    {
                        "name": "Noureldin Mohammad"
                    },
                    {
                        "name": "Saif Al-Kuwari"
                    },
                    {
                        "name": "Muataz Alhussein"
                    }
                ],
                "author_detail": {
                    "name": "Muataz Alhussein"
                },
                "author": "Muataz Alhussein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16271v2",
                "updated": "2025-10-30T09:57:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    9,
                    57,
                    54,
                    3,
                    303,
                    0
                ],
                "published": "2025-07-22T06:37:51Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    6,
                    37,
                    51,
                    1,
                    203,
                    0
                ],
                "title": "Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep\n  Knowledge Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep\n  Knowledge Extraction"
                },
                "summary": "With the emergence of large language models (LLMs), there is an expectation\nthat LLMs can effectively extract explicit information from complex real-world\ndocuments (e.g., papers, reports). However, most LLMs generate paragraph-style\nanswers that are chaotic, disorganized, and untraceable. To bridge this gap, we\nintroduce the Arranged and Organized Extraction Benchmark (AOE), a new\nbilingual benchmark with data and documents of varying lengths designed to\nsystematically evaluate the ability of LLMs to comprehend fragmented documents\nand reconstruct isolated information into one organized table. Unlike\nconventional text-to-table tasks, which rely on fixed schema and narrow task\ndomains, AOE includes 11 carefully crafted tasks across three diverse domains,\nrequiring models to generate context-specific schema tailored to varied input\nqueries. In the experiment, we evaluated both open-source and closed-source\nstate-of-the-art LLMs. The results show that even the most advanced models\nstruggled significantly. The benchmark is available at\nhttps://anonymous.4open.science/r/AOE-Benchmark/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the emergence of large language models (LLMs), there is an expectation\nthat LLMs can effectively extract explicit information from complex real-world\ndocuments (e.g., papers, reports). However, most LLMs generate paragraph-style\nanswers that are chaotic, disorganized, and untraceable. To bridge this gap, we\nintroduce the Arranged and Organized Extraction Benchmark (AOE), a new\nbilingual benchmark with data and documents of varying lengths designed to\nsystematically evaluate the ability of LLMs to comprehend fragmented documents\nand reconstruct isolated information into one organized table. Unlike\nconventional text-to-table tasks, which rely on fixed schema and narrow task\ndomains, AOE includes 11 carefully crafted tasks across three diverse domains,\nrequiring models to generate context-specific schema tailored to varied input\nqueries. In the experiment, we evaluated both open-source and closed-source\nstate-of-the-art LLMs. The results show that even the most advanced models\nstruggled significantly. The benchmark is available at\nhttps://anonymous.4open.science/r/AOE-Benchmark/."
                },
                "authors": [
                    {
                        "name": "Tianyun Zhong"
                    },
                    {
                        "name": "Guozhao Mo"
                    },
                    {
                        "name": "Yanjiang Liu"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Lingdi Kong"
                    },
                    {
                        "name": "Xuanang Chen"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Shiwei Ye"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]