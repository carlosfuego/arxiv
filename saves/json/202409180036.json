[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.10516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v1",
                "updated": "2024-09-16T17:59:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB)."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09086v1",
                "updated": "2024-09-11T12:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08286v1",
                "updated": "2024-08-28T17:28:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:28:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors"
                },
                "summary": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint."
                },
                "authors": [
                    {
                        "name": "Noushin Behboudi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Ali Afzali-Kusha"
                    }
                ],
                "author_detail": {
                    "name": "Ali Afzali-Kusha"
                },
                "author": "Ali Afzali-Kusha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.10516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v1",
                "updated": "2024-09-16T17:59:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB)."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10514v1",
                "updated": "2024-09-16T17:59:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    49,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:59:49Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    49,
                    0,
                    260,
                    0
                ],
                "title": "Constraints on axions from patchy screening of the cosmic microwave\n  background",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on axions from patchy screening of the cosmic microwave\n  background"
                },
                "summary": "The resonant conversion of cosmic microwave background (CMB) photons into\naxions within large-scale structure induces an anisotropic spectral distortion\nin CMB temperature maps. Applying state-of-the-art foreground cleaning\ntechniques to $\\textit{Planck}$ CMB observations, we construct maps of\naxion-induced \"patchy screening\" of the CMB. We cross-correlate these maps with\ndata from the $\\textit{unWISE}$ galaxy survey and find no evidence of axions.\nWe constrain the axion-photon coupling, $g_{a\\gamma\\gamma} \\lesssim 2 \\times\n10^{-12}~{\\rm GeV}^{-1}$, at the 95% confidence level for axion masses in the\nrange $10^{-13}~{\\rm eV} \\lesssim m_a \\lesssim 10^{-12}~{\\rm eV}$. These\nconstraints are competitive with the tightest astrophysical axion limits in\nthis mass range and are inferred from robust population-level statistics, which\nmakes them complementary to existing searches that rely on modeling of\nindividual systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The resonant conversion of cosmic microwave background (CMB) photons into\naxions within large-scale structure induces an anisotropic spectral distortion\nin CMB temperature maps. Applying state-of-the-art foreground cleaning\ntechniques to $\\textit{Planck}$ CMB observations, we construct maps of\naxion-induced \"patchy screening\" of the CMB. We cross-correlate these maps with\ndata from the $\\textit{unWISE}$ galaxy survey and find no evidence of axions.\nWe constrain the axion-photon coupling, $g_{a\\gamma\\gamma} \\lesssim 2 \\times\n10^{-12}~{\\rm GeV}^{-1}$, at the 95% confidence level for axion masses in the\nrange $10^{-13}~{\\rm eV} \\lesssim m_a \\lesssim 10^{-12}~{\\rm eV}$. These\nconstraints are competitive with the tightest astrophysical axion limits in\nthis mass range and are inferred from robust population-level statistics, which\nmakes them complementary to existing searches that rely on modeling of\nindividual systems."
                },
                "authors": [
                    {
                        "name": "Samuel Goldstein"
                    },
                    {
                        "name": "Fiona McCarthy"
                    },
                    {
                        "name": "Cristina Mondino"
                    },
                    {
                        "name": "J. Colin Hill"
                    },
                    {
                        "name": "Junwu Huang"
                    },
                    {
                        "name": "Matthew C. Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Matthew C. Johnson"
                },
                "author": "Matthew C. Johnson",
                "arxiv_comment": "5+15 pages; 3+15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21787v2",
                "updated": "2024-09-16T17:58:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    58,
                    42,
                    0,
                    260,
                    0
                ],
                "published": "2024-07-31T17:57:25Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    17,
                    57,
                    25,
                    2,
                    213,
                    0
                ],
                "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling"
                },
                "summary": "Scaling the amount of compute used to train language models has dramatically\nimproved their capabilities. However, when it comes to inference, we often\nlimit the amount of compute to only one attempt per problem. Here, we explore\ninference compute as another axis for scaling by increasing the number of\ngenerated samples. Across multiple tasks and models, we observe that coverage -\nthe fraction of problems solved by any attempt - scales with the number of\nsamples over four orders of magnitude. In domains like coding and formal\nproofs, where all answers can be automatically verified, these increases in\ncoverage directly translate into improved performance. When we apply repeated\nsampling to SWE-bench Lite, the fraction of issues solved with\nDeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250\nsamples, outperforming the single-attempt state-of-the-art of 43% which uses\nmore capable frontier models. Moreover, using current API pricing, amplifying\nthe cheaper DeepSeek model with five samples is more cost-effective and solves\nmore issues than paying a premium for one sample from GPT-4o or Claude 3.5\nSonnet. Interestingly, the relationship between coverage and the number of\nsamples is often log-linear and can be modelled with an exponentiated power\nlaw, suggesting the existence of inference-time scaling laws. Finally, we find\nthat identifying correct samples out of many generations remains an important\ndirection for future research in domains without automatic verifiers. When\nsolving math word problems from GSM8K and MATH, coverage with Llama-3 models\ngrows to over 95% with 10,000 samples. However, common methods to pick correct\nsolutions from a sample collection, such as majority voting or reward models,\nplateau beyond several hundred samples and fail to fully scale with the sample\nbudget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the amount of compute used to train language models has dramatically\nimproved their capabilities. However, when it comes to inference, we often\nlimit the amount of compute to only one attempt per problem. Here, we explore\ninference compute as another axis for scaling by increasing the number of\ngenerated samples. Across multiple tasks and models, we observe that coverage -\nthe fraction of problems solved by any attempt - scales with the number of\nsamples over four orders of magnitude. In domains like coding and formal\nproofs, where all answers can be automatically verified, these increases in\ncoverage directly translate into improved performance. When we apply repeated\nsampling to SWE-bench Lite, the fraction of issues solved with\nDeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250\nsamples, outperforming the single-attempt state-of-the-art of 43% which uses\nmore capable frontier models. Moreover, using current API pricing, amplifying\nthe cheaper DeepSeek model with five samples is more cost-effective and solves\nmore issues than paying a premium for one sample from GPT-4o or Claude 3.5\nSonnet. Interestingly, the relationship between coverage and the number of\nsamples is often log-linear and can be modelled with an exponentiated power\nlaw, suggesting the existence of inference-time scaling laws. Finally, we find\nthat identifying correct samples out of many generations remains an important\ndirection for future research in domains without automatic verifiers. When\nsolving math word problems from GSM8K and MATH, coverage with Llama-3 models\ngrows to over 95% with 10,000 samples. However, common methods to pick correct\nsolutions from a sample collection, such as majority voting or reward models,\nplateau beyond several hundred samples and fail to fully scale with the sample\nbudget."
                },
                "authors": [
                    {
                        "name": "Bradley Brown"
                    },
                    {
                        "name": "Jordan Juravsky"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Quoc V. Le"
                    },
                    {
                        "name": "Christopher Ré"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    }
                ],
                "author_detail": {
                    "name": "Azalia Mirhoseini"
                },
                "author": "Azalia Mirhoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10506v1",
                "updated": "2024-09-16T17:52:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    52,
                    36,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:52:36Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    52,
                    36,
                    0,
                    260,
                    0
                ],
                "title": "Context-aware Code Segmentation for C-to-Rust Translation using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-aware Code Segmentation for C-to-Rust Translation using Large\n  Language Models"
                },
                "summary": "There is strong motivation to translate C code into Rust code due to the\ncontinuing threat of memory safety vulnerabilities in existing C programs and\nthe significant attention paid to Rust as an alternative to the C language.\nWhile large language models (LLMs) show promise for automating this translation\nby generating more natural and safer code than rule-based methods, previous\nstudies have shown that LLM-generated Rust code often fails to compile, even\nfor relatively small C programs, due to significant differences between the two\nlanguages and context window limitations. We propose an LLM-based translation\nscheme that improves the success rate of translating large-scale C code into\ncompilable Rust code. Our approach involves three key techniques: (1)\npre-processing the C code to better align its structure and expressions with\nRust, (2) segmenting the code into optimally sized translation units to avoid\nexceeding the LLM's context window limits, and (3) iteratively compiling and\nrepairing errors while maintaining consistency between translation units using\ncontext-supplementing prompts. Compilation success is an essential first step\nin achieving functional equivalence, as only compilable code can be further\ntested. In experiments with 20 benchmark C programs, including those exceeding\n4 kilo lines of code, we successfully translated all programs into compilable\nRust code without losing corresponding parts of the original code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is strong motivation to translate C code into Rust code due to the\ncontinuing threat of memory safety vulnerabilities in existing C programs and\nthe significant attention paid to Rust as an alternative to the C language.\nWhile large language models (LLMs) show promise for automating this translation\nby generating more natural and safer code than rule-based methods, previous\nstudies have shown that LLM-generated Rust code often fails to compile, even\nfor relatively small C programs, due to significant differences between the two\nlanguages and context window limitations. We propose an LLM-based translation\nscheme that improves the success rate of translating large-scale C code into\ncompilable Rust code. Our approach involves three key techniques: (1)\npre-processing the C code to better align its structure and expressions with\nRust, (2) segmenting the code into optimally sized translation units to avoid\nexceeding the LLM's context window limits, and (3) iteratively compiling and\nrepairing errors while maintaining consistency between translation units using\ncontext-supplementing prompts. Compilation success is an essential first step\nin achieving functional equivalence, as only compilable code can be further\ntested. In experiments with 20 benchmark C programs, including those exceeding\n4 kilo lines of code, we successfully translated all programs into compilable\nRust code without losing corresponding parts of the original code."
                },
                "authors": [
                    {
                        "name": "Momoko Shiraishi"
                    },
                    {
                        "name": "Takahiro Shinagawa"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Shinagawa"
                },
                "author": "Takahiro Shinagawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.14220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.14220v4",
                "updated": "2024-09-16T17:47:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    47,
                    54,
                    0,
                    260,
                    0
                ],
                "published": "2023-11-23T22:41:30Z",
                "published_parsed": [
                    2023,
                    11,
                    23,
                    22,
                    41,
                    30,
                    3,
                    327,
                    0
                ],
                "title": "Assumption-Lean and Data-Adaptive Post-Prediction Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assumption-Lean and Data-Adaptive Post-Prediction Inference"
                },
                "summary": "A primary challenge facing modern scientific research is the limited\navailability of gold-standard data which can be costly, labor-intensive, or\ninvasive to obtain. With the rapid development of machine learning (ML),\nscientists can now employ ML algorithms to predict gold-standard outcomes with\nvariables that are easier to obtain. However, these predicted outcomes are\noften used directly in subsequent statistical analyses, ignoring imprecision\nand heterogeneity introduced by the prediction procedure. This will likely\nresult in false positive findings and invalid scientific conclusions. In this\nwork, we introduce PoSt-Prediction Adaptive inference (PSPA) that allows valid\nand powerful inference based on ML-predicted data. Its \"assumption-lean\"\nproperty guarantees reliable statistical inference without assumptions on the\nML prediction. Its \"data-adaptive\" feature guarantees an efficiency gain over\nexisting methods, regardless of the accuracy of ML prediction. We demonstrate\nthe statistical superiority and broad applicability of our method through\nsimulations and real-data applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A primary challenge facing modern scientific research is the limited\navailability of gold-standard data which can be costly, labor-intensive, or\ninvasive to obtain. With the rapid development of machine learning (ML),\nscientists can now employ ML algorithms to predict gold-standard outcomes with\nvariables that are easier to obtain. However, these predicted outcomes are\noften used directly in subsequent statistical analyses, ignoring imprecision\nand heterogeneity introduced by the prediction procedure. This will likely\nresult in false positive findings and invalid scientific conclusions. In this\nwork, we introduce PoSt-Prediction Adaptive inference (PSPA) that allows valid\nand powerful inference based on ML-predicted data. Its \"assumption-lean\"\nproperty guarantees reliable statistical inference without assumptions on the\nML prediction. Its \"data-adaptive\" feature guarantees an efficiency gain over\nexisting methods, regardless of the accuracy of ML prediction. We demonstrate\nthe statistical superiority and broad applicability of our method through\nsimulations and real-data applications."
                },
                "authors": [
                    {
                        "name": "Jiacheng Miao"
                    },
                    {
                        "name": "Xinran Miao"
                    },
                    {
                        "name": "Yixuan Wu"
                    },
                    {
                        "name": "Jiwei Zhao"
                    },
                    {
                        "name": "Qiongshi Lu"
                    }
                ],
                "author_detail": {
                    "name": "Qiongshi Lu"
                },
                "author": "Qiongshi Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.14220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.14220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10504v1",
                "updated": "2024-09-16T17:45:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    45,
                    40,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:45:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    45,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "DILA: Dictionary Label Attention for Mechanistic Interpretability in\n  High-dimensional Multi-label Medical Coding Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DILA: Dictionary Label Attention for Mechanistic Interpretability in\n  High-dimensional Multi-label Medical Coding Prediction"
                },
                "summary": "Predicting high-dimensional or extreme multilabels, such as in medical\ncoding, requires both accuracy and interpretability. Existing works often rely\non local interpretability methods, failing to provide comprehensive\nexplanations of the overall mechanism behind each label prediction within a\nmultilabel set. We propose a mechanistic interpretability module called\nDIctionary Label Attention (\\method) that disentangles uninterpretable dense\nembeddings into a sparse embedding space, where each nonzero element (a\ndictionary feature) represents a globally learned medical concept. Through\nhuman evaluations, we show that our sparse embeddings are more human\nunderstandable than its dense counterparts by at least 50 percent. Our\nautomated dictionary feature identification pipeline, leveraging large language\nmodels (LLMs), uncovers thousands of learned medical concepts by examining and\nsummarizing the highest activating tokens for each dictionary feature. We\nrepresent the relationships between dictionary features and medical codes\nthrough a sparse interpretable matrix, enhancing the mechanistic and global\nunderstanding of the model's predictions while maintaining competitive\nperformance and scalability without extensive human annotation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting high-dimensional or extreme multilabels, such as in medical\ncoding, requires both accuracy and interpretability. Existing works often rely\non local interpretability methods, failing to provide comprehensive\nexplanations of the overall mechanism behind each label prediction within a\nmultilabel set. We propose a mechanistic interpretability module called\nDIctionary Label Attention (\\method) that disentangles uninterpretable dense\nembeddings into a sparse embedding space, where each nonzero element (a\ndictionary feature) represents a globally learned medical concept. Through\nhuman evaluations, we show that our sparse embeddings are more human\nunderstandable than its dense counterparts by at least 50 percent. Our\nautomated dictionary feature identification pipeline, leveraging large language\nmodels (LLMs), uncovers thousands of learned medical concepts by examining and\nsummarizing the highest activating tokens for each dictionary feature. We\nrepresent the relationships between dictionary features and medical codes\nthrough a sparse interpretable matrix, enhancing the mechanistic and global\nunderstanding of the model's predictions while maintaining competitive\nperformance and scalability without extensive human annotation."
                },
                "authors": [
                    {
                        "name": "John Wu"
                    },
                    {
                        "name": "David Wu"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10527v2",
                "updated": "2024-09-16T17:44:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    44,
                    17,
                    0,
                    260,
                    0
                ],
                "published": "2024-02-16T09:29:38Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    9,
                    29,
                    38,
                    4,
                    47,
                    0
                ],
                "title": "Assessing biomedical knowledge robustness in large language models by\n  query-efficient sampling attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing biomedical knowledge robustness in large language models by\n  query-efficient sampling attacks"
                },
                "summary": "The increasing depth of parametric domain knowledge in large language models\n(LLMs) is fueling their rapid deployment in real-world applications.\nUnderstanding model vulnerabilities in high-stakes and knowledge-intensive\ntasks is essential for quantifying the trustworthiness of model predictions and\nregulating their use. The recent discovery of named entities as adversarial\nexamples (i.e. adversarial entities) in natural language processing tasks\nraises questions about their potential impact on the knowledge robustness of\npre-trained and finetuned LLMs in high-stakes and specialized domains. We\nexamined the use of type-consistent entity substitution as a template for\ncollecting adversarial entities for billion-parameter LLMs with biomedical\nknowledge. To this end, we developed an embedding-space attack based on\npowerscaled distance-weighted sampling to assess the robustness of their\nbiomedical knowledge with a low query budget and controllable coverage. Our\nmethod has favorable query efficiency and scaling over alternative approaches\nbased on random sampling and blackbox gradient-guided search, which we\ndemonstrated for adversarial distractor generation in biomedical question\nanswering. Subsequent failure mode analysis uncovered two regimes of\nadversarial entities on the attack surface with distinct characteristics and we\nshowed that entity substitution attacks can manipulate token-wise Shapley value\nexplanations, which become deceptive in this setting. Our approach complements\nstandard evaluations for high-capacity models and the results highlight the\nbrittleness of domain knowledge in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing depth of parametric domain knowledge in large language models\n(LLMs) is fueling their rapid deployment in real-world applications.\nUnderstanding model vulnerabilities in high-stakes and knowledge-intensive\ntasks is essential for quantifying the trustworthiness of model predictions and\nregulating their use. The recent discovery of named entities as adversarial\nexamples (i.e. adversarial entities) in natural language processing tasks\nraises questions about their potential impact on the knowledge robustness of\npre-trained and finetuned LLMs in high-stakes and specialized domains. We\nexamined the use of type-consistent entity substitution as a template for\ncollecting adversarial entities for billion-parameter LLMs with biomedical\nknowledge. To this end, we developed an embedding-space attack based on\npowerscaled distance-weighted sampling to assess the robustness of their\nbiomedical knowledge with a low query budget and controllable coverage. Our\nmethod has favorable query efficiency and scaling over alternative approaches\nbased on random sampling and blackbox gradient-guided search, which we\ndemonstrated for adversarial distractor generation in biomedical question\nanswering. Subsequent failure mode analysis uncovered two regimes of\nadversarial entities on the attack surface with distinct characteristics and we\nshowed that entity substitution attacks can manipulate token-wise Shapley value\nexplanations, which become deceptive in this setting. Our approach complements\nstandard evaluations for high-capacity models and the results highlight the\nbrittleness of domain knowledge in LLMs."
                },
                "authors": [
                    {
                        "name": "R. Patrick Xian"
                    },
                    {
                        "name": "Alex J. Lee"
                    },
                    {
                        "name": "Satvik Lolla"
                    },
                    {
                        "name": "Vincent Wang"
                    },
                    {
                        "name": "Qiming Cui"
                    },
                    {
                        "name": "Russell Ro"
                    },
                    {
                        "name": "Reza Abbasi-Asl"
                    }
                ],
                "author_detail": {
                    "name": "Reza Abbasi-Asl"
                },
                "author": "Reza Abbasi-Asl",
                "arxiv_comment": "28 pages incl. appendix, updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00935v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00935v3",
                "updated": "2024-09-16T17:42:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    42,
                    17,
                    0,
                    260,
                    0
                ],
                "published": "2024-01-01T19:00:55Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    19,
                    0,
                    55,
                    0,
                    1,
                    0
                ],
                "title": "Boundary Attention: Learning curves, corners, junctions and grouping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boundary Attention: Learning curves, corners, junctions and grouping"
                },
                "summary": "We present a lightweight network that infers grouping and boundaries,\nincluding curves, corners and junctions. It operates in a bottom-up fashion,\nanalogous to classical methods for sub-pixel edge localization and\nedge-linking, but with a higher-dimensional representation of local boundary\nstructure, and notions of local scale and spatial consistency that are learned\ninstead of designed. Our network uses a mechanism that we call boundary\nattention: a geometry-aware local attention operation that, when applied\ndensely and repeatedly, progressively refines a pixel-resolution field of\nvariables that specify the boundary structure in every overlapping patch within\nan image. Unlike many edge detectors that produce rasterized binary edge maps,\nour model provides a rich, unrasterized representation of the geometric\nstructure in every local region. We find that its intentional geometric bias\nallows it to be trained on simple synthetic shapes and then generalize to\nextracting boundaries from noisy low-light photographs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a lightweight network that infers grouping and boundaries,\nincluding curves, corners and junctions. It operates in a bottom-up fashion,\nanalogous to classical methods for sub-pixel edge localization and\nedge-linking, but with a higher-dimensional representation of local boundary\nstructure, and notions of local scale and spatial consistency that are learned\ninstead of designed. Our network uses a mechanism that we call boundary\nattention: a geometry-aware local attention operation that, when applied\ndensely and repeatedly, progressively refines a pixel-resolution field of\nvariables that specify the boundary structure in every overlapping patch within\nan image. Unlike many edge detectors that produce rasterized binary edge maps,\nour model provides a rich, unrasterized representation of the geometric\nstructure in every local region. We find that its intentional geometric bias\nallows it to be trained on simple synthetic shapes and then generalize to\nextracting boundaries from noisy low-light photographs."
                },
                "authors": [
                    {
                        "name": "Mia Gaia Polansky"
                    },
                    {
                        "name": "Charles Herrmann"
                    },
                    {
                        "name": "Junhwa Hur"
                    },
                    {
                        "name": "Deqing Sun"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Todd Zickler"
                    }
                ],
                "author_detail": {
                    "name": "Todd Zickler"
                },
                "author": "Todd Zickler",
                "arxiv_comment": "Project website at boundaryattention.github.io:\n  http://boundaryattention.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00935v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00935v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10502v1",
                "updated": "2024-09-16T17:42:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    42,
                    15,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:42:15Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    42,
                    15,
                    0,
                    260,
                    0
                ],
                "title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on\n  Logic Puzzles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on\n  Logic Puzzles"
                },
                "summary": "Causal language modeling using the Transformer architecture has yielded\nremarkable capabilities in Large Language Models (LLMs) over the last few\nyears. However, the extent to which fundamental search and reasoning\ncapabilities emerged within LLMs remains a topic of ongoing debate. In this\nwork, we study if causal language modeling can learn a complex task such as\nsolving Sudoku puzzles. To solve a Sudoku, the model is first required to\nsearch over all empty cells of the puzzle to decide on a cell to fill and then\napply an appropriate strategy to fill the decided cell. Sometimes, the\napplication of a strategy only results in thinning down the possible values in\na cell rather than concluding the exact value of the cell. In such cases,\nmultiple strategies are applied one after the other to fill a single cell. We\nobserve that Transformer models trained on this synthetic task can indeed learn\nto solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly)\nwhen trained on a logical sequence of steps taken by a solver. We find that\ntraining Transformers with the logical sequence of steps is necessary and\nwithout such training, they fail to learn Sudoku. We also extend our analysis\nto Zebra puzzles (known as Einstein puzzles) and show that the model solves\n$92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal\nrepresentations of the trained Transformer and find that through linear\nprobing, we can decode information about the set of possible values in any\ngiven cell from them, pointing to the presence of a strong reasoning engine\nimplicit in the Transformer weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal language modeling using the Transformer architecture has yielded\nremarkable capabilities in Large Language Models (LLMs) over the last few\nyears. However, the extent to which fundamental search and reasoning\ncapabilities emerged within LLMs remains a topic of ongoing debate. In this\nwork, we study if causal language modeling can learn a complex task such as\nsolving Sudoku puzzles. To solve a Sudoku, the model is first required to\nsearch over all empty cells of the puzzle to decide on a cell to fill and then\napply an appropriate strategy to fill the decided cell. Sometimes, the\napplication of a strategy only results in thinning down the possible values in\na cell rather than concluding the exact value of the cell. In such cases,\nmultiple strategies are applied one after the other to fill a single cell. We\nobserve that Transformer models trained on this synthetic task can indeed learn\nto solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly)\nwhen trained on a logical sequence of steps taken by a solver. We find that\ntraining Transformers with the logical sequence of steps is necessary and\nwithout such training, they fail to learn Sudoku. We also extend our analysis\nto Zebra puzzles (known as Einstein puzzles) and show that the model solves\n$92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal\nrepresentations of the trained Transformer and find that through linear\nprobing, we can decode information about the set of possible values in any\ngiven cell from them, pointing to the presence of a strong reasoning engine\nimplicit in the Transformer weights."
                },
                "authors": [
                    {
                        "name": "Kulin Shah"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11006v2",
                "updated": "2024-09-16T17:35:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    35,
                    10,
                    0,
                    260,
                    0
                ],
                "published": "2024-08-20T17:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Security Attacks on LLM-based Code Completion Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Attacks on LLM-based Code Completion Tools"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs."
                },
                "authors": [
                    {
                        "name": "Wen Cheng"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10490v1",
                "updated": "2024-09-16T17:23:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    23,
                    0,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:23:00Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    23,
                    0,
                    0,
                    260,
                    0
                ],
                "title": "Code Vulnerability Detection: A Comparative Analysis of Emerging Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Vulnerability Detection: A Comparative Analysis of Emerging Large\n  Language Models"
                },
                "summary": "The growing trend of vulnerability issues in software development as a result\nof a large dependence on open-source projects has received considerable\nattention recently. This paper investigates the effectiveness of Large Language\nModels (LLMs) in identifying vulnerabilities within codebases, with a focus on\nthe latest advancements in LLM technology. Through a comparative analysis, we\nassess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma,\nand CodeGemma, alongside established state-of-the-art models such as BERT,\nRoBERTa, and GPT-3. Our study aims to shed light on the capabilities of LLMs in\nvulnerability detection, contributing to the enhancement of software security\npractices across diverse open-source repositories. We observe that CodeGemma\nachieves the highest F1-score of 58\\ and a Recall of 87\\, amongst the recent\nadditions of large language models to detect software security vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing trend of vulnerability issues in software development as a result\nof a large dependence on open-source projects has received considerable\nattention recently. This paper investigates the effectiveness of Large Language\nModels (LLMs) in identifying vulnerabilities within codebases, with a focus on\nthe latest advancements in LLM technology. Through a comparative analysis, we\nassess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma,\nand CodeGemma, alongside established state-of-the-art models such as BERT,\nRoBERTa, and GPT-3. Our study aims to shed light on the capabilities of LLMs in\nvulnerability detection, contributing to the enhancement of software security\npractices across diverse open-source repositories. We observe that CodeGemma\nachieves the highest F1-score of 58\\ and a Recall of 87\\, amongst the recent\nadditions of large language models to detect software security vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Shaznin Sultana"
                    },
                    {
                        "name": "Sadia Afreen"
                    },
                    {
                        "name": "Nasir U. Eisty"
                    }
                ],
                "author_detail": {
                    "name": "Nasir U. Eisty"
                },
                "author": "Nasir U. Eisty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10484v1",
                "updated": "2024-09-16T17:20:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    20,
                    23,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:20:23Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    20,
                    23,
                    0,
                    260,
                    0
                ],
                "title": "XLM for Autonomous Driving Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XLM for Autonomous Driving Systems: A Comprehensive Review"
                },
                "summary": "Large Language Models (LLMs) have showcased remarkable proficiency in various\ninformation-processing tasks. These tasks span from extracting data and\nsummarizing literature to generating content, predictive modeling,\ndecision-making, and system controls. Moreover, Vision Large Models (VLMs) and\nMultimodal LLMs (MLLMs), which represent the next generation of language\nmodels, a.k.a., XLMs, can combine and integrate many data modalities with the\nstrength of language understanding, thus advancing several information-based\nsystems, such as Autonomous Driving Systems (ADS). Indeed, by combining\nlanguage communication with multimodal sensory inputs, e.g., panoramic images\nand LiDAR or radar data, accurate driving actions can be taken. In this\ncontext, we provide in this survey paper a comprehensive overview of the\npotential of XLMs towards achieving autonomous driving. Specifically, we review\nthe relevant literature on ADS and XLMs, including their architectures, tools,\nand frameworks. Then, we detail the proposed approaches to deploy XLMs for\nautonomous driving solutions. Finally, we provide the related challenges to XLM\ndeployment for ADS and point to future research directions aiming to enable XLM\nadoption in future ADS frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased remarkable proficiency in various\ninformation-processing tasks. These tasks span from extracting data and\nsummarizing literature to generating content, predictive modeling,\ndecision-making, and system controls. Moreover, Vision Large Models (VLMs) and\nMultimodal LLMs (MLLMs), which represent the next generation of language\nmodels, a.k.a., XLMs, can combine and integrate many data modalities with the\nstrength of language understanding, thus advancing several information-based\nsystems, such as Autonomous Driving Systems (ADS). Indeed, by combining\nlanguage communication with multimodal sensory inputs, e.g., panoramic images\nand LiDAR or radar data, accurate driving actions can be taken. In this\ncontext, we provide in this survey paper a comprehensive overview of the\npotential of XLMs towards achieving autonomous driving. Specifically, we review\nthe relevant literature on ADS and XLMs, including their architectures, tools,\nand frameworks. Then, we detail the proposed approaches to deploy XLMs for\nautonomous driving solutions. Finally, we provide the related challenges to XLM\ndeployment for ADS and point to future research directions aiming to enable XLM\nadoption in future ADS frameworks."
                },
                "authors": [
                    {
                        "name": "Sonda Fourati"
                    },
                    {
                        "name": "Wael Jaafar"
                    },
                    {
                        "name": "Noura Baccar"
                    },
                    {
                        "name": "Safwan Alfattani"
                    }
                ],
                "author_detail": {
                    "name": "Safwan Alfattani"
                },
                "author": "Safwan Alfattani",
                "arxiv_comment": "30 pages, 18 figures, submitted to IEEE Open Journal of Intelligent\n  Transportation Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10482v1",
                "updated": "2024-09-16T17:18:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    18,
                    11,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:18:11Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    18,
                    11,
                    0,
                    260,
                    0
                ],
                "title": "Schrodinger's Memory: Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schrodinger's Memory: Large Language Models"
                },
                "summary": "Memory is the foundation of LLMs' functionality, yet past research has lacked\nan in-depth exploration of their memory capabilities and underlying theory. In\nthis paper, we apply UAT theory to explain the memory mechanism of LLMs and\npropose a new approach for evaluating LLM performance by comparing the memory\ncapacities of different models. Through extensive experiments, we validate our\ntheory and the memory abilities of LLMs. Finally, we compare the capabilities\nof the human brain and LLMs, highlighting both their similarities and\ndifferences in terms of working mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is the foundation of LLMs' functionality, yet past research has lacked\nan in-depth exploration of their memory capabilities and underlying theory. In\nthis paper, we apply UAT theory to explain the memory mechanism of LLMs and\npropose a new approach for evaluating LLM performance by comparing the memory\ncapacities of different models. Through extensive experiments, we validate our\ntheory and the memory abilities of LLMs. Finally, we compare the capabilities\nof the human brain and LLMs, highlighting both their similarities and\ndifferences in terms of working mechanisms."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14891v2",
                "updated": "2024-09-16T17:15:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    15,
                    52,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-21T06:26:38Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    6,
                    26,
                    38,
                    4,
                    173,
                    0
                ],
                "title": "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop\n  Question Answering"
                },
                "summary": "Multi-Hop Question Answering (MHQA) tasks present a significant challenge for\nlarge language models (LLMs) due to the intensive knowledge required. Current\nsolutions, like Retrieval-Augmented Generation, typically retrieve potential\ndocuments from an external corpus to read an answer. However, the performance\nof this retrieve-then-read paradigm is constrained by the retriever and the\ninevitable noise in the retrieved documents. To mitigate these challenges, we\nintroduce a novel generate-then-ground (GenGround) framework, synergizing the\nparametric knowledge of LLMs and external documents to solve a multi-hop\nquestion. GenGround empowers LLMs to alternate two phases until the final\nanswer is derived: (1) formulate a simpler, single-hop question and directly\ngenerate the answer; (2) ground the question-answer pair in retrieved\ndocuments, amending any wrong predictions in the answer. We also propose an\ninstructional grounding distillation method to generalize our method into\nsmaller models. Extensive experiments conducted on four datasets illustrate the\nsuperiority of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Hop Question Answering (MHQA) tasks present a significant challenge for\nlarge language models (LLMs) due to the intensive knowledge required. Current\nsolutions, like Retrieval-Augmented Generation, typically retrieve potential\ndocuments from an external corpus to read an answer. However, the performance\nof this retrieve-then-read paradigm is constrained by the retriever and the\ninevitable noise in the retrieved documents. To mitigate these challenges, we\nintroduce a novel generate-then-ground (GenGround) framework, synergizing the\nparametric knowledge of LLMs and external documents to solve a multi-hop\nquestion. GenGround empowers LLMs to alternate two phases until the final\nanswer is derived: (1) formulate a simpler, single-hop question and directly\ngenerate the answer; (2) ground the question-answer pair in retrieved\ndocuments, amending any wrong predictions in the answer. We also propose an\ninstructional grounding distillation method to generalize our method into\nsmaller models. Extensive experiments conducted on four datasets illustrate the\nsuperiority of our method."
                },
                "authors": [
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "arxiv_comment": "ACL 2024 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10452v1",
                "updated": "2024-09-16T16:40:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    40,
                    40,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T16:40:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    40,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "Signed Graph Autoencoder for Explainable and Polarization-Aware Network\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signed Graph Autoencoder for Explainable and Polarization-Aware Network\n  Embeddings"
                },
                "summary": "Autoencoders based on Graph Neural Networks (GNNs) have garnered significant\nattention in recent years for their ability to extract informative latent\nrepresentations, characterizing the structure of complex topologies, such as\ngraphs. Despite the prevalence of Graph Autoencoders, there has been limited\nfocus on developing and evaluating explainable neural-based graph generative\nmodels specifically designed for signed networks. To address this gap, we\npropose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE\nextracts node-level representations that express node memberships over distinct\nextreme profiles, referred to as archetypes, within the network. This is\nachieved by projecting the graph onto a learned polytope, which governs its\npolarization. The framework employs a recently proposed likelihood for\nanalyzing signed networks based on the Skellam distribution, combined with\nrelational archetypal analysis and GNNs. Our experimental evaluation\ndemonstrates the SGAAEs' capability to successfully infer node memberships over\nthe different underlying latent structures while extracting competing\ncommunities formed through the participation of the opposing views in the\nnetwork. Additionally, we introduce the 2-level network polarization problem\nand show how SGAAE is able to characterize such a setting. The proposed model\nachieves high performance in different tasks of signed link prediction across\nfour real-world datasets, outperforming several baseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoders based on Graph Neural Networks (GNNs) have garnered significant\nattention in recent years for their ability to extract informative latent\nrepresentations, characterizing the structure of complex topologies, such as\ngraphs. Despite the prevalence of Graph Autoencoders, there has been limited\nfocus on developing and evaluating explainable neural-based graph generative\nmodels specifically designed for signed networks. To address this gap, we\npropose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE\nextracts node-level representations that express node memberships over distinct\nextreme profiles, referred to as archetypes, within the network. This is\nachieved by projecting the graph onto a learned polytope, which governs its\npolarization. The framework employs a recently proposed likelihood for\nanalyzing signed networks based on the Skellam distribution, combined with\nrelational archetypal analysis and GNNs. Our experimental evaluation\ndemonstrates the SGAAEs' capability to successfully infer node memberships over\nthe different underlying latent structures while extracting competing\ncommunities formed through the participation of the opposing views in the\nnetwork. Additionally, we introduce the 2-level network polarization problem\nand show how SGAAE is able to characterize such a setting. The proposed model\nachieves high performance in different tasks of signed link prediction across\nfour real-world datasets, outperforming several baseline models."
                },
                "authors": [
                    {
                        "name": "Nikolaos Nakis"
                    },
                    {
                        "name": "Chrysoula Kosma"
                    },
                    {
                        "name": "Giannis Nikolentzos"
                    },
                    {
                        "name": "Michalis Chatzianastasis"
                    },
                    {
                        "name": "Iakovos Evdaimon"
                    },
                    {
                        "name": "Michalis Vazirgiannis"
                    }
                ],
                "author_detail": {
                    "name": "Michalis Vazirgiannis"
                },
                "author": "Michalis Vazirgiannis",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12476v2",
                "updated": "2024-09-16T16:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    28,
                    47,
                    0,
                    260,
                    0
                ],
                "published": "2024-04-18T19:19:26Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    19,
                    26,
                    3,
                    109,
                    0
                ],
                "title": "Up-scattering production of a sterile fermion at DUNE: complementarity\n  with spallation source and direct detection experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Up-scattering production of a sterile fermion at DUNE: complementarity\n  with spallation source and direct detection experiments"
                },
                "summary": "We investigate the possible production of a MeV-scale sterile fermion through\nthe up-scattering of neutrinos on nuclei and atomic electrons at different\nfacilities. We consider a phenomenological model that adds a new fermion to the\nparticle content of the Standard Model and we allow for all possible\nLorentz-invariant non-derivative interactions (scalar, pseudoscalar, vector,\naxial-vector and tensor) of neutrinos with electrons and first-generation\nquarks. We first explore the sensitivity of the DUNE experiment to this\nscenario, by simulating elastic neutrino-electron scattering events in the near\ndetector. We consider both options of a standard and a tau-optimized neutrino\nbeams, and investigate the impact of a mobile detector that can be moved\noff-axis with respect to the beam. Next, we infer constraints on the typical\ncoupling, new fermion and mediator masses from elastic neutrino-electron\nscattering events induced by solar neutrinos in two current dark matter direct\ndetection experiments, XENONnT and LZ. Under the assumption that the new\nmediators couple also to first-generation quarks, we further set constraints on\nthe up-scattering production of the sterile fermion using coherent elastic\nneutrino-nucleus scattering data from the COHERENT experiment. Moreover, we set\nadditional constraints assuming that the sterile fermion may decay within the\ndetector. We finally compare our results and discuss how these facilities are\nsensitive to different regions of the relevant parameter space due to\nkinematics arguments and can hence provide complementary information on the\nup-scattering production of a sterile fermion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the possible production of a MeV-scale sterile fermion through\nthe up-scattering of neutrinos on nuclei and atomic electrons at different\nfacilities. We consider a phenomenological model that adds a new fermion to the\nparticle content of the Standard Model and we allow for all possible\nLorentz-invariant non-derivative interactions (scalar, pseudoscalar, vector,\naxial-vector and tensor) of neutrinos with electrons and first-generation\nquarks. We first explore the sensitivity of the DUNE experiment to this\nscenario, by simulating elastic neutrino-electron scattering events in the near\ndetector. We consider both options of a standard and a tau-optimized neutrino\nbeams, and investigate the impact of a mobile detector that can be moved\noff-axis with respect to the beam. Next, we infer constraints on the typical\ncoupling, new fermion and mediator masses from elastic neutrino-electron\nscattering events induced by solar neutrinos in two current dark matter direct\ndetection experiments, XENONnT and LZ. Under the assumption that the new\nmediators couple also to first-generation quarks, we further set constraints on\nthe up-scattering production of the sterile fermion using coherent elastic\nneutrino-nucleus scattering data from the COHERENT experiment. Moreover, we set\nadditional constraints assuming that the sterile fermion may decay within the\ndetector. We finally compare our results and discuss how these facilities are\nsensitive to different regions of the relevant parameter space due to\nkinematics arguments and can hence provide complementary information on the\nup-scattering production of a sterile fermion."
                },
                "authors": [
                    {
                        "name": "Pablo M. Candela"
                    },
                    {
                        "name": "Valentina De Romeri"
                    },
                    {
                        "name": "Pantelis Melas"
                    },
                    {
                        "name": "Dimitrios K. Papoulias"
                    },
                    {
                        "name": "Niki Saoulidou"
                    }
                ],
                "author_detail": {
                    "name": "Niki Saoulidou"
                },
                "author": "Niki Saoulidou",
                "arxiv_comment": "29 pages, 9 figures, 1 table; spin-dependent cross sections\n  corrected, typos corrected, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10444v1",
                "updated": "2024-09-16T16:28:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    28,
                    34,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T16:28:34Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    28,
                    34,
                    0,
                    260,
                    0
                ],
                "title": "LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot\n  Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot\n  Task Planning"
                },
                "summary": "Robotic assembly tasks are open challenges due to the long task horizon and\ncomplex part relations. Behavior trees (BTs) are increasingly used in robot\ntask planning for their modularity and flexibility, but manually designing them\ncan be effort-intensive. Large language models (LLMs) have recently been\napplied in robotic task planning for generating action sequences, but their\nability to generate BTs has not been fully investigated. To this end, We\npropose LLM as BT-planner, a novel framework to leverage LLMs for BT generation\nin robotic assembly task planning and execution. Four in-context learning\nmethods are introduced to utilize the natural language processing and inference\ncapabilities of LLMs to produce task plans in BT format, reducing manual effort\nand ensuring robustness and comprehensibility. We also evaluate the performance\nof fine-tuned, fewer-parameter LLMs on the same tasks. Experiments in simulated\nand real-world settings show that our framework enhances LLMs' performance in\nBT generation, improving success rates in BT generation through in-context\nlearning and supervised fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic assembly tasks are open challenges due to the long task horizon and\ncomplex part relations. Behavior trees (BTs) are increasingly used in robot\ntask planning for their modularity and flexibility, but manually designing them\ncan be effort-intensive. Large language models (LLMs) have recently been\napplied in robotic task planning for generating action sequences, but their\nability to generate BTs has not been fully investigated. To this end, We\npropose LLM as BT-planner, a novel framework to leverage LLMs for BT generation\nin robotic assembly task planning and execution. Four in-context learning\nmethods are introduced to utilize the natural language processing and inference\ncapabilities of LLMs to produce task plans in BT format, reducing manual effort\nand ensuring robustness and comprehensibility. We also evaluate the performance\nof fine-tuned, fewer-parameter LLMs on the same tasks. Experiments in simulated\nand real-world settings show that our framework enhances LLMs' performance in\nBT generation, improving success rates in BT generation through in-context\nlearning and supervised fine-tuning."
                },
                "authors": [
                    {
                        "name": "Jicong Ao"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Yansong Wu"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Sami Haddadin"
                    }
                ],
                "author_detail": {
                    "name": "Sami Haddadin"
                },
                "author": "Sami Haddadin",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00822v2",
                "updated": "2024-09-16T16:24:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    24,
                    5,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-01T19:43:40Z",
                "published_parsed": [
                    2024,
                    9,
                    1,
                    19,
                    43,
                    40,
                    6,
                    245,
                    0
                ],
                "title": "RTop-K: Ultra-Fast Row-Wise Top-K Algorithm and GPU Implementation for\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTop-K: Ultra-Fast Row-Wise Top-K Algorithm and GPU Implementation for\n  Neural Networks"
                },
                "summary": "Top-k algorithms are essential in various applications, from high-performance\ncomputing and information retrieval to big data and neural network model\ntraining. This paper introduces RTop-K, a highly efficient parallel row-wise\ntop-k selection algorithm designed for GPUs. RTop-K employs a Binary\nSearch-based approach to optimize resource allocation and provides a scalable\nsolution that significantly accelerates top-k operations. We perform a\ntheoretical analysis of the effects of early stopping in our algorithm,\ndemonstrating that it maintains the accuracy of neural network models while\nenhancing performance. Comprehensive tests show that our GPU implementation of\nRTop-K outperforms other row-wise top-k GPU implementations, with minimal\nimpact on testing accuracy when early stopping is applied. Notably, RTop-K\nachieves speed increases ranging from 4.245$\\times$ to 9.506$\\times$ with early\nstopping, and 3.936$\\times$ without early stopping, compared to\nstate-of-the-art implementations. The proposed methods offer significant\nimprovements in the training and inference of Graph Neural Networks (GNNs),\naddressing critical challenges in latency and throughput on GPU platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k algorithms are essential in various applications, from high-performance\ncomputing and information retrieval to big data and neural network model\ntraining. This paper introduces RTop-K, a highly efficient parallel row-wise\ntop-k selection algorithm designed for GPUs. RTop-K employs a Binary\nSearch-based approach to optimize resource allocation and provides a scalable\nsolution that significantly accelerates top-k operations. We perform a\ntheoretical analysis of the effects of early stopping in our algorithm,\ndemonstrating that it maintains the accuracy of neural network models while\nenhancing performance. Comprehensive tests show that our GPU implementation of\nRTop-K outperforms other row-wise top-k GPU implementations, with minimal\nimpact on testing accuracy when early stopping is applied. Notably, RTop-K\nachieves speed increases ranging from 4.245$\\times$ to 9.506$\\times$ with early\nstopping, and 3.936$\\times$ without early stopping, compared to\nstate-of-the-art implementations. The proposed methods offer significant\nimprovements in the training and inference of Graph Neural Networks (GNNs),\naddressing critical challenges in latency and throughput on GPU platforms."
                },
                "authors": [
                    {
                        "name": "Xi Xie"
                    },
                    {
                        "name": "Yuebo Luo"
                    },
                    {
                        "name": "Hongwu Peng"
                    },
                    {
                        "name": "Caiwen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Caiwen Ding"
                },
                "author": "Caiwen Ding",
                "arxiv_comment": "Need to improve the experiment part",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10432v1",
                "updated": "2024-09-16T16:07:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    7,
                    21,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T16:07:21Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    7,
                    21,
                    0,
                    260,
                    0
                ],
                "title": "Structure-preserving learning for multi-symplectic PDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-preserving learning for multi-symplectic PDEs"
                },
                "summary": "This paper presents an energy-preserving machine learning method for\ninferring reduced-order models (ROMs) by exploiting the multi-symplectic form\nof partial differential equations (PDEs). The vast majority of\nenergy-preserving reduced-order methods use symplectic Galerkin projection to\nconstruct reduced-order Hamiltonian models by projecting the full models onto a\nsymplectic subspace. However, symplectic projection requires the existence of\nfully discrete operators, and in many cases, such as black-box PDE solvers,\nthese operators are inaccessible. In this work, we propose an energy-preserving\nmachine learning method that can infer the dynamics of the given PDE using data\nonly, so that the proposed framework does not depend on the fully discrete\noperators. In this context, the proposed method is non-intrusive. The proposed\nmethod is grey box in the sense that it requires only some basic knowledge of\nthe multi-symplectic model at the partial differential equation level. We prove\nthat the proposed method satisfies spatially discrete local energy conservation\nand preserves the multi-symplectic conservation laws. We test our method on the\nlinear wave equation, the Korteweg-de Vries equation, and the\nZakharov-Kuznetsov equation. We test the generalization of our learned models\nby testing them far outside the training time interval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an energy-preserving machine learning method for\ninferring reduced-order models (ROMs) by exploiting the multi-symplectic form\nof partial differential equations (PDEs). The vast majority of\nenergy-preserving reduced-order methods use symplectic Galerkin projection to\nconstruct reduced-order Hamiltonian models by projecting the full models onto a\nsymplectic subspace. However, symplectic projection requires the existence of\nfully discrete operators, and in many cases, such as black-box PDE solvers,\nthese operators are inaccessible. In this work, we propose an energy-preserving\nmachine learning method that can infer the dynamics of the given PDE using data\nonly, so that the proposed framework does not depend on the fully discrete\noperators. In this context, the proposed method is non-intrusive. The proposed\nmethod is grey box in the sense that it requires only some basic knowledge of\nthe multi-symplectic model at the partial differential equation level. We prove\nthat the proposed method satisfies spatially discrete local energy conservation\nand preserves the multi-symplectic conservation laws. We test our method on the\nlinear wave equation, the Korteweg-de Vries equation, and the\nZakharov-Kuznetsov equation. We test the generalization of our learned models\nby testing them far outside the training time interval."
                },
                "authors": [
                    {
                        "name": "Süleyman Yıldız"
                    },
                    {
                        "name": "Pawan Goyal"
                    },
                    {
                        "name": "Peter Benner"
                    }
                ],
                "author_detail": {
                    "name": "Peter Benner"
                },
                "author": "Peter Benner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01256v2",
                "updated": "2024-09-16T16:05:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    5,
                    18,
                    0,
                    260,
                    0
                ],
                "published": "2024-01-02T15:56:48Z",
                "published_parsed": [
                    2024,
                    1,
                    2,
                    15,
                    56,
                    48,
                    1,
                    2,
                    0
                ],
                "title": "VideoStudio: Generating Consistent-Content and Multi-Scene Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoStudio: Generating Consistent-Content and Multi-Scene Videos"
                },
                "summary": "The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoStudio, for consistent-content and multi-scene video\ngeneration. Technically, VideoStudio leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoStudio identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoStudio outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoStudio outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference. Source code\nis available at \\url{https://github.com/FuchenUSTC/VideoStudio}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoStudio, for consistent-content and multi-scene video\ngeneration. Technically, VideoStudio leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoStudio identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoStudio outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoStudio outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference. Source code\nis available at \\url{https://github.com/FuchenUSTC/VideoStudio}."
                },
                "authors": [
                    {
                        "name": "Fuchen Long"
                    },
                    {
                        "name": "Zhaofan Qiu"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Tao Mei"
                    }
                ],
                "author_detail": {
                    "name": "Tao Mei"
                },
                "author": "Tao Mei",
                "arxiv_comment": "ECCV 2024. Source code is available at\n  https://github.com/FuchenUSTC/VideoStudio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.01256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07684v2",
                "updated": "2024-09-16T16:02:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    2,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-07-10T14:08:27Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    14,
                    8,
                    27,
                    2,
                    192,
                    0
                ],
                "title": "Towards Human-Like Driving: Active Inference in Autonomous Vehicle\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Human-Like Driving: Active Inference in Autonomous Vehicle\n  Control"
                },
                "summary": "This paper presents a novel approach to Autonomous Vehicle (AV) control\nthrough the application of active inference, a theory derived from neuroscience\nthat conceptualizes the brain as a predictive machine. Traditional autonomous\ndriving systems rely heavily on Modular Pipelines, Imitation Learning, or\nReinforcement Learning, each with inherent limitations in adaptability,\ngeneralization, and computational efficiency. Active inference addresses these\nchallenges by minimizing prediction error (termed \"surprise\") through a dynamic\nmodel that balances perception and action. Our method integrates active\ninference with deep learning to manage lateral control in AVs, enabling them to\nperform lane following maneuvers within a simulated urban environment. We\ndemonstrate that our model, despite its simplicity, effectively learns and\ngeneralizes from limited data without extensive retraining, significantly\nreducing computational demands. The proposed approach not only enhances the\nadaptability and performance of AVs in dynamic scenarios but also aligns\nclosely with human-like driving behavior, leveraging a generative model to\npredict and adapt to environmental changes. Results from extensive experiments\nin the CARLA simulator show promising outcomes, outperforming traditional\nmethods in terms of adaptability and efficiency, thereby advancing the\npotential of active inference in real-world autonomous driving applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to Autonomous Vehicle (AV) control\nthrough the application of active inference, a theory derived from neuroscience\nthat conceptualizes the brain as a predictive machine. Traditional autonomous\ndriving systems rely heavily on Modular Pipelines, Imitation Learning, or\nReinforcement Learning, each with inherent limitations in adaptability,\ngeneralization, and computational efficiency. Active inference addresses these\nchallenges by minimizing prediction error (termed \"surprise\") through a dynamic\nmodel that balances perception and action. Our method integrates active\ninference with deep learning to manage lateral control in AVs, enabling them to\nperform lane following maneuvers within a simulated urban environment. We\ndemonstrate that our model, despite its simplicity, effectively learns and\ngeneralizes from limited data without extensive retraining, significantly\nreducing computational demands. The proposed approach not only enhances the\nadaptability and performance of AVs in dynamic scenarios but also aligns\nclosely with human-like driving behavior, leveraging a generative model to\npredict and adapt to environmental changes. Results from extensive experiments\nin the CARLA simulator show promising outcomes, outperforming traditional\nmethods in terms of adaptability and efficiency, thereby advancing the\npotential of active inference in real-world autonomous driving applications."
                },
                "authors": [
                    {
                        "name": "Elahe Delavari"
                    },
                    {
                        "name": "John Moore"
                    },
                    {
                        "name": "Junho Hong"
                    },
                    {
                        "name": "Jaerock Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Jaerock Kwon"
                },
                "author": "Jaerock Kwon",
                "arxiv_comment": "The work is partly supported by a sponsor. Authors need to complete\n  the final report submission before any type of publication according to the\n  sponsor. The final report will be submitted in few weeks. Then, authors will\n  reinstate this paper after that",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02020v2",
                "updated": "2024-09-16T15:51:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    51,
                    35,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-03T16:12:12Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    12,
                    12,
                    1,
                    247,
                    0
                ],
                "title": "Efficient Point Cloud Classification via Offline Distillation Framework\n  and Negative-Weight Self-Distillation Technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Point Cloud Classification via Offline Distillation Framework\n  and Negative-Weight Self-Distillation Technique"
                },
                "summary": "The rapid advancement in point cloud processing technologies has\nsignificantly increased the demand for efficient and compact models that\nachieve high-accuracy classification. Knowledge distillation has emerged as a\npotent model compression technique. However, traditional KD often requires\nextensive computational resources for forward inference of large teacher\nmodels, thereby reducing training efficiency for student models and increasing\nresource demands. To address these challenges, we introduce an innovative\noffline recording strategy that avoids the simultaneous loading of both teacher\nand student models, thereby reducing hardware demands. This approach feeds a\nmultitude of augmented samples into the teacher model, recording both the data\naugmentation parameters and the corresponding logit outputs. By applying\nshape-level augmentation operations such as random scaling and translation,\nwhile excluding point-level operations like random jittering, the size of the\nrecords is significantly reduced. Additionally, to mitigate the issue of small\nstudent model over-imitating the teacher model's outputs and converging to\nsuboptimal solutions, we incorporate a negative-weight self-distillation\nstrategy. Experimental results demonstrate that the proposed distillation\nstrategy enables the student model to achieve performance comparable to\nstate-of-the-art models while maintaining lower parameter count. This approach\nstrikes an optimal balance between performance and complexity. This study\nhighlights the potential of our method to optimize knowledge distillation for\npoint cloud classification tasks, particularly in resource-constrained\nenvironments, providing a novel solution for efficient point cloud analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in point cloud processing technologies has\nsignificantly increased the demand for efficient and compact models that\nachieve high-accuracy classification. Knowledge distillation has emerged as a\npotent model compression technique. However, traditional KD often requires\nextensive computational resources for forward inference of large teacher\nmodels, thereby reducing training efficiency for student models and increasing\nresource demands. To address these challenges, we introduce an innovative\noffline recording strategy that avoids the simultaneous loading of both teacher\nand student models, thereby reducing hardware demands. This approach feeds a\nmultitude of augmented samples into the teacher model, recording both the data\naugmentation parameters and the corresponding logit outputs. By applying\nshape-level augmentation operations such as random scaling and translation,\nwhile excluding point-level operations like random jittering, the size of the\nrecords is significantly reduced. Additionally, to mitigate the issue of small\nstudent model over-imitating the teacher model's outputs and converging to\nsuboptimal solutions, we incorporate a negative-weight self-distillation\nstrategy. Experimental results demonstrate that the proposed distillation\nstrategy enables the student model to achieve performance comparable to\nstate-of-the-art models while maintaining lower parameter count. This approach\nstrikes an optimal balance between performance and complexity. This study\nhighlights the potential of our method to optimize knowledge distillation for\npoint cloud classification tasks, particularly in resource-constrained\nenvironments, providing a novel solution for efficient point cloud analysis."
                },
                "authors": [
                    {
                        "name": "Qiang Zheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10385v1",
                "updated": "2024-09-16T15:20:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    20,
                    48,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T15:20:48Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    20,
                    48,
                    0,
                    260,
                    0
                ],
                "title": "Mamba-ST: State Space Model for Efficient Style Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba-ST: State Space Model for Efficient Style Transfer"
                },
                "summary": "The goal of style transfer is, given a content image and a style source,\ngenerating a new image preserving the content but with the artistic\nrepresentation of the style source. Most of the state-of-the-art architectures\nuse transformers or diffusion-based models to perform this task, despite the\nheavy computational burden that they require. In particular, transformers use\nself- and cross-attention layers which have large memory footprint, while\ndiffusion models require high inference time. To overcome the above, this paper\nexplores a novel design of Mamba, an emergent State-Space Model (SSM), called\nMamba-ST, to perform style transfer. To do so, we adapt Mamba linear equation\nto simulate the behavior of cross-attention layers, which are able to combine\ntwo separate embeddings into a single output, but drastically reducing memory\nusage and time complexity. We modified the Mamba's inner equations so to accept\ninputs from, and combine, two separate data streams. To the best of our\nknowledge, this is the first attempt to adapt the equations of SSMs to a vision\ntask like style transfer without requiring any other module like\ncross-attention or custom normalization layers. An extensive set of experiments\ndemonstrates the superiority and efficiency of our method in performing style\ntransfer compared to transformers and diffusion models. Results show improved\nquality in terms of both ArtFID and FID metrics. Code is available at\nhttps://github.com/FilippoBotti/MambaST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of style transfer is, given a content image and a style source,\ngenerating a new image preserving the content but with the artistic\nrepresentation of the style source. Most of the state-of-the-art architectures\nuse transformers or diffusion-based models to perform this task, despite the\nheavy computational burden that they require. In particular, transformers use\nself- and cross-attention layers which have large memory footprint, while\ndiffusion models require high inference time. To overcome the above, this paper\nexplores a novel design of Mamba, an emergent State-Space Model (SSM), called\nMamba-ST, to perform style transfer. To do so, we adapt Mamba linear equation\nto simulate the behavior of cross-attention layers, which are able to combine\ntwo separate embeddings into a single output, but drastically reducing memory\nusage and time complexity. We modified the Mamba's inner equations so to accept\ninputs from, and combine, two separate data streams. To the best of our\nknowledge, this is the first attempt to adapt the equations of SSMs to a vision\ntask like style transfer without requiring any other module like\ncross-attention or custom normalization layers. An extensive set of experiments\ndemonstrates the superiority and efficiency of our method in performing style\ntransfer compared to transformers and diffusion models. Results show improved\nquality in terms of both ArtFID and FID metrics. Code is available at\nhttps://github.com/FilippoBotti/MambaST."
                },
                "authors": [
                    {
                        "name": "Filippo Botti"
                    },
                    {
                        "name": "Alex Ergasti"
                    },
                    {
                        "name": "Leonardo Rossi"
                    },
                    {
                        "name": "Tomaso Fontanini"
                    },
                    {
                        "name": "Claudio Ferrari"
                    },
                    {
                        "name": "Massimo Bertozzi"
                    },
                    {
                        "name": "Andrea Prati"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Prati"
                },
                "author": "Andrea Prati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10381v1",
                "updated": "2024-09-16T15:19:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    19,
                    38,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T15:19:38Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    19,
                    38,
                    0,
                    260,
                    0
                ],
                "title": "J-UBIK: The JAX-accelerated Universal Bayesian Imaging Kit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "J-UBIK: The JAX-accelerated Universal Bayesian Imaging Kit"
                },
                "summary": "Many advances in astronomy and astrophysics originate from accurate images of\nthe sky emission across multiple wavelengths. This often requires\nreconstructing spatially and spectrally correlated signals detected from\nmultiple instruments. To facilitate the high-fidelity imaging of these signals,\nwe introduce the universal Bayesian imaging kit (UBIK). Specifically, we\npresent J-UBIK, a flexible and modular implementation leveraging the\nJAX-accelerated NIFTy.re software as its backend. J-UBIK streamlines the\nimplementation of the key Bayesian inference components, providing for all the\nnecessary steps of Bayesian imaging pipelines. First, it provides adaptable\nprior models for different sky realizations. Second, it includes likelihood\nmodels tailored to specific instruments. So far, the package includes three\ninstruments: Chandra and eROSITA for X-ray observations, and the James Webb\nSpace Telescope (JWST) for the near- and mid-infrared. The aim is to expand\nthis set in the future. Third, these models can be integrated with various\ninference and optimization schemes, such as maximum a posteriori estimation and\nvariational inference. Explicit demos show how to integrate the individual\nmodules into a full analysis pipeline. Overall, J-UBIK enables efficient\ngeneration of high-fidelity images via Bayesian pipelines that can be tailored\nto specific research objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many advances in astronomy and astrophysics originate from accurate images of\nthe sky emission across multiple wavelengths. This often requires\nreconstructing spatially and spectrally correlated signals detected from\nmultiple instruments. To facilitate the high-fidelity imaging of these signals,\nwe introduce the universal Bayesian imaging kit (UBIK). Specifically, we\npresent J-UBIK, a flexible and modular implementation leveraging the\nJAX-accelerated NIFTy.re software as its backend. J-UBIK streamlines the\nimplementation of the key Bayesian inference components, providing for all the\nnecessary steps of Bayesian imaging pipelines. First, it provides adaptable\nprior models for different sky realizations. Second, it includes likelihood\nmodels tailored to specific instruments. So far, the package includes three\ninstruments: Chandra and eROSITA for X-ray observations, and the James Webb\nSpace Telescope (JWST) for the near- and mid-infrared. The aim is to expand\nthis set in the future. Third, these models can be integrated with various\ninference and optimization schemes, such as maximum a posteriori estimation and\nvariational inference. Explicit demos show how to integrate the individual\nmodules into a full analysis pipeline. Overall, J-UBIK enables efficient\ngeneration of high-fidelity images via Bayesian pipelines that can be tailored\nto specific research objectives."
                },
                "authors": [
                    {
                        "name": "Vincent Eberle"
                    },
                    {
                        "name": "Matteo Guardiani"
                    },
                    {
                        "name": "Margret Westerkamp"
                    },
                    {
                        "name": "Philipp Frank"
                    },
                    {
                        "name": "Julian Rüstig"
                    },
                    {
                        "name": "Julia Stadler"
                    },
                    {
                        "name": "Torsten A. Enßlin"
                    }
                ],
                "author_detail": {
                    "name": "Torsten A. Enßlin"
                },
                "author": "Torsten A. Enßlin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10372v1",
                "updated": "2024-09-16T15:15:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    15,
                    51,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T15:15:51Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    15,
                    51,
                    0,
                    260,
                    0
                ],
                "title": "Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation"
                },
                "summary": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings."
                },
                "authors": [
                    {
                        "name": "Qiliang Chen"
                    },
                    {
                        "name": "Alireza"
                    },
                    {
                        "name": "Ilami"
                    },
                    {
                        "name": "Nunzio Lore"
                    },
                    {
                        "name": "Babak Heydari"
                    }
                ],
                "author_detail": {
                    "name": "Babak Heydari"
                },
                "arxiv_affiliation": "Sepehr",
                "author": "Babak Heydari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10354v2",
                "updated": "2024-09-17T03:22:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    22,
                    45,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-16T15:04:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    4,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "Learnings from a Large-Scale Deployment of an LLM-Powered\n  Expert-in-the-Loop Healthcare Chatbot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnings from a Large-Scale Deployment of an LLM-Powered\n  Expert-in-the-Loop Healthcare Chatbot"
                },
                "summary": "Large Language Models (LLMs) are widely used in healthcare, but limitations\nlike hallucinations, incomplete information, and bias hinder their reliability.\nTo address these, researchers released the Build Your Own expert Bot (BYOeB)\nplatform, enabling developers to create LLM-powered chatbots with integrated\nexpert verification. CataractBot, its first implementation, provides\nexpert-verified responses to cataract surgery questions. A pilot evaluation\nshowed its potential; however the study had a small sample size and was\nprimarily qualitative. In this work, we conducted a large-scale 24-week\ndeployment of CataractBot involving 318 patients and attendants who sent 1,992\nmessages, with 91.71% of responses verified by seven experts. Analysis of\ninteraction logs revealed that medical questions significantly outnumbered\nlogistical ones, hallucinations were negligible, and experts rated 84.52% of\nmedical answers as accurate. As the knowledge base expanded with expert\ncorrections, system performance improved by 19.02%, reducing expert workload.\nThese insights guide the design of future LLM-powered chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in healthcare, but limitations\nlike hallucinations, incomplete information, and bias hinder their reliability.\nTo address these, researchers released the Build Your Own expert Bot (BYOeB)\nplatform, enabling developers to create LLM-powered chatbots with integrated\nexpert verification. CataractBot, its first implementation, provides\nexpert-verified responses to cataract surgery questions. A pilot evaluation\nshowed its potential; however the study had a small sample size and was\nprimarily qualitative. In this work, we conducted a large-scale 24-week\ndeployment of CataractBot involving 318 patients and attendants who sent 1,992\nmessages, with 91.71% of responses verified by seven experts. Analysis of\ninteraction logs revealed that medical questions significantly outnumbered\nlogistical ones, hallucinations were negligible, and experts rated 84.52% of\nmedical answers as accurate. As the knowledge base expanded with expert\ncorrections, system performance improved by 19.02%, reducing expert workload.\nThese insights guide the design of future LLM-powered chatbots."
                },
                "authors": [
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Geeta Fulari"
                    },
                    {
                        "name": "Kaushik Murali"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "arxiv_comment": "The first two authors contributed equally to this research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10343v1",
                "updated": "2024-09-16T14:57:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    57,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T14:57:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    57,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Large Language Model Enhanced Hard Sample Identification for Denoising\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enhanced Hard Sample Identification for Denoising\n  Recommendation"
                },
                "summary": "Implicit feedback, often used to build recommender systems, unavoidably\nconfronts noise due to factors such as misclicks and position bias. Previous\nstudies have attempted to alleviate this by identifying noisy samples based on\ntheir diverged patterns, such as higher loss values, and mitigating the noise\nthrough sample dropping or reweighting. Despite the progress, we observe\nexisting approaches struggle to distinguish hard samples and noise samples, as\nthey often exhibit similar patterns, thereby limiting their effectiveness in\ndenoising recommendations. To address this challenge, we propose a Large\nLanguage Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,\nwe construct an LLM-based scorer to evaluate the semantic consistency of items\nwith the user preference, which is quantified based on summarized historical\nuser interactions. The resulting scores are used to assess the hardness of\nsamples for the pointwise or pairwise training objectives. To ensure\nefficiency, we introduce a variance-based sample pruning strategy to filter\npotential hard samples before scoring. Besides, we propose an iterative\npreference update module designed to continuously refine summarized user\npreference, which may be biased due to false-positive user-item interactions.\nExtensive experiments on three real-world datasets and four backbone\nrecommenders demonstrate the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit feedback, often used to build recommender systems, unavoidably\nconfronts noise due to factors such as misclicks and position bias. Previous\nstudies have attempted to alleviate this by identifying noisy samples based on\ntheir diverged patterns, such as higher loss values, and mitigating the noise\nthrough sample dropping or reweighting. Despite the progress, we observe\nexisting approaches struggle to distinguish hard samples and noise samples, as\nthey often exhibit similar patterns, thereby limiting their effectiveness in\ndenoising recommendations. To address this challenge, we propose a Large\nLanguage Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,\nwe construct an LLM-based scorer to evaluate the semantic consistency of items\nwith the user preference, which is quantified based on summarized historical\nuser interactions. The resulting scores are used to assess the hardness of\nsamples for the pointwise or pairwise training objectives. To ensure\nefficiency, we introduce a variance-based sample pruning strategy to filter\npotential hard samples before scoring. Besides, we propose an iterative\npreference update module designed to continuously refine summarized user\npreference, which may be biased due to false-positive user-item interactions.\nExtensive experiments on three real-world datasets and four backbone\nrecommenders demonstrate the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Tianrui Song"
                    },
                    {
                        "name": "Wenshuo Chao"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05870v2",
                "updated": "2024-09-16T14:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-09T17:55:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    17,
                    55,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents"
                },
                "summary": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database, then generating an answer by\napplying an LLM to the retrieved documents. We demonstrate that RAG systems\nthat operate on databases with untrusted content are vulnerable to a new class\nof denial-of-service attacks we call jamming. An adversary can add a single\n``blocker'' document to the database that will be retrieved in response to a\nspecific query and result in the RAG system not answering this query -\nostensibly because it lacks the information or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. This\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not use an auxiliary LLM to generate blocker documents.\n  We evaluate jamming attacks on several LLMs and embeddings and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database, then generating an answer by\napplying an LLM to the retrieved documents. We demonstrate that RAG systems\nthat operate on databases with untrusted content are vulnerable to a new class\nof denial-of-service attacks we call jamming. An adversary can add a single\n``blocker'' document to the database that will be retrieved in response to a\nspecific query and result in the RAG system not answering this query -\nostensibly because it lacks the information or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. This\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not use an auxiliary LLM to generate blocker documents.\n  We evaluate jamming attacks on several LLMs and embeddings and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents."
                },
                "authors": [
                    {
                        "name": "Avital Shafran"
                    },
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    }
                ],
                "author_detail": {
                    "name": "Vitaly Shmatikov"
                },
                "author": "Vitaly Shmatikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10339v1",
                "updated": "2024-09-16T14:52:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    52,
                    22,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T14:52:22Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    52,
                    22,
                    0,
                    260,
                    0
                ],
                "title": "VAE-QWGAN: Improving Quantum GANs for High Resolution Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAE-QWGAN: Improving Quantum GANs for High Resolution Image Generation"
                },
                "summary": "This paper presents a novel hybrid quantum generative model, the VAE-QWGAN,\nwhich combines the strengths of a classical Variational AutoEncoder (VAE) with\na hybrid Quantum Wasserstein Generative Adversarial Network (QWGAN). The\nVAE-QWGAN integrates the VAE decoder and QGAN generator into a single quantum\nmodel with shared parameters, utilizing the VAE's encoder for latent vector\nsampling during training. To generate new data from the trained model at\ninference, input latent vectors are sampled from a Gaussian Mixture Model\n(GMM), learnt on the training latent vectors. This, in turn, enhances the\ndiversity and quality of generated images. We evaluate the model's performance\non MNIST/Fashion-MNIST datasets, and demonstrate improved quality and diversity\nof generated images compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel hybrid quantum generative model, the VAE-QWGAN,\nwhich combines the strengths of a classical Variational AutoEncoder (VAE) with\na hybrid Quantum Wasserstein Generative Adversarial Network (QWGAN). The\nVAE-QWGAN integrates the VAE decoder and QGAN generator into a single quantum\nmodel with shared parameters, utilizing the VAE's encoder for latent vector\nsampling during training. To generate new data from the trained model at\ninference, input latent vectors are sampled from a Gaussian Mixture Model\n(GMM), learnt on the training latent vectors. This, in turn, enhances the\ndiversity and quality of generated images. We evaluate the model's performance\non MNIST/Fashion-MNIST datasets, and demonstrate improved quality and diversity\nof generated images compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Aaron Mark Thomas"
                    },
                    {
                        "name": "Sharu Theresa Jose"
                    }
                ],
                "author_detail": {
                    "name": "Sharu Theresa Jose"
                },
                "author": "Sharu Theresa Jose",
                "arxiv_comment": "5 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10338v1",
                "updated": "2024-09-16T14:50:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    50,
                    29,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T14:50:29Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    50,
                    29,
                    0,
                    260,
                    0
                ],
                "title": "The 20 questions game to distinguish large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 20 questions game to distinguish large language models"
                },
                "summary": "In a parallel with the 20 questions game, we present a method to determine\nwhether two large language models (LLMs), placed in a black-box context, are\nthe same or not. The goal is to use a small set of (benign) binary questions,\ntypically under 20. We formalize the problem and first establish a baseline\nusing a random selection of questions from known benchmark datasets, achieving\nan accuracy of nearly 100% within 20 questions. After showing optimal bounds\nfor this problem, we introduce two effective questioning heuristics able to\ndiscriminate 22 LLMs by using half as many questions for the same task. These\nmethods offer significant advantages in terms of stealth and are thus of\ninterest to auditors or copyright owners facing suspicions of model leaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a parallel with the 20 questions game, we present a method to determine\nwhether two large language models (LLMs), placed in a black-box context, are\nthe same or not. The goal is to use a small set of (benign) binary questions,\ntypically under 20. We formalize the problem and first establish a baseline\nusing a random selection of questions from known benchmark datasets, achieving\nan accuracy of nearly 100% within 20 questions. After showing optimal bounds\nfor this problem, we introduce two effective questioning heuristics able to\ndiscriminate 22 LLMs by using half as many questions for the same task. These\nmethods offer significant advantages in terms of stealth and are thus of\ninterest to auditors or copyright owners facing suspicions of model leaks."
                },
                "authors": [
                    {
                        "name": "Gurvan Richardeau"
                    },
                    {
                        "name": "Erwan Le Merrer"
                    },
                    {
                        "name": "Camilla Penzo"
                    },
                    {
                        "name": "Gilles Tredan"
                    }
                ],
                "author_detail": {
                    "name": "Gilles Tredan"
                },
                "author": "Gilles Tredan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10318v1",
                "updated": "2024-09-16T14:27:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    27,
                    17,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T14:27:17Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    27,
                    17,
                    0,
                    260,
                    0
                ],
                "title": "Systematic comparison of Bayesian basket trial designs with unequal\n  sample sizes and proposal of a new method based on power priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic comparison of Bayesian basket trial designs with unequal\n  sample sizes and proposal of a new method based on power priors"
                },
                "summary": "Basket trials examine the efficacy of an intervention in multiple patient\nsubgroups simultaneously. The division into subgroups, called baskets, is based\non matching medical characteristics, which may result in small sample sizes\nwithin baskets that are also likely to differ. Sparse data complicate\nstatistical inference. Several Bayesian methods have been proposed in the\nliterature that allow information sharing between baskets to increase\nstatistical power. In this work, we provide a systematic comparison of five\ndifferent Bayesian basket trial designs when sample sizes differ between\nbaskets. We consider the power prior approach with both known and new weighting\nmethods, a design by Fujikawa et al., as well as models based on Bayesian\nhierarchical modeling and Bayesian model averaging. The results of our\nsimulation study show a high sensitivity to changing sample sizes for\nFujikawa's design and the power prior approach. Limiting the amount of shared\ninformation was found to be decisive for the robustness to varying basket\nsizes. In combination with the power prior approach, this resulted in the best\nperformance and the most reliable detection of an effect of the treatment under\ninvestigation and its absence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Basket trials examine the efficacy of an intervention in multiple patient\nsubgroups simultaneously. The division into subgroups, called baskets, is based\non matching medical characteristics, which may result in small sample sizes\nwithin baskets that are also likely to differ. Sparse data complicate\nstatistical inference. Several Bayesian methods have been proposed in the\nliterature that allow information sharing between baskets to increase\nstatistical power. In this work, we provide a systematic comparison of five\ndifferent Bayesian basket trial designs when sample sizes differ between\nbaskets. We consider the power prior approach with both known and new weighting\nmethods, a design by Fujikawa et al., as well as models based on Bayesian\nhierarchical modeling and Bayesian model averaging. The results of our\nsimulation study show a high sensitivity to changing sample sizes for\nFujikawa's design and the power prior approach. Limiting the amount of shared\ninformation was found to be decisive for the robustness to varying basket\nsizes. In combination with the power prior approach, this resulted in the best\nperformance and the most reliable detection of an effect of the treatment under\ninvestigation and its absence."
                },
                "authors": [
                    {
                        "name": "Sabrina Schmitt"
                    },
                    {
                        "name": "Lukas Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Baumann"
                },
                "author": "Lukas Baumann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10289v1",
                "updated": "2024-09-16T13:56:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    56,
                    17,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:56:17Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    56,
                    17,
                    0,
                    260,
                    0
                ],
                "title": "ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework"
                },
                "summary": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations."
                },
                "authors": [
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Zixiang Di"
                    },
                    {
                        "name": "Zhiqing Cui"
                    },
                    {
                        "name": "Guisong Yang"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11477v2",
                "updated": "2024-09-16T13:55:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    55,
                    24,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-17T12:42:34Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    12,
                    42,
                    34,
                    0,
                    169,
                    0
                ],
                "title": "How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of\n  Target Language Text?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of\n  Target Language Text?"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities in many\nlanguages beyond English. Yet, LLMs require more inference steps when\ngenerating non-English text due to their reliance on English-centric tokenizers\nand vocabulary, resulting in higher usage costs to non-English speakers.\nVocabulary expansion with target language tokens is a widely used cross-lingual\nvocabulary adaptation approach to remedy this issue. Despite its effectiveness\nin inference speedup, previous work on vocabulary expansion has focused on\nhigh-resource settings assuming access to a substantial amount of target\nlanguage data to effectively initialize the embeddings of the new tokens and\nadapt the LLM to the target language. However, vocabulary expansion in\nlow-resource settings has yet to be explored. In this paper, we investigate\nvocabulary expansion in low-resource settings by considering embedding\ninitialization methods and continual pre-training strategies. Through extensive\nexperiments across typologically diverse languages, tasks and models, we\nestablish a set of strategies to perform vocabulary expansion for faster\ninference, maintaining competitive downstream performance to baselines with\nonly 30K sentences ($\\sim$0.01GB text data) from the target language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities in many\nlanguages beyond English. Yet, LLMs require more inference steps when\ngenerating non-English text due to their reliance on English-centric tokenizers\nand vocabulary, resulting in higher usage costs to non-English speakers.\nVocabulary expansion with target language tokens is a widely used cross-lingual\nvocabulary adaptation approach to remedy this issue. Despite its effectiveness\nin inference speedup, previous work on vocabulary expansion has focused on\nhigh-resource settings assuming access to a substantial amount of target\nlanguage data to effectively initialize the embeddings of the new tokens and\nadapt the LLM to the target language. However, vocabulary expansion in\nlow-resource settings has yet to be explored. In this paper, we investigate\nvocabulary expansion in low-resource settings by considering embedding\ninitialization methods and continual pre-training strategies. Through extensive\nexperiments across typologically diverse languages, tasks and models, we\nestablish a set of strategies to perform vocabulary expansion for faster\ninference, maintaining competitive downstream performance to baselines with\nonly 30K sentences ($\\sim$0.01GB text data) from the target language."
                },
                "authors": [
                    {
                        "name": "Atsuki Yamaguchi"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14965v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14965v5",
                "updated": "2024-09-16T13:54:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    54,
                    31,
                    0,
                    260,
                    0
                ],
                "published": "2024-04-23T12:20:14Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    12,
                    20,
                    14,
                    1,
                    114,
                    0
                ],
                "title": "Vision Beyond Boundaries: An Initial Design Space of Domain-specific\n  Large Vision Models in Human-robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Beyond Boundaries: An Initial Design Space of Domain-specific\n  Large Vision Models in Human-robot Interaction"
                },
                "summary": "The emergence of large vision models (LVMs) is following in the footsteps of\nthe recent prosperity of Large Language Models (LLMs) in following years.\nHowever, there's a noticeable gap in structured research applying LVMs to\nhuman-robot interaction (HRI), despite extensive evidence supporting the\nefficacy of vision models in enhancing interactions between humans and robots.\nRecognizing the vast and anticipated potential, we introduce an initial design\nspace that incorporates domain-specific LVMs, chosen for their superior\nperformance over normal models. We delve into three primary dimensions: HRI\ncontexts, vision-based tasks, and specific domains. The empirical evaluation\nwas implemented among 15 experts across six evaluated metrics, showcasing the\nprimary efficacy in relevant decision-making scenarios. We explore the process\nof ideation and potential application scenarios, envisioning this design space\nas a foundational guideline for future HRI system design, emphasizing accurate\ndomain alignment and model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large vision models (LVMs) is following in the footsteps of\nthe recent prosperity of Large Language Models (LLMs) in following years.\nHowever, there's a noticeable gap in structured research applying LVMs to\nhuman-robot interaction (HRI), despite extensive evidence supporting the\nefficacy of vision models in enhancing interactions between humans and robots.\nRecognizing the vast and anticipated potential, we introduce an initial design\nspace that incorporates domain-specific LVMs, chosen for their superior\nperformance over normal models. We delve into three primary dimensions: HRI\ncontexts, vision-based tasks, and specific domains. The empirical evaluation\nwas implemented among 15 experts across six evaluated metrics, showcasing the\nprimary efficacy in relevant decision-making scenarios. We explore the process\nof ideation and potential application scenarios, envisioning this design space\nas a foundational guideline for future HRI system design, emphasizing accurate\ndomain alignment and model selection."
                },
                "authors": [
                    {
                        "name": "Yuchong Zhang"
                    },
                    {
                        "name": "Yong Ma"
                    },
                    {
                        "name": "Danica Kragic"
                    }
                ],
                "author_detail": {
                    "name": "Danica Kragic"
                },
                "author": "Danica Kragic",
                "arxiv_doi": "10.1145/3640471.3680244",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3640471.3680244",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.14965v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14965v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10280v1",
                "updated": "2024-09-16T13:43:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    43,
                    4,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:43:04Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    43,
                    4,
                    0,
                    260,
                    0
                ],
                "title": "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More\n  Complex Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More\n  Complex Code"
                },
                "summary": "In recent years, the application of large language models (LLMs) to\ncode-related tasks has gained significant attention. However, existing\nevaluation benchmarks often focus on limited scenarios, such as code generation\nor completion, which do not reflect the diverse challenges developers face in\nreal-world contexts. To address this, we introduce ComplexCodeEval, a benchmark\ndesigned to assess LCMs in various development tasks, including code\ngeneration, completion, API recommendation, and test case generation. It\nincludes 3,897 Java samples and 7,184 Python samples from high-star GitHub\nrepositories, each annotated with function signatures, docstrings, and API\nreferences to simulate real development environments. Our experiments across\nten LCMs reveal that context improves performance and that data leakage can\nlead to overestimation, highlighting the need for more accurate evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the application of large language models (LLMs) to\ncode-related tasks has gained significant attention. However, existing\nevaluation benchmarks often focus on limited scenarios, such as code generation\nor completion, which do not reflect the diverse challenges developers face in\nreal-world contexts. To address this, we introduce ComplexCodeEval, a benchmark\ndesigned to assess LCMs in various development tasks, including code\ngeneration, completion, API recommendation, and test case generation. It\nincludes 3,897 Java samples and 7,184 Python samples from high-star GitHub\nrepositories, each annotated with function signatures, docstrings, and API\nreferences to simulate real development environments. Our experiments across\nten LCMs reveal that context improves performance and that data leakage can\nlead to overestimation, highlighting the need for more accurate evaluations."
                },
                "authors": [
                    {
                        "name": "Jia Feng"
                    },
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Shan Gao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_doi": "10.1145/3691620.3695552",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3691620.3695552",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.10280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10277v1",
                "updated": "2024-09-16T13:39:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    39,
                    5,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:39:05Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    39,
                    5,
                    0,
                    260,
                    0
                ],
                "title": "Cognitive Kernel: An Open-source Agent System towards Generalist\n  Autopilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Kernel: An Open-source Agent System towards Generalist\n  Autopilots"
                },
                "summary": "We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems."
                },
                "authors": [
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Xiaoman Pan"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07841v2",
                "updated": "2024-09-16T13:18:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    18,
                    23,
                    0,
                    260,
                    0
                ],
                "published": "2024-02-12T17:52:05Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    17,
                    52,
                    5,
                    0,
                    43,
                    0
                ],
                "title": "Do Membership Inference Attacks Work on Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Membership Inference Attacks Work on Large Language Models?"
                },
                "summary": "Membership inference attacks (MIAs) attempt to predict whether a particular\ndatapoint is a member of a target model's training data. Despite extensive\nresearch on traditional machine learning models, there has been limited work\nstudying MIA on the pre-training data of large language models (LLMs). We\nperform a large-scale evaluation of MIAs over a suite of language models (LMs)\ntrained on the Pile, ranging from 160M to 12B parameters. We find that MIAs\nbarely outperform random guessing for most settings across varying LLM sizes\nand domains. Our further analyses reveal that this poor performance can be\nattributed to (1) the combination of a large dataset and few training\niterations, and (2) an inherently fuzzy boundary between members and\nnon-members. We identify specific settings where LLMs have been shown to be\nvulnerable to membership inference and show that the apparent success in such\nsettings can be attributed to a distribution shift, such as when members and\nnon-members are drawn from the seemingly identical domain but with different\ntemporal ranges. We release our code and data as a unified benchmark package\nthat includes all existing MIAs, supporting future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIAs) attempt to predict whether a particular\ndatapoint is a member of a target model's training data. Despite extensive\nresearch on traditional machine learning models, there has been limited work\nstudying MIA on the pre-training data of large language models (LLMs). We\nperform a large-scale evaluation of MIAs over a suite of language models (LMs)\ntrained on the Pile, ranging from 160M to 12B parameters. We find that MIAs\nbarely outperform random guessing for most settings across varying LLM sizes\nand domains. Our further analyses reveal that this poor performance can be\nattributed to (1) the combination of a large dataset and few training\niterations, and (2) an inherently fuzzy boundary between members and\nnon-members. We identify specific settings where LLMs have been shown to be\nvulnerable to membership inference and show that the apparent success in such\nsettings can be attributed to a distribution shift, such as when members and\nnon-members are drawn from the seemingly identical domain but with different\ntemporal ranges. We release our code and data as a unified benchmark package\nthat includes all existing MIAs, supporting future work."
                },
                "authors": [
                    {
                        "name": "Michael Duan"
                    },
                    {
                        "name": "Anshuman Suri"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "Sewon Min"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "David Evans"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    }
                ],
                "author_detail": {
                    "name": "Hannaneh Hajishirzi"
                },
                "author": "Hannaneh Hajishirzi",
                "arxiv_comment": "Accepted at Conference on Language Modeling (COLM), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08942v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08942v4",
                "updated": "2024-09-16T13:16:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    16,
                    22,
                    0,
                    260,
                    0
                ],
                "published": "2023-01-21T12:05:59Z",
                "published_parsed": [
                    2023,
                    1,
                    21,
                    12,
                    5,
                    59,
                    5,
                    21,
                    0
                ],
                "title": "A central limit theorem for a sequence of conditionally centered random\n  fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central limit theorem for a sequence of conditionally centered random\n  fields"
                },
                "summary": "A central limit theorem is established for a sum of random variables\nbelonging to a sequence of random fields. The fields are assumed to have zero\nmean conditional on the past history and to satisfy certain conditional\n$\\alpha$-mixing conditions in space or time. Exploiting conditional centering\nand the space-time structure, the limiting normal distribution is obtained for\nincreasing spatial domain, increasing length of the sequence, or both of these.\nThe theorem is very well suited for establishing asymptotic normality in the\ncontext of unbiased estimating function inference for a wide range of\nspace-time processes. This is pertinent given the abundance of space-time data.\nTwo examples demonstrate the applicability of the theorem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central limit theorem is established for a sum of random variables\nbelonging to a sequence of random fields. The fields are assumed to have zero\nmean conditional on the past history and to satisfy certain conditional\n$\\alpha$-mixing conditions in space or time. Exploiting conditional centering\nand the space-time structure, the limiting normal distribution is obtained for\nincreasing spatial domain, increasing length of the sequence, or both of these.\nThe theorem is very well suited for establishing asymptotic normality in the\ncontext of unbiased estimating function inference for a wide range of\nspace-time processes. This is pertinent given the abundance of space-time data.\nTwo examples demonstrate the applicability of the theorem."
                },
                "authors": [
                    {
                        "name": "Abdollah Jalilian"
                    },
                    {
                        "name": "Arnaud Poinas"
                    },
                    {
                        "name": "Ganggang Xu"
                    },
                    {
                        "name": "Rasmus Waagepetersen"
                    }
                ],
                "author_detail": {
                    "name": "Rasmus Waagepetersen"
                },
                "author": "Rasmus Waagepetersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08942v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08942v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60F05, 60G55, 60G60, 62M30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08795v2",
                "updated": "2024-09-16T12:58:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    58,
                    16,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-13T12:59:39Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    59,
                    39,
                    4,
                    257,
                    0
                ],
                "title": "LLaQo: Towards a Query-Based Coach in Expressive Music Performance\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaQo: Towards a Query-Based Coach in Expressive Music Performance\n  Assessment"
                },
                "summary": "Research in music understanding has extensively explored composition-level\nattributes such as key, genre, and instrumentation through advanced\nrepresentations, leading to cross-modal applications using large language\nmodels. However, aspects of musical performance such as stylistic expression\nand technique remain underexplored, along with the potential of using large\nlanguage models to enhance educational outcomes with customized feedback. To\nbridge this gap, we introduce LLaQo, a Large Language Query-based music coach\nthat leverages audio language modeling to provide detailed and formative\nassessments of music performances. We also introduce instruction-tuned\nquery-response datasets that cover a variety of performance dimensions from\npitch accuracy to articulation, as well as contextual performance understanding\n(such as difficulty and performance techniques). Utilizing AudioMAE encoder and\nVicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in\npredicting teachers' performance ratings, as well as in identifying piece\ndifficulty and playing techniques. Textual responses from LLaQo was moreover\nrated significantly higher compared to other baseline models in a user study\nusing audio-text matching. Our proposed model can thus provide informative\nanswers to open-ended questions related to musical performance from audio data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in music understanding has extensively explored composition-level\nattributes such as key, genre, and instrumentation through advanced\nrepresentations, leading to cross-modal applications using large language\nmodels. However, aspects of musical performance such as stylistic expression\nand technique remain underexplored, along with the potential of using large\nlanguage models to enhance educational outcomes with customized feedback. To\nbridge this gap, we introduce LLaQo, a Large Language Query-based music coach\nthat leverages audio language modeling to provide detailed and formative\nassessments of music performances. We also introduce instruction-tuned\nquery-response datasets that cover a variety of performance dimensions from\npitch accuracy to articulation, as well as contextual performance understanding\n(such as difficulty and performance techniques). Utilizing AudioMAE encoder and\nVicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in\npredicting teachers' performance ratings, as well as in identifying piece\ndifficulty and playing techniques. Textual responses from LLaQo was moreover\nrated significantly higher compared to other baseline models in a user study\nusing audio-text matching. Our proposed model can thus provide informative\nanswers to open-ended questions related to musical performance from audio data."
                },
                "authors": [
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Vincent Cheung"
                    },
                    {
                        "name": "Hayato Nishioka"
                    },
                    {
                        "name": "Simon Dixon"
                    },
                    {
                        "name": "Shinichi Furuya"
                    }
                ],
                "author_detail": {
                    "name": "Shinichi Furuya"
                },
                "author": "Shinichi Furuya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10245v1",
                "updated": "2024-09-16T12:55:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    55,
                    14,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T12:55:14Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    55,
                    14,
                    0,
                    260,
                    0
                ],
                "title": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs"
                },
                "summary": "As the demand for human-like interactions with LLMs continues to grow, so\ndoes the interest in manipulating their personality traits, which has emerged\nas a key area of research. Methods like prompt-based In-Context Knowledge\nEditing (IKE) and gradient-based Model Editor Networks (MEND) have been\nexplored but show irregularity and variability. IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLORA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,\ndespite their absence in the PEFT data. For instance, Llama-2-7B-chat generated\nemojis in 99.5% of extraversion-related test instances, while\nMistral-8B-Instruct did so in 92.5% of openness-related test instances.\nExplainability analysis indicated that the LLMs used emojis intentionally to\nexpress these traits. This paper provides a number of novel contributions.\nFirst, introducing an Opinion QA dataset for PEFT-driven personality\nmanipulation; second, developing metric models to benchmark LLM personality\ntraits; third, demonstrating PEFT's superiority over IKE in personality\nmanipulation; and finally, analyzing and validating emoji usage through\nexplainability methods such as mechanistic interpretability and in-context\nlearning explainability methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for human-like interactions with LLMs continues to grow, so\ndoes the interest in manipulating their personality traits, which has emerged\nas a key area of research. Methods like prompt-based In-Context Knowledge\nEditing (IKE) and gradient-based Model Editor Networks (MEND) have been\nexplored but show irregularity and variability. IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLORA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,\ndespite their absence in the PEFT data. For instance, Llama-2-7B-chat generated\nemojis in 99.5% of extraversion-related test instances, while\nMistral-8B-Instruct did so in 92.5% of openness-related test instances.\nExplainability analysis indicated that the LLMs used emojis intentionally to\nexpress these traits. This paper provides a number of novel contributions.\nFirst, introducing an Opinion QA dataset for PEFT-driven personality\nmanipulation; second, developing metric models to benchmark LLM personality\ntraits; third, demonstrating PEFT's superiority over IKE in personality\nmanipulation; and finally, analyzing and validating emoji usage through\nexplainability methods such as mechanistic interpretability and in-context\nlearning explainability methods."
                },
                "authors": [
                    {
                        "name": "Navya Jain"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Airlie Hilliard"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Submitted to NeurIPS 2024 Workshop on Behavioral Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10225v1",
                "updated": "2024-09-16T12:19:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    19,
                    48,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T12:19:48Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    19,
                    48,
                    0,
                    260,
                    0
                ],
                "title": "Voice control interface for surgical robot assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice control interface for surgical robot assistants"
                },
                "summary": "Traditional control interfaces for robotic-assisted minimally invasive\nsurgery impose a significant cognitive load on surgeons. To improve surgical\nefficiency, surgeon-robot collaboration capabilities, and reduce surgeon\nburden, we present a novel voice control interface for surgical robotic\nassistants. Our system integrates Whisper, state-of-the-art speech recognition,\nwithin the ROS framework to enable real-time interpretation and execution of\nvoice commands for surgical manipulator control. The proposed system consists\nof a speech recognition module, an action mapping module, and a robot control\nmodule. Experimental results demonstrate the system's high accuracy and\ninference speed, and demonstrates its feasibility for surgical applications in\na tissue triangulation task. Future work will focus on further improving its\nrobustness and clinical applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional control interfaces for robotic-assisted minimally invasive\nsurgery impose a significant cognitive load on surgeons. To improve surgical\nefficiency, surgeon-robot collaboration capabilities, and reduce surgeon\nburden, we present a novel voice control interface for surgical robotic\nassistants. Our system integrates Whisper, state-of-the-art speech recognition,\nwithin the ROS framework to enable real-time interpretation and execution of\nvoice commands for surgical manipulator control. The proposed system consists\nof a speech recognition module, an action mapping module, and a robot control\nmodule. Experimental results demonstrate the system's high accuracy and\ninference speed, and demonstrates its feasibility for surgical applications in\na tissue triangulation task. Future work will focus on further improving its\nrobustness and clinical applicability."
                },
                "authors": [
                    {
                        "name": "Ana Davila"
                    },
                    {
                        "name": "Jacinto Colan"
                    },
                    {
                        "name": "Yasuhisa Hasegawa"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhisa Hasegawa"
                },
                "author": "Yasuhisa Hasegawa",
                "arxiv_comment": "Accepted at 2024 IEEE International Symposium on\n  Micro-NanoMechatronics and Human Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10221v1",
                "updated": "2024-09-16T12:17:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    17,
                    6,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T12:17:06Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    17,
                    6,
                    0,
                    260,
                    0
                ],
                "title": "bayesCureRateModel: Bayesian Cure Rate Modeling for Time to Event Data\n  in R",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "bayesCureRateModel: Bayesian Cure Rate Modeling for Time to Event Data\n  in R"
                },
                "summary": "The family of cure models provides a unique opportunity to simultaneously\nmodel both the proportion of cured subjects (those not facing the event of\ninterest) and the distribution function of time-to-event for susceptibles\n(those facing the event). In practice, the application of cure models is mainly\nfacilitated by the availability of various R packages. However, most of these\npackages primarily focus on the mixture or promotion time cure rate model. This\narticle presents a fully Bayesian approach implemented in R to estimate a\ngeneral family of cure rate models in the presence of covariates. It builds\nupon the work by Papastamoulis and Milienos (2024) by additionally considering\nvarious options for describing the promotion time, including the Weibull,\nexponential, Gompertz, log-logistic and finite mixtures of gamma distributions,\namong others. Moreover, the user can choose any proper distribution function\nfor modeling the promotion time (provided that some specific conditions are\nmet). Posterior inference is carried out by constructing a Metropolis-coupled\nMarkov chain Monte Carlo (MCMC) sampler, which combines Gibbs sampling for the\nlatent cure indicators and Metropolis-Hastings steps with Langevin diffusion\ndynamics for parameter updates. The main MCMC algorithm is embedded within a\nparallel tempering scheme by considering heated versions of the target\nposterior distribution. The package is illustrated on a real dataset analyzing\nthe duration of the first marriage under the presence of various covariates\nsuch as the race, age and the presence of kids.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The family of cure models provides a unique opportunity to simultaneously\nmodel both the proportion of cured subjects (those not facing the event of\ninterest) and the distribution function of time-to-event for susceptibles\n(those facing the event). In practice, the application of cure models is mainly\nfacilitated by the availability of various R packages. However, most of these\npackages primarily focus on the mixture or promotion time cure rate model. This\narticle presents a fully Bayesian approach implemented in R to estimate a\ngeneral family of cure rate models in the presence of covariates. It builds\nupon the work by Papastamoulis and Milienos (2024) by additionally considering\nvarious options for describing the promotion time, including the Weibull,\nexponential, Gompertz, log-logistic and finite mixtures of gamma distributions,\namong others. Moreover, the user can choose any proper distribution function\nfor modeling the promotion time (provided that some specific conditions are\nmet). Posterior inference is carried out by constructing a Metropolis-coupled\nMarkov chain Monte Carlo (MCMC) sampler, which combines Gibbs sampling for the\nlatent cure indicators and Metropolis-Hastings steps with Langevin diffusion\ndynamics for parameter updates. The main MCMC algorithm is embedded within a\nparallel tempering scheme by considering heated versions of the target\nposterior distribution. The package is illustrated on a real dataset analyzing\nthe duration of the first marriage under the presence of various covariates\nsuch as the race, age and the presence of kids."
                },
                "authors": [
                    {
                        "name": "Panagiotis Papastamoulis"
                    },
                    {
                        "name": "Fotios Milienos"
                    }
                ],
                "author_detail": {
                    "name": "Fotios Milienos"
                },
                "author": "Fotios Milienos",
                "arxiv_comment": "34 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10213v1",
                "updated": "2024-09-16T12:04:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    4,
                    26,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T12:04:26Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    4,
                    26,
                    0,
                    260,
                    0
                ],
                "title": "Neuromorphic Facial Analysis with Cross-Modal Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Facial Analysis with Cross-Modal Supervision"
                },
                "summary": "Traditional approaches for analyzing RGB frames are capable of providing a\nfine-grained understanding of a face from different angles by inferring\nemotions, poses, shapes, landmarks. However, when it comes to subtle movements\nstandard RGB cameras might fall behind due to their latency, making it hard to\ndetect micro-movements that carry highly informative cues to infer the true\nemotions of a subject. To address this issue, the usage of event cameras to\nanalyze faces is gaining increasing interest. Nonetheless, all the expertise\nmatured for RGB processing is not directly transferrable to neuromorphic data\ndue to a strong domain shift and intrinsic differences in how data is\nrepresented. The lack of labeled data can be considered one of the main causes\nof this gap, yet gathering data is harder in the event domain since it cannot\nbe crawled from the web and labeling frames should take into account event\naggregation rates and the fact that static parts might not be visible in\ncertain frames. In this paper, we first present FACEMORPHIC, a multimodal\ntemporally synchronized face dataset comprising both RGB videos and event\nstreams. The data is labeled at a video level with facial Action Units and also\ncontains streams collected with a variety of applications in mind, ranging from\n3D shape estimation to lip-reading. We then show how temporal synchronization\ncan allow effective neuromorphic face analysis without the need to manually\nannotate videos: we instead leverage cross-modal supervision bridging the\ndomain gap by representing face shapes in a 3D space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional approaches for analyzing RGB frames are capable of providing a\nfine-grained understanding of a face from different angles by inferring\nemotions, poses, shapes, landmarks. However, when it comes to subtle movements\nstandard RGB cameras might fall behind due to their latency, making it hard to\ndetect micro-movements that carry highly informative cues to infer the true\nemotions of a subject. To address this issue, the usage of event cameras to\nanalyze faces is gaining increasing interest. Nonetheless, all the expertise\nmatured for RGB processing is not directly transferrable to neuromorphic data\ndue to a strong domain shift and intrinsic differences in how data is\nrepresented. The lack of labeled data can be considered one of the main causes\nof this gap, yet gathering data is harder in the event domain since it cannot\nbe crawled from the web and labeling frames should take into account event\naggregation rates and the fact that static parts might not be visible in\ncertain frames. In this paper, we first present FACEMORPHIC, a multimodal\ntemporally synchronized face dataset comprising both RGB videos and event\nstreams. The data is labeled at a video level with facial Action Units and also\ncontains streams collected with a variety of applications in mind, ranging from\n3D shape estimation to lip-reading. We then show how temporal synchronization\ncan allow effective neuromorphic face analysis without the need to manually\nannotate videos: we instead leverage cross-modal supervision bridging the\ndomain gap by representing face shapes in a 3D space."
                },
                "authors": [
                    {
                        "name": "Federico Becattini"
                    },
                    {
                        "name": "Luca Cultrera"
                    },
                    {
                        "name": "Lorenzo Berlincioni"
                    },
                    {
                        "name": "Claudio Ferrari"
                    },
                    {
                        "name": "Andrea Leonardo"
                    },
                    {
                        "name": "Alberto Del Bimbo"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Del Bimbo"
                },
                "author": "Alberto Del Bimbo",
                "arxiv_comment": "Accepted for publication at the ECCV 2024 workshop on Neuromorphic\n  Vision: Advantages and Applications of Event Cameras (NEVI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15512v2",
                "updated": "2024-09-16T12:02:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    2,
                    27,
                    0,
                    260,
                    0
                ],
                "published": "2024-08-28T03:48:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    48,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Towards Fully Autonomous Research Powered by LLMs: Case Study on\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully Autonomous Research Powered by LLMs: Case Study on\n  Simulations"
                },
                "summary": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research, spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLM, through sophisticated API\nintegration, to automate the entire research process, from experimental design,\nremote upload and simulation execution, data analysis, to report compilation.\nUsing a simulation problem of polymer chain conformations as a case study, we\nassessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless\nexecution on designated research missions, underscoring the potential of LLMs\nto manage complete scientific investigations autonomously. The outlined\nautomation can be iteratively performed up to twenty cycles without human\nintervention, illustrating the potential of LLMs for large-scale autonomous\nresearch endeavors. Additionally, we discussed the intrinsic traits of ASAs in\nmanaging extensive tasks, focusing on self-validation mechanisms and the\nbalance between local attention and global oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research, spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLM, through sophisticated API\nintegration, to automate the entire research process, from experimental design,\nremote upload and simulation execution, data analysis, to report compilation.\nUsing a simulation problem of polymer chain conformations as a case study, we\nassessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless\nexecution on designated research missions, underscoring the potential of LLMs\nto manage complete scientific investigations autonomously. The outlined\nautomation can be iteratively performed up to twenty cycles without human\nintervention, illustrating the potential of LLMs for large-scale autonomous\nresearch endeavors. Additionally, we discussed the intrinsic traits of ASAs in\nmanaging extensive tasks, focusing on self-validation mechanisms and the\nbalance between local attention and global oversight."
                },
                "authors": [
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Yubo Chai"
                    },
                    {
                        "name": "Jianfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Li"
                },
                "author": "Jianfeng Li",
                "arxiv_comment": "For additional code and data, please visit our GitHub repository:\n  https://github.com/zokaraa/autonomous_simulation_agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15051v3",
                "updated": "2024-09-16T12:01:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    1,
                    22,
                    0,
                    260,
                    0
                ],
                "published": "2024-07-21T04:39:06Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    4,
                    39,
                    6,
                    6,
                    203,
                    0
                ],
                "title": "Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation\n  for Video Moment Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation\n  for Video Moment Retrieval"
                },
                "summary": "In this paper, we investigate the feasibility of leveraging large language\nmodels (LLMs) for integrating general knowledge and incorporating pseudo-events\nas priors for temporal content distribution in video moment retrieval (VMR)\nmodels. The motivation behind this study arises from the limitations of using\nLLMs as decoders for generating discrete textual descriptions, which hinders\ntheir direct application to continuous outputs like salience scores and\ninter-frame embeddings that capture inter-frame relations. To overcome these\nlimitations, we propose utilizing LLM encoders instead of decoders. Through a\nfeasibility study, we demonstrate that LLM encoders effectively refine\ninter-concept relations in multimodal embeddings, even without being trained on\ntextual embeddings. We also show that the refinement capability of LLM encoders\ncan be transferred to other embeddings, such as BLIP and T5, as long as these\nembeddings exhibit similar inter-concept similarity patterns to CLIP\nembeddings. We present a general framework for integrating LLM encoders into\nexisting VMR architectures, specifically within the fusion module. Through\nexperimental validation, we demonstrate the effectiveness of our proposed\nmethods by achieving state-of-the-art performance in VMR. The source code can\nbe accessed at https://github.com/fletcherjiang/LLMEPET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the feasibility of leveraging large language\nmodels (LLMs) for integrating general knowledge and incorporating pseudo-events\nas priors for temporal content distribution in video moment retrieval (VMR)\nmodels. The motivation behind this study arises from the limitations of using\nLLMs as decoders for generating discrete textual descriptions, which hinders\ntheir direct application to continuous outputs like salience scores and\ninter-frame embeddings that capture inter-frame relations. To overcome these\nlimitations, we propose utilizing LLM encoders instead of decoders. Through a\nfeasibility study, we demonstrate that LLM encoders effectively refine\ninter-concept relations in multimodal embeddings, even without being trained on\ntextual embeddings. We also show that the refinement capability of LLM encoders\ncan be transferred to other embeddings, such as BLIP and T5, as long as these\nembeddings exhibit similar inter-concept similarity patterns to CLIP\nembeddings. We present a general framework for integrating LLM encoders into\nexisting VMR architectures, specifically within the fusion module. Through\nexperimental validation, we demonstrate the effectiveness of our proposed\nmethods by achieving state-of-the-art performance in VMR. The source code can\nbe accessed at https://github.com/fletcherjiang/LLMEPET."
                },
                "authors": [
                    {
                        "name": "Yiyang Jiang"
                    },
                    {
                        "name": "Wengyu Zhang"
                    },
                    {
                        "name": "Xulu Zhang"
                    },
                    {
                        "name": "Xiaoyong Wei"
                    },
                    {
                        "name": "Chang Wen Chen"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_doi": "10.1145/3664647.3681115",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681115",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.15051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM Multimedia 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10197v1",
                "updated": "2024-09-16T11:43:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    43,
                    19,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:43:19Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    43,
                    19,
                    0,
                    260,
                    0
                ],
                "title": "Fit and Prune: Fast and Training-free Visual Token Pruning for\n  Multi-modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fit and Prune: Fast and Training-free Visual Token Pruning for\n  Multi-modal Large Language Models"
                },
                "summary": "Recent progress in Multimodal Large Language Models(MLLMs) often use large\nimage tokens to compensate the visual shortcoming of MLLMs, which not only\nexhibits obvious redundancy but also greatly exacerbates the already high\ncomputation. Token pruning is an effective solution for speeding up MLLMs, but\nwhen and how to drop tokens still remains a challenge. In this paper, we\npropose a novel and training-free approach for the effective visual token\npruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning\nrecipe for MLLMs according to a pre-defined budget. Specifically, FitPrune\nconsiders token pruning as a statistical problem of MLLM and its objective is\nto find out an optimal pruning scheme that can minimize the divergence of the\nattention distributions before and after pruning. In practice, FitPrune can be\nquickly accomplished based on the attention statistics from a small batch of\ninference data, avoiding the expensive trials of MLLMs. According to the\npruning recipe, an MLLM can directly remove the redundant visual tokens of\ndifferent examples during inference. To validate FitPrune, we apply it to a set\nof recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct\nextensive experiments on a set of benchmarks. The experimental results show\nthat our FitPrune can not only reduce the computational complexity to a large\nextent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT\nwith only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in\nabout 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Multimodal Large Language Models(MLLMs) often use large\nimage tokens to compensate the visual shortcoming of MLLMs, which not only\nexhibits obvious redundancy but also greatly exacerbates the already high\ncomputation. Token pruning is an effective solution for speeding up MLLMs, but\nwhen and how to drop tokens still remains a challenge. In this paper, we\npropose a novel and training-free approach for the effective visual token\npruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning\nrecipe for MLLMs according to a pre-defined budget. Specifically, FitPrune\nconsiders token pruning as a statistical problem of MLLM and its objective is\nto find out an optimal pruning scheme that can minimize the divergence of the\nattention distributions before and after pruning. In practice, FitPrune can be\nquickly accomplished based on the attention statistics from a small batch of\ninference data, avoiding the expensive trials of MLLMs. According to the\npruning recipe, an MLLM can directly remove the redundant visual tokens of\ndifferent examples during inference. To validate FitPrune, we apply it to a set\nof recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct\nextensive experiments on a set of benchmarks. The experimental results show\nthat our FitPrune can not only reduce the computational complexity to a large\nextent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT\nwith only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in\nabout 5 minutes. Our code is available at https://github.com/ywh187/FitPrune."
                },
                "authors": [
                    {
                        "name": "Weihao Ye"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Wenhao Lin"
                    },
                    {
                        "name": "Yiyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yiyi Zhou"
                },
                "author": "Yiyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15002v2",
                "updated": "2024-09-16T11:38:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    38,
                    10,
                    0,
                    260,
                    0
                ],
                "published": "2024-08-27T12:34:41Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    12,
                    34,
                    41,
                    1,
                    240,
                    0
                ],
                "title": "Knowledge Discovery in Optical Music Recognition: Enhancing Information\n  Retrieval with Instance Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Discovery in Optical Music Recognition: Enhancing Information\n  Retrieval with Instance Segmentation"
                },
                "summary": "Optical Music Recognition (OMR) automates the transcription of musical\nnotation from images into machine-readable formats like MusicXML, MEI, or MIDI,\nsignificantly reducing the costs and time of manual transcription. This study\nexplores knowledge discovery in OMR by applying instance segmentation using\nMask R-CNN to enhance the detection and delineation of musical symbols in sheet\nmusic. Unlike Optical Character Recognition (OCR), OMR must handle the\nintricate semantics of Common Western Music Notation (CWMN), where symbol\nmeanings depend on shape, position, and context. Our approach leverages\ninstance segmentation to manage the density and overlap of musical symbols,\nfacilitating more precise information retrieval from music scores. Evaluations\non the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with\nour method achieving a mean Average Precision (mAP) of up to 59.70\\% in dense\nsymbol environments, achieving comparable results to object detection.\nFurthermore, using traditional computer vision techniques, we add a parallel\nstep for staff detection to infer the pitch for the recognised symbols. This\nstudy emphasises the role of pixel-wise segmentation in advancing accurate\nmusic symbol recognition, contributing to knowledge discovery in OMR. Our\nfindings indicate that instance segmentation provides more precise\nrepresentations of musical symbols, particularly in densely populated scores,\nadvancing OMR technology. We make our implementation, pre-processing scripts,\ntrained models, and evaluation results publicly available to support further\nresearch and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Music Recognition (OMR) automates the transcription of musical\nnotation from images into machine-readable formats like MusicXML, MEI, or MIDI,\nsignificantly reducing the costs and time of manual transcription. This study\nexplores knowledge discovery in OMR by applying instance segmentation using\nMask R-CNN to enhance the detection and delineation of musical symbols in sheet\nmusic. Unlike Optical Character Recognition (OCR), OMR must handle the\nintricate semantics of Common Western Music Notation (CWMN), where symbol\nmeanings depend on shape, position, and context. Our approach leverages\ninstance segmentation to manage the density and overlap of musical symbols,\nfacilitating more precise information retrieval from music scores. Evaluations\non the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with\nour method achieving a mean Average Precision (mAP) of up to 59.70\\% in dense\nsymbol environments, achieving comparable results to object detection.\nFurthermore, using traditional computer vision techniques, we add a parallel\nstep for staff detection to infer the pitch for the recognised symbols. This\nstudy emphasises the role of pixel-wise segmentation in advancing accurate\nmusic symbol recognition, contributing to knowledge discovery in OMR. Our\nfindings indicate that instance segmentation provides more precise\nrepresentations of musical symbols, particularly in densely populated scores,\nadvancing OMR technology. We make our implementation, pre-processing scripts,\ntrained models, and evaluation results publicly available to support further\nresearch and development."
                },
                "authors": [
                    {
                        "name": "Elona Shatri"
                    },
                    {
                        "name": "George Fazekas"
                    }
                ],
                "author_detail": {
                    "name": "George Fazekas"
                },
                "author": "George Fazekas",
                "arxiv_comment": "8 pages content and one references, accepted version at the\n  International Conference on Knowledge Discovery and Information Retrieval\n  2024, Porto, Portugal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10191v1",
                "updated": "2024-09-16T11:34:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    34,
                    40,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:34:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    34,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "LLMs for clinical risk prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for clinical risk prediction"
                },
                "summary": "This study compares the efficacy of GPT-4 and clinalytix Medical AI in\npredicting the clinical risk of delirium development. Findings indicate that\nGPT-4 exhibited significant deficiencies in identifying positive cases and\nstruggled to provide reliable probability estimates for delirium risk, while\nclinalytix Medical AI demonstrated superior accuracy. A thorough analysis of\nthe large language model's (LLM) outputs elucidated potential causes for these\ndiscrepancies, consistent with limitations reported in extant literature. These\nresults underscore the challenges LLMs face in accurately diagnosing conditions\nand interpreting complex clinical data. While LLMs hold substantial potential\nin healthcare, they are currently unsuitable for independent clinical\ndecision-making. Instead, they should be employed in assistive roles,\ncomplementing clinical expertise. Continued human oversight remains essential\nto ensure optimal outcomes for both patients and healthcare providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study compares the efficacy of GPT-4 and clinalytix Medical AI in\npredicting the clinical risk of delirium development. Findings indicate that\nGPT-4 exhibited significant deficiencies in identifying positive cases and\nstruggled to provide reliable probability estimates for delirium risk, while\nclinalytix Medical AI demonstrated superior accuracy. A thorough analysis of\nthe large language model's (LLM) outputs elucidated potential causes for these\ndiscrepancies, consistent with limitations reported in extant literature. These\nresults underscore the challenges LLMs face in accurately diagnosing conditions\nand interpreting complex clinical data. While LLMs hold substantial potential\nin healthcare, they are currently unsuitable for independent clinical\ndecision-making. Instead, they should be employed in assistive roles,\ncomplementing clinical expertise. Continued human oversight remains essential\nto ensure optimal outcomes for both patients and healthcare providers."
                },
                "authors": [
                    {
                        "name": "Mohamed Rezk"
                    },
                    {
                        "name": "Patricia Cabanillas Silva"
                    },
                    {
                        "name": "Fried-Michael Dahlweid"
                    }
                ],
                "author_detail": {
                    "name": "Fried-Michael Dahlweid"
                },
                "author": "Fried-Michael Dahlweid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10188v1",
                "updated": "2024-09-16T11:30:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    30,
                    39,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:30:39Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    30,
                    39,
                    0,
                    260,
                    0
                ],
                "title": "Enhancing RL Safety with Counterfactual LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing RL Safety with Counterfactual LLM Reasoning"
                },
                "summary": "Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard\nto explain. We use counterfactual large language model reasoning to enhance RL\npolicy safety post-training. We show that our approach improves and helps to\nexplain the RL policy safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard\nto explain. We use counterfactual large language model reasoning to enhance RL\npolicy safety post-training. We show that our approach improves and helps to\nexplain the RL policy safety."
                },
                "authors": [
                    {
                        "name": "Dennis Gross"
                    },
                    {
                        "name": "Helge Spieker"
                    }
                ],
                "author_detail": {
                    "name": "Helge Spieker"
                },
                "author": "Helge Spieker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10177v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10177v2",
                "updated": "2024-09-17T06:30:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    30,
                    3,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-16T11:13:14Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    13,
                    14,
                    0,
                    260,
                    0
                ],
                "title": "Augmenting Automatic Speech Recognition Models with Disfluency Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Automatic Speech Recognition Models with Disfluency Detection"
                },
                "summary": "Speech disfluency commonly occurs in conversational and spontaneous speech.\nHowever, standard Automatic Speech Recognition (ASR) models struggle to\naccurately recognize these disfluencies because they are typically trained on\nfluent transcripts. Current research mainly focuses on detecting disfluencies\nwithin transcripts, overlooking their exact location and duration in the\nspeech. Additionally, previous work often requires model fine-tuning and\naddresses limited types of disfluencies.\n  In this work, we present an inference-only approach to augment any ASR model\nwith the ability to detect open-set disfluencies. We first demonstrate that ASR\nmodels have difficulty transcribing speech disfluencies. Next, this work\nproposes a modified Connectionist Temporal Classification(CTC)-based forced\nalignment algorithm from \\cite{kurzinger2020ctc} to predict word-level\ntimestamps while effectively capturing disfluent speech. Additionally, we\ndevelop a model to classify alignment gaps between timestamps as either\ncontaining disfluent speech or silence. This model achieves an accuracy of\n81.62% and an F1-score of 80.07%. We test the augmentation pipeline of\nalignment gap detection and classification on a disfluent dataset. Our results\nshow that we captured 74.13% of the words that were initially missed by the\ntranscription, demonstrating the potential of this pipeline for downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech disfluency commonly occurs in conversational and spontaneous speech.\nHowever, standard Automatic Speech Recognition (ASR) models struggle to\naccurately recognize these disfluencies because they are typically trained on\nfluent transcripts. Current research mainly focuses on detecting disfluencies\nwithin transcripts, overlooking their exact location and duration in the\nspeech. Additionally, previous work often requires model fine-tuning and\naddresses limited types of disfluencies.\n  In this work, we present an inference-only approach to augment any ASR model\nwith the ability to detect open-set disfluencies. We first demonstrate that ASR\nmodels have difficulty transcribing speech disfluencies. Next, this work\nproposes a modified Connectionist Temporal Classification(CTC)-based forced\nalignment algorithm from \\cite{kurzinger2020ctc} to predict word-level\ntimestamps while effectively capturing disfluent speech. Additionally, we\ndevelop a model to classify alignment gaps between timestamps as either\ncontaining disfluent speech or silence. This model achieves an accuracy of\n81.62% and an F1-score of 80.07%. We test the augmentation pipeline of\nalignment gap detection and classification on a disfluent dataset. Our results\nshow that we captured 74.13% of the words that were initially missed by the\ntranscription, demonstrating the potential of this pipeline for downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Robin Amann"
                    },
                    {
                        "name": "Zhaolin Li"
                    },
                    {
                        "name": "Barbara Bruno"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "arxiv_comment": "Accepted by SLT2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10177v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03036v3",
                "updated": "2024-09-16T11:11:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    11,
                    47,
                    0,
                    260,
                    0
                ],
                "published": "2024-01-05T19:19:49Z",
                "published_parsed": [
                    2024,
                    1,
                    5,
                    19,
                    19,
                    49,
                    4,
                    5,
                    0
                ],
                "title": "Modelling and calibration of pair-rule protein patterns in Drosophila\n  embryo: From Even-skipped and Fushi-tarazu to Wingless expression networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling and calibration of pair-rule protein patterns in Drosophila\n  embryo: From Even-skipped and Fushi-tarazu to Wingless expression networks"
                },
                "summary": "We modelled and calibrated the distributions of the seven-stripe patterns of\nEven-skipped (\\textit{Eve}) and Fushi-tarazu (\\textit{Ftz}) pair-rule proteins\nalong the anteroposterior axis of the \\textit{Drosphila} embryo, established\nduring early development. We have identified the putative repressive\ncombinations for five \\textit{Eve} enhancers, and we have explored the\nrelationship between \\textit{Eve} and \\textit{Ftz} for complementary patterns.\nThe regulators of \\textit{Eve} and \\textit{Ftz} are stripe-specific DNA\nenhancers with embryo position-dependent activation rates and regulated by the\ngap family of proteins. We achieved remarkable data matching of the\n\\textit{Eve} stripe pattern, and the calibrated model reproduces gap gene\nmutation experiments. Extended work inferring the Wingless (\\textit{Wg})\nfourteen stripe pattern from \\textit{Eve} and \\textit{Ftz} enhancers have been\nproposed, clarifying the hierarchical structure of \\textit{Drosphila}'s genetic\nexpression network during early development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We modelled and calibrated the distributions of the seven-stripe patterns of\nEven-skipped (\\textit{Eve}) and Fushi-tarazu (\\textit{Ftz}) pair-rule proteins\nalong the anteroposterior axis of the \\textit{Drosphila} embryo, established\nduring early development. We have identified the putative repressive\ncombinations for five \\textit{Eve} enhancers, and we have explored the\nrelationship between \\textit{Eve} and \\textit{Ftz} for complementary patterns.\nThe regulators of \\textit{Eve} and \\textit{Ftz} are stripe-specific DNA\nenhancers with embryo position-dependent activation rates and regulated by the\ngap family of proteins. We achieved remarkable data matching of the\n\\textit{Eve} stripe pattern, and the calibrated model reproduces gap gene\nmutation experiments. Extended work inferring the Wingless (\\textit{Wg})\nfourteen stripe pattern from \\textit{Eve} and \\textit{Ftz} enhancers have been\nproposed, clarifying the hierarchical structure of \\textit{Drosphila}'s genetic\nexpression network during early development."
                },
                "authors": [
                    {
                        "name": "Catarina Dias"
                    },
                    {
                        "name": "Rui Dilão"
                    }
                ],
                "author_detail": {
                    "name": "Rui Dilão"
                },
                "author": "Rui Dilão",
                "arxiv_comment": "15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10170v1",
                "updated": "2024-09-16T11:02:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    2,
                    47,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:02:47Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    2,
                    47,
                    0,
                    260,
                    0
                ],
                "title": "Minimal Model Counting via Knowledge Compilation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal Model Counting via Knowledge Compilation"
                },
                "summary": "Counting the number of models of a Boolean formula is a fundamental problem\nin artificial intelligence and reasoning. Minimal models of a Boolean formula\nare critical in various reasoning systems, making the counting of minimal\nmodels essential for detailed inference tasks. Existing research primarily\nfocused on decision problems related to minimal models. In this work, we extend\nbeyond decision problems to address the challenge of counting minimal models.\nSpecifically, we propose a novel knowledge compilation form that facilitates\nthe efficient counting of minimal models. Our approach leverages the idea of\njustification and incorporates theories from answer set counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counting the number of models of a Boolean formula is a fundamental problem\nin artificial intelligence and reasoning. Minimal models of a Boolean formula\nare critical in various reasoning systems, making the counting of minimal\nmodels essential for detailed inference tasks. Existing research primarily\nfocused on decision problems related to minimal models. In this work, we extend\nbeyond decision problems to address the challenge of counting minimal models.\nSpecifically, we propose a novel knowledge compilation form that facilitates\nthe efficient counting of minimal models. Our approach leverages the idea of\njustification and incorporates theories from answer set counting."
                },
                "authors": [
                    {
                        "name": "Mohimenul Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Mohimenul Kabir"
                },
                "author": "Mohimenul Kabir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10164v1",
                "updated": "2024-09-16T10:54:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    54,
                    4,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T10:54:04Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    54,
                    4,
                    0,
                    260,
                    0
                ],
                "title": "Quantile Regression for Distributional Reward Models in RLHF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile Regression for Distributional Reward Models in RLHF"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has become a key method for\naligning large language models (LLMs) with human preferences through the use of\nreward models. However, traditional reward models typically generate point\nestimates, which oversimplify the diversity and complexity of human values and\npreferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel\napproach to reward modeling that learns a distribution over rewards instead of\na single scalar value. Our method uses quantile regression to estimate a full,\npotentially multimodal distribution over preferences, providing a more powerful\nand nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and\naccommodates conflicting preferences by modeling them as distinct modes in the\ndistribution. Our experimental results show that QRM outperforms comparable\ntraditional point-estimate models on RewardBench. Furthermore, we demonstrate\nthat the additional information provided by the distributional estimates can be\nutilized in downstream applications, such as risk-aware reinforcement learning,\nresulting in LLM policies that generate fewer extremely negative responses. Our\ncode and model are released at https://github.com/Nicolinho/QRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has become a key method for\naligning large language models (LLMs) with human preferences through the use of\nreward models. However, traditional reward models typically generate point\nestimates, which oversimplify the diversity and complexity of human values and\npreferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel\napproach to reward modeling that learns a distribution over rewards instead of\na single scalar value. Our method uses quantile regression to estimate a full,\npotentially multimodal distribution over preferences, providing a more powerful\nand nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and\naccommodates conflicting preferences by modeling them as distinct modes in the\ndistribution. Our experimental results show that QRM outperforms comparable\ntraditional point-estimate models on RewardBench. Furthermore, we demonstrate\nthat the additional information provided by the distributional estimates can be\nutilized in downstream applications, such as risk-aware reinforcement learning,\nresulting in LLM policies that generate fewer extremely negative responses. Our\ncode and model are released at https://github.com/Nicolinho/QRM."
                },
                "authors": [
                    {
                        "name": "Nicolai Dorka"
                    }
                ],
                "author_detail": {
                    "name": "Nicolai Dorka"
                },
                "author": "Nicolai Dorka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10157v1",
                "updated": "2024-09-16T10:41:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    41,
                    36,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T10:41:36Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    41,
                    36,
                    0,
                    260,
                    0
                ],
                "title": "Emo-DPO: Controllable Emotional Speech Synthesis through Direct\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emo-DPO: Controllable Emotional Speech Synthesis through Direct\n  Preference Optimization"
                },
                "summary": "Current emotional text-to-speech (TTS) models predominantly conduct\nsupervised training to learn the conversion from text and desired emotion to\nits emotional speech, focusing on a single emotion per text-speech pair. These\nmodels only learn the correct emotional outputs without fully comprehending\nother emotion characteristics, which limits their capabilities of capturing the\nnuances between different emotions. We propose a controllable Emo-DPO approach,\nwhich employs direct preference optimization to differentiate subtle emotional\nnuances between emotions through optimizing towards preferred emotions over\nless preferred emotional ones. Instead of relying on traditional neural\narchitectures used in existing emotional TTS models, we propose utilizing the\nemotion-aware LLM-TTS neural architecture to leverage LLMs' in-context learning\nand instruction-following capabilities. Comprehensive experiments confirm that\nour proposed method outperforms the existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current emotional text-to-speech (TTS) models predominantly conduct\nsupervised training to learn the conversion from text and desired emotion to\nits emotional speech, focusing on a single emotion per text-speech pair. These\nmodels only learn the correct emotional outputs without fully comprehending\nother emotion characteristics, which limits their capabilities of capturing the\nnuances between different emotions. We propose a controllable Emo-DPO approach,\nwhich employs direct preference optimization to differentiate subtle emotional\nnuances between emotions through optimizing towards preferred emotions over\nless preferred emotional ones. Instead of relying on traditional neural\narchitectures used in existing emotional TTS models, we propose utilizing the\nemotion-aware LLM-TTS neural architecture to leverage LLMs' in-context learning\nand instruction-following capabilities. Comprehensive experiments confirm that\nour proposed method outperforms the existing baselines."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Huayun Zhang"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15018v2",
                "updated": "2024-09-16T10:32:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    32,
                    28,
                    0,
                    260,
                    0
                ],
                "published": "2024-04-23T13:23:27Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    13,
                    23,
                    27,
                    1,
                    114,
                    0
                ],
                "title": "Conformal Predictive Systems Under Covariate Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Predictive Systems Under Covariate Shift"
                },
                "summary": "Conformal Predictive Systems (CPS) offer a versatile framework for\nconstructing predictive distributions, allowing for calibrated inference and\ninformative decision-making. However, their applicability has been limited to\nscenarios adhering to the Independent and Identically Distributed (IID) model\nassumption. This paper extends CPS to accommodate scenarios characterized by\ncovariate shifts. We therefore propose Weighted CPS (WCPS), akin to Weighted\nConformal Prediction (WCP), leveraging likelihood ratios between training and\ntesting covariate distributions. This extension enables the construction of\nnonparametric predictive distributions capable of handling covariate shifts. We\npresent theoretical underpinnings and conjectures regarding the validity and\nefficacy of WCPS and demonstrate its utility through empirical evaluations on\nboth synthetic and real-world datasets. Our simulation experiments indicate\nthat WCPS are probabilistically calibrated under covariate shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Predictive Systems (CPS) offer a versatile framework for\nconstructing predictive distributions, allowing for calibrated inference and\ninformative decision-making. However, their applicability has been limited to\nscenarios adhering to the Independent and Identically Distributed (IID) model\nassumption. This paper extends CPS to accommodate scenarios characterized by\ncovariate shifts. We therefore propose Weighted CPS (WCPS), akin to Weighted\nConformal Prediction (WCP), leveraging likelihood ratios between training and\ntesting covariate distributions. This extension enables the construction of\nnonparametric predictive distributions capable of handling covariate shifts. We\npresent theoretical underpinnings and conjectures regarding the validity and\nefficacy of WCPS and demonstrate its utility through empirical evaluations on\nboth synthetic and real-world datasets. Our simulation experiments indicate\nthat WCPS are probabilistically calibrated under covariate shift."
                },
                "authors": [
                    {
                        "name": "Jef Jonkers"
                    },
                    {
                        "name": "Glenn Van Wallendael"
                    },
                    {
                        "name": "Luc Duchateau"
                    },
                    {
                        "name": "Sofie Van Hoecke"
                    }
                ],
                "author_detail": {
                    "name": "Sofie Van Hoecke"
                },
                "author": "Sofie Van Hoecke",
                "arxiv_comment": "Accepted at the 13th Symposium on Conformal and Probabilistic\n  Prediction with Applications (COPA), 9-11 September 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10146v1",
                "updated": "2024-09-16T10:15:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    15,
                    30,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T10:15:30Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    15,
                    30,
                    0,
                    260,
                    0
                ],
                "title": "LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology\n  Learning Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology\n  Learning Challenge"
                },
                "summary": "This paper outlines the LLMs4OL 2024, the first edition of the Large Language\nModels for Ontology Learning Challenge. LLMs4OL is a community development\ninitiative collocated with the 23rd International Semantic Web Conference\n(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology\nLearning (OL), a vital process for enhancing the web with structured knowledge\nto improve interoperability. By leveraging LLMs, the challenge aims to advance\nunderstanding and innovation in OL, aligning with the goals of the Semantic Web\nto create a more intelligent and user-friendly web. In this paper, we give an\noverview of the 2024 edition of the LLMs4OL challenge and summarize the\ncontributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper outlines the LLMs4OL 2024, the first edition of the Large Language\nModels for Ontology Learning Challenge. LLMs4OL is a community development\ninitiative collocated with the 23rd International Semantic Web Conference\n(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology\nLearning (OL), a vital process for enhancing the web with structured knowledge\nto improve interoperability. By leveraging LLMs, the challenge aims to advance\nunderstanding and innovation in OL, aligning with the goals of the Semantic Web\nto create a more intelligent and user-friendly web. In this paper, we give an\noverview of the 2024 edition of the LLMs4OL challenge and summarize the\ncontributions."
                },
                "authors": [
                    {
                        "name": "Hamed Babaei Giglou"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Sören Auer"
                    }
                ],
                "author_detail": {
                    "name": "Sören Auer"
                },
                "author": "Sören Auer",
                "arxiv_comment": "15 pages, 1 figure, Will appear in \"The 1st LLMs4OL Challenge @ ISWC\n  2024\" proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04964v2",
                "updated": "2024-09-16T10:00:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    0,
                    52,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-08T04:03:55Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    4,
                    3,
                    55,
                    6,
                    252,
                    0
                ],
                "title": "Evaluation of Google Translate for Mandarin Chinese translation using\n  sentiment and semantic analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Google Translate for Mandarin Chinese translation using\n  sentiment and semantic analysis"
                },
                "summary": "Machine translation using large language models (LLMs) is having a\nsignificant global impact, making communication easier. Mandarin Chinese is the\nofficial language used for communication by the government and media in China.\nIn this study, we provide an automated assessment of translation quality of\nGoogle Translate with human experts using sentiment and semantic analysis. In\norder to demonstrate our framework, we select the classic early\ntwentieth-century novel 'The True Story of Ah Q' with selected Mandarin Chinese\nto English translations. We use Google Translate to translate the given text\ninto English and then conduct a chapter-wise sentiment analysis and semantic\nanalysis to compare the extracted sentiments across the different translations.\nOur results indicate that the precision of Google Translate differs both in\nterms of semantic and sentiment analysis when compared to human expert\ntranslations. We find that Google Translate is unable to translate some of the\nspecific words or phrases in Chinese, such as Chinese traditional allusions.\nThe mistranslations may be due to lack of contextual significance and\nhistorical knowledge of China.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine translation using large language models (LLMs) is having a\nsignificant global impact, making communication easier. Mandarin Chinese is the\nofficial language used for communication by the government and media in China.\nIn this study, we provide an automated assessment of translation quality of\nGoogle Translate with human experts using sentiment and semantic analysis. In\norder to demonstrate our framework, we select the classic early\ntwentieth-century novel 'The True Story of Ah Q' with selected Mandarin Chinese\nto English translations. We use Google Translate to translate the given text\ninto English and then conduct a chapter-wise sentiment analysis and semantic\nanalysis to compare the extracted sentiments across the different translations.\nOur results indicate that the precision of Google Translate differs both in\nterms of semantic and sentiment analysis when compared to human expert\ntranslations. We find that Google Translate is unable to translate some of the\nspecific words or phrases in Chinese, such as Chinese traditional allusions.\nThe mistranslations may be due to lack of contextual significance and\nhistorical knowledge of China."
                },
                "authors": [
                    {
                        "name": "Xuechun Wang"
                    },
                    {
                        "name": "Rodney Beard"
                    },
                    {
                        "name": "Rohitash Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Rohitash Chandra"
                },
                "author": "Rohitash Chandra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07187v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07187v3",
                "updated": "2024-09-16T09:57:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    57,
                    35,
                    0,
                    260,
                    0
                ],
                "published": "2024-01-14T02:30:19Z",
                "published_parsed": [
                    2024,
                    1,
                    14,
                    2,
                    30,
                    19,
                    6,
                    14,
                    0
                ],
                "title": "A Survey on Statistical Theory of Deep Learning: Approximation, Training\n  Dynamics, and Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Statistical Theory of Deep Learning: Approximation, Training\n  Dynamics, and Generative Models"
                },
                "summary": "In this article, we review the literature on statistical theories of neural\nnetworks from three perspectives: approximation, training dynamics and\ngenerative models. In the first part, results on excess risks for neural\nnetworks are reviewed in the nonparametric framework of regression (and\nclassification in Appendix~{\\color{blue}B}). These results rely on explicit\nconstructions of neural networks, leading to fast convergence rates of excess\nrisks. Nonetheless, their underlying analysis only applies to the global\nminimizer in the highly non-convex landscape of deep neural networks. This\nmotivates us to review the training dynamics of neural networks in the second\npart. Specifically, we review papers that attempt to answer ``how the neural\nnetwork trained via gradient-based methods finds the solution that can\ngeneralize well on unseen data.'' In particular, two well-known paradigms are\nreviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF)\nparadigm. Last but not least, we review the most recent theoretical\nadvancements in generative models including Generative Adversarial Networks\n(GANs), diffusion models, and in-context learning (ICL) in the Large Language\nModels (LLMs) from two perpsectives reviewed previously, i.e., approximation\nand training dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we review the literature on statistical theories of neural\nnetworks from three perspectives: approximation, training dynamics and\ngenerative models. In the first part, results on excess risks for neural\nnetworks are reviewed in the nonparametric framework of regression (and\nclassification in Appendix~{\\color{blue}B}). These results rely on explicit\nconstructions of neural networks, leading to fast convergence rates of excess\nrisks. Nonetheless, their underlying analysis only applies to the global\nminimizer in the highly non-convex landscape of deep neural networks. This\nmotivates us to review the training dynamics of neural networks in the second\npart. Specifically, we review papers that attempt to answer ``how the neural\nnetwork trained via gradient-based methods finds the solution that can\ngeneralize well on unseen data.'' In particular, two well-known paradigms are\nreviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF)\nparadigm. Last but not least, we review the most recent theoretical\nadvancements in generative models including Generative Adversarial Networks\n(GANs), diffusion models, and in-context learning (ICL) in the Large Language\nModels (LLMs) from two perpsectives reviewed previously, i.e., approximation\nand training dynamics."
                },
                "authors": [
                    {
                        "name": "Namjoon Suh"
                    },
                    {
                        "name": "Guang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Guang Cheng"
                },
                "author": "Guang Cheng",
                "arxiv_comment": "38 pages, 2 figures. Invited for review in Annual Review of\n  Statistics and Its Application",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.07187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07187v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10132v1",
                "updated": "2024-09-16T09:48:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    48,
                    56,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T09:48:56Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    48,
                    56,
                    0,
                    260,
                    0
                ],
                "title": "StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge\n  Editing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge\n  Editing for Large Language Models"
                },
                "summary": "As the modern tool of choice for question answering, large language models\n(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve\nsuch ideal question-answering systems, locating and then editing outdated\nknowledge in the natural language outputs is a general target of popular\nknowledge editing methods. However, this target is challenging, as both\nidentifying which tokens to edit in the reasoning steps and ensuring the\ncoherence of the revised reasoning chain are difficult tasks. We argue that\nthese challenges stem from the unstructured nature of natural language outputs.\nTo address the above challenges, we propose $\\textbf{Stru}$ctural\n$\\textbf{Edit}$ing ($\\textbf{StruEdit}$), an improved baseline for knowledge\nediting. We first prompt LLMs to produce structured outputs consisting of\nreasoning triplets. Then, StruEdit removes any potentially outdated knowledge\nand efficiently refills the structured outputs with up-to-date information in a\nsingle step. Experimental results show that StruEdit consistently delivers the\nhighest accuracy with lowest latency compared with other knowledge editing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the modern tool of choice for question answering, large language models\n(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve\nsuch ideal question-answering systems, locating and then editing outdated\nknowledge in the natural language outputs is a general target of popular\nknowledge editing methods. However, this target is challenging, as both\nidentifying which tokens to edit in the reasoning steps and ensuring the\ncoherence of the revised reasoning chain are difficult tasks. We argue that\nthese challenges stem from the unstructured nature of natural language outputs.\nTo address the above challenges, we propose $\\textbf{Stru}$ctural\n$\\textbf{Edit}$ing ($\\textbf{StruEdit}$), an improved baseline for knowledge\nediting. We first prompt LLMs to produce structured outputs consisting of\nreasoning triplets. Then, StruEdit removes any potentially outdated knowledge\nand efficiently refills the structured outputs with up-to-date information in a\nsingle step. Experimental results show that StruEdit consistently delivers the\nhighest accuracy with lowest latency compared with other knowledge editing\nmethods."
                },
                "authors": [
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10115v1",
                "updated": "2024-09-16T09:26:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    26,
                    49,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T09:26:49Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    26,
                    49,
                    0,
                    260,
                    0
                ],
                "title": "Accelerating Molecular Dynamics through Informed Resetting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Molecular Dynamics through Informed Resetting"
                },
                "summary": "We present a procedure for enhanced sampling of molecular dynamics\nsimulations through informed stochastic resetting. Many phenomena, such as\nprotein folding and crystal nucleation, occur over time scales that are\ninaccessible using standard simulation methods. We recently showed that\nstochastic resetting can accelerate molecular simulations that exhibit broad\ntransition time distributions. However, standard stochastic resetting does not\nexploit any information about the reaction progress. Here, we demonstrate that\nan informed resetting protocol leads to greater accelerations than standard\nstochastic resetting, both for molecular dynamics and Metadynamics simulations.\nThis is achieved by resetting only when a certain condition is met, e.g., when\nthe distance from the target along the reaction coordinate is larger than some\nthreshold. We then employ recently obtained theoretical results to identify the\ncondition that leads to the greatest acceleration and to infer the unbiased\nmean transition time from accelerated simulations. Our work significantly\nextends the applicability of stochastic resetting for enhanced sampling of\nmolecular simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a procedure for enhanced sampling of molecular dynamics\nsimulations through informed stochastic resetting. Many phenomena, such as\nprotein folding and crystal nucleation, occur over time scales that are\ninaccessible using standard simulation methods. We recently showed that\nstochastic resetting can accelerate molecular simulations that exhibit broad\ntransition time distributions. However, standard stochastic resetting does not\nexploit any information about the reaction progress. Here, we demonstrate that\nan informed resetting protocol leads to greater accelerations than standard\nstochastic resetting, both for molecular dynamics and Metadynamics simulations.\nThis is achieved by resetting only when a certain condition is met, e.g., when\nthe distance from the target along the reaction coordinate is larger than some\nthreshold. We then employ recently obtained theoretical results to identify the\ncondition that leads to the greatest acceleration and to infer the unbiased\nmean transition time from accelerated simulations. Our work significantly\nextends the applicability of stochastic resetting for enhanced sampling of\nmolecular simulations."
                },
                "authors": [
                    {
                        "name": "Jonathan R. Church"
                    },
                    {
                        "name": "Ofir Blumer"
                    },
                    {
                        "name": "Tommer D. Keidar"
                    },
                    {
                        "name": "Leo Ploutno"
                    },
                    {
                        "name": "Shlomi Reuveni"
                    },
                    {
                        "name": "Barak Hirshberg"
                    }
                ],
                "author_detail": {
                    "name": "Barak Hirshberg"
                },
                "author": "Barak Hirshberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04620v2",
                "updated": "2024-09-16T09:22:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    22,
                    20,
                    0,
                    260,
                    0
                ],
                "published": "2024-02-07T07:07:02Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    7,
                    7,
                    2,
                    2,
                    38,
                    0
                ],
                "title": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients"
                },
                "summary": "The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndevelop CataractBot, an experts-in-the-loop chatbot powered by LLMs, in\ncollaboration with an eye hospital in India. CataractBot answers cataract\nsurgery related questions instantly by querying a curated knowledge base, and\nprovides expert-verified responses asynchronously. It has multimodal and\nmultilingual capabilities. In an in-the-wild deployment study with 55\nparticipants, CataractBot proved valuable, providing anytime accessibility,\nsaving time, accommodating diverse literacy levels, alleviating power\ndifferences, and adding a privacy layer between patients and doctors. Users\nreported that their trust in the system was established through expert\nverification. Broadly, our results could inform future work on designing\nexpert-mediated LLM bots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndevelop CataractBot, an experts-in-the-loop chatbot powered by LLMs, in\ncollaboration with an eye hospital in India. CataractBot answers cataract\nsurgery related questions instantly by querying a curated knowledge base, and\nprovides expert-verified responses asynchronously. It has multimodal and\nmultilingual capabilities. In an in-the-wild deployment study with 55\nparticipants, CataractBot proved valuable, providing anytime accessibility,\nsaving time, accommodating diverse literacy levels, alleviating power\ndifferences, and adding a privacy layer between patients and doctors. Users\nreported that their trust in the system was established through expert\nverification. Broadly, our results could inform future work on designing\nexpert-mediated LLM bots."
                },
                "authors": [
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Satvik Golechha"
                    },
                    {
                        "name": "Shreyas Kulkarni"
                    },
                    {
                        "name": "Geeta Fulari"
                    },
                    {
                        "name": "Kaushik Murali"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10106v1",
                "updated": "2024-09-16T09:12:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    12,
                    6,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T09:12:06Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    12,
                    6,
                    0,
                    260,
                    0
                ],
                "title": "Industry 6.0: New Generation of Industry driven by Generative AI and\n  Swarm of Heterogeneous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industry 6.0: New Generation of Industry driven by Generative AI and\n  Swarm of Heterogeneous Robots"
                },
                "summary": "This paper presents the concept of Industry 6.0, introducing the world's\nfirst fully automated production system that autonomously handles the entire\nproduct design and manufacturing process based on user-provided natural\nlanguage descriptions. By leveraging generative AI, the system automates\ncritical aspects of production, including product blueprint design, component\nmanufacturing, logistics, and assembly. A heterogeneous swarm of robots, each\nequipped with individual AI through integration with Large Language Models\n(LLMs), orchestrates the production process. The robotic system includes\nmanipulator arms, delivery drones, and 3D printers capable of generating\nassembly blueprints. The system was evaluated using commercial and open-source\nLLMs, functioning through APIs and local deployment. A user study demonstrated\nthat the system reduces the average production time to 119.10 minutes,\nsignificantly outperforming a team of expert human developers, who averaged\n528.64 minutes (an improvement factor of 4.4). Furthermore, in the product\nblueprinting stage, the system surpassed human CAD operators by an\nunprecedented factor of 47, completing the task in 0.5 minutes compared to 23.5\nminutes. This breakthrough represents a major leap towards fully autonomous\nmanufacturing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the concept of Industry 6.0, introducing the world's\nfirst fully automated production system that autonomously handles the entire\nproduct design and manufacturing process based on user-provided natural\nlanguage descriptions. By leveraging generative AI, the system automates\ncritical aspects of production, including product blueprint design, component\nmanufacturing, logistics, and assembly. A heterogeneous swarm of robots, each\nequipped with individual AI through integration with Large Language Models\n(LLMs), orchestrates the production process. The robotic system includes\nmanipulator arms, delivery drones, and 3D printers capable of generating\nassembly blueprints. The system was evaluated using commercial and open-source\nLLMs, functioning through APIs and local deployment. A user study demonstrated\nthat the system reduces the average production time to 119.10 minutes,\nsignificantly outperforming a team of expert human developers, who averaged\n528.64 minutes (an improvement factor of 4.4). Furthermore, in the product\nblueprinting stage, the system surpassed human CAD operators by an\nunprecedented factor of 47, completing the task in 0.5 minutes compared to 23.5\nminutes. This breakthrough represents a major leap towards fully autonomous\nmanufacturing."
                },
                "authors": [
                    {
                        "name": "Artem Lykov"
                    },
                    {
                        "name": "Miguel Altamirano Cabrera"
                    },
                    {
                        "name": "Mikhail Konenkov"
                    },
                    {
                        "name": "Valerii Serpiva"
                    },
                    {
                        "name": "Koffivi Fid`ele Gbagbe"
                    },
                    {
                        "name": "Ali Alabbas"
                    },
                    {
                        "name": "Aleksey Fedoseev"
                    },
                    {
                        "name": "Luis Moreno"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Ziang Guo"
                    },
                    {
                        "name": "Dzmitry Tsetserukou"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Tsetserukou"
                },
                "author": "Dzmitry Tsetserukou",
                "arxiv_comment": "submitted to IEEE conf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.05653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.05653v2",
                "updated": "2024-09-16T09:10:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    10,
                    0,
                    0,
                    260,
                    0
                ],
                "published": "2023-04-12T07:16:55Z",
                "published_parsed": [
                    2023,
                    4,
                    12,
                    7,
                    16,
                    55,
                    2,
                    102,
                    0
                ],
                "title": "A Closer Look at the Explainability of Contrastive Language-Image\n  Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Closer Look at the Explainability of Contrastive Language-Image\n  Pre-training"
                },
                "summary": "Contrastive language-image pre-training (CLIP) is a powerful vision-language\nmodel that has shown great benefits for various tasks. However, we have\nidentified some issues with its explainability, which undermine its credibility\nand limit the capacity for related tasks. Specifically, we find that CLIP tends\nto focus on background regions rather than foregrounds, with noisy activations\nat irrelevant positions on the visualization results. These phenomena conflict\nwith conventional explainability methods based on the class attention map\n(CAM), where the raw model can highlight the local foreground regions using\nglobal supervision without alignment. To address these problems, we take a\ncloser look at its architecture and features. Based on thorough analyses, we\nfind the raw self-attentions link to inconsistent semantic regions, resulting\nin the opposite visualization. Besides, the noisy activations are owing to\nredundant features among categories. Building on these insights, we propose the\nCLIP Surgery for reliable CAM, a method that allows surgery-like modifications\nto the inference architecture and features, without further fine-tuning as\nclassical CAM methods. This approach significantly improves the explainability\nof CLIP, surpassing existing methods by large margins. Besides, it enables\nmultimodal visualization and extends the capacity of raw CLIP on\nopen-vocabulary tasks without extra alignment. The code is available at\nhttps://github.com/xmed-lab/CLIP_Surgery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive language-image pre-training (CLIP) is a powerful vision-language\nmodel that has shown great benefits for various tasks. However, we have\nidentified some issues with its explainability, which undermine its credibility\nand limit the capacity for related tasks. Specifically, we find that CLIP tends\nto focus on background regions rather than foregrounds, with noisy activations\nat irrelevant positions on the visualization results. These phenomena conflict\nwith conventional explainability methods based on the class attention map\n(CAM), where the raw model can highlight the local foreground regions using\nglobal supervision without alignment. To address these problems, we take a\ncloser look at its architecture and features. Based on thorough analyses, we\nfind the raw self-attentions link to inconsistent semantic regions, resulting\nin the opposite visualization. Besides, the noisy activations are owing to\nredundant features among categories. Building on these insights, we propose the\nCLIP Surgery for reliable CAM, a method that allows surgery-like modifications\nto the inference architecture and features, without further fine-tuning as\nclassical CAM methods. This approach significantly improves the explainability\nof CLIP, surpassing existing methods by large margins. Besides, it enables\nmultimodal visualization and extends the capacity of raw CLIP on\nopen-vocabulary tasks without extra alignment. The code is available at\nhttps://github.com/xmed-lab/CLIP_Surgery."
                },
                "authors": [
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Hualiang Wang"
                    },
                    {
                        "name": "Yiqun Duan"
                    },
                    {
                        "name": "Jiheng Zhang"
                    },
                    {
                        "name": "Xiaomeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaomeng Li"
                },
                "author": "Xiaomeng Li",
                "arxiv_comment": "30 pages, 11 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.05653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.05653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10102v1",
                "updated": "2024-09-16T09:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    6,
                    44,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T09:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    6,
                    44,
                    0,
                    260,
                    0
                ],
                "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal\nparadigm in the development of Large Language Models (LLMs). While much of the\ncurrent research in this field focuses on performance optimization,\nparticularly in terms of accuracy and efficiency, the trustworthiness of RAG\nsystems remains an area still under exploration. From a positive perspective,\nRAG systems are promising to enhance LLMs by providing them with useful and\nup-to-date knowledge from vast external databases, thereby mitigating the\nlong-standing problem of hallucination. While from a negative perspective, RAG\nsystems are at the risk of generating undesirable contents if the retrieved\ninformation is either inappropriate or poorly utilized. To address these\nconcerns, we propose a unified framework that assesses the trustworthiness of\nRAG systems across six key dimensions: factuality, robustness, fairness,\ntransparency, accountability, and privacy. Within this framework, we thoroughly\nreview the existing literature on each dimension. Additionally, we create the\nevaluation benchmark regarding the six dimensions and conduct comprehensive\nevaluations for a variety of proprietary and open-source models. Finally, we\nidentify the potential challenges for future research based on our\ninvestigation results. Through this work, we aim to lay a structured foundation\nfor future investigations and provide practical insights for enhancing the\ntrustworthiness of RAG systems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal\nparadigm in the development of Large Language Models (LLMs). While much of the\ncurrent research in this field focuses on performance optimization,\nparticularly in terms of accuracy and efficiency, the trustworthiness of RAG\nsystems remains an area still under exploration. From a positive perspective,\nRAG systems are promising to enhance LLMs by providing them with useful and\nup-to-date knowledge from vast external databases, thereby mitigating the\nlong-standing problem of hallucination. While from a negative perspective, RAG\nsystems are at the risk of generating undesirable contents if the retrieved\ninformation is either inappropriate or poorly utilized. To address these\nconcerns, we propose a unified framework that assesses the trustworthiness of\nRAG systems across six key dimensions: factuality, robustness, fairness,\ntransparency, accountability, and privacy. Within this framework, we thoroughly\nreview the existing literature on each dimension. Additionally, we create the\nevaluation benchmark regarding the six dimensions and conduct comprehensive\nevaluations for a variety of proprietary and open-source models. Finally, we\nidentify the potential challenges for future research based on our\ninvestigation results. Through this work, we aim to lay a structured foundation\nfor future investigations and provide practical insights for enhancing the\ntrustworthiness of RAG systems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Xiaoxi Li"
                    },
                    {
                        "name": "Jiajie Jin"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Chaozhuo Li"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10095v1",
                "updated": "2024-09-16T08:54:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    54,
                    3,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T08:54:03Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    54,
                    3,
                    0,
                    260,
                    0
                ],
                "title": "Human Insights Driven Latent Space for Different Driving Perspectives: A\n  Unified Encoder for Efficient Multi-Task Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Insights Driven Latent Space for Different Driving Perspectives: A\n  Unified Encoder for Efficient Multi-Task Inference"
                },
                "summary": "Autonomous driving holds great potential to transform road safety and traffic\nefficiency by minimizing human error and reducing congestion. A key challenge\nin realizing this potential is the accurate estimation of steering angles,\nwhich is essential for effective vehicle navigation and control. Recent\nbreakthroughs in deep learning have made it possible to estimate steering\nangles directly from raw camera inputs. However, the limited available\nnavigation data can hinder optimal feature learning, impacting the system's\nperformance in complex driving scenarios. In this paper, we propose a shared\nencoder trained on multiple computer vision tasks critical for urban\nnavigation, such as depth, pose, and 3D scene flow estimation, as well as\nsemantic, instance, panoptic, and motion segmentation. By incorporating diverse\nvisual information used by humans during navigation, this unified encoder might\nenhance steering angle estimation. To achieve effective multi-task learning\nwithin a single encoder, we introduce a multi-scale feature network for pose\nestimation to improve depth learning. Additionally, we employ knowledge\ndistillation from a multi-backbone model pretrained on these navigation tasks\nto stabilize training and boost performance. Our findings demonstrate that a\nshared backbone trained on diverse visual tasks is capable of providing overall\nperception capabilities. While our performance in steering angle estimation is\ncomparable to existing methods, the integration of human-like perception\nthrough multi-task learning holds significant potential for advancing\nautonomous driving systems. More details and the pretrained model are available\nat https://hi-computervision.github.io/uni-encoder/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving holds great potential to transform road safety and traffic\nefficiency by minimizing human error and reducing congestion. A key challenge\nin realizing this potential is the accurate estimation of steering angles,\nwhich is essential for effective vehicle navigation and control. Recent\nbreakthroughs in deep learning have made it possible to estimate steering\nangles directly from raw camera inputs. However, the limited available\nnavigation data can hinder optimal feature learning, impacting the system's\nperformance in complex driving scenarios. In this paper, we propose a shared\nencoder trained on multiple computer vision tasks critical for urban\nnavigation, such as depth, pose, and 3D scene flow estimation, as well as\nsemantic, instance, panoptic, and motion segmentation. By incorporating diverse\nvisual information used by humans during navigation, this unified encoder might\nenhance steering angle estimation. To achieve effective multi-task learning\nwithin a single encoder, we introduce a multi-scale feature network for pose\nestimation to improve depth learning. Additionally, we employ knowledge\ndistillation from a multi-backbone model pretrained on these navigation tasks\nto stabilize training and boost performance. Our findings demonstrate that a\nshared backbone trained on diverse visual tasks is capable of providing overall\nperception capabilities. While our performance in steering angle estimation is\ncomparable to existing methods, the integration of human-like perception\nthrough multi-task learning holds significant potential for advancing\nautonomous driving systems. More details and the pretrained model are available\nat https://hi-computervision.github.io/uni-encoder/."
                },
                "authors": [
                    {
                        "name": "Huy-Dung Nguyen"
                    },
                    {
                        "name": "Anass Bairouk"
                    },
                    {
                        "name": "Mirjana Maras"
                    },
                    {
                        "name": "Wei Xiao"
                    },
                    {
                        "name": "Tsun-Hsuan Wang"
                    },
                    {
                        "name": "Patrick Chareyre"
                    },
                    {
                        "name": "Ramin Hasani"
                    },
                    {
                        "name": "Marc Blanchon"
                    },
                    {
                        "name": "Daniela Rus"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Rus"
                },
                "author": "Daniela Rus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10090v1",
                "updated": "2024-09-16T08:44:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    44,
                    17,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T08:44:17Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    44,
                    17,
                    0,
                    260,
                    0
                ],
                "title": "MotionCom: Automatic and Motion-Aware Image Composition with LLM and\n  Video Diffusion Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionCom: Automatic and Motion-Aware Image Composition with LLM and\n  Video Diffusion Prior"
                },
                "summary": "This work presents MotionCom, a training-free motion-aware diffusion based\nimage composition, enabling automatic and seamless integration of target\nobjects into new scenes with dynamically coherent results without finetuning or\noptimization. Traditional approaches in this area suffer from two significant\nlimitations: they require manual planning for object placement and often\ngenerate static compositions lacking motion realism. MotionCom addresses these\nissues by utilizing a Large Vision Language Model (LVLM) for intelligent\nplanning, and a Video Diffusion prior for motion-infused image synthesis,\nstreamlining the composition process. Our multi-modal Chain-of-Thought (CoT)\nprompting with LVLM automates the strategic placement planning of foreground\nobjects, considering their potential motion and interaction within the scenes.\nComplementing this, we propose a novel method MotionPaint to distill\nmotion-aware information from pretrained video diffusion models in the\ngeneration phase, ensuring that these objects are not only seamlessly\nintegrated but also endowed with realistic motion. Extensive quantitative and\nqualitative results highlight MotionCom's superiority, showcasing its\nefficiency in streamlining the planning process and its capability to produce\ncompositions that authentically depict motion and interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents MotionCom, a training-free motion-aware diffusion based\nimage composition, enabling automatic and seamless integration of target\nobjects into new scenes with dynamically coherent results without finetuning or\noptimization. Traditional approaches in this area suffer from two significant\nlimitations: they require manual planning for object placement and often\ngenerate static compositions lacking motion realism. MotionCom addresses these\nissues by utilizing a Large Vision Language Model (LVLM) for intelligent\nplanning, and a Video Diffusion prior for motion-infused image synthesis,\nstreamlining the composition process. Our multi-modal Chain-of-Thought (CoT)\nprompting with LVLM automates the strategic placement planning of foreground\nobjects, considering their potential motion and interaction within the scenes.\nComplementing this, we propose a novel method MotionPaint to distill\nmotion-aware information from pretrained video diffusion models in the\ngeneration phase, ensuring that these objects are not only seamlessly\nintegrated but also endowed with realistic motion. Extensive quantitative and\nqualitative results highlight MotionCom's superiority, showcasing its\nefficiency in streamlining the planning process and its capability to produce\ncompositions that authentically depict motion and interaction."
                },
                "authors": [
                    {
                        "name": "Weijing Tao"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Miaomiao Cui"
                    },
                    {
                        "name": "Guosheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guosheng Lin"
                },
                "author": "Guosheng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10081v1",
                "updated": "2024-09-16T08:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    37,
                    43,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T08:37:43Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    37,
                    43,
                    0,
                    260,
                    0
                ],
                "title": "Messy Code Makes Managing ML Pipelines Difficult? Just Let LLMs Rewrite\n  the Code!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Messy Code Makes Managing ML Pipelines Difficult? Just Let LLMs Rewrite\n  the Code!"
                },
                "summary": "Machine learning (ML) applications that learn from data are increasingly used\nto automate impactful decisions. Unfortunately, these applications often fall\nshort of adequately managing critical data and complying with upcoming\nregulations. A technical reason for the persistence of these issues is that the\ndata pipelines in common ML libraries and cloud services lack fundamental\ndeclarative, data-centric abstractions. Recent research has shown how such\nabstractions enable techniques like provenance tracking and automatic\ninspection to help manage ML pipelines. Unfortunately, these approaches lack\nadoption in the real world because they require clean ML pipeline code written\nwith declarative APIs, instead of the messy imperative Python code that data\nscientists typically write for data preparation.\n  We argue that it is unrealistic to expect data scientists to change their\nestablished development practices. Instead, we propose to circumvent this \"code\nabstraction gap\" by leveraging the code generation capabilities of large\nlanguage models (LLMs). Our idea is to rewrite messy data science code to a\ncustom-tailored declarative pipeline abstraction, which we implement as a\nproof-of-concept in our prototype Lester. We detail its application for a\nchallenging compliance management example involving \"incremental view\nmaintenance\" of deployed ML pipelines. The code rewrites for our running\nexample show the potential of LLMs to make messy data science code declarative,\ne.g., by identifying hand-coded joins in Python and turning them into joins on\ndataframes, or by generating declarative feature encoders from NumPy code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) applications that learn from data are increasingly used\nto automate impactful decisions. Unfortunately, these applications often fall\nshort of adequately managing critical data and complying with upcoming\nregulations. A technical reason for the persistence of these issues is that the\ndata pipelines in common ML libraries and cloud services lack fundamental\ndeclarative, data-centric abstractions. Recent research has shown how such\nabstractions enable techniques like provenance tracking and automatic\ninspection to help manage ML pipelines. Unfortunately, these approaches lack\nadoption in the real world because they require clean ML pipeline code written\nwith declarative APIs, instead of the messy imperative Python code that data\nscientists typically write for data preparation.\n  We argue that it is unrealistic to expect data scientists to change their\nestablished development practices. Instead, we propose to circumvent this \"code\nabstraction gap\" by leveraging the code generation capabilities of large\nlanguage models (LLMs). Our idea is to rewrite messy data science code to a\ncustom-tailored declarative pipeline abstraction, which we implement as a\nproof-of-concept in our prototype Lester. We detail its application for a\nchallenging compliance management example involving \"incremental view\nmaintenance\" of deployed ML pipelines. The code rewrites for our running\nexample show the potential of LLMs to make messy data science code declarative,\ne.g., by identifying hand-coded joins in Python and turning them into joins on\ndataframes, or by generating declarative feature encoders from NumPy code."
                },
                "authors": [
                    {
                        "name": "Sebastian Schelter"
                    },
                    {
                        "name": "Stefan Grafberger"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Grafberger"
                },
                "author": "Stefan Grafberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09576v2",
                "updated": "2024-09-16T08:35:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    35,
                    51,
                    0,
                    260,
                    0
                ],
                "published": "2024-04-15T08:37:26Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    8,
                    37,
                    26,
                    0,
                    106,
                    0
                ],
                "title": "Large language models and linguistic intentionality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models and linguistic intentionality"
                },
                "summary": "Do large language models like Chat-GPT or LLaMa meaningfully use the words\nthey produce? Or are they merely clever prediction machines, simulating\nlanguage use by producing statistically plausible text? There have already been\nsome initial attempts to answer this question by showing that these models meet\nthe criteria for entering meaningful states according to metasemantic theories\nof mental content. In this paper, I will argue for a different approach - that\nwe should instead consider whether language models meet the criteria given by\nour best metasemantic theories of linguistic content. In that vein, I will\nillustrate how this can be done by applying two such theories to the case of\nlanguage models: Gareth Evans' (1982) account of naming practices and Ruth\nMillikan's (1984, 2004, 2005) teleosemantics. In doing so, I will argue that it\nis a mistake to think that the failure of LLMs to meet plausible conditions for\nmental intentionality thereby renders their outputs meaningless, and that a\ndistinguishing feature of linguistic intentionality - dependency on a\npre-existing linguistic system - allows for the plausible result LLM outputs\nare meaningful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do large language models like Chat-GPT or LLaMa meaningfully use the words\nthey produce? Or are they merely clever prediction machines, simulating\nlanguage use by producing statistically plausible text? There have already been\nsome initial attempts to answer this question by showing that these models meet\nthe criteria for entering meaningful states according to metasemantic theories\nof mental content. In this paper, I will argue for a different approach - that\nwe should instead consider whether language models meet the criteria given by\nour best metasemantic theories of linguistic content. In that vein, I will\nillustrate how this can be done by applying two such theories to the case of\nlanguage models: Gareth Evans' (1982) account of naming practices and Ruth\nMillikan's (1984, 2004, 2005) teleosemantics. In doing so, I will argue that it\nis a mistake to think that the failure of LLMs to meet plausible conditions for\nmental intentionality thereby renders their outputs meaningless, and that a\ndistinguishing feature of linguistic intentionality - dependency on a\npre-existing linguistic system - allows for the plausible result LLM outputs\nare meaningful."
                },
                "authors": [
                    {
                        "name": "Jumbly Grindrod"
                    }
                ],
                "author_detail": {
                    "name": "Jumbly Grindrod"
                },
                "author": "Jumbly Grindrod",
                "arxiv_doi": "10.1007/s11229-024-04723-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11229-024-04723-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.09576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Synthese, Vol. 204: 71 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10077v1",
                "updated": "2024-09-16T08:28:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    28,
                    5,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T08:28:05Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    28,
                    5,
                    0,
                    260,
                    0
                ],
                "title": "LLM-DER:A Named Entity Recognition Method Based on Large Language Models\n  for Chinese Coal Chemical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-DER:A Named Entity Recognition Method Based on Large Language Models\n  for Chinese Coal Chemical Domain"
                },
                "summary": "Domain-specific Named Entity Recognition (NER), whose goal is to recognize\ndomain-specific entities and their categories, provides an important support\nfor constructing domain knowledge graphs. Currently, deep learning-based\nmethods are widely used and effective in NER tasks, but due to the reliance on\nlarge-scale labeled data. As a result, the scarcity of labeled data in a\nspecific domain will limit its application.Therefore, many researches started\nto introduce few-shot methods and achieved some results. However, the entity\nstructures in specific domains are often complex, and the current few-shot\nmethods are difficult to adapt to NER tasks with complex features.Taking the\nChinese coal chemical industry domain as an example,there exists a complex\nstructure of multiple entities sharing a single entity, as well as multiple\nrelationships for the same pair of entities, which affects the NER task under\nthe sample less condition.In this paper, we propose a Large Language Models\n(LLMs)-based entity recognition framework LLM-DER for the domain-specific\nentity recognition problem in Chinese, which enriches the entity information by\ngenerating a list of relationships containing entity types through LLMs, and\ndesigning a plausibility and consistency evaluation method to remove\nmisrecognized entities, which can effectively solve the complex structural\nentity recognition problem in a specific domain.The experimental results of\nthis paper on the Resume dataset and the self-constructed coal chemical dataset\nCoal show that LLM-DER performs outstandingly in domain-specific entity\nrecognition, not only outperforming the existing GPT-3.5-turbo baseline, but\nalso exceeding the fully-supervised baseline, verifying its effectiveness in\nentity recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-specific Named Entity Recognition (NER), whose goal is to recognize\ndomain-specific entities and their categories, provides an important support\nfor constructing domain knowledge graphs. Currently, deep learning-based\nmethods are widely used and effective in NER tasks, but due to the reliance on\nlarge-scale labeled data. As a result, the scarcity of labeled data in a\nspecific domain will limit its application.Therefore, many researches started\nto introduce few-shot methods and achieved some results. However, the entity\nstructures in specific domains are often complex, and the current few-shot\nmethods are difficult to adapt to NER tasks with complex features.Taking the\nChinese coal chemical industry domain as an example,there exists a complex\nstructure of multiple entities sharing a single entity, as well as multiple\nrelationships for the same pair of entities, which affects the NER task under\nthe sample less condition.In this paper, we propose a Large Language Models\n(LLMs)-based entity recognition framework LLM-DER for the domain-specific\nentity recognition problem in Chinese, which enriches the entity information by\ngenerating a list of relationships containing entity types through LLMs, and\ndesigning a plausibility and consistency evaluation method to remove\nmisrecognized entities, which can effectively solve the complex structural\nentity recognition problem in a specific domain.The experimental results of\nthis paper on the Resume dataset and the self-constructed coal chemical dataset\nCoal show that LLM-DER performs outstandingly in domain-specific entity\nrecognition, not only outperforming the existing GPT-3.5-turbo baseline, but\nalso exceeding the fully-supervised baseline, verifying its effectiveness in\nentity recognition."
                },
                "authors": [
                    {
                        "name": "Le Xiao"
                    },
                    {
                        "name": "Yunfei Xu"
                    },
                    {
                        "name": "Jing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhao"
                },
                "author": "Jing Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18019v2",
                "updated": "2024-09-16T08:25:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    25,
                    42,
                    0,
                    260,
                    0
                ],
                "published": "2024-05-28T10:07:17Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    10,
                    7,
                    17,
                    1,
                    149,
                    0
                ],
                "title": "Mutual Information Analysis of Neuromorphic Coding for Distributed\n  Wireless Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutual Information Analysis of Neuromorphic Coding for Distributed\n  Wireless Spiking Neural Networks"
                },
                "summary": "Wireless spiking neural networks (WSNNs) allow energy-efficient\ncommunications, especially when considering edge intelligence and learning for\nboth terrestrial beyond 5G/6G and space networking systems. Recent research\nwork has revealed that distributed wireless SNNs (DWSNNs) show good performance\nin terms of inference accuracy and low energy consumption of edge devices,\nunder the constraints of limited bandwidth and spike loss probability.\nFollowing this reasoning, this technology can be promising for wireless sensor\nnetworks (WSNs) in space applications, where the energy constraint is\npredominant. In this work, we focus on neuromorphic impulse radio (IR)\ntransmission techniques for DWSNNs, quantitatively evaluating the features of\ndifferent coding algorithms that can be viewed as IR modulations. Specifically,\nthe main contribution of this work is the evaluation of information-theoretic\nmeasures that may help in quantifying performance trade-offs among existing\nneuromorphic coding techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless spiking neural networks (WSNNs) allow energy-efficient\ncommunications, especially when considering edge intelligence and learning for\nboth terrestrial beyond 5G/6G and space networking systems. Recent research\nwork has revealed that distributed wireless SNNs (DWSNNs) show good performance\nin terms of inference accuracy and low energy consumption of edge devices,\nunder the constraints of limited bandwidth and spike loss probability.\nFollowing this reasoning, this technology can be promising for wireless sensor\nnetworks (WSNs) in space applications, where the energy constraint is\npredominant. In this work, we focus on neuromorphic impulse radio (IR)\ntransmission techniques for DWSNNs, quantitatively evaluating the features of\ndifferent coding algorithms that can be viewed as IR modulations. Specifically,\nthe main contribution of this work is the evaluation of information-theoretic\nmeasures that may help in quantifying performance trade-offs among existing\nneuromorphic coding techniques."
                },
                "authors": [
                    {
                        "name": "Pietro Savazzi"
                    },
                    {
                        "name": "Anna Vizziello"
                    },
                    {
                        "name": "Fabio Dell'Acqua"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Dell'Acqua"
                },
                "author": "Fabio Dell'Acqua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06955v2",
                "updated": "2024-09-16T08:23:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    23,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-11T02:36:36Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    36,
                    36,
                    2,
                    255,
                    0
                ],
                "title": "Privacy-Preserving Federated Learning with Consistency via Knowledge\n  Distillation Using Conditional Generator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Federated Learning with Consistency via Knowledge\n  Distillation Using Conditional Generator"
                },
                "summary": "Federated Learning (FL) is gaining popularity as a distributed learning\nframework that only shares model parameters or gradient updates and keeps\nprivate data locally. However, FL is at risk of privacy leakage caused by\nprivacy inference attacks. And most existing privacy-preserving mechanisms in\nFL conflict with achieving high performance and efficiency. Therefore, we\npropose FedMD-CG, a novel FL method with highly competitive performance and\nhigh-level privacy preservation, which decouples each client's local model into\na feature extractor and a classifier, and utilizes a conditional generator\ninstead of the feature extractor to perform server-side model aggregation. To\nensure the consistency of local generators and classifiers, FedMD-CG leverages\nknowledge distillation to train local models and generators at both the latent\nfeature level and the logit level. Also, we construct additional classification\nlosses and design new diversity losses to enhance client-side training.\nFedMD-CG is robust to data heterogeneity and does not require training extra\ndiscriminators (like cGAN). We conduct extensive experiments on various image\nclassification tasks to validate the superiority of FedMD-CG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is gaining popularity as a distributed learning\nframework that only shares model parameters or gradient updates and keeps\nprivate data locally. However, FL is at risk of privacy leakage caused by\nprivacy inference attacks. And most existing privacy-preserving mechanisms in\nFL conflict with achieving high performance and efficiency. Therefore, we\npropose FedMD-CG, a novel FL method with highly competitive performance and\nhigh-level privacy preservation, which decouples each client's local model into\na feature extractor and a classifier, and utilizes a conditional generator\ninstead of the feature extractor to perform server-side model aggregation. To\nensure the consistency of local generators and classifiers, FedMD-CG leverages\nknowledge distillation to train local models and generators at both the latent\nfeature level and the logit level. Also, we construct additional classification\nlosses and design new diversity losses to enhance client-side training.\nFedMD-CG is robust to data heterogeneity and does not require training extra\ndiscriminators (like cGAN). We conduct extensive experiments on various image\nclassification tasks to validate the superiority of FedMD-CG."
                },
                "authors": [
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yunshi Lan"
                    },
                    {
                        "name": "Ming Gao"
                    },
                    {
                        "name": "Jinlong Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Shu"
                },
                "author": "Jinlong Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10066v1",
                "updated": "2024-09-16T08:01:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    1,
                    21,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T08:01:21Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    1,
                    21,
                    0,
                    260,
                    0
                ],
                "title": "LeGEND: A Top-Down Approach to Scenario Generation of Autonomous Driving\n  Systems Assisted by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeGEND: A Top-Down Approach to Scenario Generation of Autonomous Driving\n  Systems Assisted by Large Language Models"
                },
                "summary": "Autonomous driving systems (ADS) are safety-critical and require\ncomprehensive testing before their deployment on public roads. While existing\ntesting approaches primarily aim at the criticality of scenarios, they often\noverlook the diversity of the generated scenarios that is also important to\nreflect system defects in different aspects. To bridge the gap, we propose\nLeGEND, that features a top-down fashion of scenario generation: it starts with\nabstract functional scenarios, and then steps downwards to logical and concrete\nscenarios, such that scenario diversity can be controlled at the functional\nlevel. However, unlike logical scenarios that can be formally described,\nfunctional scenarios are often documented in natural languages (e.g., accident\nreports) and thus cannot be precisely parsed and processed by computers. To\ntackle that issue, LeGEND leverages the recent advances of large language\nmodels (LLMs) to transform textual functional scenarios to formal logical\nscenarios. To mitigate the distraction of useless information in functional\nscenario description, we devise a two-phase transformation that features the\nuse of an intermediate language; consequently, we adopt two LLMs in LeGEND, one\nfor extracting information from functional scenarios, the other for converting\nthe extracted information to formal logical scenarios. We experimentally\nevaluate LeGEND on Apollo, an industry-grade ADS from Baidu. Evaluation results\nshow that LeGEND can effectively identify critical scenarios, and compared to\nbaseline approaches, LeGEND exhibits evident superiority in diversity of\ngenerated scenarios. Moreover, we also demonstrate the advantages of our\ntwo-phase transformation framework, and the accuracy of the adopted LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems (ADS) are safety-critical and require\ncomprehensive testing before their deployment on public roads. While existing\ntesting approaches primarily aim at the criticality of scenarios, they often\noverlook the diversity of the generated scenarios that is also important to\nreflect system defects in different aspects. To bridge the gap, we propose\nLeGEND, that features a top-down fashion of scenario generation: it starts with\nabstract functional scenarios, and then steps downwards to logical and concrete\nscenarios, such that scenario diversity can be controlled at the functional\nlevel. However, unlike logical scenarios that can be formally described,\nfunctional scenarios are often documented in natural languages (e.g., accident\nreports) and thus cannot be precisely parsed and processed by computers. To\ntackle that issue, LeGEND leverages the recent advances of large language\nmodels (LLMs) to transform textual functional scenarios to formal logical\nscenarios. To mitigate the distraction of useless information in functional\nscenario description, we devise a two-phase transformation that features the\nuse of an intermediate language; consequently, we adopt two LLMs in LeGEND, one\nfor extracting information from functional scenarios, the other for converting\nthe extracted information to formal logical scenarios. We experimentally\nevaluate LeGEND on Apollo, an industry-grade ADS from Baidu. Evaluation results\nshow that LeGEND can effectively identify critical scenarios, and compared to\nbaseline approaches, LeGEND exhibits evident superiority in diversity of\ngenerated scenarios. Moreover, we also demonstrate the advantages of our\ntwo-phase transformation framework, and the accuracy of the adopted LLMs."
                },
                "authors": [
                    {
                        "name": "Shuncheng Tang"
                    },
                    {
                        "name": "Zhenya Zhang"
                    },
                    {
                        "name": "Jixiang Zhou"
                    },
                    {
                        "name": "Lei Lei"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Yinxing Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yinxing Xue"
                },
                "author": "Yinxing Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10064v1",
                "updated": "2024-09-16T07:58:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    58,
                    56,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:58:56Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    58,
                    56,
                    0,
                    260,
                    0
                ],
                "title": "MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid\n  via Edge LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid\n  via Edge LLM"
                },
                "summary": "Mental health disorders are among the most prevalent diseases worldwide,\naffecting nearly one in four people. Despite their widespread impact, the\nintervention rate remains below 25%, largely due to the significant cooperation\nrequired from patients for both diagnosis and intervention. The core issue\nbehind this low treatment rate is stigma, which discourages over half of those\naffected from seeking help. This paper presents MindGuard, an accessible,\nstigma-free, and professional mobile mental healthcare system designed to\nprovide mental health first aid. The heart of MindGuard is an innovative edge\nLLM, equipped with professional mental health knowledge, that seamlessly\nintegrates objective mobile sensor data with subjective Ecological Momentary\nAssessment records to deliver personalized screening and intervention\nconversations. We conduct a broad evaluation of MindGuard using open datasets\nspanning four years and real-world deployment across various mobile devices\ninvolving 20 subjects for two weeks. Remarkably, MindGuard achieves results\ncomparable to GPT-4 and outperforms its counterpart with more than 10 times the\nmodel size. We believe that MindGuard paves the way for mobile LLM\napplications, potentially revolutionizing mental healthcare practices by\nsubstituting self-reporting and intervention conversations with passive,\nintegrated monitoring within daily life, thus ensuring accessible and\nstigma-free mental health support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health disorders are among the most prevalent diseases worldwide,\naffecting nearly one in four people. Despite their widespread impact, the\nintervention rate remains below 25%, largely due to the significant cooperation\nrequired from patients for both diagnosis and intervention. The core issue\nbehind this low treatment rate is stigma, which discourages over half of those\naffected from seeking help. This paper presents MindGuard, an accessible,\nstigma-free, and professional mobile mental healthcare system designed to\nprovide mental health first aid. The heart of MindGuard is an innovative edge\nLLM, equipped with professional mental health knowledge, that seamlessly\nintegrates objective mobile sensor data with subjective Ecological Momentary\nAssessment records to deliver personalized screening and intervention\nconversations. We conduct a broad evaluation of MindGuard using open datasets\nspanning four years and real-world deployment across various mobile devices\ninvolving 20 subjects for two weeks. Remarkably, MindGuard achieves results\ncomparable to GPT-4 and outperforms its counterpart with more than 10 times the\nmodel size. We believe that MindGuard paves the way for mobile LLM\napplications, potentially revolutionizing mental healthcare practices by\nsubstituting self-reporting and intervention conversations with passive,\nintegrated monitoring within daily life, thus ensuring accessible and\nstigma-free mental health support."
                },
                "authors": [
                    {
                        "name": "Sijie Ji"
                    },
                    {
                        "name": "Xinzhe Zheng"
                    },
                    {
                        "name": "Jiawei Sun"
                    },
                    {
                        "name": "Renqi Chen"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Mani Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Mani Srivastava"
                },
                "author": "Mani Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10058v1",
                "updated": "2024-09-16T07:39:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    39,
                    58,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:39:58Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    39,
                    58,
                    0,
                    260,
                    0
                ],
                "title": "StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis\n  with Distilled Time-Varying Style Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleTTS-ZS: Efficient High-Quality Zero-Shot Text-to-Speech Synthesis\n  with Distilled Time-Varying Style Diffusion"
                },
                "summary": "The rapid development of large-scale text-to-speech (TTS) models has led to\nsignificant advancements in modeling diverse speaker prosody and voices.\nHowever, these models often face issues such as slow inference speeds, reliance\non complex pre-trained neural codec representations, and difficulties in\nachieving naturalness and high similarity to reference speakers. To address\nthese challenges, this work introduces StyleTTS-ZS, an efficient zero-shot TTS\nmodel that leverages distilled time-varying style diffusion to capture diverse\nspeaker identities and prosodies. We propose a novel approach that represents\nhuman speech using input text and fixed-length time-varying discrete style\ncodes to capture diverse prosodic variations, trained adversarially with\nmulti-modal discriminators. A diffusion model is then built to sample this\ntime-varying style code for efficient latent diffusion. Using classifier-free\nguidance, StyleTTS-ZS achieves high similarity to the reference speaker in the\nstyle diffusion process. Furthermore, to expedite sampling, the style diffusion\nmodel is distilled with perceptual loss using only 10k samples, maintaining\nspeech quality and similarity while reducing inference speed by 90%. Our model\nsurpasses previous state-of-the-art large-scale zero-shot TTS models in both\nnaturalness and similarity, offering a 10-20 faster sampling speed, making it\nan attractive alternative for efficient large-scale zero-shot TTS systems. The\naudio demo, code and models are available at https://styletts-zs.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large-scale text-to-speech (TTS) models has led to\nsignificant advancements in modeling diverse speaker prosody and voices.\nHowever, these models often face issues such as slow inference speeds, reliance\non complex pre-trained neural codec representations, and difficulties in\nachieving naturalness and high similarity to reference speakers. To address\nthese challenges, this work introduces StyleTTS-ZS, an efficient zero-shot TTS\nmodel that leverages distilled time-varying style diffusion to capture diverse\nspeaker identities and prosodies. We propose a novel approach that represents\nhuman speech using input text and fixed-length time-varying discrete style\ncodes to capture diverse prosodic variations, trained adversarially with\nmulti-modal discriminators. A diffusion model is then built to sample this\ntime-varying style code for efficient latent diffusion. Using classifier-free\nguidance, StyleTTS-ZS achieves high similarity to the reference speaker in the\nstyle diffusion process. Furthermore, to expedite sampling, the style diffusion\nmodel is distilled with perceptual loss using only 10k samples, maintaining\nspeech quality and similarity while reducing inference speed by 90%. Our model\nsurpasses previous state-of-the-art large-scale zero-shot TTS models in both\nnaturalness and similarity, offering a 10-20 faster sampling speed, making it\nan attractive alternative for efficient large-scale zero-shot TTS systems. The\naudio demo, code and models are available at https://styletts-zs.github.io/."
                },
                "authors": [
                    {
                        "name": "Yinghao Aaron Li"
                    },
                    {
                        "name": "Xilin Jiang"
                    },
                    {
                        "name": "Cong Han"
                    },
                    {
                        "name": "Nima Mesgarani"
                    }
                ],
                "author_detail": {
                    "name": "Nima Mesgarani"
                },
                "author": "Nima Mesgarani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10053v1",
                "updated": "2024-09-16T07:29:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    29,
                    40,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:29:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    29,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in\n  LLMs with Direction-Magnitude Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in\n  LLMs with Direction-Magnitude Perspective"
                },
                "summary": "Activation Editing, which involves directly editting the internal\nrepresentations of large language models (LLMs) to alter their behaviors and\nachieve desired properties, has emerged as a promising area of research.\nExisting works primarily treat LLMs' activations as points in space and modify\nthem by adding steering vectors. However, this approach is limited in its\nability to achieve greater performance improvement while maintaining the\nnecessary consistency of activation magnitudes. To overcome these issues, we\npropose a novel editing method that views activations in terms of their\ndirections and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),\nmimics the rotation transformation, thus preserving activation norms and\nresulting in an improved performance on various safety benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Editing, which involves directly editting the internal\nrepresentations of large language models (LLMs) to alter their behaviors and\nachieve desired properties, has emerged as a promising area of research.\nExisting works primarily treat LLMs' activations as points in space and modify\nthem by adding steering vectors. However, this approach is limited in its\nability to achieve greater performance improvement while maintaining the\nnecessary consistency of activation magnitudes. To overcome these issues, we\npropose a novel editing method that views activations in terms of their\ndirections and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),\nmimics the rotation transformation, thus preserving activation norms and\nresulting in an improved performance on various safety benchmarks."
                },
                "authors": [
                    {
                        "name": "Van-Cuong Pham"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10058v2",
                "updated": "2024-09-16T07:20:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    20,
                    13,
                    0,
                    260,
                    0
                ],
                "published": "2024-07-14T03:05:53Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    3,
                    5,
                    53,
                    6,
                    196,
                    0
                ],
                "title": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs"
                },
                "summary": "Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities."
                },
                "authors": [
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Chuanyuan Tan"
                    },
                    {
                        "name": "Wenliang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenliang Chen"
                },
                "author": "Wenliang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12347v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12347v6",
                "updated": "2024-09-16T07:16:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    16,
                    27,
                    0,
                    260,
                    0
                ],
                "published": "2024-08-22T12:43:14Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    43,
                    14,
                    3,
                    235,
                    0
                ],
                "title": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preregistration does not improve the transparent evaluation of severity\n  in Popper's philosophy of science or when deviations are allowed"
                },
                "summary": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One justification for preregistering research hypotheses, methods, and\nanalyses is that it improves the transparent evaluation of the severity of\nhypothesis tests. In this article, I consider two cases in which\npreregistration does not improve this evaluation. First, I argue that, although\npreregistration can facilitate the transparent evaluation of severity in Mayo's\nerror statistical philosophy of science, it does not facilitate this evaluation\nin Popper's theory-centric approach. To illustrate, I show that associated\nconcerns about Type I error rate inflation are only relevant in the error\nstatistical approach and not in a theory-centric approach. Second, I argue that\na preregistered test procedure that allows deviations in its implementation\ndoes not provide a more transparent evaluation of Mayoian severity than a\nnon-preregistered procedure. In particular, I argue that sample-based\nvalidity-enhancing deviations cause an unknown inflation of the test\nprocedure's Type I (familywise) error rate and, consequently, an unknown\nreduction in its capability to license inferences severely. I conclude that\npreregistration does not improve the transparent evaluation of severity in\nPopper's philosophy of science or when deviations are allowed."
                },
                "authors": [
                    {
                        "name": "Mark Rubin"
                    }
                ],
                "author_detail": {
                    "name": "Mark Rubin"
                },
                "author": "Mark Rubin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12347v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12347v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10044v1",
                "updated": "2024-09-16T07:13:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    13,
                    30,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:13:30Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    13,
                    30,
                    0,
                    260,
                    0
                ],
                "title": "Benchmarking Large Language Model Uncertainty for Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Model Uncertainty for Prompt Optimization"
                },
                "summary": "Prompt optimization algorithms for Large Language Models (LLMs) excel in\nmulti-step reasoning but still lack effective uncertainty estimation. This\npaper introduces a benchmark dataset to evaluate uncertainty metrics, focusing\non Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis\nof models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that\ncurrent metrics align more with Answer Uncertainty, which reflects output\nconfidence and diversity, rather than Correctness Uncertainty, highlighting the\nneed for improved metrics that are optimization-objective-aware to better guide\nprompt optimization. Our code and dataset are available at\nhttps://github.com/0Frett/PO-Uncertainty-Benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization algorithms for Large Language Models (LLMs) excel in\nmulti-step reasoning but still lack effective uncertainty estimation. This\npaper introduces a benchmark dataset to evaluate uncertainty metrics, focusing\non Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis\nof models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that\ncurrent metrics align more with Answer Uncertainty, which reflects output\nconfidence and diversity, rather than Correctness Uncertainty, highlighting the\nneed for improved metrics that are optimization-objective-aware to better guide\nprompt optimization. Our code and dataset are available at\nhttps://github.com/0Frett/PO-Uncertainty-Benchmarking."
                },
                "authors": [
                    {
                        "name": "Pei-Fu Guo"
                    },
                    {
                        "name": "Yun-Da Tsai"
                    },
                    {
                        "name": "Shou-De Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shou-De Lin"
                },
                "author": "Shou-De Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10918v4",
                "updated": "2024-09-16T07:12:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    12,
                    12,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-16T12:46:40Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    12,
                    46,
                    40,
                    6,
                    168,
                    0
                ],
                "title": "Central Answer Modeling for an Embodied Multi-LLM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Central Answer Modeling for an Embodied Multi-LLM System"
                },
                "summary": "Embodied Question Answering (EQA) is an important problem, which involves an\nagent exploring the environment to answer user queries. In the existing\nliterature, EQA has exclusively been studied in single-agent scenarios, where\nexploration can be time-consuming and costly. In this work, we consider EQA in\na multi-agent framework involving multiple large language models (LLM) based\nagents independently answering queries about a household environment. To\ngenerate one answer for each query, we use the individual responses to train a\nCentral Answer Model (CAM) that aggregates responses for a robust answer. While\nprior Question Answering (QA) work has used a central module based on answers\nfrom multiple LLM-based experts, we specifically look at applying this\nframework to embodied LLM-based agents that must physically explore the\nenvironment first to become experts on their given environment to answer\nquestions. Our work is the first to utilize a central answer model framework\nwith embodied agents that must rely on exploring an unknown environment. We set\nup a variation of EQA where instead of the agents exploring the environment\nafter the question is asked, the agents first explore the environment for a set\namount of time and then answer a set of queries. Using CAM, we observe a $46\\%$\nhigher EQA accuracy when compared against aggregation methods for ensemble LLM,\nsuch as voting schemes and debates. CAM does not require any form of agent\ncommunication, alleviating it from the associated costs. We ablate CAM with\nvarious nonlinear (neural network, random forest, decision tree, XGBoost) and\nlinear (logistic regression classifier, SVM) algorithms. We experiment in\nvarious topological graph environments and examine the case where one of the\nagents is malicious and purposes contribute responses it believes to be wrong.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Question Answering (EQA) is an important problem, which involves an\nagent exploring the environment to answer user queries. In the existing\nliterature, EQA has exclusively been studied in single-agent scenarios, where\nexploration can be time-consuming and costly. In this work, we consider EQA in\na multi-agent framework involving multiple large language models (LLM) based\nagents independently answering queries about a household environment. To\ngenerate one answer for each query, we use the individual responses to train a\nCentral Answer Model (CAM) that aggregates responses for a robust answer. While\nprior Question Answering (QA) work has used a central module based on answers\nfrom multiple LLM-based experts, we specifically look at applying this\nframework to embodied LLM-based agents that must physically explore the\nenvironment first to become experts on their given environment to answer\nquestions. Our work is the first to utilize a central answer model framework\nwith embodied agents that must rely on exploring an unknown environment. We set\nup a variation of EQA where instead of the agents exploring the environment\nafter the question is asked, the agents first explore the environment for a set\namount of time and then answer a set of queries. Using CAM, we observe a $46\\%$\nhigher EQA accuracy when compared against aggregation methods for ensemble LLM,\nsuch as voting schemes and debates. CAM does not require any form of agent\ncommunication, alleviating it from the associated costs. We ablate CAM with\nvarious nonlinear (neural network, random forest, decision tree, XGBoost) and\nlinear (logistic regression classifier, SVM) algorithms. We experiment in\nvarious topological graph environments and examine the case where one of the\nagents is malicious and purposes contribute responses it believes to be wrong."
                },
                "authors": [
                    {
                        "name": "Bhrij Patel"
                    },
                    {
                        "name": "Vishnu Sashank Dorbala"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Dinesh Manocha"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Manocha"
                },
                "author": "Dinesh Manocha",
                "arxiv_comment": "15 pages, 11 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10038v1",
                "updated": "2024-09-16T07:01:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    1,
                    41,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:01:41Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    1,
                    41,
                    0,
                    260,
                    0
                ],
                "title": "On the Diagram of Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Diagram of Thought"
                },
                "summary": "We introduce Diagram of Thought (DoT), a framework that models iterative\nreasoning in large language models (LLMs) as the construction of a directed\nacyclic graph (DAG) within a single model. Unlike traditional approaches that\nrepresent reasoning as linear chains or trees, DoT organizes propositions,\ncritiques, refinements, and verifications into a cohesive DAG structure,\nallowing the model to explore complex reasoning pathways while maintaining\nlogical consistency. Each node in the diagram corresponds to a proposition that\nhas been proposed, critiqued, refined, or verified, enabling the LLM to\niteratively improve its reasoning through natural language feedback. By\nleveraging auto-regressive next-token prediction with role-specific tokens, DoT\nfacilitates seamless transitions between proposing ideas and critically\nevaluating them, providing richer feedback than binary signals. Furthermore, we\nformalize the DoT framework using Topos Theory, providing a mathematical\nfoundation that ensures logical consistency and soundness in the reasoning\nprocess. This approach enhances both the training and inference processes\nwithin a single LLM, eliminating the need for multiple models or external\ncontrol mechanisms. DoT offers a conceptual framework for designing\nnext-generation reasoning-specialized models, emphasizing training efficiency,\nrobust reasoning capabilities, and theoretical grounding. The code is available\nat https://github.com/diagram-of-thought/diagram-of-thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Diagram of Thought (DoT), a framework that models iterative\nreasoning in large language models (LLMs) as the construction of a directed\nacyclic graph (DAG) within a single model. Unlike traditional approaches that\nrepresent reasoning as linear chains or trees, DoT organizes propositions,\ncritiques, refinements, and verifications into a cohesive DAG structure,\nallowing the model to explore complex reasoning pathways while maintaining\nlogical consistency. Each node in the diagram corresponds to a proposition that\nhas been proposed, critiqued, refined, or verified, enabling the LLM to\niteratively improve its reasoning through natural language feedback. By\nleveraging auto-regressive next-token prediction with role-specific tokens, DoT\nfacilitates seamless transitions between proposing ideas and critically\nevaluating them, providing richer feedback than binary signals. Furthermore, we\nformalize the DoT framework using Topos Theory, providing a mathematical\nfoundation that ensures logical consistency and soundness in the reasoning\nprocess. This approach enhances both the training and inference processes\nwithin a single LLM, eliminating the need for multiple models or external\ncontrol mechanisms. DoT offers a conceptual framework for designing\nnext-generation reasoning-specialized models, emphasizing training efficiency,\nrobust reasoning capabilities, and theoretical grounding. The code is available\nat https://github.com/diagram-of-thought/diagram-of-thought."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10030v1",
                "updated": "2024-09-16T06:41:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    41,
                    58,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T06:41:58Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    41,
                    58,
                    0,
                    260,
                    0
                ],
                "title": "On LASSO Inference for High Dimensional Predictive Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On LASSO Inference for High Dimensional Predictive Regression"
                },
                "summary": "LASSO introduces shrinkage bias into estimated coefficients, which can\nadversely affect the desirable asymptotic normality and invalidate the standard\ninferential procedure based on the $t$-statistic. The desparsified LASSO has\nemerged as a well-known remedy for this issue. In the context of high\ndimensional predictive regression, the desparsified LASSO faces an additional\nchallenge: the Stambaugh bias arising from nonstationary regressors. To restore\nthe standard inferential procedure, we propose a novel estimator called\nIVX-desparsified LASSO (XDlasso). XDlasso eliminates the shrinkage bias and the\nStambaugh bias simultaneously and does not require prior knowledge about the\nidentities of nonstationary and stationary regressors. We establish the\nasymptotic properties of XDlasso for hypothesis testing, and our theoretical\nfindings are supported by Monte Carlo simulations. Applying our method to\nreal-world applications from the FRED-MD database -- which includes a rich set\nof control variables -- we investigate two important empirical questions: (i)\nthe predictability of the U.S. stock returns based on the earnings-price ratio,\nand (ii) the predictability of the U.S. inflation using the unemployment rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASSO introduces shrinkage bias into estimated coefficients, which can\nadversely affect the desirable asymptotic normality and invalidate the standard\ninferential procedure based on the $t$-statistic. The desparsified LASSO has\nemerged as a well-known remedy for this issue. In the context of high\ndimensional predictive regression, the desparsified LASSO faces an additional\nchallenge: the Stambaugh bias arising from nonstationary regressors. To restore\nthe standard inferential procedure, we propose a novel estimator called\nIVX-desparsified LASSO (XDlasso). XDlasso eliminates the shrinkage bias and the\nStambaugh bias simultaneously and does not require prior knowledge about the\nidentities of nonstationary and stationary regressors. We establish the\nasymptotic properties of XDlasso for hypothesis testing, and our theoretical\nfindings are supported by Monte Carlo simulations. Applying our method to\nreal-world applications from the FRED-MD database -- which includes a rich set\nof control variables -- we investigate two important empirical questions: (i)\nthe predictability of the U.S. stock returns based on the earnings-price ratio,\nand (ii) the predictability of the U.S. inflation using the unemployment rate."
                },
                "authors": [
                    {
                        "name": "Zhan Gao"
                    },
                    {
                        "name": "Ji Hyung Lee"
                    },
                    {
                        "name": "Ziwei Mei"
                    },
                    {
                        "name": "Zhentao Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zhentao Shi"
                },
                "author": "Zhentao Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10027v1",
                "updated": "2024-09-16T06:35:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    35,
                    18,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T06:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    35,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models"
                },
                "summary": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/."
                },
                "authors": [
                    {
                        "name": "Chan Kim"
                    },
                    {
                        "name": "Keonwoo Kim"
                    },
                    {
                        "name": "Mintaek Oh"
                    },
                    {
                        "name": "Hanbi Baek"
                    },
                    {
                        "name": "Jiyang Lee"
                    },
                    {
                        "name": "Donghwi Jung"
                    },
                    {
                        "name": "Soojin Woo"
                    },
                    {
                        "name": "Younkyung Woo"
                    },
                    {
                        "name": "John Tucker"
                    },
                    {
                        "name": "Roya Firoozi"
                    },
                    {
                        "name": "Seung-Woo Seo"
                    },
                    {
                        "name": "Mac Schwager"
                    },
                    {
                        "name": "Seong-Woo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Woo Kim"
                },
                "author": "Seong-Woo Kim",
                "arxiv_comment": "19 pages, 28 figures. Project page: https://e2map.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16166v2",
                "updated": "2024-09-16T06:02:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    2,
                    0,
                    0,
                    260,
                    0
                ],
                "published": "2024-07-23T04:20:14Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    4,
                    20,
                    14,
                    1,
                    205,
                    0
                ],
                "title": "Robust Privacy Amidst Innovation with Large Language Models Through a\n  Critical Assessment of the Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Privacy Amidst Innovation with Large Language Models Through a\n  Critical Assessment of the Risks"
                },
                "summary": "This study examines integrating EHRs and NLP with large language models\n(LLMs) to improve healthcare data management and patient care. It focuses on\nusing advanced models to create secure, HIPAA-compliant synthetic patient notes\nfor biomedical research. The study used de-identified and re-identified MIMIC\nIII datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.\nText generation employed templates and keyword extraction for contextually\nrelevant notes, with one-shot generation for comparison. Privacy assessment\nchecked PHI occurrence, while text utility was tested using an ICD-9 coding\ntask. Text quality was evaluated with ROUGE and cosine similarity metrics to\nmeasure semantic similarity with source notes. Analysis of PHI occurrence and\ntext utility via the ICD-9 coding task showed that the keyword-based method had\nlow risk and good performance. One-shot generation showed the highest PHI\nexposure and PHI co-occurrence, especially in geographic location and date\ncategories. The Normalized One-shot method achieved the highest classification\naccuracy. Privacy analysis revealed a critical balance between data utility and\nprivacy protection, influencing future data use and sharing. Re-identified data\nconsistently outperformed de-identified data. This study demonstrates the\neffectiveness of keyword-based methods in generating privacy-protecting\nsynthetic clinical notes that retain data usability, potentially transforming\nclinical data-sharing practices. The superior performance of re-identified over\nde-identified data suggests a shift towards methods that enhance utility and\nprivacy by using dummy PHIs to perplex privacy attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines integrating EHRs and NLP with large language models\n(LLMs) to improve healthcare data management and patient care. It focuses on\nusing advanced models to create secure, HIPAA-compliant synthetic patient notes\nfor biomedical research. The study used de-identified and re-identified MIMIC\nIII datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.\nText generation employed templates and keyword extraction for contextually\nrelevant notes, with one-shot generation for comparison. Privacy assessment\nchecked PHI occurrence, while text utility was tested using an ICD-9 coding\ntask. Text quality was evaluated with ROUGE and cosine similarity metrics to\nmeasure semantic similarity with source notes. Analysis of PHI occurrence and\ntext utility via the ICD-9 coding task showed that the keyword-based method had\nlow risk and good performance. One-shot generation showed the highest PHI\nexposure and PHI co-occurrence, especially in geographic location and date\ncategories. The Normalized One-shot method achieved the highest classification\naccuracy. Privacy analysis revealed a critical balance between data utility and\nprivacy protection, influencing future data use and sharing. Re-identified data\nconsistently outperformed de-identified data. This study demonstrates the\neffectiveness of keyword-based methods in generating privacy-protecting\nsynthetic clinical notes that retain data usability, potentially transforming\nclinical data-sharing practices. The superior performance of re-identified over\nde-identified data suggests a shift towards methods that enhance utility and\nprivacy by using dummy PHIs to perplex privacy attacks."
                },
                "authors": [
                    {
                        "name": "Yao-Shun Chuang"
                    },
                    {
                        "name": "Atiquer Rahman Sarkar"
                    },
                    {
                        "name": "Yu-Chun Hsu"
                    },
                    {
                        "name": "Noman Mohammed"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqian Jiang"
                },
                "author": "Xiaoqian Jiang",
                "arxiv_comment": "13 pages, 4 figures, 1 table, 1 supplementary, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10011v1",
                "updated": "2024-09-16T05:50:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    50,
                    39,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T05:50:39Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    50,
                    39,
                    0,
                    260,
                    0
                ],
                "title": "HALO: Hallucination Analysis and Learning Optimization to Empower LLMs\n  with Retrieval-Augmented Context for Guided Clinical Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALO: Hallucination Analysis and Learning Optimization to Empower LLMs\n  with Retrieval-Augmented Context for Guided Clinical Decision Making"
                },
                "summary": "Large language models (LLMs) have significantly advanced natural language\nprocessing tasks, yet they are susceptible to generating inaccurate or\nunreliable responses, a phenomenon known as hallucination. In critical domains\nsuch as health and medicine, these hallucinations can pose serious risks. This\npaper introduces HALO, a novel framework designed to enhance the accuracy and\nreliability of medical question-answering (QA) systems by focusing on the\ndetection and mitigation of hallucinations. Our approach generates multiple\nvariations of a given query using LLMs and retrieves relevant information from\nexternal open knowledge bases to enrich the context. We utilize maximum\nmarginal relevance scoring to prioritize the retrieved context, which is then\nprovided to LLMs for answer generation, thereby reducing the risk of\nhallucinations. The integration of LangChain further streamlines this process,\nresulting in a notable and robust increase in the accuracy of both open-source\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\nto 70%). This framework underscores the critical importance of addressing\nhallucinations in medical QA systems, ultimately improving clinical\ndecision-making and patient care. The open-source HALO is available at:\nhttps://github.com/ResponsibleAILab/HALO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced natural language\nprocessing tasks, yet they are susceptible to generating inaccurate or\nunreliable responses, a phenomenon known as hallucination. In critical domains\nsuch as health and medicine, these hallucinations can pose serious risks. This\npaper introduces HALO, a novel framework designed to enhance the accuracy and\nreliability of medical question-answering (QA) systems by focusing on the\ndetection and mitigation of hallucinations. Our approach generates multiple\nvariations of a given query using LLMs and retrieves relevant information from\nexternal open knowledge bases to enrich the context. We utilize maximum\nmarginal relevance scoring to prioritize the retrieved context, which is then\nprovided to LLMs for answer generation, thereby reducing the risk of\nhallucinations. The integration of LangChain further streamlines this process,\nresulting in a notable and robust increase in the accuracy of both open-source\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\nto 70%). This framework underscores the critical importance of addressing\nhallucinations in medical QA systems, ultimately improving clinical\ndecision-making and patient care. The open-source HALO is available at:\nhttps://github.com/ResponsibleAILab/HALO."
                },
                "authors": [
                    {
                        "name": "Sumera Anjum"
                    },
                    {
                        "name": "Hanzhi Zhang"
                    },
                    {
                        "name": "Wenjun Zhou"
                    },
                    {
                        "name": "Eun Jin Paek"
                    },
                    {
                        "name": "Xiaopeng Zhao"
                    },
                    {
                        "name": "Yunhe Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Feng"
                },
                "author": "Yunhe Feng",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10007v1",
                "updated": "2024-09-16T05:40:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    40,
                    18,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T05:40:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    40,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL"
                },
                "summary": "In recent years,Text-to-SQL, the problem of automatically converting\nquestions posed in natural language to formal SQL queries, has emerged as an\nimportant problem at the intersection of natural language processing and data\nmanagement research. Large language models (LLMs) have delivered impressive\nperformance when used in an off-the-shelf performance, but still fall\nsignificantly short of expected expert-level performance. Errors are especially\nprobable when a nuanced understanding is needed of database schemas, questions,\nand SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a\nnovel in-context learning solution that uses an algorithmic combination of\nchain-of-thought (CoT) prompting, self-correction, and ensemble methods to\nyield a new state-of-the-art result on challenging Text-to-SQL benchmarks.\nSpecifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL\nachieves 84.2% execution accuracy on the Spider leaderboard's development set,\nexceeding both the best results of other baseline GPT-3.5-Turbo-based solutions\n(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the\nleaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years,Text-to-SQL, the problem of automatically converting\nquestions posed in natural language to formal SQL queries, has emerged as an\nimportant problem at the intersection of natural language processing and data\nmanagement research. Large language models (LLMs) have delivered impressive\nperformance when used in an off-the-shelf performance, but still fall\nsignificantly short of expected expert-level performance. Errors are especially\nprobable when a nuanced understanding is needed of database schemas, questions,\nand SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a\nnovel in-context learning solution that uses an algorithmic combination of\nchain-of-thought (CoT) prompting, self-correction, and ensemble methods to\nyield a new state-of-the-art result on challenging Text-to-SQL benchmarks.\nSpecifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL\nachieves 84.2% execution accuracy on the Spider leaderboard's development set,\nexceeding both the best results of other baseline GPT-3.5-Turbo-based solutions\n(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the\nleaderboard."
                },
                "authors": [
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mayank Kejriwal"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Kejriwal"
                },
                "author": "Mayank Kejriwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05022v3",
                "updated": "2024-09-16T05:35:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    35,
                    27,
                    0,
                    260,
                    0
                ],
                "published": "2023-10-08T05:48:30Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    5,
                    48,
                    30,
                    6,
                    281,
                    0
                ],
                "title": "Fully Spiking Neural Network for Legged Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Spiking Neural Network for Legged Robots"
                },
                "summary": "Recent advancements in legged robots using deep reinforcement learning have\nled to significant progress. Quadruped robots can perform complex tasks in\nchallenging environments, while bipedal and humanoid robots have also achieved\nbreakthroughs. Current reinforcement learning methods leverage diverse robot\nbodies and historical information to perform actions, but previous research has\nnot emphasized the speed and energy consumption of network inference and the\nbiological significance of neural networks. Most networks are traditional\nartificial neural networks that utilize multilayer perceptrons (MLP). This\npaper presents a novel Spiking Neural Network (SNN) for legged robots, showing\nexceptional performance in various simulated terrains. SNNs provide natural\nadvantages in inference speed and energy consumption, and their pulse-form\nprocessing enhances biological interpretability. This study presents a highly\nefficient SNN for legged robots that can be seamless integrated into other\nlearning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in legged robots using deep reinforcement learning have\nled to significant progress. Quadruped robots can perform complex tasks in\nchallenging environments, while bipedal and humanoid robots have also achieved\nbreakthroughs. Current reinforcement learning methods leverage diverse robot\nbodies and historical information to perform actions, but previous research has\nnot emphasized the speed and energy consumption of network inference and the\nbiological significance of neural networks. Most networks are traditional\nartificial neural networks that utilize multilayer perceptrons (MLP). This\npaper presents a novel Spiking Neural Network (SNN) for legged robots, showing\nexceptional performance in various simulated terrains. SNNs provide natural\nadvantages in inference speed and energy consumption, and their pulse-form\nprocessing enhances biological interpretability. This study presents a highly\nefficient SNN for legged robots that can be seamless integrated into other\nlearning models."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Jiang"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Jingkai Sun"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Jingtong Ma"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11764v2",
                "updated": "2024-09-16T05:28:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    28,
                    43,
                    0,
                    260,
                    0
                ],
                "published": "2024-02-19T01:28:48Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    1,
                    28,
                    48,
                    0,
                    50,
                    0
                ],
                "title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient\n  Debiasing of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient\n  Debiasing of LLMs"
                },
                "summary": "Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost."
                },
                "authors": [
                    {
                        "name": "Pengrui Han"
                    },
                    {
                        "name": "Rafal Kocielnik"
                    },
                    {
                        "name": "Adhithya Saravanan"
                    },
                    {
                        "name": "Roy Jiang"
                    },
                    {
                        "name": "Or Sharir"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "To Appear in the Proceedings of the 1st Conference on Language\n  Modeling (COLM) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13822v2",
                "updated": "2024-09-16T05:15:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    15,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-04-22T01:53:25Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    1,
                    53,
                    25,
                    0,
                    113,
                    0
                ],
                "title": "Higher-Order Graphon Theory: Fluctuations, Degeneracies, and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Higher-Order Graphon Theory: Fluctuations, Degeneracies, and Inference"
                },
                "summary": "Exchangeable random graphs, which include some of the most widely studied\nnetwork models, have emerged as the mainstay of statistical network analysis in\nrecent years. Graphons, which are the central objects in graph limit theory,\nprovide a natural way to sample exchangeable random graphs. It is well known\nthat network moments (motif/subgraph counts) identify a graphon (up to an\nisomorphism), hence, understanding the sampling distribution of subgraph counts\nin random graphs sampled from a graphon is pivotal for nonparametric network\ninference. In this paper, we derive the joint asymptotic distribution of any\nfinite collection of network moments in random graphs sampled from a graphon,\nthat includes both the non-degenerate case (where the distribution is Gaussian)\nas well as the degenerate case (where the distribution has both Gaussian or\nnon-Gaussian components). This provides the higher-order fluctuation theory for\nsubgraph counts in the graphon model. We also develop a novel multiplier\nbootstrap for graphons that consistently approximates the limiting distribution\nof the network moments (both in the Gaussian and non-Gaussian regimes). Using\nthis and a procedure for testing degeneracy, we construct joint confidence sets\nfor any finite collection of motif densities. This provides a general framework\nfor statistical inference based on network moments in the graphon model. To\nillustrate the broad scope of our results we also consider the problem of\ndetecting global structure (that is, testing whether the graphon is a constant\nfunction) based on small subgraphs. We propose a consistent test for this\nproblem, invoking celebrated results on quasi-random graphs, and derive its\nlimiting distribution both under the null and the alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exchangeable random graphs, which include some of the most widely studied\nnetwork models, have emerged as the mainstay of statistical network analysis in\nrecent years. Graphons, which are the central objects in graph limit theory,\nprovide a natural way to sample exchangeable random graphs. It is well known\nthat network moments (motif/subgraph counts) identify a graphon (up to an\nisomorphism), hence, understanding the sampling distribution of subgraph counts\nin random graphs sampled from a graphon is pivotal for nonparametric network\ninference. In this paper, we derive the joint asymptotic distribution of any\nfinite collection of network moments in random graphs sampled from a graphon,\nthat includes both the non-degenerate case (where the distribution is Gaussian)\nas well as the degenerate case (where the distribution has both Gaussian or\nnon-Gaussian components). This provides the higher-order fluctuation theory for\nsubgraph counts in the graphon model. We also develop a novel multiplier\nbootstrap for graphons that consistently approximates the limiting distribution\nof the network moments (both in the Gaussian and non-Gaussian regimes). Using\nthis and a procedure for testing degeneracy, we construct joint confidence sets\nfor any finite collection of motif densities. This provides a general framework\nfor statistical inference based on network moments in the graphon model. To\nillustrate the broad scope of our results we also consider the problem of\ndetecting global structure (that is, testing whether the graphon is a constant\nfunction) based on small subgraphs. We propose a consistent test for this\nproblem, invoking celebrated results on quasi-random graphs, and derive its\nlimiting distribution both under the null and the alternative."
                },
                "authors": [
                    {
                        "name": "Anirban Chatterjee"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Bhaswar B. Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Bhaswar B. Bhattacharya"
                },
                "author": "Bhaswar B. Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "05C80, 60F05, 05C60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10825v2",
                "updated": "2024-09-16T05:09:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    9,
                    57,
                    0,
                    260,
                    0
                ],
                "published": "2024-05-17T14:46:13Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    14,
                    46,
                    13,
                    4,
                    138,
                    0
                ],
                "title": "Large Language Model (LLM) for Telecommunications: A Comprehensive\n  Survey on Principles, Key Techniques, and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) for Telecommunications: A Comprehensive\n  Survey on Principles, Key Techniques, and Opportunities"
                },
                "summary": "Large language models (LLMs) have received considerable attention recently\ndue to their outstanding comprehension and reasoning capabilities, leading to\ngreat progress in many fields. The advancement of LLM techniques also offers\npromising opportunities to automate many tasks in the telecommunication\n(telecom) field. After pre-training and fine-tuning, LLMs can perform diverse\ndownstream tasks based on human instructions, paving the way to artificial\ngeneral intelligence (AGI)-enabled 6G. Given the great potential of LLM\ntechnologies, this work aims to provide a comprehensive overview of LLM-enabled\ntelecom networks. In particular, we first present LLM fundamentals, including\nmodel architecture, pre-training, fine-tuning, inference and utilization, model\nevaluation, and telecom deployment. Then, we introduce LLM-enabled key\ntechniques and telecom applications in terms of generation, classification,\noptimization, and prediction problems. Specifically, the LLM-enabled generation\napplications include telecom domain knowledge, code, and network configuration\ngeneration. After that, the LLM-based classification applications involve\nnetwork security, text, image, and traffic classification problems. Moreover,\nmultiple LLM-enabled optimization techniques are introduced, such as automated\nreward function design for reinforcement learning and verbal reinforcement\nlearning. Furthermore, for LLM-aided prediction problems, we discussed\ntime-series prediction models and multi-modality prediction problems for\ntelecom. Finally, we highlight the challenges and identify the future\ndirections of LLM-enabled telecom networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have received considerable attention recently\ndue to their outstanding comprehension and reasoning capabilities, leading to\ngreat progress in many fields. The advancement of LLM techniques also offers\npromising opportunities to automate many tasks in the telecommunication\n(telecom) field. After pre-training and fine-tuning, LLMs can perform diverse\ndownstream tasks based on human instructions, paving the way to artificial\ngeneral intelligence (AGI)-enabled 6G. Given the great potential of LLM\ntechnologies, this work aims to provide a comprehensive overview of LLM-enabled\ntelecom networks. In particular, we first present LLM fundamentals, including\nmodel architecture, pre-training, fine-tuning, inference and utilization, model\nevaluation, and telecom deployment. Then, we introduce LLM-enabled key\ntechniques and telecom applications in terms of generation, classification,\noptimization, and prediction problems. Specifically, the LLM-enabled generation\napplications include telecom domain knowledge, code, and network configuration\ngeneration. After that, the LLM-based classification applications involve\nnetwork security, text, image, and traffic classification problems. Moreover,\nmultiple LLM-enabled optimization techniques are introduced, such as automated\nreward function design for reinforcement learning and verbal reinforcement\nlearning. Furthermore, for LLM-aided prediction problems, we discussed\ntime-series prediction models and multi-modality prediction problems for\ntelecom. Finally, we highlight the challenges and identify the future\ndirections of LLM-enabled telecom networks."
                },
                "authors": [
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Chengming Hu"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Yufei Cui"
                    },
                    {
                        "name": "Yili Jin"
                    },
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Haolun Wu"
                    },
                    {
                        "name": "Dun Yuan"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Charlie Zhang"
                    },
                    {
                        "name": "Xianbin Wang"
                    },
                    {
                        "name": "Jiangchuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangchuan Liu"
                },
                "author": "Jiangchuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09989v1",
                "updated": "2024-09-16T04:44:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    4,
                    44,
                    52,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T04:44:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    4,
                    44,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM\n  based system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM\n  based system"
                },
                "summary": "This paper provides a comprehensive survey of sentiment analysis within the\ncontext of artificial intelligence (AI) and large language models (LLMs).\nSentiment analysis, a critical aspect of natural language processing (NLP), has\nevolved significantly from traditional rule-based methods to advanced deep\nlearning techniques. This study examines the historical development of\nsentiment analysis, highlighting the transition from lexicon-based and\npattern-based approaches to more sophisticated machine learning and deep\nlearning models. Key challenges are discussed, including handling bilingual\ntexts, detecting sarcasm, and addressing biases. The paper reviews\nstate-of-the-art approaches, identifies emerging trends, and outlines future\nresearch directions to advance the field. By synthesizing current methodologies\nand exploring future opportunities, this survey aims to understand sentiment\nanalysis in the AI and LLM context thoroughly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive survey of sentiment analysis within the\ncontext of artificial intelligence (AI) and large language models (LLMs).\nSentiment analysis, a critical aspect of natural language processing (NLP), has\nevolved significantly from traditional rule-based methods to advanced deep\nlearning techniques. This study examines the historical development of\nsentiment analysis, highlighting the transition from lexicon-based and\npattern-based approaches to more sophisticated machine learning and deep\nlearning models. Key challenges are discussed, including handling bilingual\ntexts, detecting sarcasm, and addressing biases. The paper reviews\nstate-of-the-art approaches, identifies emerging trends, and outlines future\nresearch directions to advance the field. By synthesizing current methodologies\nand exploring future opportunities, this survey aims to understand sentiment\nanalysis in the AI and LLM context thoroughly."
                },
                "authors": [
                    {
                        "name": "Shailja Gupta"
                    },
                    {
                        "name": "Rajesh Ranjan"
                    },
                    {
                        "name": "Surya Narayan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Surya Narayan Singh"
                },
                "author": "Surya Narayan Singh",
                "arxiv_comment": "2 Images",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13722v2",
                "updated": "2024-09-16T04:25:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    4,
                    25,
                    50,
                    0,
                    260,
                    0
                ],
                "published": "2024-05-22T15:14:00Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    15,
                    14,
                    0,
                    2,
                    143,
                    0
                ],
                "title": "LightningDrag: Lightning Fast and Accurate Drag-based Image Editing\n  Emerging from Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightningDrag: Lightning Fast and Accurate Drag-based Image Editing\n  Emerging from Videos"
                },
                "summary": "Accuracy and speed are critical in image editing tasks. Pan et al. introduced\na drag-based image editing framework that achieves pixel-level control using\nGenerative Adversarial Networks (GANs). A flurry of subsequent studies enhanced\nthis framework's generality by leveraging large-scale diffusion models.\nHowever, these methods often suffer from inordinately long processing times\n(exceeding 1 minute per edit) and low success rates. Addressing these issues\nhead on, we present LightningDrag, a rapid approach enabling high quality\ndrag-based image editing in ~1 second. Unlike most previous methods, we\nredefine drag-based editing as a conditional generation task, eliminating the\nneed for time-consuming latent optimization or gradient-based guidance during\ninference. In addition, the design of our pipeline allows us to train our model\non large-scale paired video frames, which contain rich motion information such\nas object translations, changing poses and orientations, zooming in and out,\netc. By learning from videos, our approach can significantly outperform\nprevious methods in terms of accuracy and consistency. Despite being trained\nsolely on videos, our model generalizes well to perform local shape\ndeformations not presented in the training data (e.g., lengthening of hair,\ntwisting rainbows, etc.). Extensive qualitative and quantitative evaluations on\nbenchmark datasets corroborate the superiority of our approach. The code and\nmodel will be released at https://github.com/magic-research/LightningDrag.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accuracy and speed are critical in image editing tasks. Pan et al. introduced\na drag-based image editing framework that achieves pixel-level control using\nGenerative Adversarial Networks (GANs). A flurry of subsequent studies enhanced\nthis framework's generality by leveraging large-scale diffusion models.\nHowever, these methods often suffer from inordinately long processing times\n(exceeding 1 minute per edit) and low success rates. Addressing these issues\nhead on, we present LightningDrag, a rapid approach enabling high quality\ndrag-based image editing in ~1 second. Unlike most previous methods, we\nredefine drag-based editing as a conditional generation task, eliminating the\nneed for time-consuming latent optimization or gradient-based guidance during\ninference. In addition, the design of our pipeline allows us to train our model\non large-scale paired video frames, which contain rich motion information such\nas object translations, changing poses and orientations, zooming in and out,\netc. By learning from videos, our approach can significantly outperform\nprevious methods in terms of accuracy and consistency. Despite being trained\nsolely on videos, our model generalizes well to perform local shape\ndeformations not presented in the training data (e.g., lengthening of hair,\ntwisting rainbows, etc.). Extensive qualitative and quantitative evaluations on\nbenchmark datasets corroborate the superiority of our approach. The code and\nmodel will be released at https://github.com/magic-research/LightningDrag."
                },
                "authors": [
                    {
                        "name": "Yujun Shi"
                    },
                    {
                        "name": "Jun Hao Liew"
                    },
                    {
                        "name": "Hanshu Yan"
                    },
                    {
                        "name": "Vincent Y. F. Tan"
                    },
                    {
                        "name": "Jiashi Feng"
                    }
                ],
                "author_detail": {
                    "name": "Jiashi Feng"
                },
                "author": "Jiashi Feng",
                "arxiv_comment": "Project page: https://lightning-drag.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05871v2",
                "updated": "2024-09-16T04:24:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    4,
                    24,
                    29,
                    0,
                    260,
                    0
                ],
                "published": "2024-02-08T18:01:18Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    18,
                    1,
                    18,
                    3,
                    39,
                    0
                ],
                "title": "Kilonova Light-Curve Interpolation with Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilonova Light-Curve Interpolation with Neural Networks"
                },
                "summary": "Kilonovae are the electromagnetic transients created by the radioactive decay\nof freshly synthesized elements in the environment surrounding a neutron star\nmerger. To study the fundamental physics in these complex environments,\nkilonova modeling requires, in part, the use of radiative transfer simulations.\nThe microphysics involved in these simulations results in high computational\ncost, prompting the use of emulators for parameter inference applications.\nUtilizing a training set of 22248 high-fidelity simulations (composed of 412\nunique ejecta parameter combinations evaluated at 54 viewing angles), we use a\nneural network to efficiently train on existing radiative transfer simulations\nand predict light curves for new parameters in a fast and computationally\nefficient manner. Our neural network can generate millions of new light curves\nin under a minute. We discuss our emulator's degree of off-sample reliability\nand parameter inference of the AT2017gfo observational data. Finally, we\ndiscuss tension introduced by multi-band inference in the parameter inference\nresults, particularly with regard to the neural network's recovery of viewing\nangle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilonovae are the electromagnetic transients created by the radioactive decay\nof freshly synthesized elements in the environment surrounding a neutron star\nmerger. To study the fundamental physics in these complex environments,\nkilonova modeling requires, in part, the use of radiative transfer simulations.\nThe microphysics involved in these simulations results in high computational\ncost, prompting the use of emulators for parameter inference applications.\nUtilizing a training set of 22248 high-fidelity simulations (composed of 412\nunique ejecta parameter combinations evaluated at 54 viewing angles), we use a\nneural network to efficiently train on existing radiative transfer simulations\nand predict light curves for new parameters in a fast and computationally\nefficient manner. Our neural network can generate millions of new light curves\nin under a minute. We discuss our emulator's degree of off-sample reliability\nand parameter inference of the AT2017gfo observational data. Finally, we\ndiscuss tension introduced by multi-band inference in the parameter inference\nresults, particularly with regard to the neural network's recovery of viewing\nangle."
                },
                "authors": [
                    {
                        "name": "Yinglei Peng"
                    },
                    {
                        "name": "Marko Ristić"
                    },
                    {
                        "name": "Atul Kedia"
                    },
                    {
                        "name": "Richard O'Shaughnessy"
                    },
                    {
                        "name": "Christopher J. Fontes"
                    },
                    {
                        "name": "Chris L. Fryer"
                    },
                    {
                        "name": "Oleg Korobkin"
                    },
                    {
                        "name": "Matthew R. Mumpower"
                    },
                    {
                        "name": "V. Ashley Villar"
                    },
                    {
                        "name": "Ryan T. Wollaeger"
                    }
                ],
                "author_detail": {
                    "name": "Ryan T. Wollaeger"
                },
                "author": "Ryan T. Wollaeger",
                "arxiv_doi": "10.1103/PhysRevResearch.6.033078",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevResearch.6.033078",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.05871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 11 figures",
                "arxiv_journal_ref": "Phys. Rev. Res. 6, 033078 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09973v1",
                "updated": "2024-09-16T04:10:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    4,
                    10,
                    44,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T04:10:44Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    4,
                    10,
                    44,
                    0,
                    260,
                    0
                ],
                "title": "Towards a Unified Theory for Semiparametric Data Fusion with\n  Individual-Level Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Unified Theory for Semiparametric Data Fusion with\n  Individual-Level Data"
                },
                "summary": "We address the goal of conducting inference about a smooth finite-dimensional\nparameter by utilizing individual-level data from various independent sources.\nRecent advancements have led to the development of a comprehensive theory\ncapable of handling scenarios where different data sources align with, possibly\ndistinct subsets of, conditional distributions of a single factorization of the\njoint target distribution. While this theory proves effective in many\nsignificant contexts, it falls short in certain common data fusion problems,\nsuch as two-sample instrumental variable analysis, settings that integrate data\nfrom epidemiological studies with diverse designs (e.g., prospective cohorts\nand retrospective case-control studies), and studies with variables prone to\nmeasurement error that are supplemented by validation studies. In this paper,\nwe extend the aforementioned comprehensive theory to allow for the fusion of\nindividual-level data from sources aligned with conditional distributions that\ndo not correspond to a single factorization of the target distribution.\nAssuming conditional and marginal distribution alignments, we provide universal\nresults that characterize the class of all influence functions of regular\nasymptotically linear estimators and the efficient influence function of any\npathwise differentiable parameter, irrespective of the number of data sources,\nthe specific parameter of interest, or the statistical model for the target\ndistribution. This theory paves the way for machine-learning debiased,\nsemiparametric efficient estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the goal of conducting inference about a smooth finite-dimensional\nparameter by utilizing individual-level data from various independent sources.\nRecent advancements have led to the development of a comprehensive theory\ncapable of handling scenarios where different data sources align with, possibly\ndistinct subsets of, conditional distributions of a single factorization of the\njoint target distribution. While this theory proves effective in many\nsignificant contexts, it falls short in certain common data fusion problems,\nsuch as two-sample instrumental variable analysis, settings that integrate data\nfrom epidemiological studies with diverse designs (e.g., prospective cohorts\nand retrospective case-control studies), and studies with variables prone to\nmeasurement error that are supplemented by validation studies. In this paper,\nwe extend the aforementioned comprehensive theory to allow for the fusion of\nindividual-level data from sources aligned with conditional distributions that\ndo not correspond to a single factorization of the target distribution.\nAssuming conditional and marginal distribution alignments, we provide universal\nresults that characterize the class of all influence functions of regular\nasymptotically linear estimators and the efficient influence function of any\npathwise differentiable parameter, irrespective of the number of data sources,\nthe specific parameter of interest, or the statistical model for the target\ndistribution. This theory paves the way for machine-learning debiased,\nsemiparametric efficient estimation."
                },
                "authors": [
                    {
                        "name": "Ellen Graham"
                    },
                    {
                        "name": "Marco Carone"
                    },
                    {
                        "name": "Andrea Rotnitzky"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Rotnitzky"
                },
                "arxiv_affiliation": "University of Washington",
                "author": "Andrea Rotnitzky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09958v1",
                "updated": "2024-09-16T03:08:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    3,
                    8,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T03:08:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    3,
                    8,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "An Offline Adaptation Framework for Constrained Multi-Objective\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Offline Adaptation Framework for Constrained Multi-Objective\n  Reinforcement Learning"
                },
                "summary": "In recent years, significant progress has been made in multi-objective\nreinforcement learning (RL) research, which aims to balance multiple objectives\nby incorporating preferences for each objective. In most existing studies,\nspecific preferences must be provided during deployment to indicate the desired\npolicies explicitly. However, designing these preferences depends heavily on\nhuman prior knowledge, which is typically obtained through extensive\nobservation of high-performing demonstrations with expected behaviors. In this\nwork, we propose a simple yet effective offline adaptation framework for\nmulti-objective RL problems without assuming handcrafted target preferences,\nbut only given several demonstrations to implicitly indicate the preferences of\nexpected policies. Additionally, we demonstrate that our framework can\nnaturally be extended to meet constraints on safety-critical objectives by\nutilizing safe demonstrations, even when the safety thresholds are unknown.\nEmpirical results on offline multi-objective and safe tasks demonstrate the\ncapability of our framework to infer policies that align with real preferences\nwhile meeting the constraints implied by the provided demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, significant progress has been made in multi-objective\nreinforcement learning (RL) research, which aims to balance multiple objectives\nby incorporating preferences for each objective. In most existing studies,\nspecific preferences must be provided during deployment to indicate the desired\npolicies explicitly. However, designing these preferences depends heavily on\nhuman prior knowledge, which is typically obtained through extensive\nobservation of high-performing demonstrations with expected behaviors. In this\nwork, we propose a simple yet effective offline adaptation framework for\nmulti-objective RL problems without assuming handcrafted target preferences,\nbut only given several demonstrations to implicitly indicate the preferences of\nexpected policies. Additionally, we demonstrate that our framework can\nnaturally be extended to meet constraints on safety-critical objectives by\nutilizing safe demonstrations, even when the safety thresholds are unknown.\nEmpirical results on offline multi-objective and safe tasks demonstrate the\ncapability of our framework to infer policies that align with real preferences\nwhile meeting the constraints implied by the provided demonstrations."
                },
                "authors": [
                    {
                        "name": "Qian Lin"
                    },
                    {
                        "name": "Zongkai Liu"
                    },
                    {
                        "name": "Danying Mo"
                    },
                    {
                        "name": "Chao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Chao Yu"
                },
                "author": "Chao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09951v1",
                "updated": "2024-09-16T02:45:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    45,
                    54,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T02:45:54Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    45,
                    54,
                    0,
                    260,
                    0
                ],
                "title": "Optimal ablation for interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal ablation for interpretability"
                },
                "summary": "Interpretability studies often involve tracing the flow of information\nthrough machine learning models to identify specific model components that\nperform relevant computations for tasks of interest. Prior work quantifies the\nimportance of a model component on a particular task by measuring the impact of\nperforming ablation on that component, or simulating model inference with the\ncomponent disabled. We propose a new method, optimal ablation (OA), and show\nthat OA-based component importance has theoretical and empirical advantages\nover measuring importance via other ablation methods. We also show that\nOA-based component importance can benefit several downstream interpretability\ntasks, including circuit discovery, localization of factual recall, and latent\nprediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability studies often involve tracing the flow of information\nthrough machine learning models to identify specific model components that\nperform relevant computations for tasks of interest. Prior work quantifies the\nimportance of a model component on a particular task by measuring the impact of\nperforming ablation on that component, or simulating model inference with the\ncomponent disabled. We propose a new method, optimal ablation (OA), and show\nthat OA-based component importance has theoretical and empirical advantages\nover measuring importance via other ablation methods. We also show that\nOA-based component importance can benefit several downstream interpretability\ntasks, including circuit discovery, localization of factual recall, and latent\nprediction."
                },
                "authors": [
                    {
                        "name": "Maximilian Li"
                    },
                    {
                        "name": "Lucas Janson"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Janson"
                },
                "author": "Lucas Janson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09947v1",
                "updated": "2024-09-16T02:38:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    38,
                    38,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T02:38:38Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    38,
                    38,
                    0,
                    260,
                    0
                ],
                "title": "Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for\n  Fine-grained Text Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for\n  Fine-grained Text Evaluations"
                },
                "summary": "Large Language Models (LLMs) show promise as a writing aid for professionals\nperforming legal analyses. However, LLMs can often hallucinate in this setting,\nin ways difficult to recognize by non-professionals and existing text\nevaluation metrics. In this work, we pose the question: when can\nmachine-generated legal analysis be evaluated as acceptable? We introduce the\nneutral notion of gaps, as opposed to hallucinations in a strict erroneous\nsense, to refer to the difference between human-written and machine-generated\nlegal analysis. Gaps do not always equate to invalid generation. Working with\nlegal experts, we consider the CLERC generation task proposed in Hou et al.\n(2024b), leading to a taxonomy, a fine-grained detector for predicting gap\ncategories, and an annotated dataset for automatic evaluation. Our best\ndetector achieves 67% F1 score and 80% precision on the test set. Employing\nthis detector as an automated metric on legal analysis generated by SOTA LLMs,\nwe find around 80% contain hallucinations of different kinds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show promise as a writing aid for professionals\nperforming legal analyses. However, LLMs can often hallucinate in this setting,\nin ways difficult to recognize by non-professionals and existing text\nevaluation metrics. In this work, we pose the question: when can\nmachine-generated legal analysis be evaluated as acceptable? We introduce the\nneutral notion of gaps, as opposed to hallucinations in a strict erroneous\nsense, to refer to the difference between human-written and machine-generated\nlegal analysis. Gaps do not always equate to invalid generation. Working with\nlegal experts, we consider the CLERC generation task proposed in Hou et al.\n(2024b), leading to a taxonomy, a fine-grained detector for predicting gap\ncategories, and an annotated dataset for automatic evaluation. Our best\ndetector achieves 67% F1 score and 80% precision on the test set. Employing\nthis detector as an automated metric on legal analysis generated by SOTA LLMs,\nwe find around 80% contain hallucinations of different kinds."
                },
                "authors": [
                    {
                        "name": "Abe Bohan Hou"
                    },
                    {
                        "name": "William Jurayj"
                    },
                    {
                        "name": "Nils Holzenberger"
                    },
                    {
                        "name": "Andrew Blair-Stanek"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09946v1",
                "updated": "2024-09-16T02:38:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    38,
                    29,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T02:38:29Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    38,
                    29,
                    0,
                    260,
                    0
                ],
                "title": "An Independent Measure of the Kinematic Dipole from SDSS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Independent Measure of the Kinematic Dipole from SDSS"
                },
                "summary": "We utilize the Sloan Digital Sky Survey (SDSS) extended Baryon Oscillation\nSpectroscopic Survey (eBOSS) and Baryon Oscillation Spectroscopic Survey (BOSS)\ncatalogs with precise spectroscopic redshifts to estimate the kinematic\nredshift dipole caused by the proper motion of the Solar system. We find that\nthe velocity extracted from the kinematic dipole is consistent with Cosmic\nMicrowave Background inferred values. Although the small sky coverage and\nlimited number density of the SDSS sources constrain us from obtaining precise\nand robust measurements, we leverage the redshift dipole method to estimate the\nkinematic dipole. The velocity measurements in this study are insensitive to\nintrinsic clustering, associated with the source count dipole. The kinematic\ndipole measured in this work and its consistency with CMB values do not\nguarantee isotropy at large scales. The anisotropy (excess dipole) measured\nwith the NRAO VLA Sky Survey (NVSS) and the WISE Catalog (CatWISE) could be due\nto the intrinsic distribution of galaxies. The results in this work focus\nsolely on the kinematic dipole term.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We utilize the Sloan Digital Sky Survey (SDSS) extended Baryon Oscillation\nSpectroscopic Survey (eBOSS) and Baryon Oscillation Spectroscopic Survey (BOSS)\ncatalogs with precise spectroscopic redshifts to estimate the kinematic\nredshift dipole caused by the proper motion of the Solar system. We find that\nthe velocity extracted from the kinematic dipole is consistent with Cosmic\nMicrowave Background inferred values. Although the small sky coverage and\nlimited number density of the SDSS sources constrain us from obtaining precise\nand robust measurements, we leverage the redshift dipole method to estimate the\nkinematic dipole. The velocity measurements in this study are insensitive to\nintrinsic clustering, associated with the source count dipole. The kinematic\ndipole measured in this work and its consistency with CMB values do not\nguarantee isotropy at large scales. The anisotropy (excess dipole) measured\nwith the NRAO VLA Sky Survey (NVSS) and the WISE Catalog (CatWISE) could be due\nto the intrinsic distribution of galaxies. The results in this work focus\nsolely on the kinematic dipole term."
                },
                "authors": [
                    {
                        "name": "Prabhakar Tiwari"
                    },
                    {
                        "name": "Dominik J. Schwarz"
                    },
                    {
                        "name": "Gong-Bo Zhao"
                    },
                    {
                        "name": "Ruth Durrer"
                    },
                    {
                        "name": "Martin Kunz"
                    },
                    {
                        "name": "Hamsa Padmanabhan"
                    }
                ],
                "author_detail": {
                    "name": "Hamsa Padmanabhan"
                },
                "arxiv_affiliation": "U. Geneva",
                "author": "Hamsa Padmanabhan",
                "arxiv_comment": "13 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07368v2",
                "updated": "2024-09-16T02:28:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    28,
                    0,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-11T15:56:15Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    56,
                    15,
                    2,
                    255,
                    0
                ],
                "title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code"
                },
                "summary": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: http://3.131.141.63:8501/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: http://3.131.141.63:8501/."
                },
                "authors": [
                    {
                        "name": "Khiem Ton"
                    },
                    {
                        "name": "Nhi Nguyen"
                    },
                    {
                        "name": "Mahmoud Nazzal"
                    },
                    {
                        "name": "Abdallah Khreishah"
                    },
                    {
                        "name": "Cristian Borcea"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Issa Khalil"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_doi": "10.1145/3658644.3691367",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3691367",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07066v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07066v4",
                "updated": "2024-09-17T01:37:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    37,
                    18,
                    1,
                    261,
                    0
                ],
                "published": "2024-04-10T14:56:40Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    14,
                    56,
                    40,
                    2,
                    101,
                    0
                ],
                "title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?"
                },
                "summary": "Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of ``Concept Depth'' to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of ``Concept Depth'' to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07066v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07066v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09930v1",
                "updated": "2024-09-16T02:08:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    8,
                    33,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T02:08:33Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    8,
                    33,
                    0,
                    260,
                    0
                ],
                "title": "Mining of Switching Sparse Networks for Missing Value Imputation in\n  Multivariate Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining of Switching Sparse Networks for Missing Value Imputation in\n  Multivariate Time Series"
                },
                "summary": "Multivariate time series data suffer from the problem of missing values,\nwhich hinders the application of many analytical methods. To achieve the\naccurate imputation of these missing values, exploiting inter-correlation by\nemploying the relationships between sequences (i.e., a network) is as important\nas the use of temporal dependency, since a sequence normally correlates with\nother sequences. Moreover, exploiting an adequate network depending on time is\nalso necessary since the network varies over time. However, in real-world\nscenarios, we normally know neither the network structure nor when the network\nchanges beforehand. Here, we propose a missing value imputation method for\nmultivariate time series, namely MissNet, that is designed to exploit temporal\ndependency with a state-space model and inter-correlation by switching sparse\nnetworks. The network encodes conditional independence between features, which\nhelps us understand the important relationships for imputation visually. Our\nalgorithm, which scales linearly with reference to the length of the data,\nalternatively infers networks and fills in missing values using the networks\nwhile discovering the switching of the networks. Extensive experiments\ndemonstrate that MissNet outperforms the state-of-the-art algorithms for\nmultivariate time series imputation and provides interpretable results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series data suffer from the problem of missing values,\nwhich hinders the application of many analytical methods. To achieve the\naccurate imputation of these missing values, exploiting inter-correlation by\nemploying the relationships between sequences (i.e., a network) is as important\nas the use of temporal dependency, since a sequence normally correlates with\nother sequences. Moreover, exploiting an adequate network depending on time is\nalso necessary since the network varies over time. However, in real-world\nscenarios, we normally know neither the network structure nor when the network\nchanges beforehand. Here, we propose a missing value imputation method for\nmultivariate time series, namely MissNet, that is designed to exploit temporal\ndependency with a state-space model and inter-correlation by switching sparse\nnetworks. The network encodes conditional independence between features, which\nhelps us understand the important relationships for imputation visually. Our\nalgorithm, which scales linearly with reference to the length of the data,\nalternatively infers networks and fills in missing values using the networks\nwhile discovering the switching of the networks. Extensive experiments\ndemonstrate that MissNet outperforms the state-of-the-art algorithms for\nmultivariate time series imputation and provides interpretable results."
                },
                "authors": [
                    {
                        "name": "Kohei Obata"
                    },
                    {
                        "name": "Koki Kawabata"
                    },
                    {
                        "name": "Yasuko Matsubara"
                    },
                    {
                        "name": "Yasushi Sakurai"
                    }
                ],
                "author_detail": {
                    "name": "Yasushi Sakurai"
                },
                "author": "Yasushi Sakurai",
                "arxiv_comment": "Accepted by KDD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.10516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v1",
                "updated": "2024-09-16T17:59:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB)."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10506v1",
                "updated": "2024-09-16T17:52:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    52,
                    36,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:52:36Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    52,
                    36,
                    0,
                    260,
                    0
                ],
                "title": "Context-aware Code Segmentation for C-to-Rust Translation using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-aware Code Segmentation for C-to-Rust Translation using Large\n  Language Models"
                },
                "summary": "There is strong motivation to translate C code into Rust code due to the\ncontinuing threat of memory safety vulnerabilities in existing C programs and\nthe significant attention paid to Rust as an alternative to the C language.\nWhile large language models (LLMs) show promise for automating this translation\nby generating more natural and safer code than rule-based methods, previous\nstudies have shown that LLM-generated Rust code often fails to compile, even\nfor relatively small C programs, due to significant differences between the two\nlanguages and context window limitations. We propose an LLM-based translation\nscheme that improves the success rate of translating large-scale C code into\ncompilable Rust code. Our approach involves three key techniques: (1)\npre-processing the C code to better align its structure and expressions with\nRust, (2) segmenting the code into optimally sized translation units to avoid\nexceeding the LLM's context window limits, and (3) iteratively compiling and\nrepairing errors while maintaining consistency between translation units using\ncontext-supplementing prompts. Compilation success is an essential first step\nin achieving functional equivalence, as only compilable code can be further\ntested. In experiments with 20 benchmark C programs, including those exceeding\n4 kilo lines of code, we successfully translated all programs into compilable\nRust code without losing corresponding parts of the original code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is strong motivation to translate C code into Rust code due to the\ncontinuing threat of memory safety vulnerabilities in existing C programs and\nthe significant attention paid to Rust as an alternative to the C language.\nWhile large language models (LLMs) show promise for automating this translation\nby generating more natural and safer code than rule-based methods, previous\nstudies have shown that LLM-generated Rust code often fails to compile, even\nfor relatively small C programs, due to significant differences between the two\nlanguages and context window limitations. We propose an LLM-based translation\nscheme that improves the success rate of translating large-scale C code into\ncompilable Rust code. Our approach involves three key techniques: (1)\npre-processing the C code to better align its structure and expressions with\nRust, (2) segmenting the code into optimally sized translation units to avoid\nexceeding the LLM's context window limits, and (3) iteratively compiling and\nrepairing errors while maintaining consistency between translation units using\ncontext-supplementing prompts. Compilation success is an essential first step\nin achieving functional equivalence, as only compilable code can be further\ntested. In experiments with 20 benchmark C programs, including those exceeding\n4 kilo lines of code, we successfully translated all programs into compilable\nRust code without losing corresponding parts of the original code."
                },
                "authors": [
                    {
                        "name": "Momoko Shiraishi"
                    },
                    {
                        "name": "Takahiro Shinagawa"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Shinagawa"
                },
                "author": "Takahiro Shinagawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10504v1",
                "updated": "2024-09-16T17:45:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    45,
                    40,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:45:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    45,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "DILA: Dictionary Label Attention for Mechanistic Interpretability in\n  High-dimensional Multi-label Medical Coding Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DILA: Dictionary Label Attention for Mechanistic Interpretability in\n  High-dimensional Multi-label Medical Coding Prediction"
                },
                "summary": "Predicting high-dimensional or extreme multilabels, such as in medical\ncoding, requires both accuracy and interpretability. Existing works often rely\non local interpretability methods, failing to provide comprehensive\nexplanations of the overall mechanism behind each label prediction within a\nmultilabel set. We propose a mechanistic interpretability module called\nDIctionary Label Attention (\\method) that disentangles uninterpretable dense\nembeddings into a sparse embedding space, where each nonzero element (a\ndictionary feature) represents a globally learned medical concept. Through\nhuman evaluations, we show that our sparse embeddings are more human\nunderstandable than its dense counterparts by at least 50 percent. Our\nautomated dictionary feature identification pipeline, leveraging large language\nmodels (LLMs), uncovers thousands of learned medical concepts by examining and\nsummarizing the highest activating tokens for each dictionary feature. We\nrepresent the relationships between dictionary features and medical codes\nthrough a sparse interpretable matrix, enhancing the mechanistic and global\nunderstanding of the model's predictions while maintaining competitive\nperformance and scalability without extensive human annotation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting high-dimensional or extreme multilabels, such as in medical\ncoding, requires both accuracy and interpretability. Existing works often rely\non local interpretability methods, failing to provide comprehensive\nexplanations of the overall mechanism behind each label prediction within a\nmultilabel set. We propose a mechanistic interpretability module called\nDIctionary Label Attention (\\method) that disentangles uninterpretable dense\nembeddings into a sparse embedding space, where each nonzero element (a\ndictionary feature) represents a globally learned medical concept. Through\nhuman evaluations, we show that our sparse embeddings are more human\nunderstandable than its dense counterparts by at least 50 percent. Our\nautomated dictionary feature identification pipeline, leveraging large language\nmodels (LLMs), uncovers thousands of learned medical concepts by examining and\nsummarizing the highest activating tokens for each dictionary feature. We\nrepresent the relationships between dictionary features and medical codes\nthrough a sparse interpretable matrix, enhancing the mechanistic and global\nunderstanding of the model's predictions while maintaining competitive\nperformance and scalability without extensive human annotation."
                },
                "authors": [
                    {
                        "name": "John Wu"
                    },
                    {
                        "name": "David Wu"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10527v2",
                "updated": "2024-09-16T17:44:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    44,
                    17,
                    0,
                    260,
                    0
                ],
                "published": "2024-02-16T09:29:38Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    9,
                    29,
                    38,
                    4,
                    47,
                    0
                ],
                "title": "Assessing biomedical knowledge robustness in large language models by\n  query-efficient sampling attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing biomedical knowledge robustness in large language models by\n  query-efficient sampling attacks"
                },
                "summary": "The increasing depth of parametric domain knowledge in large language models\n(LLMs) is fueling their rapid deployment in real-world applications.\nUnderstanding model vulnerabilities in high-stakes and knowledge-intensive\ntasks is essential for quantifying the trustworthiness of model predictions and\nregulating their use. The recent discovery of named entities as adversarial\nexamples (i.e. adversarial entities) in natural language processing tasks\nraises questions about their potential impact on the knowledge robustness of\npre-trained and finetuned LLMs in high-stakes and specialized domains. We\nexamined the use of type-consistent entity substitution as a template for\ncollecting adversarial entities for billion-parameter LLMs with biomedical\nknowledge. To this end, we developed an embedding-space attack based on\npowerscaled distance-weighted sampling to assess the robustness of their\nbiomedical knowledge with a low query budget and controllable coverage. Our\nmethod has favorable query efficiency and scaling over alternative approaches\nbased on random sampling and blackbox gradient-guided search, which we\ndemonstrated for adversarial distractor generation in biomedical question\nanswering. Subsequent failure mode analysis uncovered two regimes of\nadversarial entities on the attack surface with distinct characteristics and we\nshowed that entity substitution attacks can manipulate token-wise Shapley value\nexplanations, which become deceptive in this setting. Our approach complements\nstandard evaluations for high-capacity models and the results highlight the\nbrittleness of domain knowledge in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing depth of parametric domain knowledge in large language models\n(LLMs) is fueling their rapid deployment in real-world applications.\nUnderstanding model vulnerabilities in high-stakes and knowledge-intensive\ntasks is essential for quantifying the trustworthiness of model predictions and\nregulating their use. The recent discovery of named entities as adversarial\nexamples (i.e. adversarial entities) in natural language processing tasks\nraises questions about their potential impact on the knowledge robustness of\npre-trained and finetuned LLMs in high-stakes and specialized domains. We\nexamined the use of type-consistent entity substitution as a template for\ncollecting adversarial entities for billion-parameter LLMs with biomedical\nknowledge. To this end, we developed an embedding-space attack based on\npowerscaled distance-weighted sampling to assess the robustness of their\nbiomedical knowledge with a low query budget and controllable coverage. Our\nmethod has favorable query efficiency and scaling over alternative approaches\nbased on random sampling and blackbox gradient-guided search, which we\ndemonstrated for adversarial distractor generation in biomedical question\nanswering. Subsequent failure mode analysis uncovered two regimes of\nadversarial entities on the attack surface with distinct characteristics and we\nshowed that entity substitution attacks can manipulate token-wise Shapley value\nexplanations, which become deceptive in this setting. Our approach complements\nstandard evaluations for high-capacity models and the results highlight the\nbrittleness of domain knowledge in LLMs."
                },
                "authors": [
                    {
                        "name": "R. Patrick Xian"
                    },
                    {
                        "name": "Alex J. Lee"
                    },
                    {
                        "name": "Satvik Lolla"
                    },
                    {
                        "name": "Vincent Wang"
                    },
                    {
                        "name": "Qiming Cui"
                    },
                    {
                        "name": "Russell Ro"
                    },
                    {
                        "name": "Reza Abbasi-Asl"
                    }
                ],
                "author_detail": {
                    "name": "Reza Abbasi-Asl"
                },
                "author": "Reza Abbasi-Asl",
                "arxiv_comment": "28 pages incl. appendix, updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10502v1",
                "updated": "2024-09-16T17:42:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    42,
                    15,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:42:15Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    42,
                    15,
                    0,
                    260,
                    0
                ],
                "title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on\n  Logic Puzzles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on\n  Logic Puzzles"
                },
                "summary": "Causal language modeling using the Transformer architecture has yielded\nremarkable capabilities in Large Language Models (LLMs) over the last few\nyears. However, the extent to which fundamental search and reasoning\ncapabilities emerged within LLMs remains a topic of ongoing debate. In this\nwork, we study if causal language modeling can learn a complex task such as\nsolving Sudoku puzzles. To solve a Sudoku, the model is first required to\nsearch over all empty cells of the puzzle to decide on a cell to fill and then\napply an appropriate strategy to fill the decided cell. Sometimes, the\napplication of a strategy only results in thinning down the possible values in\na cell rather than concluding the exact value of the cell. In such cases,\nmultiple strategies are applied one after the other to fill a single cell. We\nobserve that Transformer models trained on this synthetic task can indeed learn\nto solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly)\nwhen trained on a logical sequence of steps taken by a solver. We find that\ntraining Transformers with the logical sequence of steps is necessary and\nwithout such training, they fail to learn Sudoku. We also extend our analysis\nto Zebra puzzles (known as Einstein puzzles) and show that the model solves\n$92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal\nrepresentations of the trained Transformer and find that through linear\nprobing, we can decode information about the set of possible values in any\ngiven cell from them, pointing to the presence of a strong reasoning engine\nimplicit in the Transformer weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal language modeling using the Transformer architecture has yielded\nremarkable capabilities in Large Language Models (LLMs) over the last few\nyears. However, the extent to which fundamental search and reasoning\ncapabilities emerged within LLMs remains a topic of ongoing debate. In this\nwork, we study if causal language modeling can learn a complex task such as\nsolving Sudoku puzzles. To solve a Sudoku, the model is first required to\nsearch over all empty cells of the puzzle to decide on a cell to fill and then\napply an appropriate strategy to fill the decided cell. Sometimes, the\napplication of a strategy only results in thinning down the possible values in\na cell rather than concluding the exact value of the cell. In such cases,\nmultiple strategies are applied one after the other to fill a single cell. We\nobserve that Transformer models trained on this synthetic task can indeed learn\nto solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly)\nwhen trained on a logical sequence of steps taken by a solver. We find that\ntraining Transformers with the logical sequence of steps is necessary and\nwithout such training, they fail to learn Sudoku. We also extend our analysis\nto Zebra puzzles (known as Einstein puzzles) and show that the model solves\n$92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal\nrepresentations of the trained Transformer and find that through linear\nprobing, we can decode information about the set of possible values in any\ngiven cell from them, pointing to the presence of a strong reasoning engine\nimplicit in the Transformer weights."
                },
                "authors": [
                    {
                        "name": "Kulin Shah"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12539v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12539v5",
                "updated": "2024-09-16T17:41:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    41,
                    55,
                    0,
                    260,
                    0
                ],
                "published": "2023-11-21T11:33:15Z",
                "published_parsed": [
                    2023,
                    11,
                    21,
                    11,
                    33,
                    15,
                    1,
                    325,
                    0
                ],
                "title": "GMISeg: General Medical Image Segmentation without Re-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GMISeg: General Medical Image Segmentation without Re-Training"
                },
                "summary": "Deep learning models have become the dominant method for medical image\nsegmentation. However, they often struggle to be generalisable to unknown tasks\ninvolving new anatomical structures, labels, or shapes. In these cases, the\nmodel needs to be re-trained for the new tasks, posing a significant challenge\nfor non-machine learning experts and requiring a considerable time investment.\nHere I developed a general model that can solve unknown medical image\nsegmentation tasks without requiring additional training. Given an example set\nof images and visual prompts for defining new segmentation tasks, GMISeg\n(General Medical Image Segmentation) leverages a pre-trained image encoder\nbased on ViT and applies a low-rank fine-tuning strategy to the prompt encoder\nand mask decoder to fine-tune the model without in an efficient manner. I\nevaluated the performance of the proposed method on medical image datasets with\ndifferent imaging modalities and anatomical structures. The proposed method\nfacilitated the deployment of pre-trained AI models to new segmentation works\nin a user-friendly way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become the dominant method for medical image\nsegmentation. However, they often struggle to be generalisable to unknown tasks\ninvolving new anatomical structures, labels, or shapes. In these cases, the\nmodel needs to be re-trained for the new tasks, posing a significant challenge\nfor non-machine learning experts and requiring a considerable time investment.\nHere I developed a general model that can solve unknown medical image\nsegmentation tasks without requiring additional training. Given an example set\nof images and visual prompts for defining new segmentation tasks, GMISeg\n(General Medical Image Segmentation) leverages a pre-trained image encoder\nbased on ViT and applies a low-rank fine-tuning strategy to the prompt encoder\nand mask decoder to fine-tune the model without in an efficient manner. I\nevaluated the performance of the proposed method on medical image datasets with\ndifferent imaging modalities and anatomical structures. The proposed method\nfacilitated the deployment of pre-trained AI models to new segmentation works\nin a user-friendly way."
                },
                "authors": [
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.12539v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12539v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11006v2",
                "updated": "2024-09-16T17:35:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    35,
                    10,
                    0,
                    260,
                    0
                ],
                "published": "2024-08-20T17:00:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    0,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Security Attacks on LLM-based Code Completion Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Attacks on LLM-based Code Completion Tools"
                },
                "summary": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs."
                },
                "authors": [
                    {
                        "name": "Wen Cheng"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10490v1",
                "updated": "2024-09-16T17:23:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    23,
                    0,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:23:00Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    23,
                    0,
                    0,
                    260,
                    0
                ],
                "title": "Code Vulnerability Detection: A Comparative Analysis of Emerging Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Vulnerability Detection: A Comparative Analysis of Emerging Large\n  Language Models"
                },
                "summary": "The growing trend of vulnerability issues in software development as a result\nof a large dependence on open-source projects has received considerable\nattention recently. This paper investigates the effectiveness of Large Language\nModels (LLMs) in identifying vulnerabilities within codebases, with a focus on\nthe latest advancements in LLM technology. Through a comparative analysis, we\nassess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma,\nand CodeGemma, alongside established state-of-the-art models such as BERT,\nRoBERTa, and GPT-3. Our study aims to shed light on the capabilities of LLMs in\nvulnerability detection, contributing to the enhancement of software security\npractices across diverse open-source repositories. We observe that CodeGemma\nachieves the highest F1-score of 58\\ and a Recall of 87\\, amongst the recent\nadditions of large language models to detect software security vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing trend of vulnerability issues in software development as a result\nof a large dependence on open-source projects has received considerable\nattention recently. This paper investigates the effectiveness of Large Language\nModels (LLMs) in identifying vulnerabilities within codebases, with a focus on\nthe latest advancements in LLM technology. Through a comparative analysis, we\nassess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma,\nand CodeGemma, alongside established state-of-the-art models such as BERT,\nRoBERTa, and GPT-3. Our study aims to shed light on the capabilities of LLMs in\nvulnerability detection, contributing to the enhancement of software security\npractices across diverse open-source repositories. We observe that CodeGemma\nachieves the highest F1-score of 58\\ and a Recall of 87\\, amongst the recent\nadditions of large language models to detect software security vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Shaznin Sultana"
                    },
                    {
                        "name": "Sadia Afreen"
                    },
                    {
                        "name": "Nasir U. Eisty"
                    }
                ],
                "author_detail": {
                    "name": "Nasir U. Eisty"
                },
                "author": "Nasir U. Eisty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10484v1",
                "updated": "2024-09-16T17:20:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    20,
                    23,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:20:23Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    20,
                    23,
                    0,
                    260,
                    0
                ],
                "title": "XLM for Autonomous Driving Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XLM for Autonomous Driving Systems: A Comprehensive Review"
                },
                "summary": "Large Language Models (LLMs) have showcased remarkable proficiency in various\ninformation-processing tasks. These tasks span from extracting data and\nsummarizing literature to generating content, predictive modeling,\ndecision-making, and system controls. Moreover, Vision Large Models (VLMs) and\nMultimodal LLMs (MLLMs), which represent the next generation of language\nmodels, a.k.a., XLMs, can combine and integrate many data modalities with the\nstrength of language understanding, thus advancing several information-based\nsystems, such as Autonomous Driving Systems (ADS). Indeed, by combining\nlanguage communication with multimodal sensory inputs, e.g., panoramic images\nand LiDAR or radar data, accurate driving actions can be taken. In this\ncontext, we provide in this survey paper a comprehensive overview of the\npotential of XLMs towards achieving autonomous driving. Specifically, we review\nthe relevant literature on ADS and XLMs, including their architectures, tools,\nand frameworks. Then, we detail the proposed approaches to deploy XLMs for\nautonomous driving solutions. Finally, we provide the related challenges to XLM\ndeployment for ADS and point to future research directions aiming to enable XLM\nadoption in future ADS frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased remarkable proficiency in various\ninformation-processing tasks. These tasks span from extracting data and\nsummarizing literature to generating content, predictive modeling,\ndecision-making, and system controls. Moreover, Vision Large Models (VLMs) and\nMultimodal LLMs (MLLMs), which represent the next generation of language\nmodels, a.k.a., XLMs, can combine and integrate many data modalities with the\nstrength of language understanding, thus advancing several information-based\nsystems, such as Autonomous Driving Systems (ADS). Indeed, by combining\nlanguage communication with multimodal sensory inputs, e.g., panoramic images\nand LiDAR or radar data, accurate driving actions can be taken. In this\ncontext, we provide in this survey paper a comprehensive overview of the\npotential of XLMs towards achieving autonomous driving. Specifically, we review\nthe relevant literature on ADS and XLMs, including their architectures, tools,\nand frameworks. Then, we detail the proposed approaches to deploy XLMs for\nautonomous driving solutions. Finally, we provide the related challenges to XLM\ndeployment for ADS and point to future research directions aiming to enable XLM\nadoption in future ADS frameworks."
                },
                "authors": [
                    {
                        "name": "Sonda Fourati"
                    },
                    {
                        "name": "Wael Jaafar"
                    },
                    {
                        "name": "Noura Baccar"
                    },
                    {
                        "name": "Safwan Alfattani"
                    }
                ],
                "author_detail": {
                    "name": "Safwan Alfattani"
                },
                "author": "Safwan Alfattani",
                "arxiv_comment": "30 pages, 18 figures, submitted to IEEE Open Journal of Intelligent\n  Transportation Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10482v1",
                "updated": "2024-09-16T17:18:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    18,
                    11,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:18:11Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    18,
                    11,
                    0,
                    260,
                    0
                ],
                "title": "Schrodinger's Memory: Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schrodinger's Memory: Large Language Models"
                },
                "summary": "Memory is the foundation of LLMs' functionality, yet past research has lacked\nan in-depth exploration of their memory capabilities and underlying theory. In\nthis paper, we apply UAT theory to explain the memory mechanism of LLMs and\npropose a new approach for evaluating LLM performance by comparing the memory\ncapacities of different models. Through extensive experiments, we validate our\ntheory and the memory abilities of LLMs. Finally, we compare the capabilities\nof the human brain and LLMs, highlighting both their similarities and\ndifferences in terms of working mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is the foundation of LLMs' functionality, yet past research has lacked\nan in-depth exploration of their memory capabilities and underlying theory. In\nthis paper, we apply UAT theory to explain the memory mechanism of LLMs and\npropose a new approach for evaluating LLM performance by comparing the memory\ncapacities of different models. Through extensive experiments, we validate our\ntheory and the memory abilities of LLMs. Finally, we compare the capabilities\nof the human brain and LLMs, highlighting both their similarities and\ndifferences in terms of working mechanisms."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14891v2",
                "updated": "2024-09-16T17:15:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    15,
                    52,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-21T06:26:38Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    6,
                    26,
                    38,
                    4,
                    173,
                    0
                ],
                "title": "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop\n  Question Answering"
                },
                "summary": "Multi-Hop Question Answering (MHQA) tasks present a significant challenge for\nlarge language models (LLMs) due to the intensive knowledge required. Current\nsolutions, like Retrieval-Augmented Generation, typically retrieve potential\ndocuments from an external corpus to read an answer. However, the performance\nof this retrieve-then-read paradigm is constrained by the retriever and the\ninevitable noise in the retrieved documents. To mitigate these challenges, we\nintroduce a novel generate-then-ground (GenGround) framework, synergizing the\nparametric knowledge of LLMs and external documents to solve a multi-hop\nquestion. GenGround empowers LLMs to alternate two phases until the final\nanswer is derived: (1) formulate a simpler, single-hop question and directly\ngenerate the answer; (2) ground the question-answer pair in retrieved\ndocuments, amending any wrong predictions in the answer. We also propose an\ninstructional grounding distillation method to generalize our method into\nsmaller models. Extensive experiments conducted on four datasets illustrate the\nsuperiority of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Hop Question Answering (MHQA) tasks present a significant challenge for\nlarge language models (LLMs) due to the intensive knowledge required. Current\nsolutions, like Retrieval-Augmented Generation, typically retrieve potential\ndocuments from an external corpus to read an answer. However, the performance\nof this retrieve-then-read paradigm is constrained by the retriever and the\ninevitable noise in the retrieved documents. To mitigate these challenges, we\nintroduce a novel generate-then-ground (GenGround) framework, synergizing the\nparametric knowledge of LLMs and external documents to solve a multi-hop\nquestion. GenGround empowers LLMs to alternate two phases until the final\nanswer is derived: (1) formulate a simpler, single-hop question and directly\ngenerate the answer; (2) ground the question-answer pair in retrieved\ndocuments, amending any wrong predictions in the answer. We also propose an\ninstructional grounding distillation method to generalize our method into\nsmaller models. Extensive experiments conducted on four datasets illustrate the\nsuperiority of our method."
                },
                "authors": [
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Shen Gao"
                    },
                    {
                        "name": "Pengjie Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Zhaochun Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhaochun Ren"
                },
                "author": "Zhaochun Ren",
                "arxiv_comment": "ACL 2024 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10469v1",
                "updated": "2024-09-16T17:01:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    1,
                    10,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T17:01:10Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    1,
                    10,
                    0,
                    260,
                    0
                ],
                "title": "Real-Time Whole-Body Control of Legged Robots with Model-Predictive Path\n  Integral Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Whole-Body Control of Legged Robots with Model-Predictive Path\n  Integral Control"
                },
                "summary": "This paper presents a system for enabling real-time synthesis of whole-body\nlocomotion and manipulation policies for real-world legged robots. Motivated by\nrecent advancements in robot simulation, we leverage the efficient\nparallelization capabilities of the MuJoCo simulator to achieve fast sampling\nover the robot state and action trajectories. Our results show surprisingly\neffective real-world locomotion and manipulation capabilities with a very\nsimple control strategy. We demonstrate our approach on several hardware and\nsimulation experiments: robust locomotion over flat and uneven terrains,\nclimbing over a box whose height is comparable to the robot, and pushing a box\nto a goal position. To our knowledge, this is the first successful deployment\nof whole-body sampling-based MPC on real-world legged robot hardware.\nExperiment videos and code can be found at: https://whole-body-mppi.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a system for enabling real-time synthesis of whole-body\nlocomotion and manipulation policies for real-world legged robots. Motivated by\nrecent advancements in robot simulation, we leverage the efficient\nparallelization capabilities of the MuJoCo simulator to achieve fast sampling\nover the robot state and action trajectories. Our results show surprisingly\neffective real-world locomotion and manipulation capabilities with a very\nsimple control strategy. We demonstrate our approach on several hardware and\nsimulation experiments: robust locomotion over flat and uneven terrains,\nclimbing over a box whose height is comparable to the robot, and pushing a box\nto a goal position. To our knowledge, this is the first successful deployment\nof whole-body sampling-based MPC on real-world legged robot hardware.\nExperiment videos and code can be found at: https://whole-body-mppi.github.io/"
                },
                "authors": [
                    {
                        "name": "Juan Alvarez-Padilla"
                    },
                    {
                        "name": "John Z. Zhang"
                    },
                    {
                        "name": "Sofia Kwok"
                    },
                    {
                        "name": "John M. Dolan"
                    },
                    {
                        "name": "Zachary Manchester"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Manchester"
                },
                "author": "Zachary Manchester",
                "arxiv_comment": "Under review. Code and videos are available on our website:\n  https://whole-body-mppi.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05508v2",
                "updated": "2024-09-16T16:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    44,
                    58,
                    0,
                    260,
                    0
                ],
                "published": "2024-08-10T10:16:03Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    10,
                    16,
                    3,
                    5,
                    223,
                    0
                ],
                "title": "PointMT: Efficient Point Cloud Analysis with Hybrid MLP-Transformer\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PointMT: Efficient Point Cloud Analysis with Hybrid MLP-Transformer\n  Architecture"
                },
                "summary": "In recent years, point cloud analysis methods based on the Transformer\narchitecture have made significant progress, particularly in the context of\nmultimedia applications such as 3D modeling, virtual reality, and autonomous\nsystems. However, the high computational resource demands of the Transformer\narchitecture hinder its scalability, real-time processing capabilities, and\ndeployment on mobile devices and other platforms with limited computational\nresources. This limitation remains a significant obstacle to its practical\napplication in scenarios requiring on-device intelligence and multimedia\nprocessing. To address this challenge, we propose an efficient point cloud\nanalysis architecture, \\textbf{Point} \\textbf{M}LP-\\textbf{T}ransformer\n(PointMT). This study tackles the quadratic complexity of the self-attention\nmechanism by introducing a linear complexity local attention mechanism for\neffective feature aggregation. Additionally, to counter the Transformer's focus\non token differences while neglecting channel differences, we introduce a\nparameter-free channel temperature adaptation mechanism that adaptively adjusts\nthe attention weight distribution in each channel, enhancing the precision of\nfeature aggregation. To improve the Transformer's slow convergence speed due to\nthe limited scale of point cloud datasets, we propose an MLP-Transformer hybrid\nmodule, which significantly enhances the model's convergence speed.\nFurthermore, to boost the feature representation capability of point tokens, we\nrefine the classification head, enabling point tokens to directly participate\nin prediction. Experimental results on multiple evaluation benchmarks\ndemonstrate that PointMT achieves performance comparable to state-of-the-art\nmethods while maintaining an optimal balance between performance and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, point cloud analysis methods based on the Transformer\narchitecture have made significant progress, particularly in the context of\nmultimedia applications such as 3D modeling, virtual reality, and autonomous\nsystems. However, the high computational resource demands of the Transformer\narchitecture hinder its scalability, real-time processing capabilities, and\ndeployment on mobile devices and other platforms with limited computational\nresources. This limitation remains a significant obstacle to its practical\napplication in scenarios requiring on-device intelligence and multimedia\nprocessing. To address this challenge, we propose an efficient point cloud\nanalysis architecture, \\textbf{Point} \\textbf{M}LP-\\textbf{T}ransformer\n(PointMT). This study tackles the quadratic complexity of the self-attention\nmechanism by introducing a linear complexity local attention mechanism for\neffective feature aggregation. Additionally, to counter the Transformer's focus\non token differences while neglecting channel differences, we introduce a\nparameter-free channel temperature adaptation mechanism that adaptively adjusts\nthe attention weight distribution in each channel, enhancing the precision of\nfeature aggregation. To improve the Transformer's slow convergence speed due to\nthe limited scale of point cloud datasets, we propose an MLP-Transformer hybrid\nmodule, which significantly enhances the model's convergence speed.\nFurthermore, to boost the feature representation capability of point tokens, we\nrefine the classification head, enabling point tokens to directly participate\nin prediction. Experimental results on multiple evaluation benchmarks\ndemonstrate that PointMT achieves performance comparable to state-of-the-art\nmethods while maintaining an optimal balance between performance and accuracy."
                },
                "authors": [
                    {
                        "name": "Qiang Zheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08228v3",
                "updated": "2024-09-16T16:29:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    29,
                    0,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-12T17:13:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    13,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "Improving Initial Transients of Online Learning Echo State Network\n  Control System with Feedback Adjustments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Initial Transients of Online Learning Echo State Network\n  Control System with Feedback Adjustments"
                },
                "summary": "Echo state networks (ESNs) have become increasingly popular in online\nlearning control systems due to their ease of training. However, online\nlearning ESN controllers often suffer from slow convergence during the initial\ntransient phase. Existing solutions, such as prior training, control mode\nswitching, and incorporating plant dynamic approximations, have notable\ndrawbacks, including undermining the system's online learning property or\nrelying on prior knowledge of the controlled system. This work proposes a\nsimple yet effective approach to address the slow initial convergence of online\nlearning ESN control systems by integrating a feedback proportional-derivative\n(P-D) controller. Simulation results demonstrate that the proposed control\nsystem achieves rapid convergence during the initial transient phase and shows\nstrong robustness against changes in the controlled system's dynamics and\nvariations in the online learning model's hyperparameters. We show that the\nfeedback controller accelerates convergence by guiding the online learning ESN\nto operate within a data range well-suited for learning. This study offers\npractical benefits for engineers aiming to implement online learning ESN\ncontrol systems with fast convergence and easy deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echo state networks (ESNs) have become increasingly popular in online\nlearning control systems due to their ease of training. However, online\nlearning ESN controllers often suffer from slow convergence during the initial\ntransient phase. Existing solutions, such as prior training, control mode\nswitching, and incorporating plant dynamic approximations, have notable\ndrawbacks, including undermining the system's online learning property or\nrelying on prior knowledge of the controlled system. This work proposes a\nsimple yet effective approach to address the slow initial convergence of online\nlearning ESN control systems by integrating a feedback proportional-derivative\n(P-D) controller. Simulation results demonstrate that the proposed control\nsystem achieves rapid convergence during the initial transient phase and shows\nstrong robustness against changes in the controlled system's dynamics and\nvariations in the online learning model's hyperparameters. We show that the\nfeedback controller accelerates convergence by guiding the online learning ESN\nto operate within a data range well-suited for learning. This study offers\npractical benefits for engineers aiming to implement online learning ESN\ncontrol systems with fast convergence and easy deployment."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Shen"
                },
                "author": "Junyi Shen",
                "arxiv_comment": "6 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10444v1",
                "updated": "2024-09-16T16:28:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    28,
                    34,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T16:28:34Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    28,
                    34,
                    0,
                    260,
                    0
                ],
                "title": "LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot\n  Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot\n  Task Planning"
                },
                "summary": "Robotic assembly tasks are open challenges due to the long task horizon and\ncomplex part relations. Behavior trees (BTs) are increasingly used in robot\ntask planning for their modularity and flexibility, but manually designing them\ncan be effort-intensive. Large language models (LLMs) have recently been\napplied in robotic task planning for generating action sequences, but their\nability to generate BTs has not been fully investigated. To this end, We\npropose LLM as BT-planner, a novel framework to leverage LLMs for BT generation\nin robotic assembly task planning and execution. Four in-context learning\nmethods are introduced to utilize the natural language processing and inference\ncapabilities of LLMs to produce task plans in BT format, reducing manual effort\nand ensuring robustness and comprehensibility. We also evaluate the performance\nof fine-tuned, fewer-parameter LLMs on the same tasks. Experiments in simulated\nand real-world settings show that our framework enhances LLMs' performance in\nBT generation, improving success rates in BT generation through in-context\nlearning and supervised fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic assembly tasks are open challenges due to the long task horizon and\ncomplex part relations. Behavior trees (BTs) are increasingly used in robot\ntask planning for their modularity and flexibility, but manually designing them\ncan be effort-intensive. Large language models (LLMs) have recently been\napplied in robotic task planning for generating action sequences, but their\nability to generate BTs has not been fully investigated. To this end, We\npropose LLM as BT-planner, a novel framework to leverage LLMs for BT generation\nin robotic assembly task planning and execution. Four in-context learning\nmethods are introduced to utilize the natural language processing and inference\ncapabilities of LLMs to produce task plans in BT format, reducing manual effort\nand ensuring robustness and comprehensibility. We also evaluate the performance\nof fine-tuned, fewer-parameter LLMs on the same tasks. Experiments in simulated\nand real-world settings show that our framework enhances LLMs' performance in\nBT generation, improving success rates in BT generation through in-context\nlearning and supervised fine-tuning."
                },
                "authors": [
                    {
                        "name": "Jicong Ao"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Yansong Wu"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Sami Haddadin"
                    }
                ],
                "author_detail": {
                    "name": "Sami Haddadin"
                },
                "author": "Sami Haddadin",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.01256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.01256v2",
                "updated": "2024-09-16T16:05:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    5,
                    18,
                    0,
                    260,
                    0
                ],
                "published": "2024-01-02T15:56:48Z",
                "published_parsed": [
                    2024,
                    1,
                    2,
                    15,
                    56,
                    48,
                    1,
                    2,
                    0
                ],
                "title": "VideoStudio: Generating Consistent-Content and Multi-Scene Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoStudio: Generating Consistent-Content and Multi-Scene Videos"
                },
                "summary": "The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoStudio, for consistent-content and multi-scene video\ngeneration. Technically, VideoStudio leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoStudio identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoStudio outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoStudio outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference. Source code\nis available at \\url{https://github.com/FuchenUSTC/VideoStudio}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoStudio, for consistent-content and multi-scene video\ngeneration. Technically, VideoStudio leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoStudio identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoStudio outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoStudio outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference. Source code\nis available at \\url{https://github.com/FuchenUSTC/VideoStudio}."
                },
                "authors": [
                    {
                        "name": "Fuchen Long"
                    },
                    {
                        "name": "Zhaofan Qiu"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Tao Mei"
                    }
                ],
                "author_detail": {
                    "name": "Tao Mei"
                },
                "author": "Tao Mei",
                "arxiv_comment": "ECCV 2024. Source code is available at\n  https://github.com/FuchenUSTC/VideoStudio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.01256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.01256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00921v2",
                "updated": "2024-09-16T15:28:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    28,
                    31,
                    0,
                    260,
                    0
                ],
                "published": "2024-07-01T02:55:45Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    2,
                    55,
                    45,
                    0,
                    183,
                    0
                ],
                "title": "PointViG: A Lightweight GNN-based Model for Efficient Point Cloud\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PointViG: A Lightweight GNN-based Model for Efficient Point Cloud\n  Analysis"
                },
                "summary": "In the domain of point cloud analysis, despite the significant capabilities\nof Graph Neural Networks (GNNs) in managing complex 3D datasets, existing\napproaches encounter challenges like high computational costs and scalability\nissues with extensive scenarios. These limitations restrict the practical\ndeployment of GNNs, notably in resource-constrained environments. To address\nthese issues, this study introduce <b>Point<\\b> <b>Vi<\\b>sion <b>G<\\b>NN\n(PointViG), an efficient framework for point cloud analysis. PointViG\nincorporates a lightweight graph convolutional module to efficiently aggregate\nlocal features and mitigate over-smoothing. For large-scale point cloud scenes,\nwe propose an adaptive dilated graph convolution technique that searches for\nsparse neighboring nodes within a dilated neighborhood based on semantic\ncorrelation, thereby expanding the receptive field and ensuring computational\nefficiency. Experiments demonstrate that PointViG achieves performance\ncomparable to state-of-the-art models while balancing performance and\ncomplexity. On the ModelNet40 classification task, PointViG achieved 94.3%\naccuracy with 1.5M parameters. For the S3DIS segmentation task, it achieved an\nmIoU of 71.7% with 5.3M parameters. These results underscore the potential and\nefficiency of PointViG in point cloud analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of point cloud analysis, despite the significant capabilities\nof Graph Neural Networks (GNNs) in managing complex 3D datasets, existing\napproaches encounter challenges like high computational costs and scalability\nissues with extensive scenarios. These limitations restrict the practical\ndeployment of GNNs, notably in resource-constrained environments. To address\nthese issues, this study introduce <b>Point<\\b> <b>Vi<\\b>sion <b>G<\\b>NN\n(PointViG), an efficient framework for point cloud analysis. PointViG\nincorporates a lightweight graph convolutional module to efficiently aggregate\nlocal features and mitigate over-smoothing. For large-scale point cloud scenes,\nwe propose an adaptive dilated graph convolution technique that searches for\nsparse neighboring nodes within a dilated neighborhood based on semantic\ncorrelation, thereby expanding the receptive field and ensuring computational\nefficiency. Experiments demonstrate that PointViG achieves performance\ncomparable to state-of-the-art models while balancing performance and\ncomplexity. On the ModelNet40 classification task, PointViG achieved 94.3%\naccuracy with 1.5M parameters. For the S3DIS segmentation task, it achieved an\nmIoU of 71.7% with 5.3M parameters. These results underscore the potential and\nefficiency of PointViG in point cloud analysis."
                },
                "authors": [
                    {
                        "name": "Qiang Zheng"
                    },
                    {
                        "name": "Yafei Qi"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10372v1",
                "updated": "2024-09-16T15:15:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    15,
                    51,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T15:15:51Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    15,
                    51,
                    0,
                    260,
                    0
                ],
                "title": "Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation"
                },
                "summary": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings."
                },
                "authors": [
                    {
                        "name": "Qiliang Chen"
                    },
                    {
                        "name": "Alireza"
                    },
                    {
                        "name": "Ilami"
                    },
                    {
                        "name": "Nunzio Lore"
                    },
                    {
                        "name": "Babak Heydari"
                    }
                ],
                "author_detail": {
                    "name": "Babak Heydari"
                },
                "arxiv_affiliation": "Sepehr",
                "author": "Babak Heydari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10354v2",
                "updated": "2024-09-17T03:22:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    3,
                    22,
                    45,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-16T15:04:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    15,
                    4,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "Learnings from a Large-Scale Deployment of an LLM-Powered\n  Expert-in-the-Loop Healthcare Chatbot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learnings from a Large-Scale Deployment of an LLM-Powered\n  Expert-in-the-Loop Healthcare Chatbot"
                },
                "summary": "Large Language Models (LLMs) are widely used in healthcare, but limitations\nlike hallucinations, incomplete information, and bias hinder their reliability.\nTo address these, researchers released the Build Your Own expert Bot (BYOeB)\nplatform, enabling developers to create LLM-powered chatbots with integrated\nexpert verification. CataractBot, its first implementation, provides\nexpert-verified responses to cataract surgery questions. A pilot evaluation\nshowed its potential; however the study had a small sample size and was\nprimarily qualitative. In this work, we conducted a large-scale 24-week\ndeployment of CataractBot involving 318 patients and attendants who sent 1,992\nmessages, with 91.71% of responses verified by seven experts. Analysis of\ninteraction logs revealed that medical questions significantly outnumbered\nlogistical ones, hallucinations were negligible, and experts rated 84.52% of\nmedical answers as accurate. As the knowledge base expanded with expert\ncorrections, system performance improved by 19.02%, reducing expert workload.\nThese insights guide the design of future LLM-powered chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in healthcare, but limitations\nlike hallucinations, incomplete information, and bias hinder their reliability.\nTo address these, researchers released the Build Your Own expert Bot (BYOeB)\nplatform, enabling developers to create LLM-powered chatbots with integrated\nexpert verification. CataractBot, its first implementation, provides\nexpert-verified responses to cataract surgery questions. A pilot evaluation\nshowed its potential; however the study had a small sample size and was\nprimarily qualitative. In this work, we conducted a large-scale 24-week\ndeployment of CataractBot involving 318 patients and attendants who sent 1,992\nmessages, with 91.71% of responses verified by seven experts. Analysis of\ninteraction logs revealed that medical questions significantly outnumbered\nlogistical ones, hallucinations were negligible, and experts rated 84.52% of\nmedical answers as accurate. As the knowledge base expanded with expert\ncorrections, system performance improved by 19.02%, reducing expert workload.\nThese insights guide the design of future LLM-powered chatbots."
                },
                "authors": [
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Geeta Fulari"
                    },
                    {
                        "name": "Kaushik Murali"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "arxiv_comment": "The first two authors contributed equally to this research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10343v1",
                "updated": "2024-09-16T14:57:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    57,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T14:57:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    57,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Large Language Model Enhanced Hard Sample Identification for Denoising\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enhanced Hard Sample Identification for Denoising\n  Recommendation"
                },
                "summary": "Implicit feedback, often used to build recommender systems, unavoidably\nconfronts noise due to factors such as misclicks and position bias. Previous\nstudies have attempted to alleviate this by identifying noisy samples based on\ntheir diverged patterns, such as higher loss values, and mitigating the noise\nthrough sample dropping or reweighting. Despite the progress, we observe\nexisting approaches struggle to distinguish hard samples and noise samples, as\nthey often exhibit similar patterns, thereby limiting their effectiveness in\ndenoising recommendations. To address this challenge, we propose a Large\nLanguage Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,\nwe construct an LLM-based scorer to evaluate the semantic consistency of items\nwith the user preference, which is quantified based on summarized historical\nuser interactions. The resulting scores are used to assess the hardness of\nsamples for the pointwise or pairwise training objectives. To ensure\nefficiency, we introduce a variance-based sample pruning strategy to filter\npotential hard samples before scoring. Besides, we propose an iterative\npreference update module designed to continuously refine summarized user\npreference, which may be biased due to false-positive user-item interactions.\nExtensive experiments on three real-world datasets and four backbone\nrecommenders demonstrate the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit feedback, often used to build recommender systems, unavoidably\nconfronts noise due to factors such as misclicks and position bias. Previous\nstudies have attempted to alleviate this by identifying noisy samples based on\ntheir diverged patterns, such as higher loss values, and mitigating the noise\nthrough sample dropping or reweighting. Despite the progress, we observe\nexisting approaches struggle to distinguish hard samples and noise samples, as\nthey often exhibit similar patterns, thereby limiting their effectiveness in\ndenoising recommendations. To address this challenge, we propose a Large\nLanguage Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,\nwe construct an LLM-based scorer to evaluate the semantic consistency of items\nwith the user preference, which is quantified based on summarized historical\nuser interactions. The resulting scores are used to assess the hardness of\nsamples for the pointwise or pairwise training objectives. To ensure\nefficiency, we introduce a variance-based sample pruning strategy to filter\npotential hard samples before scoring. Besides, we propose an iterative\npreference update module designed to continuously refine summarized user\npreference, which may be biased due to false-positive user-item interactions.\nExtensive experiments on three real-world datasets and four backbone\nrecommenders demonstrate the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Tianrui Song"
                    },
                    {
                        "name": "Wenshuo Chao"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10996v2",
                "updated": "2024-09-16T14:52:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    52,
                    47,
                    0,
                    260,
                    0
                ],
                "published": "2024-03-16T18:47:04Z",
                "published_parsed": [
                    2024,
                    3,
                    16,
                    18,
                    47,
                    4,
                    5,
                    76,
                    0
                ],
                "title": "A Scalable and Parallelizable Digital Twin Framework for Sustainable\n  Sim2Real Transition of Multi-Agent Reinforcement Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable and Parallelizable Digital Twin Framework for Sustainable\n  Sim2Real Transition of Multi-Agent Reinforcement Learning Systems"
                },
                "summary": "Multi-agent reinforcement learning (MARL) systems usually require\nsignificantly long training times due to their inherent complexity.\nFurthermore, deploying them in the real world demands a feature-rich\nenvironment along with multiple embodied agents, which may not be feasible due\nto budget or space limitations, not to mention energy consumption and safety\nissues. This work tries to address these pain points by presenting a\nsustainable digital twin framework capable of accelerating MARL training by\nselectively scaling parallelized workloads on-demand, and transferring the\ntrained policies from simulation to reality using minimal hardware resources.\nThe applicability of the proposed digital twin framework is highlighted through\ntwo representative use cases, which cover cooperative as well as competitive\nclasses of MARL problems. We study the effect of agent and environment\nparallelization on training time and that of systematic domain randomization on\nzero-shot sim2real transfer across both the case studies. Results indicate up\nto 76.3% reduction in training time with the proposed parallelization scheme\nand as low as 2.9% sim2real gap using the suggested deployment method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent reinforcement learning (MARL) systems usually require\nsignificantly long training times due to their inherent complexity.\nFurthermore, deploying them in the real world demands a feature-rich\nenvironment along with multiple embodied agents, which may not be feasible due\nto budget or space limitations, not to mention energy consumption and safety\nissues. This work tries to address these pain points by presenting a\nsustainable digital twin framework capable of accelerating MARL training by\nselectively scaling parallelized workloads on-demand, and transferring the\ntrained policies from simulation to reality using minimal hardware resources.\nThe applicability of the proposed digital twin framework is highlighted through\ntwo representative use cases, which cover cooperative as well as competitive\nclasses of MARL problems. We study the effect of agent and environment\nparallelization on training time and that of systematic domain randomization on\nzero-shot sim2real transfer across both the case studies. Results indicate up\nto 76.3% reduction in training time with the proposed parallelization scheme\nand as low as 2.9% sim2real gap using the suggested deployment method."
                },
                "authors": [
                    {
                        "name": "Chinmay Vilas Samak"
                    },
                    {
                        "name": "Tanmay Vilas Samak"
                    },
                    {
                        "name": "Venkat Krovi"
                    }
                ],
                "author_detail": {
                    "name": "Venkat Krovi"
                },
                "author": "Venkat Krovi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05870v2",
                "updated": "2024-09-16T14:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-09T17:55:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    17,
                    55,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents"
                },
                "summary": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database, then generating an answer by\napplying an LLM to the retrieved documents. We demonstrate that RAG systems\nthat operate on databases with untrusted content are vulnerable to a new class\nof denial-of-service attacks we call jamming. An adversary can add a single\n``blocker'' document to the database that will be retrieved in response to a\nspecific query and result in the RAG system not answering this query -\nostensibly because it lacks the information or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. This\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not use an auxiliary LLM to generate blocker documents.\n  We evaluate jamming attacks on several LLMs and embeddings and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database, then generating an answer by\napplying an LLM to the retrieved documents. We demonstrate that RAG systems\nthat operate on databases with untrusted content are vulnerable to a new class\nof denial-of-service attacks we call jamming. An adversary can add a single\n``blocker'' document to the database that will be retrieved in response to a\nspecific query and result in the RAG system not answering this query -\nostensibly because it lacks the information or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. This\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not use an auxiliary LLM to generate blocker documents.\n  We evaluate jamming attacks on several LLMs and embeddings and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents."
                },
                "authors": [
                    {
                        "name": "Avital Shafran"
                    },
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    }
                ],
                "author_detail": {
                    "name": "Vitaly Shmatikov"
                },
                "author": "Vitaly Shmatikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10338v1",
                "updated": "2024-09-16T14:50:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    50,
                    29,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T14:50:29Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    50,
                    29,
                    0,
                    260,
                    0
                ],
                "title": "The 20 questions game to distinguish large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 20 questions game to distinguish large language models"
                },
                "summary": "In a parallel with the 20 questions game, we present a method to determine\nwhether two large language models (LLMs), placed in a black-box context, are\nthe same or not. The goal is to use a small set of (benign) binary questions,\ntypically under 20. We formalize the problem and first establish a baseline\nusing a random selection of questions from known benchmark datasets, achieving\nan accuracy of nearly 100% within 20 questions. After showing optimal bounds\nfor this problem, we introduce two effective questioning heuristics able to\ndiscriminate 22 LLMs by using half as many questions for the same task. These\nmethods offer significant advantages in terms of stealth and are thus of\ninterest to auditors or copyright owners facing suspicions of model leaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a parallel with the 20 questions game, we present a method to determine\nwhether two large language models (LLMs), placed in a black-box context, are\nthe same or not. The goal is to use a small set of (benign) binary questions,\ntypically under 20. We formalize the problem and first establish a baseline\nusing a random selection of questions from known benchmark datasets, achieving\nan accuracy of nearly 100% within 20 questions. After showing optimal bounds\nfor this problem, we introduce two effective questioning heuristics able to\ndiscriminate 22 LLMs by using half as many questions for the same task. These\nmethods offer significant advantages in terms of stealth and are thus of\ninterest to auditors or copyright owners facing suspicions of model leaks."
                },
                "authors": [
                    {
                        "name": "Gurvan Richardeau"
                    },
                    {
                        "name": "Erwan Le Merrer"
                    },
                    {
                        "name": "Camilla Penzo"
                    },
                    {
                        "name": "Gilles Tredan"
                    }
                ],
                "author_detail": {
                    "name": "Gilles Tredan"
                },
                "author": "Gilles Tredan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10330v1",
                "updated": "2024-09-16T14:40:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    40,
                    47,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T14:40:47Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    40,
                    47,
                    0,
                    260,
                    0
                ],
                "title": "DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in\n  Autonomous Driving"
                },
                "summary": "Recent advancements in autonomous driving have seen a paradigm shift towards\nend-to-end learning paradigms, which map sensory inputs directly to driving\nactions, thereby enhancing the robustness and adaptability of autonomous\nvehicles. However, these models often sacrifice interpretability, posing\nsignificant challenges to trust, safety, and regulatory compliance. To address\nthese issues, we introduce DRIVE -- Dependable Robust Interpretable Visionary\nEnsemble Framework in Autonomous Driving, a comprehensive framework designed to\nimprove the dependability and stability of explanations in end-to-end\nunsupervised autonomous driving models. Our work specifically targets the\ninherent instability problems observed in the Driving through the Concept\nGridlock (DCG) model, which undermine the trustworthiness of its explanations\nand decision-making processes. We define four key attributes of DRIVE:\nconsistent interpretability, stable interpretability, consistent output, and\nstable output. These attributes collectively ensure that explanations remain\nreliable and robust across different scenarios and perturbations. Through\nextensive empirical evaluations, we demonstrate the effectiveness of our\nframework in enhancing the stability and dependability of explanations, thereby\naddressing the limitations of current models. Our contributions include an\nin-depth analysis of the dependability issues within the DCG model, a rigorous\ndefinition of DRIVE with its fundamental properties, a framework to implement\nDRIVE, and novel metrics for evaluating the dependability of concept-based\nexplainable autonomous driving models. These advancements lay the groundwork\nfor the development of more reliable and trusted autonomous driving systems,\npaving the way for their broader acceptance and deployment in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in autonomous driving have seen a paradigm shift towards\nend-to-end learning paradigms, which map sensory inputs directly to driving\nactions, thereby enhancing the robustness and adaptability of autonomous\nvehicles. However, these models often sacrifice interpretability, posing\nsignificant challenges to trust, safety, and regulatory compliance. To address\nthese issues, we introduce DRIVE -- Dependable Robust Interpretable Visionary\nEnsemble Framework in Autonomous Driving, a comprehensive framework designed to\nimprove the dependability and stability of explanations in end-to-end\nunsupervised autonomous driving models. Our work specifically targets the\ninherent instability problems observed in the Driving through the Concept\nGridlock (DCG) model, which undermine the trustworthiness of its explanations\nand decision-making processes. We define four key attributes of DRIVE:\nconsistent interpretability, stable interpretability, consistent output, and\nstable output. These attributes collectively ensure that explanations remain\nreliable and robust across different scenarios and perturbations. Through\nextensive empirical evaluations, we demonstrate the effectiveness of our\nframework in enhancing the stability and dependability of explanations, thereby\naddressing the limitations of current models. Our contributions include an\nin-depth analysis of the dependability issues within the DCG model, a rigorous\ndefinition of DRIVE with its fundamental properties, a framework to implement\nDRIVE, and novel metrics for evaluating the dependability of concept-based\nexplainable autonomous driving models. These advancements lay the groundwork\nfor the development of more reliable and trusted autonomous driving systems,\npaving the way for their broader acceptance and deployment in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Songning Lai"
                    },
                    {
                        "name": "Tianlang Xue"
                    },
                    {
                        "name": "Hongru Xiao"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Jiemin Wu"
                    },
                    {
                        "name": "Ninghui Feng"
                    },
                    {
                        "name": "Runwei Guan"
                    },
                    {
                        "name": "Haicheng Liao"
                    },
                    {
                        "name": "Zhenning Li"
                    },
                    {
                        "name": "Yutao Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Yue"
                },
                "author": "Yutao Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10461v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10461v3",
                "updated": "2024-09-16T14:24:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    14,
                    24,
                    48,
                    0,
                    260,
                    0
                ],
                "published": "2023-10-16T14:42:22Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    14,
                    42,
                    22,
                    0,
                    289,
                    0
                ],
                "title": "Model Selection of Anomaly Detectors in the Absence of Labeled\n  Validation Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Selection of Anomaly Detectors in the Absence of Labeled\n  Validation Data"
                },
                "summary": "Anomaly detection is the task of identifying abnormal samples in large\nunlabeled datasets. While the advent of foundation models has produced powerful\nzero-shot anomaly detection methods, their deployment in practice is often\nhindered by the absence of labeled validation data -- without it, their\ndetection performance cannot be evaluated reliably. In this work, we propose\nSWSA (Selection With Synthetic Anomalies): a general-purpose framework to\nselect image-based anomaly detectors without labeled validation data. Instead\nof collecting labeled validation data, we generate synthetic anomalies without\nany training or fine-tuning, using only a small support set of normal images.\nOur synthetic anomalies are used to create detection tasks that compose a\nvalidation framework for model selection. In an empirical study, we evaluate\nSWSA with three types of synthetic anomalies and on two selection tasks: model\nselection of image-based anomaly detectors and prompt selection for CLIP-based\nanomaly detection. SWSA often selects models and prompts that match selections\nmade with a ground-truth validation set, outperforming baseline selection\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection is the task of identifying abnormal samples in large\nunlabeled datasets. While the advent of foundation models has produced powerful\nzero-shot anomaly detection methods, their deployment in practice is often\nhindered by the absence of labeled validation data -- without it, their\ndetection performance cannot be evaluated reliably. In this work, we propose\nSWSA (Selection With Synthetic Anomalies): a general-purpose framework to\nselect image-based anomaly detectors without labeled validation data. Instead\nof collecting labeled validation data, we generate synthetic anomalies without\nany training or fine-tuning, using only a small support set of normal images.\nOur synthetic anomalies are used to create detection tasks that compose a\nvalidation framework for model selection. In an empirical study, we evaluate\nSWSA with three types of synthetic anomalies and on two selection tasks: model\nselection of image-based anomaly detectors and prompt selection for CLIP-based\nanomaly detection. SWSA often selects models and prompts that match selections\nmade with a ground-truth validation set, outperforming baseline selection\nstrategies."
                },
                "authors": [
                    {
                        "name": "Clement Fung"
                    },
                    {
                        "name": "Chen Qiu"
                    },
                    {
                        "name": "Aodong Li"
                    },
                    {
                        "name": "Maja Rudolph"
                    }
                ],
                "author_detail": {
                    "name": "Maja Rudolph"
                },
                "author": "Maja Rudolph",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10461v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10461v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10289v1",
                "updated": "2024-09-16T13:56:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    56,
                    17,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:56:17Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    56,
                    17,
                    0,
                    260,
                    0
                ],
                "title": "ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework"
                },
                "summary": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations."
                },
                "authors": [
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Zixiang Di"
                    },
                    {
                        "name": "Zhiqing Cui"
                    },
                    {
                        "name": "Guisong Yang"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11477v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11477v2",
                "updated": "2024-09-16T13:55:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    55,
                    24,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-17T12:42:34Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    12,
                    42,
                    34,
                    0,
                    169,
                    0
                ],
                "title": "How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of\n  Target Language Text?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of\n  Target Language Text?"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities in many\nlanguages beyond English. Yet, LLMs require more inference steps when\ngenerating non-English text due to their reliance on English-centric tokenizers\nand vocabulary, resulting in higher usage costs to non-English speakers.\nVocabulary expansion with target language tokens is a widely used cross-lingual\nvocabulary adaptation approach to remedy this issue. Despite its effectiveness\nin inference speedup, previous work on vocabulary expansion has focused on\nhigh-resource settings assuming access to a substantial amount of target\nlanguage data to effectively initialize the embeddings of the new tokens and\nadapt the LLM to the target language. However, vocabulary expansion in\nlow-resource settings has yet to be explored. In this paper, we investigate\nvocabulary expansion in low-resource settings by considering embedding\ninitialization methods and continual pre-training strategies. Through extensive\nexperiments across typologically diverse languages, tasks and models, we\nestablish a set of strategies to perform vocabulary expansion for faster\ninference, maintaining competitive downstream performance to baselines with\nonly 30K sentences ($\\sim$0.01GB text data) from the target language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities in many\nlanguages beyond English. Yet, LLMs require more inference steps when\ngenerating non-English text due to their reliance on English-centric tokenizers\nand vocabulary, resulting in higher usage costs to non-English speakers.\nVocabulary expansion with target language tokens is a widely used cross-lingual\nvocabulary adaptation approach to remedy this issue. Despite its effectiveness\nin inference speedup, previous work on vocabulary expansion has focused on\nhigh-resource settings assuming access to a substantial amount of target\nlanguage data to effectively initialize the embeddings of the new tokens and\nadapt the LLM to the target language. However, vocabulary expansion in\nlow-resource settings has yet to be explored. In this paper, we investigate\nvocabulary expansion in low-resource settings by considering embedding\ninitialization methods and continual pre-training strategies. Through extensive\nexperiments across typologically diverse languages, tasks and models, we\nestablish a set of strategies to perform vocabulary expansion for faster\ninference, maintaining competitive downstream performance to baselines with\nonly 30K sentences ($\\sim$0.01GB text data) from the target language."
                },
                "authors": [
                    {
                        "name": "Atsuki Yamaguchi"
                    },
                    {
                        "name": "Aline Villavicencio"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11477v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11477v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14965v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14965v5",
                "updated": "2024-09-16T13:54:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    54,
                    31,
                    0,
                    260,
                    0
                ],
                "published": "2024-04-23T12:20:14Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    12,
                    20,
                    14,
                    1,
                    114,
                    0
                ],
                "title": "Vision Beyond Boundaries: An Initial Design Space of Domain-specific\n  Large Vision Models in Human-robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Beyond Boundaries: An Initial Design Space of Domain-specific\n  Large Vision Models in Human-robot Interaction"
                },
                "summary": "The emergence of large vision models (LVMs) is following in the footsteps of\nthe recent prosperity of Large Language Models (LLMs) in following years.\nHowever, there's a noticeable gap in structured research applying LVMs to\nhuman-robot interaction (HRI), despite extensive evidence supporting the\nefficacy of vision models in enhancing interactions between humans and robots.\nRecognizing the vast and anticipated potential, we introduce an initial design\nspace that incorporates domain-specific LVMs, chosen for their superior\nperformance over normal models. We delve into three primary dimensions: HRI\ncontexts, vision-based tasks, and specific domains. The empirical evaluation\nwas implemented among 15 experts across six evaluated metrics, showcasing the\nprimary efficacy in relevant decision-making scenarios. We explore the process\nof ideation and potential application scenarios, envisioning this design space\nas a foundational guideline for future HRI system design, emphasizing accurate\ndomain alignment and model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large vision models (LVMs) is following in the footsteps of\nthe recent prosperity of Large Language Models (LLMs) in following years.\nHowever, there's a noticeable gap in structured research applying LVMs to\nhuman-robot interaction (HRI), despite extensive evidence supporting the\nefficacy of vision models in enhancing interactions between humans and robots.\nRecognizing the vast and anticipated potential, we introduce an initial design\nspace that incorporates domain-specific LVMs, chosen for their superior\nperformance over normal models. We delve into three primary dimensions: HRI\ncontexts, vision-based tasks, and specific domains. The empirical evaluation\nwas implemented among 15 experts across six evaluated metrics, showcasing the\nprimary efficacy in relevant decision-making scenarios. We explore the process\nof ideation and potential application scenarios, envisioning this design space\nas a foundational guideline for future HRI system design, emphasizing accurate\ndomain alignment and model selection."
                },
                "authors": [
                    {
                        "name": "Yuchong Zhang"
                    },
                    {
                        "name": "Yong Ma"
                    },
                    {
                        "name": "Danica Kragic"
                    }
                ],
                "author_detail": {
                    "name": "Danica Kragic"
                },
                "author": "Danica Kragic",
                "arxiv_doi": "10.1145/3640471.3680244",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3640471.3680244",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.14965v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14965v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.21051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.21051v3",
                "updated": "2024-09-16T13:44:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    44,
                    37,
                    0,
                    260,
                    0
                ],
                "published": "2024-05-31T17:43:59Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    17,
                    43,
                    59,
                    4,
                    152,
                    0
                ],
                "title": "Good Modelling Software Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Good Modelling Software Practices"
                },
                "summary": "Frequently in socio-environmental sciences, models are used as tools to\nrepresent, understand, project and predict the behaviour of these complex\nsystems. Along the modelling chain, Good Modelling Practices have been evolving\nthat ensure -- amongst others -- that models are transparent and their results\nreplicable. Whenever such models are represented in software, Good Modelling\nmeet Good Software Practices, such as a tractable development workflow, good\ncode, collaborative development and governance, continuous integration and\ndeployment; and they meet Good Scientific Practices, such as attribution of\ncopyrights and acknowledgement of intellectual property, publication of a\nsoftware paper and archiving. Too often in existing socio-environmental model\nsoftware, these practices have been regarded as an add-on to be considered at a\nlater stage only; modellers have shied away from publishing their model as open\nsource out of fear that having to add good practices is too demanding. We here\nargue for making a habit of following a list of simple and not so simple\npractices early on in the implementation of the model life cycle. We\ncontextualise cherry-picked and hands-on practices for supporting Good\nModelling Practice, and we demonstrate their application in the example context\nof the Viable North Sea fisheries socio-ecological systems model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequently in socio-environmental sciences, models are used as tools to\nrepresent, understand, project and predict the behaviour of these complex\nsystems. Along the modelling chain, Good Modelling Practices have been evolving\nthat ensure -- amongst others -- that models are transparent and their results\nreplicable. Whenever such models are represented in software, Good Modelling\nmeet Good Software Practices, such as a tractable development workflow, good\ncode, collaborative development and governance, continuous integration and\ndeployment; and they meet Good Scientific Practices, such as attribution of\ncopyrights and acknowledgement of intellectual property, publication of a\nsoftware paper and archiving. Too often in existing socio-environmental model\nsoftware, these practices have been regarded as an add-on to be considered at a\nlater stage only; modellers have shied away from publishing their model as open\nsource out of fear that having to add good practices is too demanding. We here\nargue for making a habit of following a list of simple and not so simple\npractices early on in the implementation of the model life cycle. We\ncontextualise cherry-picked and hands-on practices for supporting Good\nModelling Practice, and we demonstrate their application in the example context\nof the Viable North Sea fisheries socio-ecological systems model."
                },
                "authors": [
                    {
                        "name": "Carsten Lemmen"
                    },
                    {
                        "name": "Philipp Sebastian Sommer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Sebastian Sommer"
                },
                "author": "Philipp Sebastian Sommer",
                "arxiv_comment": "2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.21051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.21051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.0; D.2.4; D.2.5; D.2.11; D.2.12; G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10280v1",
                "updated": "2024-09-16T13:43:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    43,
                    4,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:43:04Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    43,
                    4,
                    0,
                    260,
                    0
                ],
                "title": "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More\n  Complex Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComplexCodeEval: A Benchmark for Evaluating Large Code Models on More\n  Complex Code"
                },
                "summary": "In recent years, the application of large language models (LLMs) to\ncode-related tasks has gained significant attention. However, existing\nevaluation benchmarks often focus on limited scenarios, such as code generation\nor completion, which do not reflect the diverse challenges developers face in\nreal-world contexts. To address this, we introduce ComplexCodeEval, a benchmark\ndesigned to assess LCMs in various development tasks, including code\ngeneration, completion, API recommendation, and test case generation. It\nincludes 3,897 Java samples and 7,184 Python samples from high-star GitHub\nrepositories, each annotated with function signatures, docstrings, and API\nreferences to simulate real development environments. Our experiments across\nten LCMs reveal that context improves performance and that data leakage can\nlead to overestimation, highlighting the need for more accurate evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the application of large language models (LLMs) to\ncode-related tasks has gained significant attention. However, existing\nevaluation benchmarks often focus on limited scenarios, such as code generation\nor completion, which do not reflect the diverse challenges developers face in\nreal-world contexts. To address this, we introduce ComplexCodeEval, a benchmark\ndesigned to assess LCMs in various development tasks, including code\ngeneration, completion, API recommendation, and test case generation. It\nincludes 3,897 Java samples and 7,184 Python samples from high-star GitHub\nrepositories, each annotated with function signatures, docstrings, and API\nreferences to simulate real development environments. Our experiments across\nten LCMs reveal that context improves performance and that data leakage can\nlead to overestimation, highlighting the need for more accurate evaluations."
                },
                "authors": [
                    {
                        "name": "Jia Feng"
                    },
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Shan Gao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "arxiv_doi": "10.1145/3691620.3695552",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3691620.3695552",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.10280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10277v1",
                "updated": "2024-09-16T13:39:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    39,
                    5,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:39:05Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    39,
                    5,
                    0,
                    260,
                    0
                ],
                "title": "Cognitive Kernel: An Open-source Agent System towards Generalist\n  Autopilots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Kernel: An Open-source Agent System towards Generalist\n  Autopilots"
                },
                "summary": "We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cognitive Kernel, an open-source agent system towards the goal\nof generalist autopilots. Unlike copilot systems, which primarily rely on users\nto provide essential state information (e.g., task descriptions) and assist\nusers by answering questions or auto-completing contents, autopilot systems\nmust complete tasks from start to finish independently, which requires the\nsystem to acquire the state information from the environments actively. To\nachieve this, an autopilot system should be capable of understanding user\nintents, actively gathering necessary information from various real-world\nsources, and making wise decisions. Cognitive Kernel adopts a model-centric\ndesign. In our implementation, the central policy model (a fine-tuned LLM)\ninitiates interactions with the environment using a combination of atomic\nactions, such as opening files, clicking buttons, saving intermediate results\nto memory, or calling the LLM itself. This differs from the widely used\nenvironment-centric design, where a task-specific environment with predefined\nactions is fixed, and the policy model is limited to selecting the correct\naction from a given set of options. Our design facilitates seamless information\nflow across various sources and provides greater flexibility. We evaluate our\nsystem in three use cases: real-time information management, private\ninformation management, and long-term memory management. The results\ndemonstrate that Cognitive Kernel achieves better or comparable performance to\nother closed-source systems in these scenarios. Cognitive Kernel is fully\ndockerized, ensuring everyone can deploy it privately and securely. We\nopen-source the system and the backbone model to encourage further research on\nLLM-driven autopilot systems."
                },
                "authors": [
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Xiaoman Pan"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07841v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07841v2",
                "updated": "2024-09-16T13:18:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    18,
                    23,
                    0,
                    260,
                    0
                ],
                "published": "2024-02-12T17:52:05Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    17,
                    52,
                    5,
                    0,
                    43,
                    0
                ],
                "title": "Do Membership Inference Attacks Work on Large Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Membership Inference Attacks Work on Large Language Models?"
                },
                "summary": "Membership inference attacks (MIAs) attempt to predict whether a particular\ndatapoint is a member of a target model's training data. Despite extensive\nresearch on traditional machine learning models, there has been limited work\nstudying MIA on the pre-training data of large language models (LLMs). We\nperform a large-scale evaluation of MIAs over a suite of language models (LMs)\ntrained on the Pile, ranging from 160M to 12B parameters. We find that MIAs\nbarely outperform random guessing for most settings across varying LLM sizes\nand domains. Our further analyses reveal that this poor performance can be\nattributed to (1) the combination of a large dataset and few training\niterations, and (2) an inherently fuzzy boundary between members and\nnon-members. We identify specific settings where LLMs have been shown to be\nvulnerable to membership inference and show that the apparent success in such\nsettings can be attributed to a distribution shift, such as when members and\nnon-members are drawn from the seemingly identical domain but with different\ntemporal ranges. We release our code and data as a unified benchmark package\nthat includes all existing MIAs, supporting future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIAs) attempt to predict whether a particular\ndatapoint is a member of a target model's training data. Despite extensive\nresearch on traditional machine learning models, there has been limited work\nstudying MIA on the pre-training data of large language models (LLMs). We\nperform a large-scale evaluation of MIAs over a suite of language models (LMs)\ntrained on the Pile, ranging from 160M to 12B parameters. We find that MIAs\nbarely outperform random guessing for most settings across varying LLM sizes\nand domains. Our further analyses reveal that this poor performance can be\nattributed to (1) the combination of a large dataset and few training\niterations, and (2) an inherently fuzzy boundary between members and\nnon-members. We identify specific settings where LLMs have been shown to be\nvulnerable to membership inference and show that the apparent success in such\nsettings can be attributed to a distribution shift, such as when members and\nnon-members are drawn from the seemingly identical domain but with different\ntemporal ranges. We release our code and data as a unified benchmark package\nthat includes all existing MIAs, supporting future work."
                },
                "authors": [
                    {
                        "name": "Michael Duan"
                    },
                    {
                        "name": "Anshuman Suri"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    },
                    {
                        "name": "Sewon Min"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "David Evans"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    }
                ],
                "author_detail": {
                    "name": "Hannaneh Hajishirzi"
                },
                "author": "Hannaneh Hajishirzi",
                "arxiv_comment": "Accepted at Conference on Language Modeling (COLM), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07841v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07841v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08795v2",
                "updated": "2024-09-16T12:58:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    58,
                    16,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-13T12:59:39Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    59,
                    39,
                    4,
                    257,
                    0
                ],
                "title": "LLaQo: Towards a Query-Based Coach in Expressive Music Performance\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaQo: Towards a Query-Based Coach in Expressive Music Performance\n  Assessment"
                },
                "summary": "Research in music understanding has extensively explored composition-level\nattributes such as key, genre, and instrumentation through advanced\nrepresentations, leading to cross-modal applications using large language\nmodels. However, aspects of musical performance such as stylistic expression\nand technique remain underexplored, along with the potential of using large\nlanguage models to enhance educational outcomes with customized feedback. To\nbridge this gap, we introduce LLaQo, a Large Language Query-based music coach\nthat leverages audio language modeling to provide detailed and formative\nassessments of music performances. We also introduce instruction-tuned\nquery-response datasets that cover a variety of performance dimensions from\npitch accuracy to articulation, as well as contextual performance understanding\n(such as difficulty and performance techniques). Utilizing AudioMAE encoder and\nVicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in\npredicting teachers' performance ratings, as well as in identifying piece\ndifficulty and playing techniques. Textual responses from LLaQo was moreover\nrated significantly higher compared to other baseline models in a user study\nusing audio-text matching. Our proposed model can thus provide informative\nanswers to open-ended questions related to musical performance from audio data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research in music understanding has extensively explored composition-level\nattributes such as key, genre, and instrumentation through advanced\nrepresentations, leading to cross-modal applications using large language\nmodels. However, aspects of musical performance such as stylistic expression\nand technique remain underexplored, along with the potential of using large\nlanguage models to enhance educational outcomes with customized feedback. To\nbridge this gap, we introduce LLaQo, a Large Language Query-based music coach\nthat leverages audio language modeling to provide detailed and formative\nassessments of music performances. We also introduce instruction-tuned\nquery-response datasets that cover a variety of performance dimensions from\npitch accuracy to articulation, as well as contextual performance understanding\n(such as difficulty and performance techniques). Utilizing AudioMAE encoder and\nVicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in\npredicting teachers' performance ratings, as well as in identifying piece\ndifficulty and playing techniques. Textual responses from LLaQo was moreover\nrated significantly higher compared to other baseline models in a user study\nusing audio-text matching. Our proposed model can thus provide informative\nanswers to open-ended questions related to musical performance from audio data."
                },
                "authors": [
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Vincent Cheung"
                    },
                    {
                        "name": "Hayato Nishioka"
                    },
                    {
                        "name": "Simon Dixon"
                    },
                    {
                        "name": "Shinichi Furuya"
                    }
                ],
                "author_detail": {
                    "name": "Shinichi Furuya"
                },
                "author": "Shinichi Furuya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10245v1",
                "updated": "2024-09-16T12:55:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    55,
                    14,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T12:55:14Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    55,
                    14,
                    0,
                    260,
                    0
                ],
                "title": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs"
                },
                "summary": "As the demand for human-like interactions with LLMs continues to grow, so\ndoes the interest in manipulating their personality traits, which has emerged\nas a key area of research. Methods like prompt-based In-Context Knowledge\nEditing (IKE) and gradient-based Model Editor Networks (MEND) have been\nexplored but show irregularity and variability. IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLORA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,\ndespite their absence in the PEFT data. For instance, Llama-2-7B-chat generated\nemojis in 99.5% of extraversion-related test instances, while\nMistral-8B-Instruct did so in 92.5% of openness-related test instances.\nExplainability analysis indicated that the LLMs used emojis intentionally to\nexpress these traits. This paper provides a number of novel contributions.\nFirst, introducing an Opinion QA dataset for PEFT-driven personality\nmanipulation; second, developing metric models to benchmark LLM personality\ntraits; third, demonstrating PEFT's superiority over IKE in personality\nmanipulation; and finally, analyzing and validating emoji usage through\nexplainability methods such as mechanistic interpretability and in-context\nlearning explainability methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for human-like interactions with LLMs continues to grow, so\ndoes the interest in manipulating their personality traits, which has emerged\nas a key area of research. Methods like prompt-based In-Context Knowledge\nEditing (IKE) and gradient-based Model Editor Networks (MEND) have been\nexplored but show irregularity and variability. IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLORA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,\ndespite their absence in the PEFT data. For instance, Llama-2-7B-chat generated\nemojis in 99.5% of extraversion-related test instances, while\nMistral-8B-Instruct did so in 92.5% of openness-related test instances.\nExplainability analysis indicated that the LLMs used emojis intentionally to\nexpress these traits. This paper provides a number of novel contributions.\nFirst, introducing an Opinion QA dataset for PEFT-driven personality\nmanipulation; second, developing metric models to benchmark LLM personality\ntraits; third, demonstrating PEFT's superiority over IKE in personality\nmanipulation; and finally, analyzing and validating emoji usage through\nexplainability methods such as mechanistic interpretability and in-context\nlearning explainability methods."
                },
                "authors": [
                    {
                        "name": "Navya Jain"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Airlie Hilliard"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Submitted to NeurIPS 2024 Workshop on Behavioral Machine Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10227v1",
                "updated": "2024-09-16T12:23:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    23,
                    27,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T12:23:27Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    23,
                    27,
                    0,
                    260,
                    0
                ],
                "title": "Programmable multifunctional integrated microwave photonic circuit on\n  thin-film lithium niobate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programmable multifunctional integrated microwave photonic circuit on\n  thin-film lithium niobate"
                },
                "summary": "Microwave photonics, with its advanced high-frequency signal processing\ncapabilities, is expected to play a crucial role in next-generation wireless\ncommunications and radar systems. The realization of highly integrated,\nhigh-performance, and multifunctional microwave photonic links will pave the\nway for its widespread deployment in practical applications, which is a\nsignificant challenge. Here, leveraging thin-film lithium niobate intensity\nmodulator and programmable cascaded microring resonators, we demonstrate for\nthe first time a tunable microwave photonic notch filter that simultaneously\nachieves high level of integration along with high dynamic range, high link\ngain, low noise figure, and ultra-high rejection ratio. Additionally, this\nprogrammable on-chip system is multifunctional, allowing for the dual-band\nnotch filter and the suppression of the high-power interference signal. This\nwork demonstrates the potential applications of the thin-film lithium niobate\nplatform in the field of high-performance integrated microwave photonic\nfiltering and signal processing, facilitating the advancement of microwave\nphotonic system towards practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microwave photonics, with its advanced high-frequency signal processing\ncapabilities, is expected to play a crucial role in next-generation wireless\ncommunications and radar systems. The realization of highly integrated,\nhigh-performance, and multifunctional microwave photonic links will pave the\nway for its widespread deployment in practical applications, which is a\nsignificant challenge. Here, leveraging thin-film lithium niobate intensity\nmodulator and programmable cascaded microring resonators, we demonstrate for\nthe first time a tunable microwave photonic notch filter that simultaneously\nachieves high level of integration along with high dynamic range, high link\ngain, low noise figure, and ultra-high rejection ratio. Additionally, this\nprogrammable on-chip system is multifunctional, allowing for the dual-band\nnotch filter and the suppression of the high-power interference signal. This\nwork demonstrates the potential applications of the thin-film lithium niobate\nplatform in the field of high-performance integrated microwave photonic\nfiltering and signal processing, facilitating the advancement of microwave\nphotonic system towards practical applications."
                },
                "authors": [
                    {
                        "name": "Chuangchuang Wei"
                    },
                    {
                        "name": "Hanke Feng"
                    },
                    {
                        "name": "Kaixuan Ye"
                    },
                    {
                        "name": "Maarten Eijkel"
                    },
                    {
                        "name": "Yvan Klaver"
                    },
                    {
                        "name": "Zhaoxi Chen"
                    },
                    {
                        "name": "Akshay Keloth"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "David Marpaung"
                    }
                ],
                "author_detail": {
                    "name": "David Marpaung"
                },
                "arxiv_affiliation": "Nonlinear Nanophotonics Group, MESA+ Institute of Nanotechnology, University of Twente, Enschede, Netherlands",
                "author": "David Marpaung",
                "arxiv_comment": "18 pages, 8 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15512v2",
                "updated": "2024-09-16T12:02:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    2,
                    27,
                    0,
                    260,
                    0
                ],
                "published": "2024-08-28T03:48:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    3,
                    48,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Towards Fully Autonomous Research Powered by LLMs: Case Study on\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fully Autonomous Research Powered by LLMs: Case Study on\n  Simulations"
                },
                "summary": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research, spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLM, through sophisticated API\nintegration, to automate the entire research process, from experimental design,\nremote upload and simulation execution, data analysis, to report compilation.\nUsing a simulation problem of polymer chain conformations as a case study, we\nassessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless\nexecution on designated research missions, underscoring the potential of LLMs\nto manage complete scientific investigations autonomously. The outlined\nautomation can be iteratively performed up to twenty cycles without human\nintervention, illustrating the potential of LLMs for large-scale autonomous\nresearch endeavors. Additionally, we discussed the intrinsic traits of ASAs in\nmanaging extensive tasks, focusing on self-validation mechanisms and the\nbalance between local attention and global oversight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research, spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLM, through sophisticated API\nintegration, to automate the entire research process, from experimental design,\nremote upload and simulation execution, data analysis, to report compilation.\nUsing a simulation problem of polymer chain conformations as a case study, we\nassessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless\nexecution on designated research missions, underscoring the potential of LLMs\nto manage complete scientific investigations autonomously. The outlined\nautomation can be iteratively performed up to twenty cycles without human\nintervention, illustrating the potential of LLMs for large-scale autonomous\nresearch endeavors. Additionally, we discussed the intrinsic traits of ASAs in\nmanaging extensive tasks, focusing on self-validation mechanisms and the\nbalance between local attention and global oversight."
                },
                "authors": [
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Yubo Chai"
                    },
                    {
                        "name": "Jianfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Li"
                },
                "author": "Jianfeng Li",
                "arxiv_comment": "For additional code and data, please visit our GitHub repository:\n  https://github.com/zokaraa/autonomous_simulation_agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15051v3",
                "updated": "2024-09-16T12:01:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    12,
                    1,
                    22,
                    0,
                    260,
                    0
                ],
                "published": "2024-07-21T04:39:06Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    4,
                    39,
                    6,
                    6,
                    203,
                    0
                ],
                "title": "Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation\n  for Video Moment Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation\n  for Video Moment Retrieval"
                },
                "summary": "In this paper, we investigate the feasibility of leveraging large language\nmodels (LLMs) for integrating general knowledge and incorporating pseudo-events\nas priors for temporal content distribution in video moment retrieval (VMR)\nmodels. The motivation behind this study arises from the limitations of using\nLLMs as decoders for generating discrete textual descriptions, which hinders\ntheir direct application to continuous outputs like salience scores and\ninter-frame embeddings that capture inter-frame relations. To overcome these\nlimitations, we propose utilizing LLM encoders instead of decoders. Through a\nfeasibility study, we demonstrate that LLM encoders effectively refine\ninter-concept relations in multimodal embeddings, even without being trained on\ntextual embeddings. We also show that the refinement capability of LLM encoders\ncan be transferred to other embeddings, such as BLIP and T5, as long as these\nembeddings exhibit similar inter-concept similarity patterns to CLIP\nembeddings. We present a general framework for integrating LLM encoders into\nexisting VMR architectures, specifically within the fusion module. Through\nexperimental validation, we demonstrate the effectiveness of our proposed\nmethods by achieving state-of-the-art performance in VMR. The source code can\nbe accessed at https://github.com/fletcherjiang/LLMEPET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the feasibility of leveraging large language\nmodels (LLMs) for integrating general knowledge and incorporating pseudo-events\nas priors for temporal content distribution in video moment retrieval (VMR)\nmodels. The motivation behind this study arises from the limitations of using\nLLMs as decoders for generating discrete textual descriptions, which hinders\ntheir direct application to continuous outputs like salience scores and\ninter-frame embeddings that capture inter-frame relations. To overcome these\nlimitations, we propose utilizing LLM encoders instead of decoders. Through a\nfeasibility study, we demonstrate that LLM encoders effectively refine\ninter-concept relations in multimodal embeddings, even without being trained on\ntextual embeddings. We also show that the refinement capability of LLM encoders\ncan be transferred to other embeddings, such as BLIP and T5, as long as these\nembeddings exhibit similar inter-concept similarity patterns to CLIP\nembeddings. We present a general framework for integrating LLM encoders into\nexisting VMR architectures, specifically within the fusion module. Through\nexperimental validation, we demonstrate the effectiveness of our proposed\nmethods by achieving state-of-the-art performance in VMR. The source code can\nbe accessed at https://github.com/fletcherjiang/LLMEPET."
                },
                "authors": [
                    {
                        "name": "Yiyang Jiang"
                    },
                    {
                        "name": "Wengyu Zhang"
                    },
                    {
                        "name": "Xulu Zhang"
                    },
                    {
                        "name": "Xiaoyong Wei"
                    },
                    {
                        "name": "Chang Wen Chen"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_doi": "10.1145/3664647.3681115",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681115",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.15051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM Multimedia 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10204v1",
                "updated": "2024-09-16T11:55:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    55,
                    6,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:55:06Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    55,
                    6,
                    0,
                    260,
                    0
                ],
                "title": "Embedded Image-to-Image Translation for Efficient Sim-to-Real Transfer\n  in Learning-based Robot-Assisted Soft Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded Image-to-Image Translation for Efficient Sim-to-Real Transfer\n  in Learning-based Robot-Assisted Soft Manipulation"
                },
                "summary": "Recent advances in robotic learning in simulation have shown impressive\nresults in accelerating learning complex manipulation skills. However, the\nsim-to-real gap, caused by discrepancies between simulation and reality, poses\nsignificant challenges for the effective deployment of autonomous surgical\nsystems. We propose a novel approach utilizing image translation models to\nmitigate domain mismatches and facilitate efficient robot skill learning in a\nsimulated environment. Our method involves the use of contrastive unpaired\nImage-to-image translation, allowing for the acquisition of embedded\nrepresentations from these transformed images. Subsequently, these embeddings\nare used to improve the efficiency of training surgical manipulation models. We\nconducted experiments to evaluate the performance of our approach,\ndemonstrating that it significantly enhances task success rates and reduces the\nsteps required for task completion compared to traditional methods. The results\nindicate that our proposed system effectively bridges the sim-to-real gap,\nproviding a robust framework for advancing the autonomy of surgical robots in\nminimally invasive procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in robotic learning in simulation have shown impressive\nresults in accelerating learning complex manipulation skills. However, the\nsim-to-real gap, caused by discrepancies between simulation and reality, poses\nsignificant challenges for the effective deployment of autonomous surgical\nsystems. We propose a novel approach utilizing image translation models to\nmitigate domain mismatches and facilitate efficient robot skill learning in a\nsimulated environment. Our method involves the use of contrastive unpaired\nImage-to-image translation, allowing for the acquisition of embedded\nrepresentations from these transformed images. Subsequently, these embeddings\nare used to improve the efficiency of training surgical manipulation models. We\nconducted experiments to evaluate the performance of our approach,\ndemonstrating that it significantly enhances task success rates and reduces the\nsteps required for task completion compared to traditional methods. The results\nindicate that our proposed system effectively bridges the sim-to-real gap,\nproviding a robust framework for advancing the autonomy of surgical robots in\nminimally invasive procedures."
                },
                "authors": [
                    {
                        "name": "Jacinto Colan"
                    },
                    {
                        "name": "Keisuke Sugita"
                    },
                    {
                        "name": "Ana Davila"
                    },
                    {
                        "name": "Yutaro Yamada"
                    },
                    {
                        "name": "Yasuhisa Hasegawa"
                    }
                ],
                "author_detail": {
                    "name": "Yasuhisa Hasegawa"
                },
                "author": "Yasuhisa Hasegawa",
                "arxiv_comment": "Accepted at 2024 IEEE International Symposium on\n  Micro-NanoMechatronics and Human Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10192v1",
                "updated": "2024-09-16T11:35:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    35,
                    17,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:35:17Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    35,
                    17,
                    0,
                    260,
                    0
                ],
                "title": "PrePaMS: Privacy-Preserving Participant Management System for Studies\n  with Rewards and Prerequisites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrePaMS: Privacy-Preserving Participant Management System for Studies\n  with Rewards and Prerequisites"
                },
                "summary": "Taking part in surveys, experiments, and studies is often compensated by\nrewards to increase the number of participants and encourage attendance. While\nprivacy requirements are usually considered for participation, privacy aspects\nof the reward procedure are mostly ignored. To this end, we introduce PrePaMS,\nan efficient participation management system that supports prerequisite checks\nand participation rewards in a privacy-preserving way. Our system organizes\nparticipations with potential (dis-)qualifying dependencies and enables secure\nreward payoffs. By leveraging a set of proven cryptographic primitives and\nmechanisms such as anonymous credentials and zero-knowledge proofs,\nparticipations are protected so that service providers and organizers cannot\nderive the identity of participants even within the reward process. In this\npaper, we have designed and implemented a prototype of PrePaMS to show its\neffectiveness and evaluated its performance under realistic workloads. PrePaMS\ncovers the information whether subjects have participated in surveys,\nexperiments, or studies. When combined with other secure solutions for the\nactual data collection within these events, PrePaMS can represent a cornerstone\nfor more privacy-preserving empirical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taking part in surveys, experiments, and studies is often compensated by\nrewards to increase the number of participants and encourage attendance. While\nprivacy requirements are usually considered for participation, privacy aspects\nof the reward procedure are mostly ignored. To this end, we introduce PrePaMS,\nan efficient participation management system that supports prerequisite checks\nand participation rewards in a privacy-preserving way. Our system organizes\nparticipations with potential (dis-)qualifying dependencies and enables secure\nreward payoffs. By leveraging a set of proven cryptographic primitives and\nmechanisms such as anonymous credentials and zero-knowledge proofs,\nparticipations are protected so that service providers and organizers cannot\nderive the identity of participants even within the reward process. In this\npaper, we have designed and implemented a prototype of PrePaMS to show its\neffectiveness and evaluated its performance under realistic workloads. PrePaMS\ncovers the information whether subjects have participated in surveys,\nexperiments, or studies. When combined with other secure solutions for the\nactual data collection within these events, PrePaMS can represent a cornerstone\nfor more privacy-preserving empirical research."
                },
                "authors": [
                    {
                        "name": "Echo Meißner"
                    },
                    {
                        "name": "Frank Kargl"
                    },
                    {
                        "name": "Benjamin Erb"
                    },
                    {
                        "name": "Felix Engelmann"
                    }
                ],
                "author_detail": {
                    "name": "Felix Engelmann"
                },
                "author": "Felix Engelmann",
                "arxiv_comment": "Prototype source code: https://github.com/vs-uulm/prepams/ Public\n  test deployment: https://vs-uulm.github.io/prepams/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10191v1",
                "updated": "2024-09-16T11:34:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    34,
                    40,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:34:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    34,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "LLMs for clinical risk prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for clinical risk prediction"
                },
                "summary": "This study compares the efficacy of GPT-4 and clinalytix Medical AI in\npredicting the clinical risk of delirium development. Findings indicate that\nGPT-4 exhibited significant deficiencies in identifying positive cases and\nstruggled to provide reliable probability estimates for delirium risk, while\nclinalytix Medical AI demonstrated superior accuracy. A thorough analysis of\nthe large language model's (LLM) outputs elucidated potential causes for these\ndiscrepancies, consistent with limitations reported in extant literature. These\nresults underscore the challenges LLMs face in accurately diagnosing conditions\nand interpreting complex clinical data. While LLMs hold substantial potential\nin healthcare, they are currently unsuitable for independent clinical\ndecision-making. Instead, they should be employed in assistive roles,\ncomplementing clinical expertise. Continued human oversight remains essential\nto ensure optimal outcomes for both patients and healthcare providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study compares the efficacy of GPT-4 and clinalytix Medical AI in\npredicting the clinical risk of delirium development. Findings indicate that\nGPT-4 exhibited significant deficiencies in identifying positive cases and\nstruggled to provide reliable probability estimates for delirium risk, while\nclinalytix Medical AI demonstrated superior accuracy. A thorough analysis of\nthe large language model's (LLM) outputs elucidated potential causes for these\ndiscrepancies, consistent with limitations reported in extant literature. These\nresults underscore the challenges LLMs face in accurately diagnosing conditions\nand interpreting complex clinical data. While LLMs hold substantial potential\nin healthcare, they are currently unsuitable for independent clinical\ndecision-making. Instead, they should be employed in assistive roles,\ncomplementing clinical expertise. Continued human oversight remains essential\nto ensure optimal outcomes for both patients and healthcare providers."
                },
                "authors": [
                    {
                        "name": "Mohamed Rezk"
                    },
                    {
                        "name": "Patricia Cabanillas Silva"
                    },
                    {
                        "name": "Fried-Michael Dahlweid"
                    }
                ],
                "author_detail": {
                    "name": "Fried-Michael Dahlweid"
                },
                "author": "Fried-Michael Dahlweid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10188v1",
                "updated": "2024-09-16T11:30:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    30,
                    39,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:30:39Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    30,
                    39,
                    0,
                    260,
                    0
                ],
                "title": "Enhancing RL Safety with Counterfactual LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing RL Safety with Counterfactual LLM Reasoning"
                },
                "summary": "Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard\nto explain. We use counterfactual large language model reasoning to enhance RL\npolicy safety post-training. We show that our approach improves and helps to\nexplain the RL policy safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard\nto explain. We use counterfactual large language model reasoning to enhance RL\npolicy safety post-training. We show that our approach improves and helps to\nexplain the RL policy safety."
                },
                "authors": [
                    {
                        "name": "Dennis Gross"
                    },
                    {
                        "name": "Helge Spieker"
                    }
                ],
                "author_detail": {
                    "name": "Helge Spieker"
                },
                "author": "Helge Spieker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10178v1",
                "updated": "2024-09-16T11:17:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    17,
                    33,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:17:33Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    17,
                    33,
                    0,
                    260,
                    0
                ],
                "title": "ExelMap: Explainable Element-based HD-Map Change Detection and Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExelMap: Explainable Element-based HD-Map Change Detection and Update"
                },
                "summary": "Acquisition and maintenance are central problems in deploying high-definition\n(HD) maps for autonomous driving, with two lines of research prevalent in\ncurrent literature: Online HD map generation and HD map change detection.\nHowever, the generated map's quality is currently insufficient for safe\ndeployment, and many change detection approaches fail to precisely localize and\nextract the changed map elements, hence lacking explainability and hindering a\npotential fleet-based cooperative HD map update. In this paper, we propose the\nnovel task of explainable element-based HD map change detection and update. In\nextending recent approaches that use online mapping techniques informed with an\noutdated map prior for HD map updating, we present ExelMap, an explainable\nelement-based map updating strategy that specifically identifies changed map\nelements. In this context, we discuss how currently used metrics fail to\ncapture change detection performance, while allowing for unfair comparison\nbetween prior-less and prior-informed map generation methods. Finally, we\npresent an experimental study on real-world changes related to pedestrian\ncrossings of the Argoverse 2 Map Change Dataset. To the best of our knowledge,\nthis is the first comprehensive problem investigation of real-world end-to-end\nelement-based HD map change detection and update, and ExelMap the first\nproposed solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acquisition and maintenance are central problems in deploying high-definition\n(HD) maps for autonomous driving, with two lines of research prevalent in\ncurrent literature: Online HD map generation and HD map change detection.\nHowever, the generated map's quality is currently insufficient for safe\ndeployment, and many change detection approaches fail to precisely localize and\nextract the changed map elements, hence lacking explainability and hindering a\npotential fleet-based cooperative HD map update. In this paper, we propose the\nnovel task of explainable element-based HD map change detection and update. In\nextending recent approaches that use online mapping techniques informed with an\noutdated map prior for HD map updating, we present ExelMap, an explainable\nelement-based map updating strategy that specifically identifies changed map\nelements. In this context, we discuss how currently used metrics fail to\ncapture change detection performance, while allowing for unfair comparison\nbetween prior-less and prior-informed map generation methods. Finally, we\npresent an experimental study on real-world changes related to pedestrian\ncrossings of the Argoverse 2 Map Change Dataset. To the best of our knowledge,\nthis is the first comprehensive problem investigation of real-world end-to-end\nelement-based HD map change detection and update, and ExelMap the first\nproposed solution."
                },
                "authors": [
                    {
                        "name": "Lena Wild"
                    },
                    {
                        "name": "Ludvig Ericson"
                    },
                    {
                        "name": "Rafael Valencia"
                    },
                    {
                        "name": "Patric Jensfelt"
                    }
                ],
                "author_detail": {
                    "name": "Patric Jensfelt"
                },
                "author": "Patric Jensfelt",
                "arxiv_comment": "17 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10164v1",
                "updated": "2024-09-16T10:54:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    54,
                    4,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T10:54:04Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    54,
                    4,
                    0,
                    260,
                    0
                ],
                "title": "Quantile Regression for Distributional Reward Models in RLHF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile Regression for Distributional Reward Models in RLHF"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has become a key method for\naligning large language models (LLMs) with human preferences through the use of\nreward models. However, traditional reward models typically generate point\nestimates, which oversimplify the diversity and complexity of human values and\npreferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel\napproach to reward modeling that learns a distribution over rewards instead of\na single scalar value. Our method uses quantile regression to estimate a full,\npotentially multimodal distribution over preferences, providing a more powerful\nand nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and\naccommodates conflicting preferences by modeling them as distinct modes in the\ndistribution. Our experimental results show that QRM outperforms comparable\ntraditional point-estimate models on RewardBench. Furthermore, we demonstrate\nthat the additional information provided by the distributional estimates can be\nutilized in downstream applications, such as risk-aware reinforcement learning,\nresulting in LLM policies that generate fewer extremely negative responses. Our\ncode and model are released at https://github.com/Nicolinho/QRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has become a key method for\naligning large language models (LLMs) with human preferences through the use of\nreward models. However, traditional reward models typically generate point\nestimates, which oversimplify the diversity and complexity of human values and\npreferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel\napproach to reward modeling that learns a distribution over rewards instead of\na single scalar value. Our method uses quantile regression to estimate a full,\npotentially multimodal distribution over preferences, providing a more powerful\nand nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and\naccommodates conflicting preferences by modeling them as distinct modes in the\ndistribution. Our experimental results show that QRM outperforms comparable\ntraditional point-estimate models on RewardBench. Furthermore, we demonstrate\nthat the additional information provided by the distributional estimates can be\nutilized in downstream applications, such as risk-aware reinforcement learning,\nresulting in LLM policies that generate fewer extremely negative responses. Our\ncode and model are released at https://github.com/Nicolinho/QRM."
                },
                "authors": [
                    {
                        "name": "Nicolai Dorka"
                    }
                ],
                "author_detail": {
                    "name": "Nicolai Dorka"
                },
                "author": "Nicolai Dorka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10157v1",
                "updated": "2024-09-16T10:41:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    41,
                    36,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T10:41:36Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    41,
                    36,
                    0,
                    260,
                    0
                ],
                "title": "Emo-DPO: Controllable Emotional Speech Synthesis through Direct\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emo-DPO: Controllable Emotional Speech Synthesis through Direct\n  Preference Optimization"
                },
                "summary": "Current emotional text-to-speech (TTS) models predominantly conduct\nsupervised training to learn the conversion from text and desired emotion to\nits emotional speech, focusing on a single emotion per text-speech pair. These\nmodels only learn the correct emotional outputs without fully comprehending\nother emotion characteristics, which limits their capabilities of capturing the\nnuances between different emotions. We propose a controllable Emo-DPO approach,\nwhich employs direct preference optimization to differentiate subtle emotional\nnuances between emotions through optimizing towards preferred emotions over\nless preferred emotional ones. Instead of relying on traditional neural\narchitectures used in existing emotional TTS models, we propose utilizing the\nemotion-aware LLM-TTS neural architecture to leverage LLMs' in-context learning\nand instruction-following capabilities. Comprehensive experiments confirm that\nour proposed method outperforms the existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current emotional text-to-speech (TTS) models predominantly conduct\nsupervised training to learn the conversion from text and desired emotion to\nits emotional speech, focusing on a single emotion per text-speech pair. These\nmodels only learn the correct emotional outputs without fully comprehending\nother emotion characteristics, which limits their capabilities of capturing the\nnuances between different emotions. We propose a controllable Emo-DPO approach,\nwhich employs direct preference optimization to differentiate subtle emotional\nnuances between emotions through optimizing towards preferred emotions over\nless preferred emotional ones. Instead of relying on traditional neural\narchitectures used in existing emotional TTS models, we propose utilizing the\nemotion-aware LLM-TTS neural architecture to leverage LLMs' in-context learning\nand instruction-following capabilities. Comprehensive experiments confirm that\nour proposed method outperforms the existing baselines."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Huayun Zhang"
                    },
                    {
                        "name": "Nancy F. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Nancy F. Chen"
                },
                "author": "Nancy F. Chen",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10146v1",
                "updated": "2024-09-16T10:15:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    15,
                    30,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T10:15:30Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    15,
                    30,
                    0,
                    260,
                    0
                ],
                "title": "LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology\n  Learning Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology\n  Learning Challenge"
                },
                "summary": "This paper outlines the LLMs4OL 2024, the first edition of the Large Language\nModels for Ontology Learning Challenge. LLMs4OL is a community development\ninitiative collocated with the 23rd International Semantic Web Conference\n(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology\nLearning (OL), a vital process for enhancing the web with structured knowledge\nto improve interoperability. By leveraging LLMs, the challenge aims to advance\nunderstanding and innovation in OL, aligning with the goals of the Semantic Web\nto create a more intelligent and user-friendly web. In this paper, we give an\noverview of the 2024 edition of the LLMs4OL challenge and summarize the\ncontributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper outlines the LLMs4OL 2024, the first edition of the Large Language\nModels for Ontology Learning Challenge. LLMs4OL is a community development\ninitiative collocated with the 23rd International Semantic Web Conference\n(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology\nLearning (OL), a vital process for enhancing the web with structured knowledge\nto improve interoperability. By leveraging LLMs, the challenge aims to advance\nunderstanding and innovation in OL, aligning with the goals of the Semantic Web\nto create a more intelligent and user-friendly web. In this paper, we give an\noverview of the 2024 edition of the LLMs4OL challenge and summarize the\ncontributions."
                },
                "authors": [
                    {
                        "name": "Hamed Babaei Giglou"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Sören Auer"
                    }
                ],
                "author_detail": {
                    "name": "Sören Auer"
                },
                "author": "Sören Auer",
                "arxiv_comment": "15 pages, 1 figure, Will appear in \"The 1st LLMs4OL Challenge @ ISWC\n  2024\" proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04964v2",
                "updated": "2024-09-16T10:00:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    10,
                    0,
                    52,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-08T04:03:55Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    4,
                    3,
                    55,
                    6,
                    252,
                    0
                ],
                "title": "Evaluation of Google Translate for Mandarin Chinese translation using\n  sentiment and semantic analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Google Translate for Mandarin Chinese translation using\n  sentiment and semantic analysis"
                },
                "summary": "Machine translation using large language models (LLMs) is having a\nsignificant global impact, making communication easier. Mandarin Chinese is the\nofficial language used for communication by the government and media in China.\nIn this study, we provide an automated assessment of translation quality of\nGoogle Translate with human experts using sentiment and semantic analysis. In\norder to demonstrate our framework, we select the classic early\ntwentieth-century novel 'The True Story of Ah Q' with selected Mandarin Chinese\nto English translations. We use Google Translate to translate the given text\ninto English and then conduct a chapter-wise sentiment analysis and semantic\nanalysis to compare the extracted sentiments across the different translations.\nOur results indicate that the precision of Google Translate differs both in\nterms of semantic and sentiment analysis when compared to human expert\ntranslations. We find that Google Translate is unable to translate some of the\nspecific words or phrases in Chinese, such as Chinese traditional allusions.\nThe mistranslations may be due to lack of contextual significance and\nhistorical knowledge of China.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine translation using large language models (LLMs) is having a\nsignificant global impact, making communication easier. Mandarin Chinese is the\nofficial language used for communication by the government and media in China.\nIn this study, we provide an automated assessment of translation quality of\nGoogle Translate with human experts using sentiment and semantic analysis. In\norder to demonstrate our framework, we select the classic early\ntwentieth-century novel 'The True Story of Ah Q' with selected Mandarin Chinese\nto English translations. We use Google Translate to translate the given text\ninto English and then conduct a chapter-wise sentiment analysis and semantic\nanalysis to compare the extracted sentiments across the different translations.\nOur results indicate that the precision of Google Translate differs both in\nterms of semantic and sentiment analysis when compared to human expert\ntranslations. We find that Google Translate is unable to translate some of the\nspecific words or phrases in Chinese, such as Chinese traditional allusions.\nThe mistranslations may be due to lack of contextual significance and\nhistorical knowledge of China."
                },
                "authors": [
                    {
                        "name": "Xuechun Wang"
                    },
                    {
                        "name": "Rodney Beard"
                    },
                    {
                        "name": "Rohitash Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Rohitash Chandra"
                },
                "author": "Rohitash Chandra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07187v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07187v3",
                "updated": "2024-09-16T09:57:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    57,
                    35,
                    0,
                    260,
                    0
                ],
                "published": "2024-01-14T02:30:19Z",
                "published_parsed": [
                    2024,
                    1,
                    14,
                    2,
                    30,
                    19,
                    6,
                    14,
                    0
                ],
                "title": "A Survey on Statistical Theory of Deep Learning: Approximation, Training\n  Dynamics, and Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Statistical Theory of Deep Learning: Approximation, Training\n  Dynamics, and Generative Models"
                },
                "summary": "In this article, we review the literature on statistical theories of neural\nnetworks from three perspectives: approximation, training dynamics and\ngenerative models. In the first part, results on excess risks for neural\nnetworks are reviewed in the nonparametric framework of regression (and\nclassification in Appendix~{\\color{blue}B}). These results rely on explicit\nconstructions of neural networks, leading to fast convergence rates of excess\nrisks. Nonetheless, their underlying analysis only applies to the global\nminimizer in the highly non-convex landscape of deep neural networks. This\nmotivates us to review the training dynamics of neural networks in the second\npart. Specifically, we review papers that attempt to answer ``how the neural\nnetwork trained via gradient-based methods finds the solution that can\ngeneralize well on unseen data.'' In particular, two well-known paradigms are\nreviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF)\nparadigm. Last but not least, we review the most recent theoretical\nadvancements in generative models including Generative Adversarial Networks\n(GANs), diffusion models, and in-context learning (ICL) in the Large Language\nModels (LLMs) from two perpsectives reviewed previously, i.e., approximation\nand training dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we review the literature on statistical theories of neural\nnetworks from three perspectives: approximation, training dynamics and\ngenerative models. In the first part, results on excess risks for neural\nnetworks are reviewed in the nonparametric framework of regression (and\nclassification in Appendix~{\\color{blue}B}). These results rely on explicit\nconstructions of neural networks, leading to fast convergence rates of excess\nrisks. Nonetheless, their underlying analysis only applies to the global\nminimizer in the highly non-convex landscape of deep neural networks. This\nmotivates us to review the training dynamics of neural networks in the second\npart. Specifically, we review papers that attempt to answer ``how the neural\nnetwork trained via gradient-based methods finds the solution that can\ngeneralize well on unseen data.'' In particular, two well-known paradigms are\nreviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF)\nparadigm. Last but not least, we review the most recent theoretical\nadvancements in generative models including Generative Adversarial Networks\n(GANs), diffusion models, and in-context learning (ICL) in the Large Language\nModels (LLMs) from two perpsectives reviewed previously, i.e., approximation\nand training dynamics."
                },
                "authors": [
                    {
                        "name": "Namjoon Suh"
                    },
                    {
                        "name": "Guang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Guang Cheng"
                },
                "author": "Guang Cheng",
                "arxiv_comment": "38 pages, 2 figures. Invited for review in Annual Review of\n  Statistics and Its Application",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.07187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07187v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10132v1",
                "updated": "2024-09-16T09:48:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    48,
                    56,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T09:48:56Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    48,
                    56,
                    0,
                    260,
                    0
                ],
                "title": "StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge\n  Editing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge\n  Editing for Large Language Models"
                },
                "summary": "As the modern tool of choice for question answering, large language models\n(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve\nsuch ideal question-answering systems, locating and then editing outdated\nknowledge in the natural language outputs is a general target of popular\nknowledge editing methods. However, this target is challenging, as both\nidentifying which tokens to edit in the reasoning steps and ensuring the\ncoherence of the revised reasoning chain are difficult tasks. We argue that\nthese challenges stem from the unstructured nature of natural language outputs.\nTo address the above challenges, we propose $\\textbf{Stru}$ctural\n$\\textbf{Edit}$ing ($\\textbf{StruEdit}$), an improved baseline for knowledge\nediting. We first prompt LLMs to produce structured outputs consisting of\nreasoning triplets. Then, StruEdit removes any potentially outdated knowledge\nand efficiently refills the structured outputs with up-to-date information in a\nsingle step. Experimental results show that StruEdit consistently delivers the\nhighest accuracy with lowest latency compared with other knowledge editing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the modern tool of choice for question answering, large language models\n(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve\nsuch ideal question-answering systems, locating and then editing outdated\nknowledge in the natural language outputs is a general target of popular\nknowledge editing methods. However, this target is challenging, as both\nidentifying which tokens to edit in the reasoning steps and ensuring the\ncoherence of the revised reasoning chain are difficult tasks. We argue that\nthese challenges stem from the unstructured nature of natural language outputs.\nTo address the above challenges, we propose $\\textbf{Stru}$ctural\n$\\textbf{Edit}$ing ($\\textbf{StruEdit}$), an improved baseline for knowledge\nediting. We first prompt LLMs to produce structured outputs consisting of\nreasoning triplets. Then, StruEdit removes any potentially outdated knowledge\nand efficiently refills the structured outputs with up-to-date information in a\nsingle step. Experimental results show that StruEdit consistently delivers the\nhighest accuracy with lowest latency compared with other knowledge editing\nmethods."
                },
                "authors": [
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Hongcheng Gao"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04620v2",
                "updated": "2024-09-16T09:22:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    22,
                    20,
                    0,
                    260,
                    0
                ],
                "published": "2024-02-07T07:07:02Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    7,
                    7,
                    2,
                    2,
                    38,
                    0
                ],
                "title": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients"
                },
                "summary": "The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndevelop CataractBot, an experts-in-the-loop chatbot powered by LLMs, in\ncollaboration with an eye hospital in India. CataractBot answers cataract\nsurgery related questions instantly by querying a curated knowledge base, and\nprovides expert-verified responses asynchronously. It has multimodal and\nmultilingual capabilities. In an in-the-wild deployment study with 55\nparticipants, CataractBot proved valuable, providing anytime accessibility,\nsaving time, accommodating diverse literacy levels, alleviating power\ndifferences, and adding a privacy layer between patients and doctors. Users\nreported that their trust in the system was established through expert\nverification. Broadly, our results could inform future work on designing\nexpert-mediated LLM bots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndevelop CataractBot, an experts-in-the-loop chatbot powered by LLMs, in\ncollaboration with an eye hospital in India. CataractBot answers cataract\nsurgery related questions instantly by querying a curated knowledge base, and\nprovides expert-verified responses asynchronously. It has multimodal and\nmultilingual capabilities. In an in-the-wild deployment study with 55\nparticipants, CataractBot proved valuable, providing anytime accessibility,\nsaving time, accommodating diverse literacy levels, alleviating power\ndifferences, and adding a privacy layer between patients and doctors. Users\nreported that their trust in the system was established through expert\nverification. Broadly, our results could inform future work on designing\nexpert-mediated LLM bots."
                },
                "authors": [
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Satvik Golechha"
                    },
                    {
                        "name": "Shreyas Kulkarni"
                    },
                    {
                        "name": "Geeta Fulari"
                    },
                    {
                        "name": "Kaushik Murali"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10106v1",
                "updated": "2024-09-16T09:12:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    12,
                    6,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T09:12:06Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    12,
                    6,
                    0,
                    260,
                    0
                ],
                "title": "Industry 6.0: New Generation of Industry driven by Generative AI and\n  Swarm of Heterogeneous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industry 6.0: New Generation of Industry driven by Generative AI and\n  Swarm of Heterogeneous Robots"
                },
                "summary": "This paper presents the concept of Industry 6.0, introducing the world's\nfirst fully automated production system that autonomously handles the entire\nproduct design and manufacturing process based on user-provided natural\nlanguage descriptions. By leveraging generative AI, the system automates\ncritical aspects of production, including product blueprint design, component\nmanufacturing, logistics, and assembly. A heterogeneous swarm of robots, each\nequipped with individual AI through integration with Large Language Models\n(LLMs), orchestrates the production process. The robotic system includes\nmanipulator arms, delivery drones, and 3D printers capable of generating\nassembly blueprints. The system was evaluated using commercial and open-source\nLLMs, functioning through APIs and local deployment. A user study demonstrated\nthat the system reduces the average production time to 119.10 minutes,\nsignificantly outperforming a team of expert human developers, who averaged\n528.64 minutes (an improvement factor of 4.4). Furthermore, in the product\nblueprinting stage, the system surpassed human CAD operators by an\nunprecedented factor of 47, completing the task in 0.5 minutes compared to 23.5\nminutes. This breakthrough represents a major leap towards fully autonomous\nmanufacturing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the concept of Industry 6.0, introducing the world's\nfirst fully automated production system that autonomously handles the entire\nproduct design and manufacturing process based on user-provided natural\nlanguage descriptions. By leveraging generative AI, the system automates\ncritical aspects of production, including product blueprint design, component\nmanufacturing, logistics, and assembly. A heterogeneous swarm of robots, each\nequipped with individual AI through integration with Large Language Models\n(LLMs), orchestrates the production process. The robotic system includes\nmanipulator arms, delivery drones, and 3D printers capable of generating\nassembly blueprints. The system was evaluated using commercial and open-source\nLLMs, functioning through APIs and local deployment. A user study demonstrated\nthat the system reduces the average production time to 119.10 minutes,\nsignificantly outperforming a team of expert human developers, who averaged\n528.64 minutes (an improvement factor of 4.4). Furthermore, in the product\nblueprinting stage, the system surpassed human CAD operators by an\nunprecedented factor of 47, completing the task in 0.5 minutes compared to 23.5\nminutes. This breakthrough represents a major leap towards fully autonomous\nmanufacturing."
                },
                "authors": [
                    {
                        "name": "Artem Lykov"
                    },
                    {
                        "name": "Miguel Altamirano Cabrera"
                    },
                    {
                        "name": "Mikhail Konenkov"
                    },
                    {
                        "name": "Valerii Serpiva"
                    },
                    {
                        "name": "Koffivi Fid`ele Gbagbe"
                    },
                    {
                        "name": "Ali Alabbas"
                    },
                    {
                        "name": "Aleksey Fedoseev"
                    },
                    {
                        "name": "Luis Moreno"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Ziang Guo"
                    },
                    {
                        "name": "Dzmitry Tsetserukou"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Tsetserukou"
                },
                "author": "Dzmitry Tsetserukou",
                "arxiv_comment": "submitted to IEEE conf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10102v1",
                "updated": "2024-09-16T09:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    6,
                    44,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T09:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    9,
                    6,
                    44,
                    0,
                    260,
                    0
                ],
                "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal\nparadigm in the development of Large Language Models (LLMs). While much of the\ncurrent research in this field focuses on performance optimization,\nparticularly in terms of accuracy and efficiency, the trustworthiness of RAG\nsystems remains an area still under exploration. From a positive perspective,\nRAG systems are promising to enhance LLMs by providing them with useful and\nup-to-date knowledge from vast external databases, thereby mitigating the\nlong-standing problem of hallucination. While from a negative perspective, RAG\nsystems are at the risk of generating undesirable contents if the retrieved\ninformation is either inappropriate or poorly utilized. To address these\nconcerns, we propose a unified framework that assesses the trustworthiness of\nRAG systems across six key dimensions: factuality, robustness, fairness,\ntransparency, accountability, and privacy. Within this framework, we thoroughly\nreview the existing literature on each dimension. Additionally, we create the\nevaluation benchmark regarding the six dimensions and conduct comprehensive\nevaluations for a variety of proprietary and open-source models. Finally, we\nidentify the potential challenges for future research based on our\ninvestigation results. Through this work, we aim to lay a structured foundation\nfor future investigations and provide practical insights for enhancing the\ntrustworthiness of RAG systems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal\nparadigm in the development of Large Language Models (LLMs). While much of the\ncurrent research in this field focuses on performance optimization,\nparticularly in terms of accuracy and efficiency, the trustworthiness of RAG\nsystems remains an area still under exploration. From a positive perspective,\nRAG systems are promising to enhance LLMs by providing them with useful and\nup-to-date knowledge from vast external databases, thereby mitigating the\nlong-standing problem of hallucination. While from a negative perspective, RAG\nsystems are at the risk of generating undesirable contents if the retrieved\ninformation is either inappropriate or poorly utilized. To address these\nconcerns, we propose a unified framework that assesses the trustworthiness of\nRAG systems across six key dimensions: factuality, robustness, fairness,\ntransparency, accountability, and privacy. Within this framework, we thoroughly\nreview the existing literature on each dimension. Additionally, we create the\nevaluation benchmark regarding the six dimensions and conduct comprehensive\nevaluations for a variety of proprietary and open-source models. Finally, we\nidentify the potential challenges for future research based on our\ninvestigation results. Through this work, we aim to lay a structured foundation\nfor future investigations and provide practical insights for enhancing the\ntrustworthiness of RAG systems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Xiaoxi Li"
                    },
                    {
                        "name": "Jiajie Jin"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Chaozhuo Li"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tsung-Yi Ho"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10090v1",
                "updated": "2024-09-16T08:44:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    44,
                    17,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T08:44:17Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    44,
                    17,
                    0,
                    260,
                    0
                ],
                "title": "MotionCom: Automatic and Motion-Aware Image Composition with LLM and\n  Video Diffusion Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionCom: Automatic and Motion-Aware Image Composition with LLM and\n  Video Diffusion Prior"
                },
                "summary": "This work presents MotionCom, a training-free motion-aware diffusion based\nimage composition, enabling automatic and seamless integration of target\nobjects into new scenes with dynamically coherent results without finetuning or\noptimization. Traditional approaches in this area suffer from two significant\nlimitations: they require manual planning for object placement and often\ngenerate static compositions lacking motion realism. MotionCom addresses these\nissues by utilizing a Large Vision Language Model (LVLM) for intelligent\nplanning, and a Video Diffusion prior for motion-infused image synthesis,\nstreamlining the composition process. Our multi-modal Chain-of-Thought (CoT)\nprompting with LVLM automates the strategic placement planning of foreground\nobjects, considering their potential motion and interaction within the scenes.\nComplementing this, we propose a novel method MotionPaint to distill\nmotion-aware information from pretrained video diffusion models in the\ngeneration phase, ensuring that these objects are not only seamlessly\nintegrated but also endowed with realistic motion. Extensive quantitative and\nqualitative results highlight MotionCom's superiority, showcasing its\nefficiency in streamlining the planning process and its capability to produce\ncompositions that authentically depict motion and interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents MotionCom, a training-free motion-aware diffusion based\nimage composition, enabling automatic and seamless integration of target\nobjects into new scenes with dynamically coherent results without finetuning or\noptimization. Traditional approaches in this area suffer from two significant\nlimitations: they require manual planning for object placement and often\ngenerate static compositions lacking motion realism. MotionCom addresses these\nissues by utilizing a Large Vision Language Model (LVLM) for intelligent\nplanning, and a Video Diffusion prior for motion-infused image synthesis,\nstreamlining the composition process. Our multi-modal Chain-of-Thought (CoT)\nprompting with LVLM automates the strategic placement planning of foreground\nobjects, considering their potential motion and interaction within the scenes.\nComplementing this, we propose a novel method MotionPaint to distill\nmotion-aware information from pretrained video diffusion models in the\ngeneration phase, ensuring that these objects are not only seamlessly\nintegrated but also endowed with realistic motion. Extensive quantitative and\nqualitative results highlight MotionCom's superiority, showcasing its\nefficiency in streamlining the planning process and its capability to produce\ncompositions that authentically depict motion and interaction."
                },
                "authors": [
                    {
                        "name": "Weijing Tao"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Miaomiao Cui"
                    },
                    {
                        "name": "Guosheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guosheng Lin"
                },
                "author": "Guosheng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10081v1",
                "updated": "2024-09-16T08:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    37,
                    43,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T08:37:43Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    37,
                    43,
                    0,
                    260,
                    0
                ],
                "title": "Messy Code Makes Managing ML Pipelines Difficult? Just Let LLMs Rewrite\n  the Code!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Messy Code Makes Managing ML Pipelines Difficult? Just Let LLMs Rewrite\n  the Code!"
                },
                "summary": "Machine learning (ML) applications that learn from data are increasingly used\nto automate impactful decisions. Unfortunately, these applications often fall\nshort of adequately managing critical data and complying with upcoming\nregulations. A technical reason for the persistence of these issues is that the\ndata pipelines in common ML libraries and cloud services lack fundamental\ndeclarative, data-centric abstractions. Recent research has shown how such\nabstractions enable techniques like provenance tracking and automatic\ninspection to help manage ML pipelines. Unfortunately, these approaches lack\nadoption in the real world because they require clean ML pipeline code written\nwith declarative APIs, instead of the messy imperative Python code that data\nscientists typically write for data preparation.\n  We argue that it is unrealistic to expect data scientists to change their\nestablished development practices. Instead, we propose to circumvent this \"code\nabstraction gap\" by leveraging the code generation capabilities of large\nlanguage models (LLMs). Our idea is to rewrite messy data science code to a\ncustom-tailored declarative pipeline abstraction, which we implement as a\nproof-of-concept in our prototype Lester. We detail its application for a\nchallenging compliance management example involving \"incremental view\nmaintenance\" of deployed ML pipelines. The code rewrites for our running\nexample show the potential of LLMs to make messy data science code declarative,\ne.g., by identifying hand-coded joins in Python and turning them into joins on\ndataframes, or by generating declarative feature encoders from NumPy code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) applications that learn from data are increasingly used\nto automate impactful decisions. Unfortunately, these applications often fall\nshort of adequately managing critical data and complying with upcoming\nregulations. A technical reason for the persistence of these issues is that the\ndata pipelines in common ML libraries and cloud services lack fundamental\ndeclarative, data-centric abstractions. Recent research has shown how such\nabstractions enable techniques like provenance tracking and automatic\ninspection to help manage ML pipelines. Unfortunately, these approaches lack\nadoption in the real world because they require clean ML pipeline code written\nwith declarative APIs, instead of the messy imperative Python code that data\nscientists typically write for data preparation.\n  We argue that it is unrealistic to expect data scientists to change their\nestablished development practices. Instead, we propose to circumvent this \"code\nabstraction gap\" by leveraging the code generation capabilities of large\nlanguage models (LLMs). Our idea is to rewrite messy data science code to a\ncustom-tailored declarative pipeline abstraction, which we implement as a\nproof-of-concept in our prototype Lester. We detail its application for a\nchallenging compliance management example involving \"incremental view\nmaintenance\" of deployed ML pipelines. The code rewrites for our running\nexample show the potential of LLMs to make messy data science code declarative,\ne.g., by identifying hand-coded joins in Python and turning them into joins on\ndataframes, or by generating declarative feature encoders from NumPy code."
                },
                "authors": [
                    {
                        "name": "Sebastian Schelter"
                    },
                    {
                        "name": "Stefan Grafberger"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Grafberger"
                },
                "author": "Stefan Grafberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09576v2",
                "updated": "2024-09-16T08:35:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    35,
                    51,
                    0,
                    260,
                    0
                ],
                "published": "2024-04-15T08:37:26Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    8,
                    37,
                    26,
                    0,
                    106,
                    0
                ],
                "title": "Large language models and linguistic intentionality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models and linguistic intentionality"
                },
                "summary": "Do large language models like Chat-GPT or LLaMa meaningfully use the words\nthey produce? Or are they merely clever prediction machines, simulating\nlanguage use by producing statistically plausible text? There have already been\nsome initial attempts to answer this question by showing that these models meet\nthe criteria for entering meaningful states according to metasemantic theories\nof mental content. In this paper, I will argue for a different approach - that\nwe should instead consider whether language models meet the criteria given by\nour best metasemantic theories of linguistic content. In that vein, I will\nillustrate how this can be done by applying two such theories to the case of\nlanguage models: Gareth Evans' (1982) account of naming practices and Ruth\nMillikan's (1984, 2004, 2005) teleosemantics. In doing so, I will argue that it\nis a mistake to think that the failure of LLMs to meet plausible conditions for\nmental intentionality thereby renders their outputs meaningless, and that a\ndistinguishing feature of linguistic intentionality - dependency on a\npre-existing linguistic system - allows for the plausible result LLM outputs\nare meaningful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do large language models like Chat-GPT or LLaMa meaningfully use the words\nthey produce? Or are they merely clever prediction machines, simulating\nlanguage use by producing statistically plausible text? There have already been\nsome initial attempts to answer this question by showing that these models meet\nthe criteria for entering meaningful states according to metasemantic theories\nof mental content. In this paper, I will argue for a different approach - that\nwe should instead consider whether language models meet the criteria given by\nour best metasemantic theories of linguistic content. In that vein, I will\nillustrate how this can be done by applying two such theories to the case of\nlanguage models: Gareth Evans' (1982) account of naming practices and Ruth\nMillikan's (1984, 2004, 2005) teleosemantics. In doing so, I will argue that it\nis a mistake to think that the failure of LLMs to meet plausible conditions for\nmental intentionality thereby renders their outputs meaningless, and that a\ndistinguishing feature of linguistic intentionality - dependency on a\npre-existing linguistic system - allows for the plausible result LLM outputs\nare meaningful."
                },
                "authors": [
                    {
                        "name": "Jumbly Grindrod"
                    }
                ],
                "author_detail": {
                    "name": "Jumbly Grindrod"
                },
                "author": "Jumbly Grindrod",
                "arxiv_doi": "10.1007/s11229-024-04723-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11229-024-04723-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.09576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Synthese, Vol. 204: 71 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10077v1",
                "updated": "2024-09-16T08:28:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    28,
                    5,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T08:28:05Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    28,
                    5,
                    0,
                    260,
                    0
                ],
                "title": "LLM-DER:A Named Entity Recognition Method Based on Large Language Models\n  for Chinese Coal Chemical Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-DER:A Named Entity Recognition Method Based on Large Language Models\n  for Chinese Coal Chemical Domain"
                },
                "summary": "Domain-specific Named Entity Recognition (NER), whose goal is to recognize\ndomain-specific entities and their categories, provides an important support\nfor constructing domain knowledge graphs. Currently, deep learning-based\nmethods are widely used and effective in NER tasks, but due to the reliance on\nlarge-scale labeled data. As a result, the scarcity of labeled data in a\nspecific domain will limit its application.Therefore, many researches started\nto introduce few-shot methods and achieved some results. However, the entity\nstructures in specific domains are often complex, and the current few-shot\nmethods are difficult to adapt to NER tasks with complex features.Taking the\nChinese coal chemical industry domain as an example,there exists a complex\nstructure of multiple entities sharing a single entity, as well as multiple\nrelationships for the same pair of entities, which affects the NER task under\nthe sample less condition.In this paper, we propose a Large Language Models\n(LLMs)-based entity recognition framework LLM-DER for the domain-specific\nentity recognition problem in Chinese, which enriches the entity information by\ngenerating a list of relationships containing entity types through LLMs, and\ndesigning a plausibility and consistency evaluation method to remove\nmisrecognized entities, which can effectively solve the complex structural\nentity recognition problem in a specific domain.The experimental results of\nthis paper on the Resume dataset and the self-constructed coal chemical dataset\nCoal show that LLM-DER performs outstandingly in domain-specific entity\nrecognition, not only outperforming the existing GPT-3.5-turbo baseline, but\nalso exceeding the fully-supervised baseline, verifying its effectiveness in\nentity recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-specific Named Entity Recognition (NER), whose goal is to recognize\ndomain-specific entities and their categories, provides an important support\nfor constructing domain knowledge graphs. Currently, deep learning-based\nmethods are widely used and effective in NER tasks, but due to the reliance on\nlarge-scale labeled data. As a result, the scarcity of labeled data in a\nspecific domain will limit its application.Therefore, many researches started\nto introduce few-shot methods and achieved some results. However, the entity\nstructures in specific domains are often complex, and the current few-shot\nmethods are difficult to adapt to NER tasks with complex features.Taking the\nChinese coal chemical industry domain as an example,there exists a complex\nstructure of multiple entities sharing a single entity, as well as multiple\nrelationships for the same pair of entities, which affects the NER task under\nthe sample less condition.In this paper, we propose a Large Language Models\n(LLMs)-based entity recognition framework LLM-DER for the domain-specific\nentity recognition problem in Chinese, which enriches the entity information by\ngenerating a list of relationships containing entity types through LLMs, and\ndesigning a plausibility and consistency evaluation method to remove\nmisrecognized entities, which can effectively solve the complex structural\nentity recognition problem in a specific domain.The experimental results of\nthis paper on the Resume dataset and the self-constructed coal chemical dataset\nCoal show that LLM-DER performs outstandingly in domain-specific entity\nrecognition, not only outperforming the existing GPT-3.5-turbo baseline, but\nalso exceeding the fully-supervised baseline, verifying its effectiveness in\nentity recognition."
                },
                "authors": [
                    {
                        "name": "Le Xiao"
                    },
                    {
                        "name": "Yunfei Xu"
                    },
                    {
                        "name": "Jing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhao"
                },
                "author": "Jing Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10071v1",
                "updated": "2024-09-16T08:21:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    21,
                    22,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T08:21:22Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    21,
                    22,
                    0,
                    260,
                    0
                ],
                "title": "Towards Physically-Realizable Adversarial Attacks in Embodied Vision\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Physically-Realizable Adversarial Attacks in Embodied Vision\n  Navigation"
                },
                "summary": "The deployment of embodied navigation agents in safety-critical environments\nraises concerns about their vulnerability to adversarial attacks on deep neural\nnetworks. However, current attack methods often lack practicality due to\nchallenges in transitioning from the digital to the physical world, while\nexisting physical attacks for object detection fail to achieve both multi-view\neffectiveness and naturalness. To address this, we propose a practical attack\nmethod for embodied navigation by attaching adversarial patches with learnable\ntextures and opacity to objects. Specifically, to ensure effectiveness across\nvarying viewpoints, we employ a multi-view optimization strategy based on\nobject-aware sampling, which uses feedback from the navigation model to\noptimize the patch's texture. To make the patch inconspicuous to human\nobservers, we introduce a two-stage opacity optimization mechanism, where\nopacity is refined after texture optimization. Experimental results show our\nadversarial patches reduce navigation success rates by about 40%, outperforming\nprevious methods in practicality, effectiveness, and naturalness. Code is\navailable at:\n[https://github.com/chen37058/Physical-Attacks-in-Embodied-Navigation].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of embodied navigation agents in safety-critical environments\nraises concerns about their vulnerability to adversarial attacks on deep neural\nnetworks. However, current attack methods often lack practicality due to\nchallenges in transitioning from the digital to the physical world, while\nexisting physical attacks for object detection fail to achieve both multi-view\neffectiveness and naturalness. To address this, we propose a practical attack\nmethod for embodied navigation by attaching adversarial patches with learnable\ntextures and opacity to objects. Specifically, to ensure effectiveness across\nvarying viewpoints, we employ a multi-view optimization strategy based on\nobject-aware sampling, which uses feedback from the navigation model to\noptimize the patch's texture. To make the patch inconspicuous to human\nobservers, we introduce a two-stage opacity optimization mechanism, where\nopacity is refined after texture optimization. Experimental results show our\nadversarial patches reduce navigation success rates by about 40%, outperforming\nprevious methods in practicality, effectiveness, and naturalness. Code is\navailable at:\n[https://github.com/chen37058/Physical-Attacks-in-Embodied-Navigation]."
                },
                "authors": [
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Jiawei Tu"
                    },
                    {
                        "name": "Chao Qi"
                    },
                    {
                        "name": "Yonghao Dang"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Jianqin Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianqin Yin"
                },
                "author": "Jianqin Yin",
                "arxiv_comment": "8 pages, 6 figures, submitted to the 2025 IEEE International\n  Conference on Robotics & Automation (ICRA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10066v1",
                "updated": "2024-09-16T08:01:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    1,
                    21,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T08:01:21Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    8,
                    1,
                    21,
                    0,
                    260,
                    0
                ],
                "title": "LeGEND: A Top-Down Approach to Scenario Generation of Autonomous Driving\n  Systems Assisted by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeGEND: A Top-Down Approach to Scenario Generation of Autonomous Driving\n  Systems Assisted by Large Language Models"
                },
                "summary": "Autonomous driving systems (ADS) are safety-critical and require\ncomprehensive testing before their deployment on public roads. While existing\ntesting approaches primarily aim at the criticality of scenarios, they often\noverlook the diversity of the generated scenarios that is also important to\nreflect system defects in different aspects. To bridge the gap, we propose\nLeGEND, that features a top-down fashion of scenario generation: it starts with\nabstract functional scenarios, and then steps downwards to logical and concrete\nscenarios, such that scenario diversity can be controlled at the functional\nlevel. However, unlike logical scenarios that can be formally described,\nfunctional scenarios are often documented in natural languages (e.g., accident\nreports) and thus cannot be precisely parsed and processed by computers. To\ntackle that issue, LeGEND leverages the recent advances of large language\nmodels (LLMs) to transform textual functional scenarios to formal logical\nscenarios. To mitigate the distraction of useless information in functional\nscenario description, we devise a two-phase transformation that features the\nuse of an intermediate language; consequently, we adopt two LLMs in LeGEND, one\nfor extracting information from functional scenarios, the other for converting\nthe extracted information to formal logical scenarios. We experimentally\nevaluate LeGEND on Apollo, an industry-grade ADS from Baidu. Evaluation results\nshow that LeGEND can effectively identify critical scenarios, and compared to\nbaseline approaches, LeGEND exhibits evident superiority in diversity of\ngenerated scenarios. Moreover, we also demonstrate the advantages of our\ntwo-phase transformation framework, and the accuracy of the adopted LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems (ADS) are safety-critical and require\ncomprehensive testing before their deployment on public roads. While existing\ntesting approaches primarily aim at the criticality of scenarios, they often\noverlook the diversity of the generated scenarios that is also important to\nreflect system defects in different aspects. To bridge the gap, we propose\nLeGEND, that features a top-down fashion of scenario generation: it starts with\nabstract functional scenarios, and then steps downwards to logical and concrete\nscenarios, such that scenario diversity can be controlled at the functional\nlevel. However, unlike logical scenarios that can be formally described,\nfunctional scenarios are often documented in natural languages (e.g., accident\nreports) and thus cannot be precisely parsed and processed by computers. To\ntackle that issue, LeGEND leverages the recent advances of large language\nmodels (LLMs) to transform textual functional scenarios to formal logical\nscenarios. To mitigate the distraction of useless information in functional\nscenario description, we devise a two-phase transformation that features the\nuse of an intermediate language; consequently, we adopt two LLMs in LeGEND, one\nfor extracting information from functional scenarios, the other for converting\nthe extracted information to formal logical scenarios. We experimentally\nevaluate LeGEND on Apollo, an industry-grade ADS from Baidu. Evaluation results\nshow that LeGEND can effectively identify critical scenarios, and compared to\nbaseline approaches, LeGEND exhibits evident superiority in diversity of\ngenerated scenarios. Moreover, we also demonstrate the advantages of our\ntwo-phase transformation framework, and the accuracy of the adopted LLMs."
                },
                "authors": [
                    {
                        "name": "Shuncheng Tang"
                    },
                    {
                        "name": "Zhenya Zhang"
                    },
                    {
                        "name": "Jixiang Zhou"
                    },
                    {
                        "name": "Lei Lei"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Yinxing Xue"
                    }
                ],
                "author_detail": {
                    "name": "Yinxing Xue"
                },
                "author": "Yinxing Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10064v1",
                "updated": "2024-09-16T07:58:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    58,
                    56,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:58:56Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    58,
                    56,
                    0,
                    260,
                    0
                ],
                "title": "MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid\n  via Edge LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid\n  via Edge LLM"
                },
                "summary": "Mental health disorders are among the most prevalent diseases worldwide,\naffecting nearly one in four people. Despite their widespread impact, the\nintervention rate remains below 25%, largely due to the significant cooperation\nrequired from patients for both diagnosis and intervention. The core issue\nbehind this low treatment rate is stigma, which discourages over half of those\naffected from seeking help. This paper presents MindGuard, an accessible,\nstigma-free, and professional mobile mental healthcare system designed to\nprovide mental health first aid. The heart of MindGuard is an innovative edge\nLLM, equipped with professional mental health knowledge, that seamlessly\nintegrates objective mobile sensor data with subjective Ecological Momentary\nAssessment records to deliver personalized screening and intervention\nconversations. We conduct a broad evaluation of MindGuard using open datasets\nspanning four years and real-world deployment across various mobile devices\ninvolving 20 subjects for two weeks. Remarkably, MindGuard achieves results\ncomparable to GPT-4 and outperforms its counterpart with more than 10 times the\nmodel size. We believe that MindGuard paves the way for mobile LLM\napplications, potentially revolutionizing mental healthcare practices by\nsubstituting self-reporting and intervention conversations with passive,\nintegrated monitoring within daily life, thus ensuring accessible and\nstigma-free mental health support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health disorders are among the most prevalent diseases worldwide,\naffecting nearly one in four people. Despite their widespread impact, the\nintervention rate remains below 25%, largely due to the significant cooperation\nrequired from patients for both diagnosis and intervention. The core issue\nbehind this low treatment rate is stigma, which discourages over half of those\naffected from seeking help. This paper presents MindGuard, an accessible,\nstigma-free, and professional mobile mental healthcare system designed to\nprovide mental health first aid. The heart of MindGuard is an innovative edge\nLLM, equipped with professional mental health knowledge, that seamlessly\nintegrates objective mobile sensor data with subjective Ecological Momentary\nAssessment records to deliver personalized screening and intervention\nconversations. We conduct a broad evaluation of MindGuard using open datasets\nspanning four years and real-world deployment across various mobile devices\ninvolving 20 subjects for two weeks. Remarkably, MindGuard achieves results\ncomparable to GPT-4 and outperforms its counterpart with more than 10 times the\nmodel size. We believe that MindGuard paves the way for mobile LLM\napplications, potentially revolutionizing mental healthcare practices by\nsubstituting self-reporting and intervention conversations with passive,\nintegrated monitoring within daily life, thus ensuring accessible and\nstigma-free mental health support."
                },
                "authors": [
                    {
                        "name": "Sijie Ji"
                    },
                    {
                        "name": "Xinzhe Zheng"
                    },
                    {
                        "name": "Jiawei Sun"
                    },
                    {
                        "name": "Renqi Chen"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Mani Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Mani Srivastava"
                },
                "author": "Mani Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10053v1",
                "updated": "2024-09-16T07:29:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    29,
                    40,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:29:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    29,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in\n  LLMs with Direction-Magnitude Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Householder Pseudo-Rotation: A Novel Approach to Activation Editing in\n  LLMs with Direction-Magnitude Perspective"
                },
                "summary": "Activation Editing, which involves directly editting the internal\nrepresentations of large language models (LLMs) to alter their behaviors and\nachieve desired properties, has emerged as a promising area of research.\nExisting works primarily treat LLMs' activations as points in space and modify\nthem by adding steering vectors. However, this approach is limited in its\nability to achieve greater performance improvement while maintaining the\nnecessary consistency of activation magnitudes. To overcome these issues, we\npropose a novel editing method that views activations in terms of their\ndirections and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),\nmimics the rotation transformation, thus preserving activation norms and\nresulting in an improved performance on various safety benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Editing, which involves directly editting the internal\nrepresentations of large language models (LLMs) to alter their behaviors and\nachieve desired properties, has emerged as a promising area of research.\nExisting works primarily treat LLMs' activations as points in space and modify\nthem by adding steering vectors. However, this approach is limited in its\nability to achieve greater performance improvement while maintaining the\nnecessary consistency of activation magnitudes. To overcome these issues, we\npropose a novel editing method that views activations in terms of their\ndirections and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),\nmimics the rotation transformation, thus preserving activation norms and\nresulting in an improved performance on various safety benchmarks."
                },
                "authors": [
                    {
                        "name": "Van-Cuong Pham"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10049v1",
                "updated": "2024-09-16T07:21:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    21,
                    18,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:21:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    21,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "A Social Force Model for Multi-Agent Systems With Application to Robots\n  Traversal in Cluttered Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Social Force Model for Multi-Agent Systems With Application to Robots\n  Traversal in Cluttered Environments"
                },
                "summary": "This letter presents a model to address the collaborative effects in\nmulti-agent systems from the perspective of microscopic mechanism. The model\nutilizes distributed control for robot swarms in traversal applications.\nInspired by pedestrian planning dynamics, the model employs three types of\nforces to regulate the behavior of agents: intrinsic propulsion, interaction\namong agents, and repulsion from obstacles. These forces are able to balance\nthe convergence, divergence and avoidance effects among agents. Additionally,\nwe present a planning and decision method based on resultant forces to enable\nreal-world deployment of the model. Experimental results demonstrate the\neffectiveness on system path optimization in unknown cluttered environments.\nThe sensor data is swiftly digital filtered and the data transmitted is\nsignificantly compressed. Consequently, the model has low computation costs and\nminimal communication loads, thereby promoting environmental adaptability and\nsystem scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This letter presents a model to address the collaborative effects in\nmulti-agent systems from the perspective of microscopic mechanism. The model\nutilizes distributed control for robot swarms in traversal applications.\nInspired by pedestrian planning dynamics, the model employs three types of\nforces to regulate the behavior of agents: intrinsic propulsion, interaction\namong agents, and repulsion from obstacles. These forces are able to balance\nthe convergence, divergence and avoidance effects among agents. Additionally,\nwe present a planning and decision method based on resultant forces to enable\nreal-world deployment of the model. Experimental results demonstrate the\neffectiveness on system path optimization in unknown cluttered environments.\nThe sensor data is swiftly digital filtered and the data transmitted is\nsignificantly compressed. Consequently, the model has low computation costs and\nminimal communication loads, thereby promoting environmental adaptability and\nsystem scalability."
                },
                "authors": [
                    {
                        "name": "Chenxi Li"
                    },
                    {
                        "name": "Weining Lu"
                    },
                    {
                        "name": "Qingquan Lin"
                    },
                    {
                        "name": "Litong Meng"
                    },
                    {
                        "name": "Haolu Li"
                    },
                    {
                        "name": "Bin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Liang"
                },
                "author": "Bin Liang",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10058v2",
                "updated": "2024-09-16T07:20:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    20,
                    13,
                    0,
                    260,
                    0
                ],
                "published": "2024-07-14T03:05:53Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    3,
                    5,
                    53,
                    6,
                    196,
                    0
                ],
                "title": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Refuse: Towards Mitigating Privacy Risks in LLMs"
                },
                "summary": "Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities."
                },
                "authors": [
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Chuanyuan Tan"
                    },
                    {
                        "name": "Wenliang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenliang Chen"
                },
                "author": "Wenliang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10044v1",
                "updated": "2024-09-16T07:13:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    13,
                    30,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:13:30Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    13,
                    30,
                    0,
                    260,
                    0
                ],
                "title": "Benchmarking Large Language Model Uncertainty for Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Model Uncertainty for Prompt Optimization"
                },
                "summary": "Prompt optimization algorithms for Large Language Models (LLMs) excel in\nmulti-step reasoning but still lack effective uncertainty estimation. This\npaper introduces a benchmark dataset to evaluate uncertainty metrics, focusing\non Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis\nof models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that\ncurrent metrics align more with Answer Uncertainty, which reflects output\nconfidence and diversity, rather than Correctness Uncertainty, highlighting the\nneed for improved metrics that are optimization-objective-aware to better guide\nprompt optimization. Our code and dataset are available at\nhttps://github.com/0Frett/PO-Uncertainty-Benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization algorithms for Large Language Models (LLMs) excel in\nmulti-step reasoning but still lack effective uncertainty estimation. This\npaper introduces a benchmark dataset to evaluate uncertainty metrics, focusing\non Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis\nof models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that\ncurrent metrics align more with Answer Uncertainty, which reflects output\nconfidence and diversity, rather than Correctness Uncertainty, highlighting the\nneed for improved metrics that are optimization-objective-aware to better guide\nprompt optimization. Our code and dataset are available at\nhttps://github.com/0Frett/PO-Uncertainty-Benchmarking."
                },
                "authors": [
                    {
                        "name": "Pei-Fu Guo"
                    },
                    {
                        "name": "Yun-Da Tsai"
                    },
                    {
                        "name": "Shou-De Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shou-De Lin"
                },
                "author": "Shou-De Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10918v4",
                "updated": "2024-09-16T07:12:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    12,
                    12,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-16T12:46:40Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    12,
                    46,
                    40,
                    6,
                    168,
                    0
                ],
                "title": "Central Answer Modeling for an Embodied Multi-LLM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Central Answer Modeling for an Embodied Multi-LLM System"
                },
                "summary": "Embodied Question Answering (EQA) is an important problem, which involves an\nagent exploring the environment to answer user queries. In the existing\nliterature, EQA has exclusively been studied in single-agent scenarios, where\nexploration can be time-consuming and costly. In this work, we consider EQA in\na multi-agent framework involving multiple large language models (LLM) based\nagents independently answering queries about a household environment. To\ngenerate one answer for each query, we use the individual responses to train a\nCentral Answer Model (CAM) that aggregates responses for a robust answer. While\nprior Question Answering (QA) work has used a central module based on answers\nfrom multiple LLM-based experts, we specifically look at applying this\nframework to embodied LLM-based agents that must physically explore the\nenvironment first to become experts on their given environment to answer\nquestions. Our work is the first to utilize a central answer model framework\nwith embodied agents that must rely on exploring an unknown environment. We set\nup a variation of EQA where instead of the agents exploring the environment\nafter the question is asked, the agents first explore the environment for a set\namount of time and then answer a set of queries. Using CAM, we observe a $46\\%$\nhigher EQA accuracy when compared against aggregation methods for ensemble LLM,\nsuch as voting schemes and debates. CAM does not require any form of agent\ncommunication, alleviating it from the associated costs. We ablate CAM with\nvarious nonlinear (neural network, random forest, decision tree, XGBoost) and\nlinear (logistic regression classifier, SVM) algorithms. We experiment in\nvarious topological graph environments and examine the case where one of the\nagents is malicious and purposes contribute responses it believes to be wrong.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Question Answering (EQA) is an important problem, which involves an\nagent exploring the environment to answer user queries. In the existing\nliterature, EQA has exclusively been studied in single-agent scenarios, where\nexploration can be time-consuming and costly. In this work, we consider EQA in\na multi-agent framework involving multiple large language models (LLM) based\nagents independently answering queries about a household environment. To\ngenerate one answer for each query, we use the individual responses to train a\nCentral Answer Model (CAM) that aggregates responses for a robust answer. While\nprior Question Answering (QA) work has used a central module based on answers\nfrom multiple LLM-based experts, we specifically look at applying this\nframework to embodied LLM-based agents that must physically explore the\nenvironment first to become experts on their given environment to answer\nquestions. Our work is the first to utilize a central answer model framework\nwith embodied agents that must rely on exploring an unknown environment. We set\nup a variation of EQA where instead of the agents exploring the environment\nafter the question is asked, the agents first explore the environment for a set\namount of time and then answer a set of queries. Using CAM, we observe a $46\\%$\nhigher EQA accuracy when compared against aggregation methods for ensemble LLM,\nsuch as voting schemes and debates. CAM does not require any form of agent\ncommunication, alleviating it from the associated costs. We ablate CAM with\nvarious nonlinear (neural network, random forest, decision tree, XGBoost) and\nlinear (logistic regression classifier, SVM) algorithms. We experiment in\nvarious topological graph environments and examine the case where one of the\nagents is malicious and purposes contribute responses it believes to be wrong."
                },
                "authors": [
                    {
                        "name": "Bhrij Patel"
                    },
                    {
                        "name": "Vishnu Sashank Dorbala"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Dinesh Manocha"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Manocha"
                },
                "author": "Dinesh Manocha",
                "arxiv_comment": "15 pages, 11 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10042v1",
                "updated": "2024-09-16T07:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    12,
                    4,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:12:04Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    12,
                    4,
                    0,
                    260,
                    0
                ],
                "title": "Cross: A Delay Based Congestion Control Method for RTP Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross: A Delay Based Congestion Control Method for RTP Media"
                },
                "summary": "After more than a decade of development, real time communication (RTC) for\nvideo telephony has made significantly progress. However, emerging high-quality\nRTC applications with high definition and high frame rate requires sufficient\nbandwidth. The default congestion control mechanism specifically tuned for\nvideo telephony leaves plenty of room for optimization under high-rate\nscenarios. It is necessary to develop new rate control solutions to utilize\nbandwidth efficiently and to provide better experience for such services. A\ndelay-based congestion control method called Cross is proposed, which regulates\nrate based on queue load with a multiplicative increase and multiplicative\ndecrease fashion. A simulation module is developed to validate the\neffectiveness of these congestion control algorithms for RTC services. The\nmodule is released with the hope to provide convenience for RTC research\ncommunity. Simulation results demonstrate that Cross can achieve low queuing\ndelay and maintain high channel utilization under random loss environments.\nOnline deployment shows that Cross can reduce the video freezing ratio by up to\n58.45\\% on average when compared with a benchmark algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "After more than a decade of development, real time communication (RTC) for\nvideo telephony has made significantly progress. However, emerging high-quality\nRTC applications with high definition and high frame rate requires sufficient\nbandwidth. The default congestion control mechanism specifically tuned for\nvideo telephony leaves plenty of room for optimization under high-rate\nscenarios. It is necessary to develop new rate control solutions to utilize\nbandwidth efficiently and to provide better experience for such services. A\ndelay-based congestion control method called Cross is proposed, which regulates\nrate based on queue load with a multiplicative increase and multiplicative\ndecrease fashion. A simulation module is developed to validate the\neffectiveness of these congestion control algorithms for RTC services. The\nmodule is released with the hope to provide convenience for RTC research\ncommunity. Simulation results demonstrate that Cross can achieve low queuing\ndelay and maintain high channel utilization under random loss environments.\nOnline deployment shows that Cross can reduce the video freezing ratio by up to\n58.45\\% on average when compared with a benchmark algorithm."
                },
                "authors": [
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Changpeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Changpeng Yang"
                },
                "author": "Changpeng Yang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10038v1",
                "updated": "2024-09-16T07:01:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    1,
                    41,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T07:01:41Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    7,
                    1,
                    41,
                    0,
                    260,
                    0
                ],
                "title": "On the Diagram of Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Diagram of Thought"
                },
                "summary": "We introduce Diagram of Thought (DoT), a framework that models iterative\nreasoning in large language models (LLMs) as the construction of a directed\nacyclic graph (DAG) within a single model. Unlike traditional approaches that\nrepresent reasoning as linear chains or trees, DoT organizes propositions,\ncritiques, refinements, and verifications into a cohesive DAG structure,\nallowing the model to explore complex reasoning pathways while maintaining\nlogical consistency. Each node in the diagram corresponds to a proposition that\nhas been proposed, critiqued, refined, or verified, enabling the LLM to\niteratively improve its reasoning through natural language feedback. By\nleveraging auto-regressive next-token prediction with role-specific tokens, DoT\nfacilitates seamless transitions between proposing ideas and critically\nevaluating them, providing richer feedback than binary signals. Furthermore, we\nformalize the DoT framework using Topos Theory, providing a mathematical\nfoundation that ensures logical consistency and soundness in the reasoning\nprocess. This approach enhances both the training and inference processes\nwithin a single LLM, eliminating the need for multiple models or external\ncontrol mechanisms. DoT offers a conceptual framework for designing\nnext-generation reasoning-specialized models, emphasizing training efficiency,\nrobust reasoning capabilities, and theoretical grounding. The code is available\nat https://github.com/diagram-of-thought/diagram-of-thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Diagram of Thought (DoT), a framework that models iterative\nreasoning in large language models (LLMs) as the construction of a directed\nacyclic graph (DAG) within a single model. Unlike traditional approaches that\nrepresent reasoning as linear chains or trees, DoT organizes propositions,\ncritiques, refinements, and verifications into a cohesive DAG structure,\nallowing the model to explore complex reasoning pathways while maintaining\nlogical consistency. Each node in the diagram corresponds to a proposition that\nhas been proposed, critiqued, refined, or verified, enabling the LLM to\niteratively improve its reasoning through natural language feedback. By\nleveraging auto-regressive next-token prediction with role-specific tokens, DoT\nfacilitates seamless transitions between proposing ideas and critically\nevaluating them, providing richer feedback than binary signals. Furthermore, we\nformalize the DoT framework using Topos Theory, providing a mathematical\nfoundation that ensures logical consistency and soundness in the reasoning\nprocess. This approach enhances both the training and inference processes\nwithin a single LLM, eliminating the need for multiple models or external\ncontrol mechanisms. DoT offers a conceptual framework for designing\nnext-generation reasoning-specialized models, emphasizing training efficiency,\nrobust reasoning capabilities, and theoretical grounding. The code is available\nat https://github.com/diagram-of-thought/diagram-of-thought."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10027v1",
                "updated": "2024-09-16T06:35:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    35,
                    18,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T06:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    35,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models"
                },
                "summary": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/."
                },
                "authors": [
                    {
                        "name": "Chan Kim"
                    },
                    {
                        "name": "Keonwoo Kim"
                    },
                    {
                        "name": "Mintaek Oh"
                    },
                    {
                        "name": "Hanbi Baek"
                    },
                    {
                        "name": "Jiyang Lee"
                    },
                    {
                        "name": "Donghwi Jung"
                    },
                    {
                        "name": "Soojin Woo"
                    },
                    {
                        "name": "Younkyung Woo"
                    },
                    {
                        "name": "John Tucker"
                    },
                    {
                        "name": "Roya Firoozi"
                    },
                    {
                        "name": "Seung-Woo Seo"
                    },
                    {
                        "name": "Mac Schwager"
                    },
                    {
                        "name": "Seong-Woo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Woo Kim"
                },
                "author": "Seong-Woo Kim",
                "arxiv_comment": "19 pages, 28 figures. Project page: https://e2map.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10019v1",
                "updated": "2024-09-16T06:16:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    16,
                    1,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T06:16:01Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    16,
                    1,
                    0,
                    260,
                    0
                ],
                "title": "Learning Agile Swimming: An End-to-End Approach without CPGs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Agile Swimming: An End-to-End Approach without CPGs"
                },
                "summary": "The pursuit of agile and efficient underwater robots, especially bio-mimetic\nrobotic fish, has been impeded by challenges in creating motion controllers\nthat are able to fully exploit their hydrodynamic capabilities. This paper\naddresses these challenges by introducing a novel, model-free, end-to-end\ncontrol framework that leverages Deep Reinforcement Learning (DRL) to enable\nagile and energy-efficient swimming of robotic fish. Unlike existing methods\nthat rely on predefined trigonometric swimming patterns like Central Pattern\nGenerators (CPG), our approach directly outputs low-level actuator commands\nwithout strong constraint, enabling the robotic fish to learn agile swimming\nbehaviors. In addition, by integrating a high-performance Computational Fluid\nDynamics (CFD) simulator with innovative sim-to-real strategies, such as\nnormalized density matching and servo response matching, the proposed framework\nsignificantly mitigates the sim-to-real gap, facilitating direct transfer of\ncontrol policies to real-world environments without fine-tuning. Comparative\nexperiments demonstrate that our method achieves faster swimming speeds,\nsmaller turning radii, and reduced energy consumption compared to the\nconventional CPG-PID-based controllers. Furthermore, the proposed framework\nshows promise in addressing complex tasks in diverse scenario, paving the way\nfor more effective deployment of robotic fish in real aquatic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of agile and efficient underwater robots, especially bio-mimetic\nrobotic fish, has been impeded by challenges in creating motion controllers\nthat are able to fully exploit their hydrodynamic capabilities. This paper\naddresses these challenges by introducing a novel, model-free, end-to-end\ncontrol framework that leverages Deep Reinforcement Learning (DRL) to enable\nagile and energy-efficient swimming of robotic fish. Unlike existing methods\nthat rely on predefined trigonometric swimming patterns like Central Pattern\nGenerators (CPG), our approach directly outputs low-level actuator commands\nwithout strong constraint, enabling the robotic fish to learn agile swimming\nbehaviors. In addition, by integrating a high-performance Computational Fluid\nDynamics (CFD) simulator with innovative sim-to-real strategies, such as\nnormalized density matching and servo response matching, the proposed framework\nsignificantly mitigates the sim-to-real gap, facilitating direct transfer of\ncontrol policies to real-world environments without fine-tuning. Comparative\nexperiments demonstrate that our method achieves faster swimming speeds,\nsmaller turning radii, and reduced energy consumption compared to the\nconventional CPG-PID-based controllers. Furthermore, the proposed framework\nshows promise in addressing complex tasks in diverse scenario, paving the way\nfor more effective deployment of robotic fish in real aquatic environments."
                },
                "authors": [
                    {
                        "name": "Xiaozhu Lin"
                    },
                    {
                        "name": "Xiaopei Liu"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10015v1",
                "updated": "2024-09-16T06:04:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    4,
                    49,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T06:04:49Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    4,
                    49,
                    0,
                    260,
                    0
                ],
                "title": "RPC: A Modular Framework for Robot Planning, Control, and Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPC: A Modular Framework for Robot Planning, Control, and Deployment"
                },
                "summary": "This paper presents an open-source, lightweight, yet comprehensive software\nframework, named RPC, which integrates physics-based simulators, planning and\ncontrol libraries, debugging tools, and a user-friendly operator interface. RPC\nenables users to thoroughly evaluate and develop control algorithms for robotic\nsystems. While existing software frameworks provide some of these capabilities,\nintegrating them into a cohesive system can be challenging and cumbersome. To\novercome this challenge, we have modularized each component in RPC to ensure\neasy and seamless integration or replacement with new modules. Additionally,\nour framework currently supports a variety of model-based planning and control\nalgorithms for robotic manipulators and legged robots, alongside essential\ndebugging tools, making it easier for users to design and execute complex\nrobotics tasks. The code and usage instructions of RPC are available at\nhttps://github.com/shbang91/rpc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source, lightweight, yet comprehensive software\nframework, named RPC, which integrates physics-based simulators, planning and\ncontrol libraries, debugging tools, and a user-friendly operator interface. RPC\nenables users to thoroughly evaluate and develop control algorithms for robotic\nsystems. While existing software frameworks provide some of these capabilities,\nintegrating them into a cohesive system can be challenging and cumbersome. To\novercome this challenge, we have modularized each component in RPC to ensure\neasy and seamless integration or replacement with new modules. Additionally,\nour framework currently supports a variety of model-based planning and control\nalgorithms for robotic manipulators and legged robots, alongside essential\ndebugging tools, making it easier for users to design and execute complex\nrobotics tasks. The code and usage instructions of RPC are available at\nhttps://github.com/shbang91/rpc."
                },
                "authors": [
                    {
                        "name": "Seung Hyeon Bang"
                    },
                    {
                        "name": "Carlos Gonzalez"
                    },
                    {
                        "name": "Gabriel Moore"
                    },
                    {
                        "name": "Dong Ho Kang"
                    },
                    {
                        "name": "Mingyo Seo"
                    },
                    {
                        "name": "Luis Sentis"
                    }
                ],
                "author_detail": {
                    "name": "Luis Sentis"
                },
                "author": "Luis Sentis",
                "arxiv_comment": "7pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16166v2",
                "updated": "2024-09-16T06:02:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    2,
                    0,
                    0,
                    260,
                    0
                ],
                "published": "2024-07-23T04:20:14Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    4,
                    20,
                    14,
                    1,
                    205,
                    0
                ],
                "title": "Robust Privacy Amidst Innovation with Large Language Models Through a\n  Critical Assessment of the Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Privacy Amidst Innovation with Large Language Models Through a\n  Critical Assessment of the Risks"
                },
                "summary": "This study examines integrating EHRs and NLP with large language models\n(LLMs) to improve healthcare data management and patient care. It focuses on\nusing advanced models to create secure, HIPAA-compliant synthetic patient notes\nfor biomedical research. The study used de-identified and re-identified MIMIC\nIII datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.\nText generation employed templates and keyword extraction for contextually\nrelevant notes, with one-shot generation for comparison. Privacy assessment\nchecked PHI occurrence, while text utility was tested using an ICD-9 coding\ntask. Text quality was evaluated with ROUGE and cosine similarity metrics to\nmeasure semantic similarity with source notes. Analysis of PHI occurrence and\ntext utility via the ICD-9 coding task showed that the keyword-based method had\nlow risk and good performance. One-shot generation showed the highest PHI\nexposure and PHI co-occurrence, especially in geographic location and date\ncategories. The Normalized One-shot method achieved the highest classification\naccuracy. Privacy analysis revealed a critical balance between data utility and\nprivacy protection, influencing future data use and sharing. Re-identified data\nconsistently outperformed de-identified data. This study demonstrates the\neffectiveness of keyword-based methods in generating privacy-protecting\nsynthetic clinical notes that retain data usability, potentially transforming\nclinical data-sharing practices. The superior performance of re-identified over\nde-identified data suggests a shift towards methods that enhance utility and\nprivacy by using dummy PHIs to perplex privacy attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines integrating EHRs and NLP with large language models\n(LLMs) to improve healthcare data management and patient care. It focuses on\nusing advanced models to create secure, HIPAA-compliant synthetic patient notes\nfor biomedical research. The study used de-identified and re-identified MIMIC\nIII datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.\nText generation employed templates and keyword extraction for contextually\nrelevant notes, with one-shot generation for comparison. Privacy assessment\nchecked PHI occurrence, while text utility was tested using an ICD-9 coding\ntask. Text quality was evaluated with ROUGE and cosine similarity metrics to\nmeasure semantic similarity with source notes. Analysis of PHI occurrence and\ntext utility via the ICD-9 coding task showed that the keyword-based method had\nlow risk and good performance. One-shot generation showed the highest PHI\nexposure and PHI co-occurrence, especially in geographic location and date\ncategories. The Normalized One-shot method achieved the highest classification\naccuracy. Privacy analysis revealed a critical balance between data utility and\nprivacy protection, influencing future data use and sharing. Re-identified data\nconsistently outperformed de-identified data. This study demonstrates the\neffectiveness of keyword-based methods in generating privacy-protecting\nsynthetic clinical notes that retain data usability, potentially transforming\nclinical data-sharing practices. The superior performance of re-identified over\nde-identified data suggests a shift towards methods that enhance utility and\nprivacy by using dummy PHIs to perplex privacy attacks."
                },
                "authors": [
                    {
                        "name": "Yao-Shun Chuang"
                    },
                    {
                        "name": "Atiquer Rahman Sarkar"
                    },
                    {
                        "name": "Yu-Chun Hsu"
                    },
                    {
                        "name": "Noman Mohammed"
                    },
                    {
                        "name": "Xiaoqian Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqian Jiang"
                },
                "author": "Xiaoqian Jiang",
                "arxiv_comment": "13 pages, 4 figures, 1 table, 1 supplementary, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10011v1",
                "updated": "2024-09-16T05:50:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    50,
                    39,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T05:50:39Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    50,
                    39,
                    0,
                    260,
                    0
                ],
                "title": "HALO: Hallucination Analysis and Learning Optimization to Empower LLMs\n  with Retrieval-Augmented Context for Guided Clinical Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALO: Hallucination Analysis and Learning Optimization to Empower LLMs\n  with Retrieval-Augmented Context for Guided Clinical Decision Making"
                },
                "summary": "Large language models (LLMs) have significantly advanced natural language\nprocessing tasks, yet they are susceptible to generating inaccurate or\nunreliable responses, a phenomenon known as hallucination. In critical domains\nsuch as health and medicine, these hallucinations can pose serious risks. This\npaper introduces HALO, a novel framework designed to enhance the accuracy and\nreliability of medical question-answering (QA) systems by focusing on the\ndetection and mitigation of hallucinations. Our approach generates multiple\nvariations of a given query using LLMs and retrieves relevant information from\nexternal open knowledge bases to enrich the context. We utilize maximum\nmarginal relevance scoring to prioritize the retrieved context, which is then\nprovided to LLMs for answer generation, thereby reducing the risk of\nhallucinations. The integration of LangChain further streamlines this process,\nresulting in a notable and robust increase in the accuracy of both open-source\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\nto 70%). This framework underscores the critical importance of addressing\nhallucinations in medical QA systems, ultimately improving clinical\ndecision-making and patient care. The open-source HALO is available at:\nhttps://github.com/ResponsibleAILab/HALO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced natural language\nprocessing tasks, yet they are susceptible to generating inaccurate or\nunreliable responses, a phenomenon known as hallucination. In critical domains\nsuch as health and medicine, these hallucinations can pose serious risks. This\npaper introduces HALO, a novel framework designed to enhance the accuracy and\nreliability of medical question-answering (QA) systems by focusing on the\ndetection and mitigation of hallucinations. Our approach generates multiple\nvariations of a given query using LLMs and retrieves relevant information from\nexternal open knowledge bases to enrich the context. We utilize maximum\nmarginal relevance scoring to prioritize the retrieved context, which is then\nprovided to LLMs for answer generation, thereby reducing the risk of\nhallucinations. The integration of LangChain further streamlines this process,\nresulting in a notable and robust increase in the accuracy of both open-source\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\nto 70%). This framework underscores the critical importance of addressing\nhallucinations in medical QA systems, ultimately improving clinical\ndecision-making and patient care. The open-source HALO is available at:\nhttps://github.com/ResponsibleAILab/HALO."
                },
                "authors": [
                    {
                        "name": "Sumera Anjum"
                    },
                    {
                        "name": "Hanzhi Zhang"
                    },
                    {
                        "name": "Wenjun Zhou"
                    },
                    {
                        "name": "Eun Jin Paek"
                    },
                    {
                        "name": "Xiaopeng Zhao"
                    },
                    {
                        "name": "Yunhe Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Feng"
                },
                "author": "Yunhe Feng",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10007v1",
                "updated": "2024-09-16T05:40:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    40,
                    18,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T05:40:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    40,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL"
                },
                "summary": "In recent years,Text-to-SQL, the problem of automatically converting\nquestions posed in natural language to formal SQL queries, has emerged as an\nimportant problem at the intersection of natural language processing and data\nmanagement research. Large language models (LLMs) have delivered impressive\nperformance when used in an off-the-shelf performance, but still fall\nsignificantly short of expected expert-level performance. Errors are especially\nprobable when a nuanced understanding is needed of database schemas, questions,\nand SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a\nnovel in-context learning solution that uses an algorithmic combination of\nchain-of-thought (CoT) prompting, self-correction, and ensemble methods to\nyield a new state-of-the-art result on challenging Text-to-SQL benchmarks.\nSpecifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL\nachieves 84.2% execution accuracy on the Spider leaderboard's development set,\nexceeding both the best results of other baseline GPT-3.5-Turbo-based solutions\n(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the\nleaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years,Text-to-SQL, the problem of automatically converting\nquestions posed in natural language to formal SQL queries, has emerged as an\nimportant problem at the intersection of natural language processing and data\nmanagement research. Large language models (LLMs) have delivered impressive\nperformance when used in an off-the-shelf performance, but still fall\nsignificantly short of expected expert-level performance. Errors are especially\nprobable when a nuanced understanding is needed of database schemas, questions,\nand SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a\nnovel in-context learning solution that uses an algorithmic combination of\nchain-of-thought (CoT) prompting, self-correction, and ensemble methods to\nyield a new state-of-the-art result on challenging Text-to-SQL benchmarks.\nSpecifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL\nachieves 84.2% execution accuracy on the Spider leaderboard's development set,\nexceeding both the best results of other baseline GPT-3.5-Turbo-based solutions\n(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the\nleaderboard."
                },
                "authors": [
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mayank Kejriwal"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Kejriwal"
                },
                "author": "Mayank Kejriwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11764v2",
                "updated": "2024-09-16T05:28:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    28,
                    43,
                    0,
                    260,
                    0
                ],
                "published": "2024-02-19T01:28:48Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    1,
                    28,
                    48,
                    0,
                    50,
                    0
                ],
                "title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient\n  Debiasing of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient\n  Debiasing of LLMs"
                },
                "summary": "Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost."
                },
                "authors": [
                    {
                        "name": "Pengrui Han"
                    },
                    {
                        "name": "Rafal Kocielnik"
                    },
                    {
                        "name": "Adhithya Saravanan"
                    },
                    {
                        "name": "Roy Jiang"
                    },
                    {
                        "name": "Or Sharir"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "To Appear in the Proceedings of the 1st Conference on Language\n  Modeling (COLM) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10000v1",
                "updated": "2024-09-16T05:16:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    16,
                    19,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T05:16:19Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    16,
                    19,
                    0,
                    260,
                    0
                ],
                "title": "Development and Testing of a Vine Robot for Urban Search and Rescue in\n  Confined Rubble Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Testing of a Vine Robot for Urban Search and Rescue in\n  Confined Rubble Environments"
                },
                "summary": "The request for fast response and safe operation after natural and man-made\ndisasters in urban environments has spurred the development of robotic systems\ndesigned to assist in search and rescue operations within complex rubble sites.\nTraditional Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs)\nface significant limitations in such confined and obstructed environments. This\npaper introduces a novel vine robot designed to navigate dense rubble, drawing\ninspiration from natural growth mechanisms found in plants. Unlike conventional\nrobots, vine robots are soft robots that can grow by everting their material,\nallowing them to navigate through narrow spaces and obstacles. The prototype\npresented in this study incorporates pneumatic muscles for steering and\noscillation, an equation-based robot length control plus feedback pressure\nregulating system for extending and retracting the robot body. We conducted a\nseries of controlled experiments in an artificial rubble testbed to assess the\nrobot performance under varying environmental conditions and robot parameters,\nincluding volume ratio, environmental weight, oscillation, and steering. The\nresults show that the vine robot can achieve significant penetration depths in\ncluttered environments with mixed obstacle sizes and weights, and can maintain\nrepeated trajectories, demonstrating potential for mapping and navigating\ncomplex underground paths. Our findings highlight the suitability of the vine\nrobot for urban search and rescue missions, with further research planned to\nenhance its robustness and deployability in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The request for fast response and safe operation after natural and man-made\ndisasters in urban environments has spurred the development of robotic systems\ndesigned to assist in search and rescue operations within complex rubble sites.\nTraditional Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs)\nface significant limitations in such confined and obstructed environments. This\npaper introduces a novel vine robot designed to navigate dense rubble, drawing\ninspiration from natural growth mechanisms found in plants. Unlike conventional\nrobots, vine robots are soft robots that can grow by everting their material,\nallowing them to navigate through narrow spaces and obstacles. The prototype\npresented in this study incorporates pneumatic muscles for steering and\noscillation, an equation-based robot length control plus feedback pressure\nregulating system for extending and retracting the robot body. We conducted a\nseries of controlled experiments in an artificial rubble testbed to assess the\nrobot performance under varying environmental conditions and robot parameters,\nincluding volume ratio, environmental weight, oscillation, and steering. The\nresults show that the vine robot can achieve significant penetration depths in\ncluttered environments with mixed obstacle sizes and weights, and can maintain\nrepeated trajectories, demonstrating potential for mapping and navigating\ncomplex underground paths. Our findings highlight the suitability of the vine\nrobot for urban search and rescue missions, with further research planned to\nenhance its robustness and deployability in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Zheyu Zhou"
                    },
                    {
                        "name": "Yaqing Wang"
                    },
                    {
                        "name": "Elliot W. Hawkes"
                    },
                    {
                        "name": "Chen Li"
                    }
                ],
                "author_detail": {
                    "name": "Chen Li"
                },
                "author": "Chen Li",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10825v2",
                "updated": "2024-09-16T05:09:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    5,
                    9,
                    57,
                    0,
                    260,
                    0
                ],
                "published": "2024-05-17T14:46:13Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    14,
                    46,
                    13,
                    4,
                    138,
                    0
                ],
                "title": "Large Language Model (LLM) for Telecommunications: A Comprehensive\n  Survey on Principles, Key Techniques, and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) for Telecommunications: A Comprehensive\n  Survey on Principles, Key Techniques, and Opportunities"
                },
                "summary": "Large language models (LLMs) have received considerable attention recently\ndue to their outstanding comprehension and reasoning capabilities, leading to\ngreat progress in many fields. The advancement of LLM techniques also offers\npromising opportunities to automate many tasks in the telecommunication\n(telecom) field. After pre-training and fine-tuning, LLMs can perform diverse\ndownstream tasks based on human instructions, paving the way to artificial\ngeneral intelligence (AGI)-enabled 6G. Given the great potential of LLM\ntechnologies, this work aims to provide a comprehensive overview of LLM-enabled\ntelecom networks. In particular, we first present LLM fundamentals, including\nmodel architecture, pre-training, fine-tuning, inference and utilization, model\nevaluation, and telecom deployment. Then, we introduce LLM-enabled key\ntechniques and telecom applications in terms of generation, classification,\noptimization, and prediction problems. Specifically, the LLM-enabled generation\napplications include telecom domain knowledge, code, and network configuration\ngeneration. After that, the LLM-based classification applications involve\nnetwork security, text, image, and traffic classification problems. Moreover,\nmultiple LLM-enabled optimization techniques are introduced, such as automated\nreward function design for reinforcement learning and verbal reinforcement\nlearning. Furthermore, for LLM-aided prediction problems, we discussed\ntime-series prediction models and multi-modality prediction problems for\ntelecom. Finally, we highlight the challenges and identify the future\ndirections of LLM-enabled telecom networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have received considerable attention recently\ndue to their outstanding comprehension and reasoning capabilities, leading to\ngreat progress in many fields. The advancement of LLM techniques also offers\npromising opportunities to automate many tasks in the telecommunication\n(telecom) field. After pre-training and fine-tuning, LLMs can perform diverse\ndownstream tasks based on human instructions, paving the way to artificial\ngeneral intelligence (AGI)-enabled 6G. Given the great potential of LLM\ntechnologies, this work aims to provide a comprehensive overview of LLM-enabled\ntelecom networks. In particular, we first present LLM fundamentals, including\nmodel architecture, pre-training, fine-tuning, inference and utilization, model\nevaluation, and telecom deployment. Then, we introduce LLM-enabled key\ntechniques and telecom applications in terms of generation, classification,\noptimization, and prediction problems. Specifically, the LLM-enabled generation\napplications include telecom domain knowledge, code, and network configuration\ngeneration. After that, the LLM-based classification applications involve\nnetwork security, text, image, and traffic classification problems. Moreover,\nmultiple LLM-enabled optimization techniques are introduced, such as automated\nreward function design for reinforcement learning and verbal reinforcement\nlearning. Furthermore, for LLM-aided prediction problems, we discussed\ntime-series prediction models and multi-modality prediction problems for\ntelecom. Finally, we highlight the challenges and identify the future\ndirections of LLM-enabled telecom networks."
                },
                "authors": [
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Chengming Hu"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Yufei Cui"
                    },
                    {
                        "name": "Yili Jin"
                    },
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Haolun Wu"
                    },
                    {
                        "name": "Dun Yuan"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Charlie Zhang"
                    },
                    {
                        "name": "Xianbin Wang"
                    },
                    {
                        "name": "Jiangchuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangchuan Liu"
                },
                "author": "Jiangchuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09989v1",
                "updated": "2024-09-16T04:44:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    4,
                    44,
                    52,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T04:44:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    4,
                    44,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM\n  based system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM\n  based system"
                },
                "summary": "This paper provides a comprehensive survey of sentiment analysis within the\ncontext of artificial intelligence (AI) and large language models (LLMs).\nSentiment analysis, a critical aspect of natural language processing (NLP), has\nevolved significantly from traditional rule-based methods to advanced deep\nlearning techniques. This study examines the historical development of\nsentiment analysis, highlighting the transition from lexicon-based and\npattern-based approaches to more sophisticated machine learning and deep\nlearning models. Key challenges are discussed, including handling bilingual\ntexts, detecting sarcasm, and addressing biases. The paper reviews\nstate-of-the-art approaches, identifies emerging trends, and outlines future\nresearch directions to advance the field. By synthesizing current methodologies\nand exploring future opportunities, this survey aims to understand sentiment\nanalysis in the AI and LLM context thoroughly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a comprehensive survey of sentiment analysis within the\ncontext of artificial intelligence (AI) and large language models (LLMs).\nSentiment analysis, a critical aspect of natural language processing (NLP), has\nevolved significantly from traditional rule-based methods to advanced deep\nlearning techniques. This study examines the historical development of\nsentiment analysis, highlighting the transition from lexicon-based and\npattern-based approaches to more sophisticated machine learning and deep\nlearning models. Key challenges are discussed, including handling bilingual\ntexts, detecting sarcasm, and addressing biases. The paper reviews\nstate-of-the-art approaches, identifies emerging trends, and outlines future\nresearch directions to advance the field. By synthesizing current methodologies\nand exploring future opportunities, this survey aims to understand sentiment\nanalysis in the AI and LLM context thoroughly."
                },
                "authors": [
                    {
                        "name": "Shailja Gupta"
                    },
                    {
                        "name": "Rajesh Ranjan"
                    },
                    {
                        "name": "Surya Narayan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Surya Narayan Singh"
                },
                "author": "Surya Narayan Singh",
                "arxiv_comment": "2 Images",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09958v1",
                "updated": "2024-09-16T03:08:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    3,
                    8,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T03:08:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    3,
                    8,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "An Offline Adaptation Framework for Constrained Multi-Objective\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Offline Adaptation Framework for Constrained Multi-Objective\n  Reinforcement Learning"
                },
                "summary": "In recent years, significant progress has been made in multi-objective\nreinforcement learning (RL) research, which aims to balance multiple objectives\nby incorporating preferences for each objective. In most existing studies,\nspecific preferences must be provided during deployment to indicate the desired\npolicies explicitly. However, designing these preferences depends heavily on\nhuman prior knowledge, which is typically obtained through extensive\nobservation of high-performing demonstrations with expected behaviors. In this\nwork, we propose a simple yet effective offline adaptation framework for\nmulti-objective RL problems without assuming handcrafted target preferences,\nbut only given several demonstrations to implicitly indicate the preferences of\nexpected policies. Additionally, we demonstrate that our framework can\nnaturally be extended to meet constraints on safety-critical objectives by\nutilizing safe demonstrations, even when the safety thresholds are unknown.\nEmpirical results on offline multi-objective and safe tasks demonstrate the\ncapability of our framework to infer policies that align with real preferences\nwhile meeting the constraints implied by the provided demonstrations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, significant progress has been made in multi-objective\nreinforcement learning (RL) research, which aims to balance multiple objectives\nby incorporating preferences for each objective. In most existing studies,\nspecific preferences must be provided during deployment to indicate the desired\npolicies explicitly. However, designing these preferences depends heavily on\nhuman prior knowledge, which is typically obtained through extensive\nobservation of high-performing demonstrations with expected behaviors. In this\nwork, we propose a simple yet effective offline adaptation framework for\nmulti-objective RL problems without assuming handcrafted target preferences,\nbut only given several demonstrations to implicitly indicate the preferences of\nexpected policies. Additionally, we demonstrate that our framework can\nnaturally be extended to meet constraints on safety-critical objectives by\nutilizing safe demonstrations, even when the safety thresholds are unknown.\nEmpirical results on offline multi-objective and safe tasks demonstrate the\ncapability of our framework to infer policies that align with real preferences\nwhile meeting the constraints implied by the provided demonstrations."
                },
                "authors": [
                    {
                        "name": "Qian Lin"
                    },
                    {
                        "name": "Zongkai Liu"
                    },
                    {
                        "name": "Danying Mo"
                    },
                    {
                        "name": "Chao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Chao Yu"
                },
                "author": "Chao Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09947v1",
                "updated": "2024-09-16T02:38:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    38,
                    38,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T02:38:38Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    38,
                    38,
                    0,
                    260,
                    0
                ],
                "title": "Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for\n  Fine-grained Text Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for\n  Fine-grained Text Evaluations"
                },
                "summary": "Large Language Models (LLMs) show promise as a writing aid for professionals\nperforming legal analyses. However, LLMs can often hallucinate in this setting,\nin ways difficult to recognize by non-professionals and existing text\nevaluation metrics. In this work, we pose the question: when can\nmachine-generated legal analysis be evaluated as acceptable? We introduce the\nneutral notion of gaps, as opposed to hallucinations in a strict erroneous\nsense, to refer to the difference between human-written and machine-generated\nlegal analysis. Gaps do not always equate to invalid generation. Working with\nlegal experts, we consider the CLERC generation task proposed in Hou et al.\n(2024b), leading to a taxonomy, a fine-grained detector for predicting gap\ncategories, and an annotated dataset for automatic evaluation. Our best\ndetector achieves 67% F1 score and 80% precision on the test set. Employing\nthis detector as an automated metric on legal analysis generated by SOTA LLMs,\nwe find around 80% contain hallucinations of different kinds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show promise as a writing aid for professionals\nperforming legal analyses. However, LLMs can often hallucinate in this setting,\nin ways difficult to recognize by non-professionals and existing text\nevaluation metrics. In this work, we pose the question: when can\nmachine-generated legal analysis be evaluated as acceptable? We introduce the\nneutral notion of gaps, as opposed to hallucinations in a strict erroneous\nsense, to refer to the difference between human-written and machine-generated\nlegal analysis. Gaps do not always equate to invalid generation. Working with\nlegal experts, we consider the CLERC generation task proposed in Hou et al.\n(2024b), leading to a taxonomy, a fine-grained detector for predicting gap\ncategories, and an annotated dataset for automatic evaluation. Our best\ndetector achieves 67% F1 score and 80% precision on the test set. Employing\nthis detector as an automated metric on legal analysis generated by SOTA LLMs,\nwe find around 80% contain hallucinations of different kinds."
                },
                "authors": [
                    {
                        "name": "Abe Bohan Hou"
                    },
                    {
                        "name": "William Jurayj"
                    },
                    {
                        "name": "Nils Holzenberger"
                    },
                    {
                        "name": "Andrew Blair-Stanek"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07368v2",
                "updated": "2024-09-16T02:28:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    28,
                    0,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-11T15:56:15Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    56,
                    15,
                    2,
                    255,
                    0
                ],
                "title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code"
                },
                "summary": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: http://3.131.141.63:8501/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: http://3.131.141.63:8501/."
                },
                "authors": [
                    {
                        "name": "Khiem Ton"
                    },
                    {
                        "name": "Nhi Nguyen"
                    },
                    {
                        "name": "Mahmoud Nazzal"
                    },
                    {
                        "name": "Abdallah Khreishah"
                    },
                    {
                        "name": "Cristian Borcea"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Issa Khalil"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_doi": "10.1145/3658644.3691367",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3691367",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07066v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07066v4",
                "updated": "2024-09-17T01:37:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    1,
                    37,
                    18,
                    1,
                    261,
                    0
                ],
                "published": "2024-04-10T14:56:40Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    14,
                    56,
                    40,
                    2,
                    101,
                    0
                ],
                "title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?"
                },
                "summary": "Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of ``Concept Depth'' to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of ``Concept Depth'' to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07066v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07066v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09927v1",
                "updated": "2024-09-16T02:04:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    4,
                    33,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T02:04:33Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    2,
                    4,
                    33,
                    0,
                    260,
                    0
                ],
                "title": "Towards Data Contamination Detection for Modern Large Language Models:\n  Limitations, Inconsistencies, and Oracle Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Data Contamination Detection for Modern Large Language Models:\n  Limitations, Inconsistencies, and Oracle Challenges"
                },
                "summary": "As large language models achieve increasingly impressive results, questions\narise about whether such performance is from generalizability or mere data\nmemorization. Thus, numerous data contamination detection methods have been\nproposed. However, these approaches are often validated with traditional\nbenchmarks and early-stage LLMs, leaving uncertainty about their effectiveness\nwhen evaluating state-of-the-art LLMs on the contamination of more challenging\nbenchmarks. To address this gap and provide a dual investigation of SOTA LLM\ncontamination status and detection method robustness, we evaluate five\ncontamination detection approaches with four state-of-the-art LLMs across eight\nchallenging datasets often used in modern LLM evaluation. Our analysis reveals\nthat (1) Current methods have non-trivial limitations in their assumptions and\npractical applications; (2) Notable difficulties exist in detecting\ncontamination introduced during instruction fine-tuning with answer\naugmentation; and (3) Limited consistencies between SOTA contamination\ndetection techniques. These findings highlight the complexity of contamination\ndetection in advanced LLMs and the urgent need for further research on robust\nand generalizable contamination evaluation. Our code is available at\nhttps://github.com/vsamuel2003/data-contamination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models achieve increasingly impressive results, questions\narise about whether such performance is from generalizability or mere data\nmemorization. Thus, numerous data contamination detection methods have been\nproposed. However, these approaches are often validated with traditional\nbenchmarks and early-stage LLMs, leaving uncertainty about their effectiveness\nwhen evaluating state-of-the-art LLMs on the contamination of more challenging\nbenchmarks. To address this gap and provide a dual investigation of SOTA LLM\ncontamination status and detection method robustness, we evaluate five\ncontamination detection approaches with four state-of-the-art LLMs across eight\nchallenging datasets often used in modern LLM evaluation. Our analysis reveals\nthat (1) Current methods have non-trivial limitations in their assumptions and\npractical applications; (2) Notable difficulties exist in detecting\ncontamination introduced during instruction fine-tuning with answer\naugmentation; and (3) Limited consistencies between SOTA contamination\ndetection techniques. These findings highlight the complexity of contamination\ndetection in advanced LLMs and the urgent need for further research on robust\nand generalizable contamination evaluation. Our code is available at\nhttps://github.com/vsamuel2003/data-contamination."
                },
                "authors": [
                    {
                        "name": "Vinay Samuel"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Henry Peng Zou"
                    }
                ],
                "author_detail": {
                    "name": "Henry Peng Zou"
                },
                "author": "Henry Peng Zou",
                "arxiv_comment": "12 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04755v2",
                "updated": "2024-09-16T01:23:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    1,
                    23,
                    27,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-07T08:54:55Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    8,
                    54,
                    55,
                    4,
                    159,
                    0
                ],
                "title": "LLM Whisperer: An Inconspicuous Attack to Bias LLM Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Whisperer: An Inconspicuous Attack to Bias LLM Responses"
                },
                "summary": "Writing effective prompts for large language models (LLM) can be unintuitive\nand burdensome. In response, services that optimize or suggest prompts have\nemerged. While such services can reduce user effort, they also introduce a\nrisk: the prompt provider can subtly manipulate prompts to produce heavily\nbiased LLM responses. In this work, we show that subtle synonym replacements in\nprompts can increase the likelihood (by a difference up to 78%) that LLMs\nmention a target concept (e.g., a brand, political party, nation). We\nsubstantiate our observations through a user study, showing our adversarially\nperturbed prompts 1) are indistinguishable from unaltered prompts by humans, 2)\npush LLMs to recommend target concepts more often, and 3) make users more\nlikely to notice target concepts, all without arousing suspicion. The\npracticality of this attack has the potential to undermine user autonomy. Among\nother measures, we recommend implementing warnings against using prompts from\nuntrusted parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing effective prompts for large language models (LLM) can be unintuitive\nand burdensome. In response, services that optimize or suggest prompts have\nemerged. While such services can reduce user effort, they also introduce a\nrisk: the prompt provider can subtly manipulate prompts to produce heavily\nbiased LLM responses. In this work, we show that subtle synonym replacements in\nprompts can increase the likelihood (by a difference up to 78%) that LLMs\nmention a target concept (e.g., a brand, political party, nation). We\nsubstantiate our observations through a user study, showing our adversarially\nperturbed prompts 1) are indistinguishable from unaltered prompts by humans, 2)\npush LLMs to recommend target concepts more often, and 3) make users more\nlikely to notice target concepts, all without arousing suspicion. The\npracticality of this attack has the potential to undermine user autonomy. Among\nother measures, we recommend implementing warnings against using prompts from\nuntrusted parties."
                },
                "authors": [
                    {
                        "name": "Weiran Lin"
                    },
                    {
                        "name": "Anna Gerchanovsky"
                    },
                    {
                        "name": "Omer Akgul"
                    },
                    {
                        "name": "Lujo Bauer"
                    },
                    {
                        "name": "Matt Fredrikson"
                    },
                    {
                        "name": "Zifan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zifan Wang"
                },
                "author": "Zifan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09916v1",
                "updated": "2024-09-16T01:08:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    1,
                    8,
                    18,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T01:08:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    1,
                    8,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "SFR-RAG: Towards Contextually Faithful LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SFR-RAG: Towards Contextually Faithful LLMs"
                },
                "summary": "Retrieval Augmented Generation (RAG), a paradigm that integrates external\ncontextual information with large language models (LLMs) to enhance factual\naccuracy and relevance, has emerged as a pivotal area in generative AI. The\nLLMs used in RAG applications are required to faithfully and completely\ncomprehend the provided context and users' questions, avoid hallucination,\nhandle unanswerable, counterfactual or otherwise low-quality and irrelevant\ncontexts, perform complex multi-hop reasoning and produce reliable citations.\nIn this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with\nan emphasis on context-grounded generation and hallucination minimization. We\nalso present ContextualBench, a new evaluation framework compiling multiple\npopular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with\nconsistent RAG settings to ensure reproducibility and consistency in model\nassessments. Experimental results demonstrate that our SFR-RAG-9B model\noutperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving\nstate-of-the-art results in 3 out of 7 benchmarks in ContextualBench with\nsignificantly fewer parameters. The model is also shown to be resilient to\nalteration in the contextual information and behave appropriately when relevant\ncontext is removed. Additionally, the SFR-RAG model maintains competitive\nperformance in general instruction-following tasks and function-calling\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG), a paradigm that integrates external\ncontextual information with large language models (LLMs) to enhance factual\naccuracy and relevance, has emerged as a pivotal area in generative AI. The\nLLMs used in RAG applications are required to faithfully and completely\ncomprehend the provided context and users' questions, avoid hallucination,\nhandle unanswerable, counterfactual or otherwise low-quality and irrelevant\ncontexts, perform complex multi-hop reasoning and produce reliable citations.\nIn this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with\nan emphasis on context-grounded generation and hallucination minimization. We\nalso present ContextualBench, a new evaluation framework compiling multiple\npopular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with\nconsistent RAG settings to ensure reproducibility and consistency in model\nassessments. Experimental results demonstrate that our SFR-RAG-9B model\noutperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving\nstate-of-the-art results in 3 out of 7 benchmarks in ContextualBench with\nsignificantly fewer parameters. The model is also shown to be resilient to\nalteration in the contextual information and behave appropriately when relevant\ncontext is removed. Additionally, the SFR-RAG model maintains competitive\nperformance in general instruction-following tasks and function-calling\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Shrey Pandit"
                    },
                    {
                        "name": "Senthil Purushwalkam"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Hailin Chen"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Zixuan Ke"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09915v1",
                "updated": "2024-09-16T01:07:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    1,
                    7,
                    16,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T01:07:16Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    1,
                    7,
                    16,
                    0,
                    260,
                    0
                ],
                "title": "Forearm Ultrasound based Gesture Recognition on Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forearm Ultrasound based Gesture Recognition on Edge"
                },
                "summary": "Ultrasound imaging of the forearm has demonstrated significant potential for\naccurate hand gesture classification. Despite this progress, there has been\nlimited focus on developing a stand-alone end- to-end gesture recognition\nsystem which makes it mobile, real-time and more user friendly. To bridge this\ngap, this paper explores the deployment of deep neural networks for forearm\nultrasound-based hand gesture recognition on edge devices. Utilizing\nquantization techniques, we achieve substantial reductions in model size while\nmaintaining high accuracy and low latency. Our best model, with Float16\nquantization, achieves a test accuracy of 92% and an inference time of 0.31\nseconds on a Raspberry Pi. These results demonstrate the feasibility of\nefficient, real-time gesture recognition on resource-limited edge devices,\npaving the way for wearable ultrasound-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrasound imaging of the forearm has demonstrated significant potential for\naccurate hand gesture classification. Despite this progress, there has been\nlimited focus on developing a stand-alone end- to-end gesture recognition\nsystem which makes it mobile, real-time and more user friendly. To bridge this\ngap, this paper explores the deployment of deep neural networks for forearm\nultrasound-based hand gesture recognition on edge devices. Utilizing\nquantization techniques, we achieve substantial reductions in model size while\nmaintaining high accuracy and low latency. Our best model, with Float16\nquantization, achieves a test accuracy of 92% and an inference time of 0.31\nseconds on a Raspberry Pi. These results demonstrate the feasibility of\nefficient, real-time gesture recognition on resource-limited edge devices,\npaving the way for wearable ultrasound-based systems."
                },
                "authors": [
                    {
                        "name": "Keshav Bimbraw"
                    },
                    {
                        "name": "Haichong K. Zhang"
                    },
                    {
                        "name": "Bashima Islam"
                    }
                ],
                "author_detail": {
                    "name": "Bashima Islam"
                },
                "author": "Bashima Islam",
                "arxiv_comment": "Please contact the authors for code and any additional questions\n  pertaining to the project. You can reach Keshav Bimbraw at bimbrawkeshav at\n  gmail dot com",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09907v1",
                "updated": "2024-09-16T00:42:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    0,
                    42,
                    45,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T00:42:45Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    0,
                    42,
                    45,
                    0,
                    260,
                    0
                ],
                "title": "Rapid Adaptation of Earth Observation Foundation Models for Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Adaptation of Earth Observation Foundation Models for Segmentation"
                },
                "summary": "This study investigates the efficacy of Low-Rank Adaptation (LoRA) in\nfine-tuning Earth Observation (EO) foundation models for flood segmentation. We\nhypothesize that LoRA, a parameter-efficient technique, can significantly\naccelerate the adaptation of large-scale EO models to this critical task while\nmaintaining high performance. We apply LoRA to fine-tune a state-of-the-art EO\nfoundation model pre-trained on diverse satellite imagery, using a curated\ndataset of flood events. Our results demonstrate that LoRA-based fine-tuning\n(r-256) improves F1 score by 6.66 points and IoU by 0.11 compared to a frozen\nencoder baseline, while significantly reducing computational costs. Notably,\nLoRA outperforms full fine-tuning, which proves computationally infeasible on\nour hardware. We further assess generalization through out-of-distribution\n(OOD) testing on a geographically distinct flood event. While LoRA\nconfigurations show improved OOD performance over the baseline. This work\ncontributes to research on efficient adaptation of foundation models for\nspecialized EO tasks, with implications for rapid response systems in disaster\nmanagement. Our findings demonstrate LoRA's potential for enabling faster\ndeployment of accurate flood segmentation models in resource-constrained,\ntime-critical scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the efficacy of Low-Rank Adaptation (LoRA) in\nfine-tuning Earth Observation (EO) foundation models for flood segmentation. We\nhypothesize that LoRA, a parameter-efficient technique, can significantly\naccelerate the adaptation of large-scale EO models to this critical task while\nmaintaining high performance. We apply LoRA to fine-tune a state-of-the-art EO\nfoundation model pre-trained on diverse satellite imagery, using a curated\ndataset of flood events. Our results demonstrate that LoRA-based fine-tuning\n(r-256) improves F1 score by 6.66 points and IoU by 0.11 compared to a frozen\nencoder baseline, while significantly reducing computational costs. Notably,\nLoRA outperforms full fine-tuning, which proves computationally infeasible on\nour hardware. We further assess generalization through out-of-distribution\n(OOD) testing on a geographically distinct flood event. While LoRA\nconfigurations show improved OOD performance over the baseline. This work\ncontributes to research on efficient adaptation of foundation models for\nspecialized EO tasks, with implications for rapid response systems in disaster\nmanagement. Our findings demonstrate LoRA's potential for enabling faster\ndeployment of accurate flood segmentation models in resource-constrained,\ntime-critical scenarios."
                },
                "authors": [
                    {
                        "name": "Karthick Panner Selvam"
                    },
                    {
                        "name": "Raul Ramos-Pollan"
                    },
                    {
                        "name": "Freddie Kalaitzis"
                    }
                ],
                "author_detail": {
                    "name": "Freddie Kalaitzis"
                },
                "author": "Freddie Kalaitzis",
                "arxiv_comment": "9 pages 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11569v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11569v3",
                "updated": "2024-09-16T00:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    0,
                    35,
                    15,
                    0,
                    260,
                    0
                ],
                "published": "2024-06-17T14:06:13Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    14,
                    6,
                    13,
                    0,
                    169,
                    0
                ],
                "title": "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated\n  Meta-Learning: Convergence-Generalization Trade-Offs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated\n  Meta-Learning: Convergence-Generalization Trade-Offs"
                },
                "summary": "For modern artificial intelligence (AI) applications such as large language\nmodels (LLMs), the training paradigm has recently shifted to pre-training\nfollowed by fine-tuning. Furthermore, owing to dwindling open repositories of\ndata and thanks to efforts to democratize access to AI models, pre-training is\nexpected to increasingly migrate from the current centralized deployments to\nfederated learning (FL) implementations. Meta-learning provides a general\nframework in which pre-training and fine-tuning can be formalized.\nMeta-learning-based personalized FL (meta-pFL) moves beyond basic\npersonalization by targeting generalization to new agents and tasks. This paper\nstudies the generalization performance of meta-pFL for a wireless setting in\nwhich the agents participating in the pre-training phase, i.e., meta-learning,\nare connected via a shared wireless channel to the server. Adopting\nover-the-air computing, we study the trade-off between generalization to new\nagents and tasks, on the one hand, and convergence, on the other hand. The\ntrade-off arises from the fact that channel impairments may enhance\ngeneralization, while degrading convergence. Extensive numerical results\nvalidate the theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For modern artificial intelligence (AI) applications such as large language\nmodels (LLMs), the training paradigm has recently shifted to pre-training\nfollowed by fine-tuning. Furthermore, owing to dwindling open repositories of\ndata and thanks to efforts to democratize access to AI models, pre-training is\nexpected to increasingly migrate from the current centralized deployments to\nfederated learning (FL) implementations. Meta-learning provides a general\nframework in which pre-training and fine-tuning can be formalized.\nMeta-learning-based personalized FL (meta-pFL) moves beyond basic\npersonalization by targeting generalization to new agents and tasks. This paper\nstudies the generalization performance of meta-pFL for a wireless setting in\nwhich the agents participating in the pre-training phase, i.e., meta-learning,\nare connected via a shared wireless channel to the server. Adopting\nover-the-air computing, we study the trade-off between generalization to new\nagents and tasks, on the one hand, and convergence, on the other hand. The\ntrade-off arises from the fact that channel impairments may enhance\ngeneralization, while degrading convergence. Extensive numerical results\nvalidate the theory."
                },
                "authors": [
                    {
                        "name": "Haifeng Wen"
                    },
                    {
                        "name": "Hong Xing"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "39 pages, 7 figures, submitted for possible journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11569v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11569v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09905v1",
                "updated": "2024-09-16T00:24:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    0,
                    24,
                    40,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T00:24:40Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    0,
                    24,
                    40,
                    0,
                    260,
                    0
                ],
                "title": "Rediscovering the Latent Dimensions of Personality with Large Language\n  Models as Trait Descriptors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rediscovering the Latent Dimensions of Personality with Large Language\n  Models as Trait Descriptors"
                },
                "summary": "Assessing personality traits using large language models (LLMs) has emerged\nas an interesting and challenging area of research. While previous methods\nemploy explicit questionnaires, often derived from the Big Five model of\npersonality, we hypothesize that LLMs implicitly encode notions of personality\nwhen modeling next-token responses. To demonstrate this, we introduce a novel\napproach that uncovers latent personality dimensions in LLMs by applying\nsingular value de-composition (SVD) to the log-probabilities of\ntrait-descriptive adjectives. Our experiments show that LLMs \"rediscover\" core\npersonality traits such as extraversion, agreeableness, conscientiousness,\nneuroticism, and openness without relying on direct questionnaire inputs, with\nthe top-5 factors corresponding to Big Five traits explaining 74.3% of the\nvariance in the latent space. Moreover, we can use the derived principal\ncomponents to assess personality along the Big Five dimensions, and achieve\nimprovements in average personality prediction accuracy of up to 5% over\nfine-tuned models, and up to 21% over direct LLM-based scoring techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing personality traits using large language models (LLMs) has emerged\nas an interesting and challenging area of research. While previous methods\nemploy explicit questionnaires, often derived from the Big Five model of\npersonality, we hypothesize that LLMs implicitly encode notions of personality\nwhen modeling next-token responses. To demonstrate this, we introduce a novel\napproach that uncovers latent personality dimensions in LLMs by applying\nsingular value de-composition (SVD) to the log-probabilities of\ntrait-descriptive adjectives. Our experiments show that LLMs \"rediscover\" core\npersonality traits such as extraversion, agreeableness, conscientiousness,\nneuroticism, and openness without relying on direct questionnaire inputs, with\nthe top-5 factors corresponding to Big Five traits explaining 74.3% of the\nvariance in the latent space. Moreover, we can use the derived principal\ncomponents to assess personality along the Big Five dimensions, and achieve\nimprovements in average personality prediction accuracy of up to 5% over\nfine-tuned models, and up to 21% over direct LLM-based scoring techniques."
                },
                "authors": [
                    {
                        "name": "Joseph Suh"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Minwoo Kang"
                    },
                    {
                        "name": "David M. Chan"
                    }
                ],
                "author_detail": {
                    "name": "David M. Chan"
                },
                "author": "David M. Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.12169v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.12169v5",
                "updated": "2024-09-15T23:04:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    23,
                    4,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2023-07-22T21:18:41Z",
                "published_parsed": [
                    2023,
                    7,
                    22,
                    21,
                    18,
                    41,
                    5,
                    203,
                    0
                ],
                "title": "Rail-only: A Low-Cost High-Performance Network for Training LLMs with\n  Trillion Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rail-only: A Low-Cost High-Performance Network for Training LLMs with\n  Trillion Parameters"
                },
                "summary": "This paper presents a low-cost network architecture for training large\nlanguage models (LLMs) at hyperscale. We study the optimal parallelization\nstrategy of LLMs and propose a novel datacenter network design tailored to\nLLM's unique communication pattern. We show that LLM training generates sparse\ncommunication patterns in the network and, therefore, does not require\nany-to-any full-bisection network to complete efficiently. As a result, our\ndesign eliminates the spine layer in traditional GPU clusters. We name this\ndesign a Rail-only network and demonstrate that it achieves the same training\nperformance while reducing the network cost by 38% to 77% and network power\nconsumption by 37% to 75% compared to a conventional GPU datacenter. Our\narchitecture also supports Mixture-of-Expert (MoE) models with all-to-all\ncommunication through forwarding, with only 8.2% to 11.2% completion time\noverhead for all-to-all traffic. We study the failure robustness of Rail-only\nnetworks and provide insights into the performance impact of different network\nand training parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a low-cost network architecture for training large\nlanguage models (LLMs) at hyperscale. We study the optimal parallelization\nstrategy of LLMs and propose a novel datacenter network design tailored to\nLLM's unique communication pattern. We show that LLM training generates sparse\ncommunication patterns in the network and, therefore, does not require\nany-to-any full-bisection network to complete efficiently. As a result, our\ndesign eliminates the spine layer in traditional GPU clusters. We name this\ndesign a Rail-only network and demonstrate that it achieves the same training\nperformance while reducing the network cost by 38% to 77% and network power\nconsumption by 37% to 75% compared to a conventional GPU datacenter. Our\narchitecture also supports Mixture-of-Expert (MoE) models with all-to-all\ncommunication through forwarding, with only 8.2% to 11.2% completion time\noverhead for all-to-all traffic. We study the failure robustness of Rail-only\nnetworks and provide insights into the performance impact of different network\nand training parameters."
                },
                "authors": [
                    {
                        "name": "Weiyang Wang"
                    },
                    {
                        "name": "Manya Ghobadi"
                    },
                    {
                        "name": "Kayvon Shakeri"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Naader Hasani"
                    }
                ],
                "author_detail": {
                    "name": "Naader Hasani"
                },
                "author": "Naader Hasani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.12169v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.12169v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09882v1",
                "updated": "2024-09-15T22:27:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    22,
                    27,
                    37,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T22:27:37Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    22,
                    27,
                    37,
                    6,
                    259,
                    0
                ],
                "title": "Safe Control of Quadruped in Varying Dynamics via Safety Index\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Control of Quadruped in Varying Dynamics via Safety Index\n  Adaptation"
                },
                "summary": "Varying dynamics pose a fundamental difficulty when deploying safe control\nlaws in the real world. Safety Index Synthesis (SIS) deeply relies on the\nsystem dynamics and once the dynamics change, the previously synthesized safety\nindex becomes invalid. In this work, we show the real-time efficacy of Safety\nIndex Adaptation (SIA) in varying dynamics. SIA enables real-time adaptation to\nthe changing dynamics so that the adapted safe control law can still guarantee\n1) forward invariance within a safe region and 2) finite time convergence to\nthat safe region. This work employs SIA on a package-carrying quadruped robot,\nwhere the payload weight changes in real-time. SIA updates the safety index\nwhen the dynamics change, e.g., a change in payload weight, so that the\nquadruped can avoid obstacles while achieving its performance objectives.\nNumerical study provides theoretical guarantees for SIA and a series of\nhardware experiments demonstrate the effectiveness of SIA in real-world\ndeployment in avoiding obstacles under varying dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Varying dynamics pose a fundamental difficulty when deploying safe control\nlaws in the real world. Safety Index Synthesis (SIS) deeply relies on the\nsystem dynamics and once the dynamics change, the previously synthesized safety\nindex becomes invalid. In this work, we show the real-time efficacy of Safety\nIndex Adaptation (SIA) in varying dynamics. SIA enables real-time adaptation to\nthe changing dynamics so that the adapted safe control law can still guarantee\n1) forward invariance within a safe region and 2) finite time convergence to\nthat safe region. This work employs SIA on a package-carrying quadruped robot,\nwhere the payload weight changes in real-time. SIA updates the safety index\nwhen the dynamics change, e.g., a change in payload weight, so that the\nquadruped can avoid obstacles while achieving its performance objectives.\nNumerical study provides theoretical guarantees for SIA and a series of\nhardware experiments demonstrate the effectiveness of SIA in real-world\ndeployment in avoiding obstacles under varying dynamics."
                },
                "authors": [
                    {
                        "name": "Kai S. Yun"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chase Dunaway"
                    },
                    {
                        "name": "John M. Dolan"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09881v1",
                "updated": "2024-09-15T22:22:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    22,
                    22,
                    27,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T22:22:27Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    22,
                    22,
                    27,
                    6,
                    259,
                    0
                ],
                "title": "Proximal Ranking Policy Optimization for Practical Safety in\n  Counterfactual Learning to Rank",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proximal Ranking Policy Optimization for Practical Safety in\n  Counterfactual Learning to Rank"
                },
                "summary": "Counterfactual learning to rank (CLTR) can be risky and, in various\ncircumstances, can produce sub-optimal models that hurt performance when\ndeployed. Safe CLTR was introduced to mitigate these risks when using inverse\npropensity scoring to correct for position bias. However, the existing safety\nmeasure for CLTR is not applicable to state-of-the-art CLTR methods, cannot\nhandle trust bias, and relies on specific assumptions about user behavior. We\npropose a novel approach, proximal ranking policy optimization (PRPO), that\nprovides safety in deployment without assumptions about user behavior. PRPO\nremoves incentives for learning ranking behavior that is too dissimilar to a\nsafe ranking model. Thereby, PRPO imposes a limit on how much learned models\ncan degrade performance metrics, without relying on any specific user\nassumptions. Our experiments show that PRPO provides higher performance than\nthe existing safe inverse propensity scoring approach. PRPO always maintains\nsafety, even in maximally adversarial situations. By avoiding assumptions, PRPO\nis the first method with unconditional safety in deployment that translates to\nrobust safety for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual learning to rank (CLTR) can be risky and, in various\ncircumstances, can produce sub-optimal models that hurt performance when\ndeployed. Safe CLTR was introduced to mitigate these risks when using inverse\npropensity scoring to correct for position bias. However, the existing safety\nmeasure for CLTR is not applicable to state-of-the-art CLTR methods, cannot\nhandle trust bias, and relies on specific assumptions about user behavior. We\npropose a novel approach, proximal ranking policy optimization (PRPO), that\nprovides safety in deployment without assumptions about user behavior. PRPO\nremoves incentives for learning ranking behavior that is too dissimilar to a\nsafe ranking model. Thereby, PRPO imposes a limit on how much learned models\ncan degrade performance metrics, without relying on any specific user\nassumptions. Our experiments show that PRPO provides higher performance than\nthe existing safe inverse propensity scoring approach. PRPO always maintains\nsafety, even in maximally adversarial situations. By avoiding assumptions, PRPO\nis the first method with unconditional safety in deployment that translates to\nrobust safety for real-world applications."
                },
                "authors": [
                    {
                        "name": "Shashank Gupta"
                    },
                    {
                        "name": "Harrie Oosterhuis"
                    },
                    {
                        "name": "Maarten de Rijke"
                    }
                ],
                "author_detail": {
                    "name": "Maarten de Rijke"
                },
                "author": "Maarten de Rijke",
                "arxiv_comment": "Accepted at the CONSEQUENCES 2024 workshop, co-located with ACM\n  RecSys 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09860v1",
                "updated": "2024-09-15T20:48:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    20,
                    48,
                    47,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T20:48:47Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    20,
                    48,
                    47,
                    6,
                    259,
                    0
                ],
                "title": "Revisiting Physical-World Adversarial Attack on Traffic Sign\n  Recognition: A Commercial Systems Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Physical-World Adversarial Attack on Traffic Sign\n  Recognition: A Commercial Systems Perspective"
                },
                "summary": "Traffic Sign Recognition (TSR) is crucial for safe and correct driving\nautomation. Recent works revealed a general vulnerability of TSR models to\nphysical-world adversarial attacks, which can be low-cost, highly deployable,\nand capable of causing severe attack effects such as hiding a critical traffic\nsign or spoofing a fake one. However, so far existing works generally only\nconsidered evaluating the attack effects on academic TSR models, leaving the\nimpacts of such attacks on real-world commercial TSR systems largely unclear.\nIn this paper, we conduct the first large-scale measurement of physical-world\nadversarial attacks against commercial TSR systems. Our testing results reveal\nthat it is possible for existing attack works from academia to have highly\nreliable (100\\%) attack success against certain commercial TSR system\nfunctionality, but such attack capabilities are not generalizable, leading to\nmuch lower-than-expected attack success rates overall. We find that one\npotential major factor is a spatial memorization design that commonly exists in\ntoday's commercial TSR systems. We design new attack success metrics that can\nmathematically model the impacts of such design on the TSR system-level attack\nsuccess, and use them to revisit existing attacks. Through these efforts, we\nuncover 7 novel observations, some of which directly challenge the observations\nor claims in prior works due to the introduction of the new metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic Sign Recognition (TSR) is crucial for safe and correct driving\nautomation. Recent works revealed a general vulnerability of TSR models to\nphysical-world adversarial attacks, which can be low-cost, highly deployable,\nand capable of causing severe attack effects such as hiding a critical traffic\nsign or spoofing a fake one. However, so far existing works generally only\nconsidered evaluating the attack effects on academic TSR models, leaving the\nimpacts of such attacks on real-world commercial TSR systems largely unclear.\nIn this paper, we conduct the first large-scale measurement of physical-world\nadversarial attacks against commercial TSR systems. Our testing results reveal\nthat it is possible for existing attack works from academia to have highly\nreliable (100\\%) attack success against certain commercial TSR system\nfunctionality, but such attack capabilities are not generalizable, leading to\nmuch lower-than-expected attack success rates overall. We find that one\npotential major factor is a spatial memorization design that commonly exists in\ntoday's commercial TSR systems. We design new attack success metrics that can\nmathematically model the impacts of such design on the TSR system-level attack\nsuccess, and use them to revisit existing attacks. Through these efforts, we\nuncover 7 novel observations, some of which directly challenge the observations\nor claims in prior works due to the introduction of the new metrics."
                },
                "authors": [
                    {
                        "name": "Ningfei Wang"
                    },
                    {
                        "name": "Shaoyuan Xie"
                    },
                    {
                        "name": "Takami Sato"
                    },
                    {
                        "name": "Yunpeng Luo"
                    },
                    {
                        "name": "Kaidi Xu"
                    },
                    {
                        "name": "Qi Alfred Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qi Alfred Chen"
                },
                "author": "Qi Alfred Chen",
                "arxiv_doi": "10.14722/ndss.2025.23090",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14722/ndss.2025.23090",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.09860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by NDSS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09825v1",
                "updated": "2024-09-15T18:56:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    18,
                    56,
                    20,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T18:56:20Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    18,
                    56,
                    20,
                    6,
                    259,
                    0
                ],
                "title": "GP-GPT: Large Language Model for Gene-Phenotype Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GP-GPT: Large Language Model for Gene-Phenotype Mapping"
                },
                "summary": "Pre-trained large language models(LLMs) have attracted increasing attention\nin biomedical domains due to their success in natural language processing.\nHowever, the complex traits and heterogeneity of multi-sources genomics data\npose significant challenges when adapting these models to the bioinformatics\nand biomedical field. To address these challenges, we present GP-GPT, the first\nspecialized large language model for genetic-phenotype knowledge representation\nand genomics relation analysis. Our model is fine-tuned in two stages on a\ncomprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,\nand medical genetics, derived from multiple large-scale validated datasets and\nscientific publications. GP-GPT demonstrates proficiency in accurately\nretrieving medical genetics information and performing common genomics analysis\ntasks, such as genomics information retrieval and relationship determination.\nComparative experiments across domain-specific tasks reveal that GP-GPT\noutperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These\nresults highlight GP-GPT's potential to enhance genetic disease relation\nresearch and facilitate accurate and efficient analysis in the fields of\ngenomics and medical genetics. Our investigation demonstrated the subtle\nchanges of bio-factor entities' representations in the GP-GPT, which suggested\nthe opportunities for the application of LLMs to advancing gene-phenotype\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained large language models(LLMs) have attracted increasing attention\nin biomedical domains due to their success in natural language processing.\nHowever, the complex traits and heterogeneity of multi-sources genomics data\npose significant challenges when adapting these models to the bioinformatics\nand biomedical field. To address these challenges, we present GP-GPT, the first\nspecialized large language model for genetic-phenotype knowledge representation\nand genomics relation analysis. Our model is fine-tuned in two stages on a\ncomprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,\nand medical genetics, derived from multiple large-scale validated datasets and\nscientific publications. GP-GPT demonstrates proficiency in accurately\nretrieving medical genetics information and performing common genomics analysis\ntasks, such as genomics information retrieval and relationship determination.\nComparative experiments across domain-specific tasks reveal that GP-GPT\noutperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These\nresults highlight GP-GPT's potential to enhance genetic disease relation\nresearch and facilitate accurate and efficient analysis in the fields of\ngenomics and medical genetics. Our investigation demonstrated the subtle\nchanges of bio-factor entities' representations in the GP-GPT, which suggested\nthe opportunities for the application of LLMs to advancing gene-phenotype\nresearch."
                },
                "authors": [
                    {
                        "name": "Yanjun Lyu"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Wei Ruan"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Xiaowei Yu"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Minheng Chen"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Rongjie Liu"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Dajiang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Dajiang Zhu"
                },
                "author": "Dajiang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09822v1",
                "updated": "2024-09-15T18:43:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    18,
                    43,
                    11,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T18:43:11Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    18,
                    43,
                    11,
                    6,
                    259,
                    0
                ],
                "title": "Causal Inference with Large Language Model: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference with Large Language Model: A Survey"
                },
                "summary": "Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies."
                },
                "authors": [
                    {
                        "name": "Jing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jing Ma"
                },
                "author": "Jing Ma",
                "arxiv_comment": "15 pages, 2 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04218v2",
                "updated": "2024-09-15T17:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    17,
                    52,
                    7,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-06T12:17:23Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    17,
                    23,
                    4,
                    250,
                    0
                ],
                "title": "MpoxMamba: A Grouped Mamba-based Lightweight Hybrid Network for Mpox\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MpoxMamba: A Grouped Mamba-based Lightweight Hybrid Network for Mpox\n  Detection"
                },
                "summary": "Due to the lack of effective mpox detection tools, the mpox virus continues\nto spread worldwide and has once again been declared a public health emergency\nof international concern by the World Health Organization. Lightweight deep\nlearning model-based detection systems are crucial to alleviate mpox outbreaks\nsince they are suitable for widespread deployment, especially in\nresource-limited scenarios. However, the key to its successful application\ndepends on ensuring that the model can effectively model local features and\nlong-range dependencies in mpox lesions while maintaining lightweight. Inspired\nby the success of Mamba in modeling long-range dependencies and its linear\ncomplexity, we proposed a lightweight hybrid architecture called MpoxMamba for\nefficient mpox detection. MpoxMamba utilizes depth-wise separable convolutions\nto extract local feature representations in mpox skin lesions and greatly\nenhances the model's ability to model the global contextual information by\ngrouped Mamba modules. Notably, MpoxMamba's parameter size and FLOPs are 0.77M\nand 0.53G, respectively. Experimental results on two widely recognized\nbenchmark datasets demonstrate that MpoxMamba outperforms state-of-the-art\nlightweight models and existing mpox detection methods. Importantly, we\ndeveloped a web-based online application to provide free mpox detection\n(http://5227i971s5.goho.co:30290). The source codes of MpoxMamba are available\nat https://github.com/YubiaoYue/MpoxMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the lack of effective mpox detection tools, the mpox virus continues\nto spread worldwide and has once again been declared a public health emergency\nof international concern by the World Health Organization. Lightweight deep\nlearning model-based detection systems are crucial to alleviate mpox outbreaks\nsince they are suitable for widespread deployment, especially in\nresource-limited scenarios. However, the key to its successful application\ndepends on ensuring that the model can effectively model local features and\nlong-range dependencies in mpox lesions while maintaining lightweight. Inspired\nby the success of Mamba in modeling long-range dependencies and its linear\ncomplexity, we proposed a lightweight hybrid architecture called MpoxMamba for\nefficient mpox detection. MpoxMamba utilizes depth-wise separable convolutions\nto extract local feature representations in mpox skin lesions and greatly\nenhances the model's ability to model the global contextual information by\ngrouped Mamba modules. Notably, MpoxMamba's parameter size and FLOPs are 0.77M\nand 0.53G, respectively. Experimental results on two widely recognized\nbenchmark datasets demonstrate that MpoxMamba outperforms state-of-the-art\nlightweight models and existing mpox detection methods. Importantly, we\ndeveloped a web-based online application to provide free mpox detection\n(http://5227i971s5.goho.co:30290). The source codes of MpoxMamba are available\nat https://github.com/YubiaoYue/MpoxMamba."
                },
                "authors": [
                    {
                        "name": "Yubiao Yue"
                    },
                    {
                        "name": "Jun Xue"
                    },
                    {
                        "name": "Haihuang Liang"
                    },
                    {
                        "name": "Zhenzhang Li"
                    },
                    {
                        "name": "Yufeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yufeng Wang"
                },
                "author": "Yufeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19544v2",
                "updated": "2024-09-15T17:42:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    17,
                    42,
                    20,
                    6,
                    259,
                    0
                ],
                "published": "2024-05-29T22:12:52Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    22,
                    12,
                    52,
                    2,
                    150,
                    0
                ],
                "title": "One-Shot Safety Alignment for Large Language Models via Optimal\n  Dualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Shot Safety Alignment for Large Language Models via Optimal\n  Dualization"
                },
                "summary": "The growing safety concerns surrounding Large Language Models (LLMs) raise an\nurgent need to align them with diverse human preferences to simultaneously\nenhance their helpfulness and safety. A promising approach is to enforce safety\nconstraints through Reinforcement Learning from Human Feedback (RLHF). For such\nconstrained RLHF, common Lagrangian-based primal-dual policy optimization\nmethods are computationally expensive and often unstable. This paper presents a\ndualization perspective that reduces constrained alignment to an equivalent\nunconstrained alignment problem. We do so by pre-optimizing a smooth and convex\ndual function that has a closed form. This shortcut eliminates the need for\ncumbersome primal-dual policy iterations, thus greatly reducing the\ncomputational burden and improving training stability. Our strategy leads to\ntwo practical algorithms in model-based and preference-based scenarios (MoCAN\nand PeCAN, respectively). A broad range of experiments demonstrate the\neffectiveness of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing safety concerns surrounding Large Language Models (LLMs) raise an\nurgent need to align them with diverse human preferences to simultaneously\nenhance their helpfulness and safety. A promising approach is to enforce safety\nconstraints through Reinforcement Learning from Human Feedback (RLHF). For such\nconstrained RLHF, common Lagrangian-based primal-dual policy optimization\nmethods are computationally expensive and often unstable. This paper presents a\ndualization perspective that reduces constrained alignment to an equivalent\nunconstrained alignment problem. We do so by pre-optimizing a smooth and convex\ndual function that has a closed form. This shortcut eliminates the need for\ncumbersome primal-dual policy iterations, thus greatly reducing the\ncomputational burden and improving training stability. Our strategy leads to\ntwo practical algorithms in model-based and preference-based scenarios (MoCAN\nand PeCAN, respectively). A broad range of experiments demonstrate the\neffectiveness of our methods."
                },
                "authors": [
                    {
                        "name": "Xinmeng Huang"
                    },
                    {
                        "name": "Shuo Li"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Osbert Bastani"
                    },
                    {
                        "name": "Hamed Hassani"
                    },
                    {
                        "name": "Dongsheng Ding"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Ding"
                },
                "author": "Dongsheng Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16871v2",
                "updated": "2024-09-15T17:03:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    17,
                    3,
                    26,
                    6,
                    259,
                    0
                ],
                "published": "2024-03-25T15:37:43Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    15,
                    37,
                    43,
                    0,
                    85,
                    0
                ],
                "title": "Conformal Off-Policy Prediction for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Off-Policy Prediction for Multi-Agent Systems"
                },
                "summary": "Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy\nusing only data collected under a nominal (behavioural) policy, is a paramount\nproblem in data-driven analysis of safety-critical systems where the deployment\nof a new policy may be unsafe. To achieve dependable off-policy predictions,\nrecent work on Conformal Off-Policy Prediction (COPP) leverage the conformal\nprediction framework to derive prediction regions with probabilistic guarantees\nunder the target process. Existing COPP methods can account for the\ndistribution shifts induced by policy switching, but are limited to\nsingle-agent systems and scalar outcomes (e.g., rewards). In this work, we\nintroduce MA-COPP, the first conformal prediction method to solve OPP problems\ninvolving multi-agent systems, deriving joint prediction regions for all\nagents' trajectories when one or more ego agents change their policies. Unlike\nthe single-agent scenario, this setting introduces higher complexity as the\ndistribution shifts affect predictions for all agents, not just the ego agents,\nand the prediction task involves full multi-dimensional trajectories, not just\nreward values. A key contribution of MA-COPP is to avoid enumeration or\nexhaustive search of the output space of agent trajectories, which is instead\nrequired by existing COPP methods to construct the prediction region. We\nachieve this by showing that an over-approximation of the true joint prediction\nregion (JPR) can be constructed, without enumeration, from the maximum density\nratio of the JPR trajectories. We evaluate the effectiveness of MA-COPP in\nmulti-agent systems from the PettingZoo library and the F1TENTH autonomous\nracing environment, achieving nominal coverage in higher dimensions and various\nshift settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy\nusing only data collected under a nominal (behavioural) policy, is a paramount\nproblem in data-driven analysis of safety-critical systems where the deployment\nof a new policy may be unsafe. To achieve dependable off-policy predictions,\nrecent work on Conformal Off-Policy Prediction (COPP) leverage the conformal\nprediction framework to derive prediction regions with probabilistic guarantees\nunder the target process. Existing COPP methods can account for the\ndistribution shifts induced by policy switching, but are limited to\nsingle-agent systems and scalar outcomes (e.g., rewards). In this work, we\nintroduce MA-COPP, the first conformal prediction method to solve OPP problems\ninvolving multi-agent systems, deriving joint prediction regions for all\nagents' trajectories when one or more ego agents change their policies. Unlike\nthe single-agent scenario, this setting introduces higher complexity as the\ndistribution shifts affect predictions for all agents, not just the ego agents,\nand the prediction task involves full multi-dimensional trajectories, not just\nreward values. A key contribution of MA-COPP is to avoid enumeration or\nexhaustive search of the output space of agent trajectories, which is instead\nrequired by existing COPP methods to construct the prediction region. We\nachieve this by showing that an over-approximation of the true joint prediction\nregion (JPR) can be constructed, without enumeration, from the maximum density\nratio of the JPR trajectories. We evaluate the effectiveness of MA-COPP in\nmulti-agent systems from the PettingZoo library and the F1TENTH autonomous\nracing environment, achieving nominal coverage in higher dimensions and various\nshift settings."
                },
                "authors": [
                    {
                        "name": "Tom Kuipers"
                    },
                    {
                        "name": "Renukanandan Tumu"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Milad Kazemi"
                    },
                    {
                        "name": "Rahul Mangharam"
                    },
                    {
                        "name": "Nicola Paoletti"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Paoletti"
                },
                "author": "Nicola Paoletti",
                "arxiv_comment": "Accepted for publication in the 63rd IEEE Conference on Decision and\n  Control (CDC) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09785v2",
                "updated": "2024-09-17T09:32:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    32,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-15T16:32:49Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    16,
                    32,
                    49,
                    6,
                    259,
                    0
                ],
                "title": "Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition"
                },
                "summary": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations."
                },
                "authors": [
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Yuan Gong"
                    },
                    {
                        "name": "Yuanchao Li"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Yen-Ting Lin"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Piotr Żelasko"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    },
                    {
                        "name": "Yu Tsao"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Sabato Marco Siniscalchi"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Peter Bell"
                    },
                    {
                        "name": "Catherine Lai"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community:\n  https://huggingface.co/GenSEC-LLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.19158v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.19158v3",
                "updated": "2024-09-15T16:18:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    16,
                    18,
                    31,
                    6,
                    259,
                    0
                ],
                "published": "2023-10-29T21:18:25Z",
                "published_parsed": [
                    2023,
                    10,
                    29,
                    21,
                    18,
                    25,
                    6,
                    302,
                    0
                ],
                "title": "Perspectives from India: Opportunities and Challenges for AI Replication\n  Prediction to Improve Confidence in Published Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perspectives from India: Opportunities and Challenges for AI Replication\n  Prediction to Improve Confidence in Published Research"
                },
                "summary": "Over the past decade, a crisis of confidence in scientific literature has\ngained attention, particularly in the West. In response, we have seen changes\nin policy and practice amongst individual researchers and institutions. Greater\nattention is given to the transparency of workflows and the appropriate use of\nstatistical methods. Advances in scholarly big data and machine learning have\nled to the development of AI-driven tools for the evaluation of published\nfindings. In this study, we conduct 19 semi-structured interviews with Indian\nresearchers to understand their perspectives on challenges and opportunities\nfor AI technologies to improve confidence in published research. Our findings\nhighlight the importance of social and cultural context for the design and\ndeployment of AI tools for research assessment. Our work suggests that such\ntechnologies must work alongside rather than replace human research assessment\nmechanisms. They must be explainable and situated within well-functioning\nhuman-centered peer review processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, a crisis of confidence in scientific literature has\ngained attention, particularly in the West. In response, we have seen changes\nin policy and practice amongst individual researchers and institutions. Greater\nattention is given to the transparency of workflows and the appropriate use of\nstatistical methods. Advances in scholarly big data and machine learning have\nled to the development of AI-driven tools for the evaluation of published\nfindings. In this study, we conduct 19 semi-structured interviews with Indian\nresearchers to understand their perspectives on challenges and opportunities\nfor AI technologies to improve confidence in published research. Our findings\nhighlight the importance of social and cultural context for the design and\ndeployment of AI tools for research assessment. Our work suggests that such\ntechnologies must work alongside rather than replace human research assessment\nmechanisms. They must be explainable and situated within well-functioning\nhuman-centered peer review processes."
                },
                "authors": [
                    {
                        "name": "Tatiana Chakravorti"
                    },
                    {
                        "name": "Chuhao Wu"
                    },
                    {
                        "name": "Sai Koneru"
                    },
                    {
                        "name": "Sarah Rajtmajer"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Rajtmajer"
                },
                "author": "Sarah Rajtmajer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.19158v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.19158v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10453v2",
                "updated": "2024-09-15T15:58:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    15,
                    58,
                    45,
                    6,
                    259,
                    0
                ],
                "published": "2024-02-16T05:03:01Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    5,
                    3,
                    1,
                    4,
                    47,
                    0
                ],
                "title": "Steering Conversational Large Language Models for Long Emotional Support\n  Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Conversational Large Language Models for Long Emotional Support\n  Conversations"
                },
                "summary": "In this study, we address the challenge of enabling large language models\n(LLMs) to consistently adhere to emotional support strategies in extended\nconversations. We focus on the steerability of the Llama-2 and Llama-3 suite of\nmodels, examining their ability to maintain these strategies throughout\ninteractions. To assess this, we introduce the Strategy Relevant Attention\n(SRA) metric, which quantifies the model's adherence to the prompted strategy\nthrough attention maps. To facilitate our study, we create a\nstrategy-conditioned synthetic conversational dataset derived from the ESConv\ndataset. We also propose various baselines informed by our proposed SRA metric\nto address the challenge and propose a fine-tuned model that significantly\nenhances the steerability of the base model in following the strategy\nthroughout the conversation. The code and data are publicly available on our\nGitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we address the challenge of enabling large language models\n(LLMs) to consistently adhere to emotional support strategies in extended\nconversations. We focus on the steerability of the Llama-2 and Llama-3 suite of\nmodels, examining their ability to maintain these strategies throughout\ninteractions. To assess this, we introduce the Strategy Relevant Attention\n(SRA) metric, which quantifies the model's adherence to the prompted strategy\nthrough attention maps. To facilitate our study, we create a\nstrategy-conditioned synthetic conversational dataset derived from the ESConv\ndataset. We also propose various baselines informed by our proposed SRA metric\nto address the challenge and propose a fine-tuned model that significantly\nenhances the steerability of the base model in following the strategy\nthroughout the conversation. The code and data are publicly available on our\nGitHub."
                },
                "authors": [
                    {
                        "name": "Navid Madani"
                    },
                    {
                        "name": "Sougata Saha"
                    },
                    {
                        "name": "Rohini Srihari"
                    }
                ],
                "author_detail": {
                    "name": "Rohini Srihari"
                },
                "author": "Rohini Srihari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09774v1",
                "updated": "2024-09-15T15:46:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    15,
                    46,
                    3,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T15:46:03Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    15,
                    46,
                    3,
                    6,
                    259,
                    0
                ],
                "title": "Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization"
                },
                "summary": "Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications."
                },
                "authors": [
                    {
                        "name": "Haoyuan Sun"
                    },
                    {
                        "name": "Bo Xia"
                    },
                    {
                        "name": "Yongzhe Chang"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]