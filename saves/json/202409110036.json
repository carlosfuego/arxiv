[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.05862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05862v2",
                "updated": "2024-09-10T02:28:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    28,
                    40,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T17:59:13Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    13,
                    0,
                    253,
                    0
                ],
                "title": "Evaluating Multiview Object Consistency in Humans and Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Multiview Object Consistency in Humans and Image Models"
                },
                "summary": "We introduce a benchmark to directly evaluate the alignment between human\nobservers and vision models on a 3D shape inference task. We leverage an\nexperimental design from the cognitive sciences which requires zero-shot visual\ninferences about object shape: given a set of images, participants identify\nwhich contain the same/different objects, despite considerable viewpoint\nvariation. We draw from a diverse range of images that include common objects\n(e.g., chairs) as well as abstract shapes (i.e., procedurally generated\n`nonsense' objects). After constructing over 2000 unique image sets, we\nadminister these tasks to human participants, collecting 35K trials of\nbehavioral data from over 500 participants. This includes explicit choice\nbehaviors as well as intermediate measures, such as reaction time and gaze\ndata. We then evaluate the performance of common vision models (e.g., DINOv2,\nMAE, CLIP). We find that humans outperform all models by a wide margin. Using a\nmulti-scale evaluation approach, we identify underlying similarities and\ndifferences between models and humans: while human-model performance is\ncorrelated, humans allocate more time/processing on challenging trials. All\nimages, data, and code can be accessed via our project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a benchmark to directly evaluate the alignment between human\nobservers and vision models on a 3D shape inference task. We leverage an\nexperimental design from the cognitive sciences which requires zero-shot visual\ninferences about object shape: given a set of images, participants identify\nwhich contain the same/different objects, despite considerable viewpoint\nvariation. We draw from a diverse range of images that include common objects\n(e.g., chairs) as well as abstract shapes (i.e., procedurally generated\n`nonsense' objects). After constructing over 2000 unique image sets, we\nadminister these tasks to human participants, collecting 35K trials of\nbehavioral data from over 500 participants. This includes explicit choice\nbehaviors as well as intermediate measures, such as reaction time and gaze\ndata. We then evaluate the performance of common vision models (e.g., DINOv2,\nMAE, CLIP). We find that humans outperform all models by a wide margin. Using a\nmulti-scale evaluation approach, we identify underlying similarities and\ndifferences between models and humans: while human-model performance is\ncorrelated, humans allocate more time/processing on challenging trials. All\nimages, data, and code can be accessed via our project page."
                },
                "authors": [
                    {
                        "name": "Tyler Bonnen"
                    },
                    {
                        "name": "Stephanie Fu"
                    },
                    {
                        "name": "Yutong Bai"
                    },
                    {
                        "name": "Thomas O'Connell"
                    },
                    {
                        "name": "Yoni Friedman"
                    },
                    {
                        "name": "Nancy Kanwisher"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Alexei A. Efros"
                    }
                ],
                "author_detail": {
                    "name": "Alexei A. Efros"
                },
                "author": "Alexei A. Efros",
                "arxiv_comment": "Project page: https://tzler.github.io/MOCHI/ Code:\n  https://github.com/tzler/mochi_code Huggingface dataset:\n  https://huggingface.co/datasets/tzler/MOCHI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14770v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14770v5",
                "updated": "2024-09-09T17:37:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    37,
                    16,
                    0,
                    253,
                    0
                ],
                "published": "2023-05-24T06:19:14Z",
                "published_parsed": [
                    2023,
                    5,
                    24,
                    6,
                    19,
                    14,
                    2,
                    144,
                    0
                ],
                "title": "Using Natural Language Explanations to Rescale Human Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Natural Language Explanations to Rescale Human Judgments"
                },
                "summary": "The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover human judgments. However, annotators' judgments for subjective tasks can\ndiffer in many ways: they may reflect different qualitative judgments about an\nexample, and they may be mapped to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover human judgments. However, annotators' judgments for subjective tasks can\ndiffer in many ways: they may reflect different qualitative judgments about an\nexample, and they may be mapped to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric."
                },
                "authors": [
                    {
                        "name": "Manya Wadhwa"
                    },
                    {
                        "name": "Jifan Chen"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "arxiv_comment": "Data available at\n  https://github.com/ManyaWadhwa/explanation_based_rescaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14770v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14770v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05824v1",
                "updated": "2024-09-09T17:30:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    30,
                    20,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:30:20Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    30,
                    20,
                    0,
                    253,
                    0
                ],
                "title": "Are Large Language Models a Threat to Programming Platforms? An\n  Exploratory Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models a Threat to Programming Platforms? An\n  Exploratory Study"
                },
                "summary": "Competitive programming platforms like LeetCode, Codeforces, and HackerRank\nevaluate programming skills, often used by recruiters for screening. With the\nrise of advanced Large Language Models (LLMs) such as ChatGPT, Gemini, and Meta\nAI, their problem-solving ability on these platforms needs assessment. This\nstudy explores LLMs' ability to tackle diverse programming challenges across\nplatforms with varying difficulty, offering insights into their real-time and\noffline performance and comparing them with human programmers.\n  We tested 98 problems from LeetCode, 126 from Codeforces, covering 15\ncategories. Nine online contests from Codeforces and LeetCode were conducted,\nalong with two certification tests on HackerRank, to assess real-time\nperformance. Prompts and feedback mechanisms were used to guide LLMs, and\ncorrelations were explored across different scenarios.\n  LLMs, like ChatGPT (71.43% success on LeetCode), excelled in LeetCode and\nHackerRank certifications but struggled in virtual contests, particularly on\nCodeforces. They performed better than users in LeetCode archives, excelling in\ntime and memory efficiency but underperforming in harder Codeforces contests.\nWhile not immediately threatening, LLMs performance on these platforms is\nconcerning, and future improvements will need addressing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive programming platforms like LeetCode, Codeforces, and HackerRank\nevaluate programming skills, often used by recruiters for screening. With the\nrise of advanced Large Language Models (LLMs) such as ChatGPT, Gemini, and Meta\nAI, their problem-solving ability on these platforms needs assessment. This\nstudy explores LLMs' ability to tackle diverse programming challenges across\nplatforms with varying difficulty, offering insights into their real-time and\noffline performance and comparing them with human programmers.\n  We tested 98 problems from LeetCode, 126 from Codeforces, covering 15\ncategories. Nine online contests from Codeforces and LeetCode were conducted,\nalong with two certification tests on HackerRank, to assess real-time\nperformance. Prompts and feedback mechanisms were used to guide LLMs, and\ncorrelations were explored across different scenarios.\n  LLMs, like ChatGPT (71.43% success on LeetCode), excelled in LeetCode and\nHackerRank certifications but struggled in virtual contests, particularly on\nCodeforces. They performed better than users in LeetCode archives, excelling in\ntime and memory efficiency but underperforming in harder Codeforces contests.\nWhile not immediately threatening, LLMs performance on these platforms is\nconcerning, and future improvements will need addressing."
                },
                "authors": [
                    {
                        "name": "Md Mustakim Billah"
                    },
                    {
                        "name": "Palash Ranjan Roy"
                    },
                    {
                        "name": "Zadia Codabux"
                    },
                    {
                        "name": "Banani Roy"
                    }
                ],
                "author_detail": {
                    "name": "Banani Roy"
                },
                "author": "Banani Roy",
                "arxiv_doi": "10.1145/3674805.3686689",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3674805.3686689",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.05824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in ESEM 2024",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05816v1",
                "updated": "2024-09-09T17:23:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    23,
                    29,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:23:29Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    23,
                    29,
                    0,
                    253,
                    0
                ],
                "title": "Improving Pretraining Data Using Perplexity Correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Pretraining Data Using Perplexity Correlations"
                },
                "summary": "Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier."
                },
                "authors": [
                    {
                        "name": "Tristan Thrush"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05806v1",
                "updated": "2024-09-09T17:11:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    11,
                    51,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:11:51Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    11,
                    51,
                    0,
                    253,
                    0
                ],
                "title": "Benchmarking Chinese Knowledge Rectification in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Chinese Knowledge Rectification in Large Language Models"
                },
                "summary": "While Large Language Models (LLMs) exhibit remarkable generative\ncapabilities, they are not without flaws, particularly in the form of\nhallucinations. This issue is even more pronounced when LLMs are applied to\nspecific languages and domains. For example, LLMs may generate nonsense\ninformation when handling Chinese ancient poetry, proverbs, or idioms, owing to\nthe lack of specific knowledge. To this end, this paper introduces a benchmark\nfor rectifying Chinese knowledge in LLMs via knowledge editing. Specifically,\nwe introduce a new Chinese dataset, CKnowEdit, by collecting seven type of\nknowledge from various sources, including classical texts, idioms, and content\nfrom Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony,\nantithesis, and logical constructs inherent in the Chinese language. Through\nthe analysis of this dataset, we uncover the challenges faced by current LLMs\nin mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques on this dataset unveil the substantial scope for advancement\nin the rectification of Chinese knowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) exhibit remarkable generative\ncapabilities, they are not without flaws, particularly in the form of\nhallucinations. This issue is even more pronounced when LLMs are applied to\nspecific languages and domains. For example, LLMs may generate nonsense\ninformation when handling Chinese ancient poetry, proverbs, or idioms, owing to\nthe lack of specific knowledge. To this end, this paper introduces a benchmark\nfor rectifying Chinese knowledge in LLMs via knowledge editing. Specifically,\nwe introduce a new Chinese dataset, CKnowEdit, by collecting seven type of\nknowledge from various sources, including classical texts, idioms, and content\nfrom Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony,\nantithesis, and logical constructs inherent in the Chinese language. Through\nthe analysis of this dataset, we uncover the challenges faced by current LLMs\nin mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques on this dataset unveil the substantial scope for advancement\nin the rectification of Chinese knowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit."
                },
                "authors": [
                    {
                        "name": "Tianhe Lu"
                    },
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Ongoing work; code and dataset are available at\n  https://github.com/zjunlp/EasyEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20520v2",
                "updated": "2024-09-09T17:08:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    8,
                    47,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-30T03:32:27Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    3,
                    32,
                    27,
                    1,
                    212,
                    0
                ],
                "title": "Uncertainty Quantification under Noisy Constraints, with Applications to\n  Raking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification under Noisy Constraints, with Applications to\n  Raking"
                },
                "summary": "We consider statistical inference problems under uncertain equality\nconstraints, and provide asymptotically valid uncertainty estimates for\ninferred parameters. The proposed approach leverages the implicit function\ntheorem and primal-dual optimality conditions for a particular problem class.\nThe motivating application is multi-dimensional raking, where observations are\nadjusted to match marginals; for example, adjusting estimated deaths across\nrace, county, and cause in order to match state all-race all-cause totals. We\nreview raking from a convex optimization perspective, providing explicit\nprimal-dual formulations, algorithms, and optimality conditions for a wide\narray of raking applications, which are then leveraged to obtain the\nuncertainty estimates. Empirical results show that the approach obtains, at the\ncost of a single solve, nearly the same uncertainty estimates as\ncomputationally intensive Monte Carlo techniques that pass thousands of\nobserved and of marginal draws through the entire raking process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider statistical inference problems under uncertain equality\nconstraints, and provide asymptotically valid uncertainty estimates for\ninferred parameters. The proposed approach leverages the implicit function\ntheorem and primal-dual optimality conditions for a particular problem class.\nThe motivating application is multi-dimensional raking, where observations are\nadjusted to match marginals; for example, adjusting estimated deaths across\nrace, county, and cause in order to match state all-race all-cause totals. We\nreview raking from a convex optimization perspective, providing explicit\nprimal-dual formulations, algorithms, and optimality conditions for a wide\narray of raking applications, which are then leveraged to obtain the\nuncertainty estimates. Empirical results show that the approach obtains, at the\ncost of a single solve, nearly the same uncertainty estimates as\ncomputationally intensive Monte Carlo techniques that pass thousands of\nobserved and of marginal draws through the entire raking process."
                },
                "authors": [
                    {
                        "name": "Ariane Ducellier"
                    },
                    {
                        "name": "Alexander Hsu"
                    },
                    {
                        "name": "Parkes Kendrick"
                    },
                    {
                        "name": "Bill Gustafson"
                    },
                    {
                        "name": "Laura Dwyer-Lindgren"
                    },
                    {
                        "name": "Christopher Murray"
                    },
                    {
                        "name": "Peng Zheng"
                    },
                    {
                        "name": "Aleksandr Aravkin"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr Aravkin"
                },
                "arxiv_affiliation": "Department of Applied Mathematics, University of Washington, Seattle, WA",
                "author": "Aleksandr Aravkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46N10, 62D05, 62E20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05795v1",
                "updated": "2024-09-09T17:00:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    0,
                    53,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:00:53Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    0,
                    53,
                    0,
                    253,
                    0
                ],
                "title": "Environmental dependence on galaxy-halo connections for satellites using\n  HSC weak lensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environmental dependence on galaxy-halo connections for satellites using\n  HSC weak lensing"
                },
                "summary": "We present the luminosity-halo mass relations of satellite (sLHMRs) galaxies\nin the SDSS redMaPPer cluster catalogue and the effects of the dense cluster\nenvironment on subhalo mass evolution. We use data from the Subaru Hyper\nSuprime-Cam survey Year-3 catalogue of galaxy shapes to measure the weak\nlensing signal around these satellites. This signal serves as a probe of the\nmatter distribution around the satellites, thereby providing the masses of\ntheir associated subhalos. We bin our satellites based on physical observable\nquantities such as their luminosity or the host cluster's richness, combined\nwith their cluster-centric radial separations. Our results indicate that\nalthough more luminous satellites tend to reside in more massive halos, the\nsLHMRs depend on the distance of the satellite from the cluster centre.\nSubhalos near the cluster centre (within $<0.3 h^{-1}Mpc$) are stripped of\nmass. Consequently, the ratio of subhalo mass to luminosity decreases near the\ncluster centre. For low luminosity galaxies ($L < 10^{10} h^{-2}L_{\\odot}$),\nthe lack of evidence of increasing subhalo masses with luminosity shows the\nimpact of tidal stripping. We also present stellar-to-subhalo mass relations\n(sSHMRs) for our satellite sample evolving at different cluster-centric\nseparations. Inferred sSHMRs in the outer radial bin appear to match that\nobserved for the field galaxies. We show that the sSHMRs from the\nmock-redMaPPer run on galaxy catalogues generated by the empirical\nUniverseMachine galaxy formation model are in good agreement with our\nobservational results. Satellites, when binned based on the host cluster's\nrichness, show very little dependence of the subhalo mass on the richness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the luminosity-halo mass relations of satellite (sLHMRs) galaxies\nin the SDSS redMaPPer cluster catalogue and the effects of the dense cluster\nenvironment on subhalo mass evolution. We use data from the Subaru Hyper\nSuprime-Cam survey Year-3 catalogue of galaxy shapes to measure the weak\nlensing signal around these satellites. This signal serves as a probe of the\nmatter distribution around the satellites, thereby providing the masses of\ntheir associated subhalos. We bin our satellites based on physical observable\nquantities such as their luminosity or the host cluster's richness, combined\nwith their cluster-centric radial separations. Our results indicate that\nalthough more luminous satellites tend to reside in more massive halos, the\nsLHMRs depend on the distance of the satellite from the cluster centre.\nSubhalos near the cluster centre (within $<0.3 h^{-1}Mpc$) are stripped of\nmass. Consequently, the ratio of subhalo mass to luminosity decreases near the\ncluster centre. For low luminosity galaxies ($L < 10^{10} h^{-2}L_{\\odot}$),\nthe lack of evidence of increasing subhalo masses with luminosity shows the\nimpact of tidal stripping. We also present stellar-to-subhalo mass relations\n(sSHMRs) for our satellite sample evolving at different cluster-centric\nseparations. Inferred sSHMRs in the outer radial bin appear to match that\nobserved for the field galaxies. We show that the sSHMRs from the\nmock-redMaPPer run on galaxy catalogues generated by the empirical\nUniverseMachine galaxy formation model are in good agreement with our\nobservational results. Satellites, when binned based on the host cluster's\nrichness, show very little dependence of the subhalo mass on the richness."
                },
                "authors": [
                    {
                        "name": "Amit Kumar"
                    },
                    {
                        "name": "Surhud More"
                    }
                ],
                "author_detail": {
                    "name": "Surhud More"
                },
                "author": "Surhud More",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15256v2",
                "updated": "2024-09-09T16:42:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    42,
                    22,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-21T19:58:53Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    19,
                    58,
                    53,
                    6,
                    203,
                    0
                ],
                "title": "Weak-instrument-robust subvector inference in instrumental variables\n  regression: A subvector Lagrange multiplier test and properties of subvector\n  Anderson-Rubin confidence sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-instrument-robust subvector inference in instrumental variables\n  regression: A subvector Lagrange multiplier test and properties of subvector\n  Anderson-Rubin confidence sets"
                },
                "summary": "We propose a weak-instrument-robust subvector Lagrange multiplier test for\ninstrumental variables regression. We show that it is asymptotically\nsize-correct under a technical condition. This is the first\nweak-instrument-robust subvector test for instrumental variables regression to\nrecover the degrees of freedom of the commonly used non-weak-instrument-robust\nWald test. Additionally, we provide a closed-form solution for subvector\nconfidence sets obtained by inverting the subvector Anderson-Rubin test. We\nshow that they are centered around a k-class estimator. Also, we show that the\nsubvector confidence sets for single coefficients of the causal parameter are\njointly bounded if and only if Anderson's likelihood-ratio test rejects the\nhypothesis that the first-stage regression parameter is of reduced rank, that\nis, that the causal parameter is not identified. Finally, we show that if a\nconfidence set obtained by inverting the Anderson-Rubin test is bounded and\nnonempty, it is equal to a Wald-based confidence set with a data-dependent\nconfidence level. We explicitly compute this Wald-based confidence test.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a weak-instrument-robust subvector Lagrange multiplier test for\ninstrumental variables regression. We show that it is asymptotically\nsize-correct under a technical condition. This is the first\nweak-instrument-robust subvector test for instrumental variables regression to\nrecover the degrees of freedom of the commonly used non-weak-instrument-robust\nWald test. Additionally, we provide a closed-form solution for subvector\nconfidence sets obtained by inverting the subvector Anderson-Rubin test. We\nshow that they are centered around a k-class estimator. Also, we show that the\nsubvector confidence sets for single coefficients of the causal parameter are\njointly bounded if and only if Anderson's likelihood-ratio test rejects the\nhypothesis that the first-stage regression parameter is of reduced rank, that\nis, that the causal parameter is not identified. Finally, we show that if a\nconfidence set obtained by inverting the Anderson-Rubin test is bounded and\nnonempty, it is equal to a Wald-based confidence set with a data-dependent\nconfidence level. We explicitly compute this Wald-based confidence test."
                },
                "authors": [
                    {
                        "name": "Malte Londschien"
                    },
                    {
                        "name": "Peter Bühlmann"
                    }
                ],
                "author_detail": {
                    "name": "Peter Bühlmann"
                },
                "author": "Peter Bühlmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14774v2",
                "updated": "2024-09-09T16:41:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    41,
                    36,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-27T04:31:58Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    31,
                    58,
                    1,
                    240,
                    0
                ],
                "title": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning"
                },
                "summary": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings."
                },
                "authors": [
                    {
                        "name": "Simran Kaur"
                    },
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Sanjeev Arora"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Arora"
                },
                "author": "Sanjeev Arora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05771v1",
                "updated": "2024-09-09T16:33:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    33,
                    16,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:33:16Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    33,
                    16,
                    0,
                    253,
                    0
                ],
                "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language\n  Models"
                },
                "summary": "Research has repeatedly demonstrated that intermediate hidden states\nextracted from large language models are able to predict measured brain\nresponse to natural language stimuli. Yet, very little is known about the\nrepresentation properties that enable this high prediction performance. Why is\nit the intermediate layers, and not the output layers, that are most capable\nfor this unique and highly general transfer task? In this work, we show that\nevidence from language encoding models in fMRI supports the existence of a\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\nshow that this abstraction process naturally arises over the course of training\na language model and that the first \"composition\" phase of this abstraction\nprocess is compressed into fewer layers as training continues. Finally, we\ndemonstrate a strong correspondence between layerwise encoding performance and\nthe intrinsic dimensionality of representations from LLMs. We give initial\nevidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research has repeatedly demonstrated that intermediate hidden states\nextracted from large language models are able to predict measured brain\nresponse to natural language stimuli. Yet, very little is known about the\nrepresentation properties that enable this high prediction performance. Why is\nit the intermediate layers, and not the output layers, that are most capable\nfor this unique and highly general transfer task? In this work, we show that\nevidence from language encoding models in fMRI supports the existence of a\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\nshow that this abstraction process naturally arises over the course of training\na language model and that the first \"composition\" phase of this abstraction\nprocess is compressed into fewer layers as training continues. Finally, we\ndemonstrate a strong correspondence between layerwise encoding performance and\nthe intrinsic dimensionality of representations from LLMs. We give initial\nevidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties."
                },
                "authors": [
                    {
                        "name": "Emily Cheng"
                    },
                    {
                        "name": "Richard J. Antonello"
                    }
                ],
                "author_detail": {
                    "name": "Richard J. Antonello"
                },
                "author": "Richard J. Antonello",
                "arxiv_comment": "Equal contribution from both authors. Submitted to NeurIPS NeuroAI\n  workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05768v1",
                "updated": "2024-09-09T16:32:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    32,
                    14,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:32:14Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    32,
                    14,
                    0,
                    253,
                    0
                ],
                "title": "Model Input Verification of Large Scale Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Input Verification of Large Scale Simulations"
                },
                "summary": "Reliable simulations are critical for analyzing and understanding complex\nsystems, but their accuracy depends on correct input data. Incorrect inputs\nsuch as invalid or out-of-range values, missing data, and format\ninconsistencies can cause simulation crashes or unnoticed result distortions,\nultimately undermining the validity of the conclusions. This paper presents a\nmethodology for verifying the validity of input data in simulations, a process\nwe term model input verification (MIV). We implement this approach in FabGuard,\na toolset that uses established data schema and validation tools for the\nspecific needs of simulation modeling. We introduce a formalism for\ncategorizing MIV patterns and offer a streamlined verification pipeline that\nintegrates into existing simulation workflows. FabGuard's applicability is\ndemonstrated across three diverse domains: conflict-driven migration, disaster\nevacuation, and disease spread models. We also explore the use of Large\nLanguage Models (LLMs) for automating constraint generation and inference. In a\ncase study with a migration simulation, LLMs not only correctly inferred 22 out\nof 23 developer-defined constraints, but also identified errors in existing\nconstraints and proposed new, valid constraints. Our evaluation demonstrates\nthat MIV is feasible on large datasets, with FabGuard efficiently processing\n12,000 input files in 140 seconds and maintaining consistent performance across\nvarying file sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable simulations are critical for analyzing and understanding complex\nsystems, but their accuracy depends on correct input data. Incorrect inputs\nsuch as invalid or out-of-range values, missing data, and format\ninconsistencies can cause simulation crashes or unnoticed result distortions,\nultimately undermining the validity of the conclusions. This paper presents a\nmethodology for verifying the validity of input data in simulations, a process\nwe term model input verification (MIV). We implement this approach in FabGuard,\na toolset that uses established data schema and validation tools for the\nspecific needs of simulation modeling. We introduce a formalism for\ncategorizing MIV patterns and offer a streamlined verification pipeline that\nintegrates into existing simulation workflows. FabGuard's applicability is\ndemonstrated across three diverse domains: conflict-driven migration, disaster\nevacuation, and disease spread models. We also explore the use of Large\nLanguage Models (LLMs) for automating constraint generation and inference. In a\ncase study with a migration simulation, LLMs not only correctly inferred 22 out\nof 23 developer-defined constraints, but also identified errors in existing\nconstraints and proposed new, valid constraints. Our evaluation demonstrates\nthat MIV is feasible on large datasets, with FabGuard efficiently processing\n12,000 input files in 140 seconds and maintaining consistent performance across\nvarying file sizes."
                },
                "authors": [
                    {
                        "name": "Rumyana Neykova"
                    },
                    {
                        "name": "Derek Groen"
                    }
                ],
                "author_detail": {
                    "name": "Derek Groen"
                },
                "author": "Derek Groen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10999v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10999v3",
                "updated": "2024-09-09T16:28:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    28,
                    9,
                    0,
                    253,
                    0
                ],
                "published": "2024-06-16T16:25:22Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    16,
                    25,
                    22,
                    6,
                    168,
                    0
                ],
                "title": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions"
                },
                "summary": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications."
                },
                "authors": [
                    {
                        "name": "Liman Wang"
                    },
                    {
                        "name": "Hanyang Zhong"
                    },
                    {
                        "name": "Wenting Cao"
                    },
                    {
                        "name": "Zeyuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zeyuan Sun"
                },
                "author": "Zeyuan Sun",
                "arxiv_comment": "This article is currently under review. All data will be open on\n  GitHub once the review is complete.\n  https://github.com/limanwang/Balancing-Rigor-and-Utility",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10999v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10999v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05757v1",
                "updated": "2024-09-09T16:14:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    14,
                    47,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:14:47Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    14,
                    47,
                    0,
                    253,
                    0
                ],
                "title": "jaxspec : a fast and robust Python library for X-ray spectral fitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "jaxspec : a fast and robust Python library for X-ray spectral fitting"
                },
                "summary": "Context. Inferring spectral parameters from X-ray data is one of the\ncornerstones of high-energy astrophysics, and is achieved using software stacks\nthat have been developed over the last twenty years and more. However, as\nmodels get more complex and spectra reach higher resolutions, these established\nsoftware solutions become more feature-heavy, difficult to maintain and less\nefficient. Aims. We present jaxspec, a Python package for performing this task\nquickly and robustly in a fully Bayesian framework. Based on the JAX ecosystem,\njaxspec allows the generation of differentiable likelihood functions compilable\non core or graphical process units (resp. CPU and GPU), enabling the use of\nrobust algorithms for Bayesian inference. Methods. We demonstrate the\neffectiveness of jaxspec samplers, in particular the No U-Turn Sampler, using a\ncomposite model and comparing what we obtain with the existing frameworks. We\nalso demonstrate its ability to process high-resolution spectroscopy data and\nusing original methods, by reproducing the results of the Hitomi collaboration\non the Perseus cluster, while solving the inference problem using variational\ninference on a GPU. Results. We obtain identical results when compared to other\nsoftwares and approaches, meaning that jaxspec provides reliable results while\nbeing $\\sim 10$ times faster than existing alternatives. In addition, we show\nthat variational inference can produce convincing results even on\nhigh-resolution data in less than 10 minutes on a GPU. Conclusions. With this\npackage, we aim to pursue the goal of opening up X-ray spectroscopy to the\nexisting ecosystem of machine learning and Bayesian inference, enabling\nresearchers to apply new methods to solve increasingly complex problems in the\nbest possible way. Our long-term ambition is the scientific exploitation of the\ndata from the newAthena X-ray Integral Field Unit (X-IFU).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Inferring spectral parameters from X-ray data is one of the\ncornerstones of high-energy astrophysics, and is achieved using software stacks\nthat have been developed over the last twenty years and more. However, as\nmodels get more complex and spectra reach higher resolutions, these established\nsoftware solutions become more feature-heavy, difficult to maintain and less\nefficient. Aims. We present jaxspec, a Python package for performing this task\nquickly and robustly in a fully Bayesian framework. Based on the JAX ecosystem,\njaxspec allows the generation of differentiable likelihood functions compilable\non core or graphical process units (resp. CPU and GPU), enabling the use of\nrobust algorithms for Bayesian inference. Methods. We demonstrate the\neffectiveness of jaxspec samplers, in particular the No U-Turn Sampler, using a\ncomposite model and comparing what we obtain with the existing frameworks. We\nalso demonstrate its ability to process high-resolution spectroscopy data and\nusing original methods, by reproducing the results of the Hitomi collaboration\non the Perseus cluster, while solving the inference problem using variational\ninference on a GPU. Results. We obtain identical results when compared to other\nsoftwares and approaches, meaning that jaxspec provides reliable results while\nbeing $\\sim 10$ times faster than existing alternatives. In addition, we show\nthat variational inference can produce convincing results even on\nhigh-resolution data in less than 10 minutes on a GPU. Conclusions. With this\npackage, we aim to pursue the goal of opening up X-ray spectroscopy to the\nexisting ecosystem of machine learning and Bayesian inference, enabling\nresearchers to apply new methods to solve increasingly complex problems in the\nbest possible way. Our long-term ambition is the scientific exploitation of the\ndata from the newAthena X-ray Integral Field Unit (X-IFU)."
                },
                "authors": [
                    {
                        "name": "Simon Dupourqué"
                    },
                    {
                        "name": "Didier Barret"
                    },
                    {
                        "name": "Camille M. Diez"
                    },
                    {
                        "name": "Sébastien Guillot"
                    },
                    {
                        "name": "Erwan Quintin"
                    }
                ],
                "author_detail": {
                    "name": "Erwan Quintin"
                },
                "author": "Erwan Quintin",
                "arxiv_comment": "Accepted for publication in A&A. Code and documentation can be found\n  at jaxspec.readthedocs.io/en/latest/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05747v1",
                "updated": "2024-09-09T16:02:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    2,
                    27,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:02:27Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    2,
                    27,
                    0,
                    253,
                    0
                ],
                "title": "A Novel Idea Generation Tool using a Structured Conversational AI (CAI)\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Idea Generation Tool using a Structured Conversational AI (CAI)\n  System"
                },
                "summary": "This paper presents a novel conversational AI-enabled active ideation\ninterface as a creative idea-generation tool to assist novice designers in\nmitigating the initial latency and ideation bottlenecks that are commonly\nobserved. It is a dynamic, interactive, and contextually responsive approach,\nactively involving a large language model (LLM) from the domain of natural\nlanguage processing (NLP) in artificial intelligence (AI) to produce multiple\nstatements of potential ideas for different design problems. Integrating such\nAI models with ideation creates what we refer to as an Active Ideation\nscenario, which helps foster continuous dialogue-based interaction,\ncontext-sensitive conversation, and prolific idea generation. A pilot study was\nconducted with thirty novice designers to generate ideas for given problems\nusing traditional methods and the new CAI-based interface. The key parameters\nof fluency, novelty, and variety were used to compare the outcomes\nqualitatively by a panel of experts. The findings demonstrated the\neffectiveness of the proposed tool for generating prolific, diverse and novel\nideas. The interface was enhanced by incorporating a prompt-engineered\nstructured dialogue style for each ideation stage to make it uniform and more\nconvenient for the designers. The resulting responses of such a structured CAI\ninterface were found to be more succinct and aligned towards the subsequent\ndesign stage, namely conceptualization. The paper thus established the rich\npotential of using Generative AI (Gen-AI) for the early ill-structured phase of\nthe creative product design process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel conversational AI-enabled active ideation\ninterface as a creative idea-generation tool to assist novice designers in\nmitigating the initial latency and ideation bottlenecks that are commonly\nobserved. It is a dynamic, interactive, and contextually responsive approach,\nactively involving a large language model (LLM) from the domain of natural\nlanguage processing (NLP) in artificial intelligence (AI) to produce multiple\nstatements of potential ideas for different design problems. Integrating such\nAI models with ideation creates what we refer to as an Active Ideation\nscenario, which helps foster continuous dialogue-based interaction,\ncontext-sensitive conversation, and prolific idea generation. A pilot study was\nconducted with thirty novice designers to generate ideas for given problems\nusing traditional methods and the new CAI-based interface. The key parameters\nof fluency, novelty, and variety were used to compare the outcomes\nqualitatively by a panel of experts. The findings demonstrated the\neffectiveness of the proposed tool for generating prolific, diverse and novel\nideas. The interface was enhanced by incorporating a prompt-engineered\nstructured dialogue style for each ideation stage to make it uniform and more\nconvenient for the designers. The resulting responses of such a structured CAI\ninterface were found to be more succinct and aligned towards the subsequent\ndesign stage, namely conceptualization. The paper thus established the rich\npotential of using Generative AI (Gen-AI) for the early ill-structured phase of\nthe creative product design process."
                },
                "authors": [
                    {
                        "name": "B. Sankar"
                    },
                    {
                        "name": "Dibakar Sen"
                    }
                ],
                "author_detail": {
                    "name": "Dibakar Sen"
                },
                "author": "Dibakar Sen",
                "arxiv_comment": "21 pages, 16 figures, AIEDAM Journal Article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; J.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05746v1",
                "updated": "2024-09-09T16:01:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    1,
                    58,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:01:58Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    1,
                    58,
                    0,
                    253,
                    0
                ],
                "title": "LLMs Will Always Hallucinate, and We Need to Live With This",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Will Always Hallucinate, and We Need to Live With This"
                },
                "summary": "As Large Language Models become more ubiquitous across domains, it becomes\nimportant to examine their inherent limitations critically. This work argues\nthat hallucinations in language models are not just occasional errors but an\ninevitable feature of these systems. We demonstrate that hallucinations stem\nfrom the fundamental mathematical and logical structure of LLMs. It is,\ntherefore, impossible to eliminate them through architectural improvements,\ndataset enhancements, or fact-checking mechanisms. Our analysis draws on\ncomputational theory and Godel's First Incompleteness Theorem, which references\nthe undecidability of problems like the Halting, Emptiness, and Acceptance\nProblems. We demonstrate that every stage of the LLM process-from training data\ncompilation to fact retrieval, intent classification, and text generation-will\nhave a non-zero probability of producing hallucinations. This work introduces\nthe concept of Structural Hallucination as an intrinsic nature of these\nsystems. By establishing the mathematical certainty of hallucinations, we\nchallenge the prevailing notion that they can be fully mitigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models become more ubiquitous across domains, it becomes\nimportant to examine their inherent limitations critically. This work argues\nthat hallucinations in language models are not just occasional errors but an\ninevitable feature of these systems. We demonstrate that hallucinations stem\nfrom the fundamental mathematical and logical structure of LLMs. It is,\ntherefore, impossible to eliminate them through architectural improvements,\ndataset enhancements, or fact-checking mechanisms. Our analysis draws on\ncomputational theory and Godel's First Incompleteness Theorem, which references\nthe undecidability of problems like the Halting, Emptiness, and Acceptance\nProblems. We demonstrate that every stage of the LLM process-from training data\ncompilation to fact retrieval, intent classification, and text generation-will\nhave a non-zero probability of producing hallucinations. This work introduces\nthe concept of Structural Hallucination as an intrinsic nature of these\nsystems. By establishing the mathematical certainty of hallucinations, we\nchallenge the prevailing notion that they can be fully mitigated."
                },
                "authors": [
                    {
                        "name": "Sourav Banerjee"
                    },
                    {
                        "name": "Ayushi Agarwal"
                    },
                    {
                        "name": "Saloni Singla"
                    }
                ],
                "author_detail": {
                    "name": "Saloni Singla"
                },
                "author": "Saloni Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18799v2",
                "updated": "2024-09-09T16:00:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    0,
                    4,
                    0,
                    253,
                    0
                ],
                "published": "2023-11-30T18:43:51Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    18,
                    43,
                    51,
                    3,
                    334,
                    0
                ],
                "title": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\n  representations to LLMs and Emergent Cross-modal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\n  representations to LLMs and Emergent Cross-modal Reasoning"
                },
                "summary": "Recent research has achieved significant advancements in visual reasoning\ntasks through learning image-to-language projections and leveraging the\nimpressive reasoning abilities of Large Language Models (LLMs). This paper\nintroduces an efficient and effective framework that integrates multiple\nmodalities (images, 3D, audio and video) to a frozen LLM and demonstrates an\nemergent ability for cross-modal reasoning (2+ modality inputs). Our approach\nexplores two distinct projection mechanisms: Q-Formers and Linear Projections\n(LPs). Through extensive experimentation across all four modalities on 16\nbenchmarks, we explore both methods and assess their adaptability in integrated\nand separate cross-modal reasoning. The Q-Former projection demonstrates\nsuperior performance in single modality scenarios and adaptability in joint\nversus discriminative reasoning involving two or more modalities. However, it\nexhibits lower generalization capabilities than linear projection in contexts\nwhere task-modality data are limited. To enable this framework, we devise a\nscalable pipeline that automatically generates high-quality, instruction-tuning\ndatasets from readily available captioning data across different modalities,\nand contribute 24K QA data for audio and 250K QA data for 3D. To facilitate\nfurther research in cross-modal reasoning, we introduce the DisCRn\n(Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA\nsamples and 28K image-3D QA samples that require the model to reason\ndiscriminatively across disparate input modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has achieved significant advancements in visual reasoning\ntasks through learning image-to-language projections and leveraging the\nimpressive reasoning abilities of Large Language Models (LLMs). This paper\nintroduces an efficient and effective framework that integrates multiple\nmodalities (images, 3D, audio and video) to a frozen LLM and demonstrates an\nemergent ability for cross-modal reasoning (2+ modality inputs). Our approach\nexplores two distinct projection mechanisms: Q-Formers and Linear Projections\n(LPs). Through extensive experimentation across all four modalities on 16\nbenchmarks, we explore both methods and assess their adaptability in integrated\nand separate cross-modal reasoning. The Q-Former projection demonstrates\nsuperior performance in single modality scenarios and adaptability in joint\nversus discriminative reasoning involving two or more modalities. However, it\nexhibits lower generalization capabilities than linear projection in contexts\nwhere task-modality data are limited. To enable this framework, we devise a\nscalable pipeline that automatically generates high-quality, instruction-tuning\ndatasets from readily available captioning data across different modalities,\nand contribute 24K QA data for audio and 250K QA data for 3D. To facilitate\nfurther research in cross-modal reasoning, we introduce the DisCRn\n(Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA\nsamples and 28K image-3D QA samples that require the model to reason\ndiscriminatively across disparate input modalities."
                },
                "authors": [
                    {
                        "name": "Artemis Panagopoulou"
                    },
                    {
                        "name": "Le Xue"
                    },
                    {
                        "name": "Ning Yu"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Niebles"
                },
                "author": "Juan Carlos Niebles",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05737v1",
                "updated": "2024-09-09T15:49:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    49,
                    27,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:49:27Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    49,
                    27,
                    0,
                    253,
                    0
                ],
                "title": "Examining the Relationship Between the Persistent Emission and the\n  Accretion Rate During a Type I X-ray Burst",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Relationship Between the Persistent Emission and the\n  Accretion Rate During a Type I X-ray Burst"
                },
                "summary": "The accretion flow onto a neutron star will be impacted due to irradiation by\na Type I X-ray burst. The burst radiation exerts Poynting-Robertson (PR) drag\non the accretion disk, leading to an enhanced mass accretion rate. Observations\nof X-ray bursts often find evidence that the normalization of the\ndisk-generated persistent emission (commonly denoted by the factor $f_a$)\nincreases during a burst, and changes in $f_a$ have been used to infer the\nevolution in the mass accretion rate due to PR drag. Here, we examine this\nproposed relationship between $f_a$ and mass accretion rate enhancement using\ntime-resolved data from simulations of accretion disks impacted by Type I X-ray\nbursts. We consider bursts from both spinning and non-spinning neutron stars\nand track both the change in accretion rate due to PR grad and the disk\nemission spectra during the burst. Regardless of the neutron star spin, we find\nthat $f_a$ strongly correlates with the disk temperature and only weakly\nfollows the mass accretion rate (the Pearson correlation coefficients are $\\leq\n0.63$ in the latter case). Additionally, heating causes the disk to emit at\nhigher energies, reducing its contribution to a soft excess. We conclude that\n$f_a$ cannot accurately capture the mass accretion rate enhancement and is\nrather a tracer of the disk temperature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The accretion flow onto a neutron star will be impacted due to irradiation by\na Type I X-ray burst. The burst radiation exerts Poynting-Robertson (PR) drag\non the accretion disk, leading to an enhanced mass accretion rate. Observations\nof X-ray bursts often find evidence that the normalization of the\ndisk-generated persistent emission (commonly denoted by the factor $f_a$)\nincreases during a burst, and changes in $f_a$ have been used to infer the\nevolution in the mass accretion rate due to PR drag. Here, we examine this\nproposed relationship between $f_a$ and mass accretion rate enhancement using\ntime-resolved data from simulations of accretion disks impacted by Type I X-ray\nbursts. We consider bursts from both spinning and non-spinning neutron stars\nand track both the change in accretion rate due to PR grad and the disk\nemission spectra during the burst. Regardless of the neutron star spin, we find\nthat $f_a$ strongly correlates with the disk temperature and only weakly\nfollows the mass accretion rate (the Pearson correlation coefficients are $\\leq\n0.63$ in the latter case). Additionally, heating causes the disk to emit at\nhigher energies, reducing its contribution to a soft excess. We conclude that\n$f_a$ cannot accurately capture the mass accretion rate enhancement and is\nrather a tracer of the disk temperature."
                },
                "authors": [
                    {
                        "name": "J. Speicher"
                    },
                    {
                        "name": "D. R. Ballantyne"
                    },
                    {
                        "name": "P. C. Fragile"
                    }
                ],
                "author_detail": {
                    "name": "P. C. Fragile"
                },
                "arxiv_affiliation": "Department of Physics & Astronomy, College of Charleston",
                "author": "P. C. Fragile",
                "arxiv_comment": "Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05735v1",
                "updated": "2024-09-09T15:44:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    44,
                    39,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:44:39Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    44,
                    39,
                    0,
                    253,
                    0
                ],
                "title": "A System and Benchmark for LLM-based Q\\&A on Heterogeneous Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System and Benchmark for LLM-based Q\\&A on Heterogeneous Data"
                },
                "summary": "In many industrial settings, users wish to ask questions whose answers may be\nfound in structured data sources such as a spreadsheets, databases, APIs, or\ncombinations thereof. Often, the user doesn't know how to identify or access\nthe right data source. This problem is compounded even further if multiple (and\npotentially siloed) data sources must be assembled to derive the answer.\nRecently, various Text-to-SQL applications that leverage Large Language Models\n(LLMs) have addressed some of these problems by enabling users to ask questions\nin natural language. However, these applications remain impractical in\nrealistic industrial settings because they fail to cope with the data source\nheterogeneity that typifies such environments. In this paper, we address\nheterogeneity by introducing the siwarex platform, which enables seamless\nnatural language access to both databases and APIs. To demonstrate the\neffectiveness of siwarex, we extend the popular Spider dataset and benchmark by\nreplacing some of its tables by data retrieval APIs. We find that siwarex does\na good job of coping with data source heterogeneity. Our modified Spider\nbenchmark will soon be available to the research community",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many industrial settings, users wish to ask questions whose answers may be\nfound in structured data sources such as a spreadsheets, databases, APIs, or\ncombinations thereof. Often, the user doesn't know how to identify or access\nthe right data source. This problem is compounded even further if multiple (and\npotentially siloed) data sources must be assembled to derive the answer.\nRecently, various Text-to-SQL applications that leverage Large Language Models\n(LLMs) have addressed some of these problems by enabling users to ask questions\nin natural language. However, these applications remain impractical in\nrealistic industrial settings because they fail to cope with the data source\nheterogeneity that typifies such environments. In this paper, we address\nheterogeneity by introducing the siwarex platform, which enables seamless\nnatural language access to both databases and APIs. To demonstrate the\neffectiveness of siwarex, we extend the popular Spider dataset and benchmark by\nreplacing some of its tables by data retrieval APIs. We find that siwarex does\na good job of coping with data source heterogeneity. Our modified Spider\nbenchmark will soon be available to the research community"
                },
                "authors": [
                    {
                        "name": "Achille Fokoue"
                    },
                    {
                        "name": "Srideepika Jayaraman"
                    },
                    {
                        "name": "Elham Khabiri"
                    },
                    {
                        "name": "Jeffrey O. Kephart"
                    },
                    {
                        "name": "Yingjie Li"
                    },
                    {
                        "name": "Dhruv Shah"
                    },
                    {
                        "name": "Youssef Drissi"
                    },
                    {
                        "name": "Fenno F. Heath III"
                    },
                    {
                        "name": "Anu Bhamidipaty"
                    },
                    {
                        "name": "Fateh A. Tipu"
                    },
                    {
                        "name": "Robert J. Baseman"
                    }
                ],
                "author_detail": {
                    "name": "Robert J. Baseman"
                },
                "author": "Robert J. Baseman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05733v1",
                "updated": "2024-09-09T15:42:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    42,
                    28,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:42:28Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    42,
                    28,
                    0,
                    253,
                    0
                ],
                "title": "Markov Chain Variance Estimation: A Stochastic Approximation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain Variance Estimation: A Stochastic Approximation Approach"
                },
                "summary": "We consider the problem of estimating the asymptotic variance of a function\ndefined on a Markov chain, an important step for statistical inference of the\nstationary mean. We design the first recursive estimator that requires $O(1)$\ncomputation at each step, does not require storing any historical samples or\nany prior knowledge of run-length, and has optimal $O(\\frac{1}{n})$ rate of\nconvergence for the mean-squared error (MSE) with provable finite sample\nguarantees. Here, $n$ refers to the total number of samples generated. The\npreviously best-known rate of convergence in MSE was $O(\\frac{\\log n}{n})$,\nachieved by jackknifed estimators, which also do not enjoy these other\ndesirable properties. Our estimator is based on linear stochastic approximation\nof an equivalent formulation of the asymptotic variance in terms of the\nsolution of the Poisson equation.\n  We generalize our estimator in several directions, including estimating the\ncovariance matrix for vector-valued functions, estimating the stationary\nvariance of a Markov chain, and approximately estimating the asymptotic\nvariance in settings where the state space of the underlying Markov chain is\nlarge. We also show applications of our estimator in average reward\nreinforcement learning (RL), where we work with asymptotic variance as a risk\nmeasure to model safety-critical applications. We design a temporal-difference\ntype algorithm tailored for policy evaluation in this context. We consider both\nthe tabular and linear function approximation settings. Our work paves the way\nfor developing actor-critic style algorithms for variance-constrained RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of estimating the asymptotic variance of a function\ndefined on a Markov chain, an important step for statistical inference of the\nstationary mean. We design the first recursive estimator that requires $O(1)$\ncomputation at each step, does not require storing any historical samples or\nany prior knowledge of run-length, and has optimal $O(\\frac{1}{n})$ rate of\nconvergence for the mean-squared error (MSE) with provable finite sample\nguarantees. Here, $n$ refers to the total number of samples generated. The\npreviously best-known rate of convergence in MSE was $O(\\frac{\\log n}{n})$,\nachieved by jackknifed estimators, which also do not enjoy these other\ndesirable properties. Our estimator is based on linear stochastic approximation\nof an equivalent formulation of the asymptotic variance in terms of the\nsolution of the Poisson equation.\n  We generalize our estimator in several directions, including estimating the\ncovariance matrix for vector-valued functions, estimating the stationary\nvariance of a Markov chain, and approximately estimating the asymptotic\nvariance in settings where the state space of the underlying Markov chain is\nlarge. We also show applications of our estimator in average reward\nreinforcement learning (RL), where we work with asymptotic variance as a risk\nmeasure to model safety-critical applications. We design a temporal-difference\ntype algorithm tailored for policy evaluation in this context. We consider both\nthe tabular and linear function approximation settings. Our work paves the way\nfor developing actor-critic style algorithms for variance-constrained RL."
                },
                "authors": [
                    {
                        "name": "Shubhada Agrawal"
                    },
                    {
                        "name": "Prashanth L. A."
                    },
                    {
                        "name": "Siva Theja Maguluri"
                    }
                ],
                "author_detail": {
                    "name": "Siva Theja Maguluri"
                },
                "author": "Siva Theja Maguluri",
                "arxiv_comment": "61 pages, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05732v1",
                "updated": "2024-09-09T15:42:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    42,
                    19,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:42:19Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    42,
                    19,
                    0,
                    253,
                    0
                ],
                "title": "Towards Democratizing Multilingual Large Language Models For Medicine\n  Through A Two-Stage Instruction Fine-tuning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Democratizing Multilingual Large Language Models For Medicine\n  Through A Two-Stage Instruction Fine-tuning Approach"
                },
                "summary": "Open-source, multilingual medical large language models (LLMs) have the\npotential to serve linguistically diverse populations across different regions.\nAdapting generic LLMs for healthcare often requires continual pretraining, but\nthis approach is computationally expensive and sometimes impractical.\nInstruction fine-tuning on a specific task may not always guarantee optimal\nperformance due to the lack of broader domain knowledge that the model needs to\nunderstand and reason effectively in diverse scenarios. To address these\nchallenges, we introduce two multilingual instruction fine-tuning datasets,\nMMed-IFT and MMed-IFT-MC, containing over 200k high-quality medical samples in\nsix languages. We propose a two-stage training paradigm: the first stage\ninjects general medical knowledge using MMed-IFT, while the second stage\nfine-tunes task-specific multiple-choice questions with MMed-IFT-MC. Our method\nachieves competitive results on both English and multilingual benchmarks,\nstriking a balance between computational efficiency and performance. We plan to\nmake our dataset and model weights public at\n\\url{https://github.com/SpassMed/Med-Llama3} in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source, multilingual medical large language models (LLMs) have the\npotential to serve linguistically diverse populations across different regions.\nAdapting generic LLMs for healthcare often requires continual pretraining, but\nthis approach is computationally expensive and sometimes impractical.\nInstruction fine-tuning on a specific task may not always guarantee optimal\nperformance due to the lack of broader domain knowledge that the model needs to\nunderstand and reason effectively in diverse scenarios. To address these\nchallenges, we introduce two multilingual instruction fine-tuning datasets,\nMMed-IFT and MMed-IFT-MC, containing over 200k high-quality medical samples in\nsix languages. We propose a two-stage training paradigm: the first stage\ninjects general medical knowledge using MMed-IFT, while the second stage\nfine-tunes task-specific multiple-choice questions with MMed-IFT-MC. Our method\nachieves competitive results on both English and multilingual benchmarks,\nstriking a balance between computational efficiency and performance. We plan to\nmake our dataset and model weights public at\n\\url{https://github.com/SpassMed/Med-Llama3} in the future."
                },
                "authors": [
                    {
                        "name": "Meng Zhou"
                    },
                    {
                        "name": "Surajsinh Parmar"
                    },
                    {
                        "name": "Anubhav Bhatti"
                    }
                ],
                "author_detail": {
                    "name": "Anubhav Bhatti"
                },
                "author": "Anubhav Bhatti",
                "arxiv_comment": "Technical Report v1, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05726v1",
                "updated": "2024-09-09T15:39:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    39,
                    59,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:39:59Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    39,
                    59,
                    0,
                    253,
                    0
                ],
                "title": "Optical Spiking Neurons Enable High-Speed and Energy-Efficient Optical\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Spiking Neurons Enable High-Speed and Energy-Efficient Optical\n  Neural Networks"
                },
                "summary": "Optical neural networks (ONNs) perform extensive computations using photons\ninstead of electrons, resulting in passively energy-efficient and low-latency\ncomputing. Among various ONNs, the diffractive optical neural networks (DONNs)\nparticularly excel in energy efficiency, bandwidth, and parallelism, therefore\nattract considerable attention. However, their performance is limited by the\ninherent constraints of traditional frame-based sensors, which process and\nproduce dense and redundant information at low operating frequency. Inspired by\nthe spiking neurons in human neural system, which utilize a thresholding\nmechanism to transmit information sparsely and efficiently, we propose\nintegrating a threshold-locking method into neuromorphic vision sensors to\ngenerate sparse and binary information, achieving microsecond-level accurate\nperception similar to human spiking neurons. By introducing novel Binary Dual\nAdaptive Training (BAT) and Optically Parallel Mixture of Experts (OPMoE)\ninference methods, the high-speed, spike-based diffractive optical neural\nnetwork (S2NN) demonstrates an ultra-fast operating speed of 3649 FPS, which is\n30 fold faster than that of reported DONNs, delivering a remarkable\ncomputational speed of 417.96 TOPS and a system energy efficiency of 12.6\nTOPS/W. Our work demonstrates the potential of incorporating neuromorphic\narchitecture to facilitate optical neural network applications in real-world\nscenarios for both low-level and high-level machine vision tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical neural networks (ONNs) perform extensive computations using photons\ninstead of electrons, resulting in passively energy-efficient and low-latency\ncomputing. Among various ONNs, the diffractive optical neural networks (DONNs)\nparticularly excel in energy efficiency, bandwidth, and parallelism, therefore\nattract considerable attention. However, their performance is limited by the\ninherent constraints of traditional frame-based sensors, which process and\nproduce dense and redundant information at low operating frequency. Inspired by\nthe spiking neurons in human neural system, which utilize a thresholding\nmechanism to transmit information sparsely and efficiently, we propose\nintegrating a threshold-locking method into neuromorphic vision sensors to\ngenerate sparse and binary information, achieving microsecond-level accurate\nperception similar to human spiking neurons. By introducing novel Binary Dual\nAdaptive Training (BAT) and Optically Parallel Mixture of Experts (OPMoE)\ninference methods, the high-speed, spike-based diffractive optical neural\nnetwork (S2NN) demonstrates an ultra-fast operating speed of 3649 FPS, which is\n30 fold faster than that of reported DONNs, delivering a remarkable\ncomputational speed of 417.96 TOPS and a system energy efficiency of 12.6\nTOPS/W. Our work demonstrates the potential of incorporating neuromorphic\narchitecture to facilitate optical neural network applications in real-world\nscenarios for both low-level and high-level machine vision tasks."
                },
                "authors": [
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Zefeng Huang"
                    },
                    {
                        "name": "Yuetong Fang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Bojun Cheng"
                    },
                    {
                        "name": "Shaoliang Yu"
                    },
                    {
                        "name": "Zhongrui Wang"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05716v1",
                "updated": "2024-09-09T15:25:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    25,
                    53,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:25:53Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    25,
                    53,
                    0,
                    253,
                    0
                ],
                "title": "Implications of feedback solutions to the $S_8$ tension for the baryon\n  fractions of galaxy groups and clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implications of feedback solutions to the $S_8$ tension for the baryon\n  fractions of galaxy groups and clusters"
                },
                "summary": "Recent large-scale structure (LSS) surveys have revealed a persistent tension\nin the value of $S_8$ compared to predictions from the standard cosmological\nmodel. This tension may suggest the need for new physics beyond the standard\nmodel, but an accurate characterisation of baryonic effects is essential to\navoid biases. Although some studies indicate that baryonic effects are too\nsmall to resolve this tension, others propose that more aggressive feedback\nmechanisms could reconcile differences between cosmic microwave background\n(CMB) measurements and low-redshift LSS observations. In this paper, we\ninvestigate the role of baryonic effects in alleviating the $S_8$ tension. We\nextend the SP(k) model (Salcido et al. 2023), which was trained on hundreds of\ncosmological hydrodynamical simulations to map the suppression of the matter\npower spectrum to the baryon fraction in groups and clusters, to predict the\nrequired baryon fraction for a given $P(k)$ suppression. We then compare\npredictions from recent cosmic shear (weak lensing) analyses with the latest\nbaryon budget measurements from X-ray and weak gravitational lensing studies.\nOur findings show that studies marginalising over baryonic effects while fixing\ncosmological parameters to a Planck-like cosmology predict strong $P(k)$\nsuppression and baryon fractions that are much lower than existing low-redshift\nbaryon budget estimates of galaxy groups and clusters. Conversely, most studies\nthat marginalise over both cosmological parameters and baryonic effects imply\nbaryon fractions that are consistent with observations but lower values of\n$S_8$ than inferred from the CMB. Unless the observed baryon fractions are\nbiased high by a factor of several, these results suggest that a mechanism\nbeyond baryonic physics alone is required to modify or slow down the growth of\nstructure in the universe in order to resolve the $S_8$ tension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large-scale structure (LSS) surveys have revealed a persistent tension\nin the value of $S_8$ compared to predictions from the standard cosmological\nmodel. This tension may suggest the need for new physics beyond the standard\nmodel, but an accurate characterisation of baryonic effects is essential to\navoid biases. Although some studies indicate that baryonic effects are too\nsmall to resolve this tension, others propose that more aggressive feedback\nmechanisms could reconcile differences between cosmic microwave background\n(CMB) measurements and low-redshift LSS observations. In this paper, we\ninvestigate the role of baryonic effects in alleviating the $S_8$ tension. We\nextend the SP(k) model (Salcido et al. 2023), which was trained on hundreds of\ncosmological hydrodynamical simulations to map the suppression of the matter\npower spectrum to the baryon fraction in groups and clusters, to predict the\nrequired baryon fraction for a given $P(k)$ suppression. We then compare\npredictions from recent cosmic shear (weak lensing) analyses with the latest\nbaryon budget measurements from X-ray and weak gravitational lensing studies.\nOur findings show that studies marginalising over baryonic effects while fixing\ncosmological parameters to a Planck-like cosmology predict strong $P(k)$\nsuppression and baryon fractions that are much lower than existing low-redshift\nbaryon budget estimates of galaxy groups and clusters. Conversely, most studies\nthat marginalise over both cosmological parameters and baryonic effects imply\nbaryon fractions that are consistent with observations but lower values of\n$S_8$ than inferred from the CMB. Unless the observed baryon fractions are\nbiased high by a factor of several, these results suggest that a mechanism\nbeyond baryonic physics alone is required to modify or slow down the growth of\nstructure in the universe in order to resolve the $S_8$ tension."
                },
                "authors": [
                    {
                        "name": "Jaime Salcido"
                    },
                    {
                        "name": "Ian G. McCarthy"
                    }
                ],
                "author_detail": {
                    "name": "Ian G. McCarthy"
                },
                "author": "Ian G. McCarthy",
                "arxiv_comment": "Submitted to MNRAS. 12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05715v1",
                "updated": "2024-09-09T15:25:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    25,
                    41,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:25:41Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    25,
                    41,
                    0,
                    253,
                    0
                ],
                "title": "Uniform Estimation and Inference for Nonparametric Partitioning-Based\n  M-Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uniform Estimation and Inference for Nonparametric Partitioning-Based\n  M-Estimators"
                },
                "summary": "This paper presents uniform estimation and inference theory for a large class\nof nonparametric partitioning-based M-estimators. The main theoretical results\ninclude: (i) uniform consistency for convex and non-convex objective functions;\n(ii) optimal uniform Bahadur representations; (iii) optimal uniform (and mean\nsquare) convergence rates; (iv) valid strong approximations and feasible\nuniform inference methods; and (v) extensions to functional transformations of\nunderlying estimators. Uniformity is established over both the evaluation point\nof the nonparametric functional parameter and a Euclidean parameter indexing\nthe class of loss functions. The results also account explicitly for the\nsmoothness degree of the loss function (if any), and allow for a possibly\nnon-identity (inverse) link function. We illustrate the main theoretical and\nmethodological results with four substantive applications: quantile regression,\ndistribution regression, $L_p$ regression, and Logistic regression; many other\npossibly non-smooth, nonlinear, generalized, robust M-estimation settings are\ncovered by our theoretical results. We provide detailed comparisons with the\nexisting literature and demonstrate substantive improvements: we achieve the\nbest (in some cases optimal) known results under improved (in some cases\nminimal) requirements in terms of regularity conditions and side rate\nrestrictions. The supplemental appendix reports other technical results that\nmay be of independent interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents uniform estimation and inference theory for a large class\nof nonparametric partitioning-based M-estimators. The main theoretical results\ninclude: (i) uniform consistency for convex and non-convex objective functions;\n(ii) optimal uniform Bahadur representations; (iii) optimal uniform (and mean\nsquare) convergence rates; (iv) valid strong approximations and feasible\nuniform inference methods; and (v) extensions to functional transformations of\nunderlying estimators. Uniformity is established over both the evaluation point\nof the nonparametric functional parameter and a Euclidean parameter indexing\nthe class of loss functions. The results also account explicitly for the\nsmoothness degree of the loss function (if any), and allow for a possibly\nnon-identity (inverse) link function. We illustrate the main theoretical and\nmethodological results with four substantive applications: quantile regression,\ndistribution regression, $L_p$ regression, and Logistic regression; many other\npossibly non-smooth, nonlinear, generalized, robust M-estimation settings are\ncovered by our theoretical results. We provide detailed comparisons with the\nexisting literature and demonstrate substantive improvements: we achieve the\nbest (in some cases optimal) known results under improved (in some cases\nminimal) requirements in terms of regularity conditions and side rate\nrestrictions. The supplemental appendix reports other technical results that\nmay be of independent interest."
                },
                "authors": [
                    {
                        "name": "Matias D. Cattaneo"
                    },
                    {
                        "name": "Yingjie Feng"
                    },
                    {
                        "name": "Boris Shigida"
                    }
                ],
                "author_detail": {
                    "name": "Boris Shigida"
                },
                "author": "Boris Shigida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03491v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03491v2",
                "updated": "2024-09-09T15:24:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    24,
                    15,
                    0,
                    253,
                    0
                ],
                "published": "2024-05-06T14:02:59Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    14,
                    2,
                    59,
                    0,
                    127,
                    0
                ],
                "title": "Jointly Learning Cost and Constraints from Demonstrations for Safe\n  Trajectory Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jointly Learning Cost and Constraints from Demonstrations for Safe\n  Trajectory Generation"
                },
                "summary": "Learning from Demonstration allows robots to mimic human actions. However,\nthese methods do not model constraints crucial to ensure safety of the learned\nskill. Moreover, even when explicitly modelling constraints, they rely on the\nassumption of a known cost function, which limits their practical usability for\ntask with unknown cost. In this work we propose a two-step optimization process\nthat allow to estimate cost and constraints by decoupling the learning of cost\nfunctions from the identification of unknown constraints within the\ndemonstrated trajectories. Initially, we identify the cost function by\nisolating the effect of constraints on parts of the demonstrations.\nSubsequently, a constraint leaning method is used to identify the unknown\nconstraints. Our approach is validated both on simulated trajectories and a\nreal robotic manipulation task. Our experiments show the impact that incorrect\ncost estimation has on the learned constraints and illustrate how the proposed\nmethod is able to infer unknown constraints, such as obstacles, from\ndemonstrated trajectories without any initial knowledge of the cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Demonstration allows robots to mimic human actions. However,\nthese methods do not model constraints crucial to ensure safety of the learned\nskill. Moreover, even when explicitly modelling constraints, they rely on the\nassumption of a known cost function, which limits their practical usability for\ntask with unknown cost. In this work we propose a two-step optimization process\nthat allow to estimate cost and constraints by decoupling the learning of cost\nfunctions from the identification of unknown constraints within the\ndemonstrated trajectories. Initially, we identify the cost function by\nisolating the effect of constraints on parts of the demonstrations.\nSubsequently, a constraint leaning method is used to identify the unknown\nconstraints. Our approach is validated both on simulated trajectories and a\nreal robotic manipulation task. Our experiments show the impact that incorrect\ncost estimation has on the learned constraints and illustrate how the proposed\nmethod is able to infer unknown constraints, such as obstacles, from\ndemonstrated trajectories without any initial knowledge of the cost."
                },
                "authors": [
                    {
                        "name": "Shivam Chaubey"
                    },
                    {
                        "name": "Francesco Verdoja"
                    },
                    {
                        "name": "Ville Kyrki"
                    }
                ],
                "author_detail": {
                    "name": "Ville Kyrki"
                },
                "author": "Ville Kyrki",
                "arxiv_comment": "(Accepted/In press) 2024 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03491v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03491v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.00654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.00654v2",
                "updated": "2024-09-09T15:23:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    23,
                    14,
                    0,
                    253,
                    0
                ],
                "published": "2023-04-02T23:41:02Z",
                "published_parsed": [
                    2023,
                    4,
                    2,
                    23,
                    41,
                    2,
                    6,
                    92,
                    0
                ],
                "title": "Mixed additive modelling of global alien species co-invasions of plants\n  and insects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed additive modelling of global alien species co-invasions of plants\n  and insects"
                },
                "summary": "Alien species refer to non-native species introduced into an ecosystem,\npotentially causing harm to the environment, economy, or human health. Presence\nof confounding factors has so far prevented a comprehensive picture of relative\nimportance of various drivers of such invasions. In this manuscript, we aim to\ndevelop and apply a general mixed additive Relational Event Model (REM) to\ndescribe the pattern of global invasions of alien species. An alien species\ninvasion can be seen as a relational event, where the species - sender -\nreaches a region - receiver - at a specific time in history. We consider the\nFirst Record Database and focus on co-invasions by insects and plants between\n1880 and 2005. REM aims to describe underlying hazard of each sender-receiver\npair. Besides potentially time-varying, exogenous, and endogenous covariates,\nour mixed additive REM incorporates time-varying and random effects. Our\nefficient inference procedure relies on case-control sampling, yielding the\nsame likelihood as that of a degenerate logistic regression. Resulting\ncomputational efficiency means that complex models for large dynamic networks\ncan be estimated in seconds on a standard computer. We also present a framework\nfor testing the goodness-of-fit of REMs via cumulative martingale-residuals.\nImplementation is performed through R package mgcv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alien species refer to non-native species introduced into an ecosystem,\npotentially causing harm to the environment, economy, or human health. Presence\nof confounding factors has so far prevented a comprehensive picture of relative\nimportance of various drivers of such invasions. In this manuscript, we aim to\ndevelop and apply a general mixed additive Relational Event Model (REM) to\ndescribe the pattern of global invasions of alien species. An alien species\ninvasion can be seen as a relational event, where the species - sender -\nreaches a region - receiver - at a specific time in history. We consider the\nFirst Record Database and focus on co-invasions by insects and plants between\n1880 and 2005. REM aims to describe underlying hazard of each sender-receiver\npair. Besides potentially time-varying, exogenous, and endogenous covariates,\nour mixed additive REM incorporates time-varying and random effects. Our\nefficient inference procedure relies on case-control sampling, yielding the\nsame likelihood as that of a degenerate logistic regression. Resulting\ncomputational efficiency means that complex models for large dynamic networks\ncan be estimated in seconds on a standard computer. We also present a framework\nfor testing the goodness-of-fit of REMs via cumulative martingale-residuals.\nImplementation is performed through R package mgcv."
                },
                "authors": [
                    {
                        "name": "Martina Boschi"
                    },
                    {
                        "name": "Rūta Juozaitienė"
                    },
                    {
                        "name": "Ernst-Jan Camiel Wit"
                    }
                ],
                "author_detail": {
                    "name": "Ernst-Jan Camiel Wit"
                },
                "author": "Ernst-Jan Camiel Wit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.00654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.00654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05703v1",
                "updated": "2024-09-09T15:14:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    14,
                    31,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:14:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    14,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "The Influence of Task and Group Disparities over Users' Attitudes Toward\n  Using Large Language Models for Psychotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Influence of Task and Group Disparities over Users' Attitudes Toward\n  Using Large Language Models for Psychotherapy"
                },
                "summary": "The population suffering from mental health disorders has kept increasing in\nrecent years. With the advancements in large language models (LLMs) in diverse\nfields, LLM-based psychotherapy has also attracted increasingly more attention.\nHowever, the factors influencing users' attitudes to LLM-based psychotherapy\nhave rarely been explored. As the first attempt, this paper investigated the\ninfluence of task and group disparities on user attitudes toward LLM-based\npsychotherapy tools. Utilizing the Technology Acceptance Model (TAM) and\nAutomation Acceptance Model (AAM), based on an online survey, we collected and\nanalyzed responses from 222 LLM-based psychotherapy users in mainland China.\nThe results revealed that group disparity (i.e., mental health conditions) can\ninfluence users' attitudes toward LLM tools. Further, one of the typical task\ndisparities, i.e., the privacy concern, was not found to have a significant\neffect on trust and usage intention. These findings can guide the design of\nfuture LLM-based psychotherapy services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The population suffering from mental health disorders has kept increasing in\nrecent years. With the advancements in large language models (LLMs) in diverse\nfields, LLM-based psychotherapy has also attracted increasingly more attention.\nHowever, the factors influencing users' attitudes to LLM-based psychotherapy\nhave rarely been explored. As the first attempt, this paper investigated the\ninfluence of task and group disparities on user attitudes toward LLM-based\npsychotherapy tools. Utilizing the Technology Acceptance Model (TAM) and\nAutomation Acceptance Model (AAM), based on an online survey, we collected and\nanalyzed responses from 222 LLM-based psychotherapy users in mainland China.\nThe results revealed that group disparity (i.e., mental health conditions) can\ninfluence users' attitudes toward LLM tools. Further, one of the typical task\ndisparities, i.e., the privacy concern, was not found to have a significant\neffect on trust and usage intention. These findings can guide the design of\nfuture LLM-based psychotherapy services."
                },
                "authors": [
                    {
                        "name": "Qihang He"
                    },
                    {
                        "name": "Jiyao Wang"
                    },
                    {
                        "name": "Dengbo He"
                    }
                ],
                "author_detail": {
                    "name": "Dengbo He"
                },
                "author": "Dengbo He",
                "arxiv_comment": "Accepted by HFES 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03727v3",
                "updated": "2024-09-09T15:04:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    4,
                    15,
                    0,
                    253,
                    0
                ],
                "published": "2024-05-06T08:09:46Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    8,
                    9,
                    46,
                    0,
                    127,
                    0
                ],
                "title": "Large Language Models Synergize with Automated Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Synergize with Automated Machine Learning"
                },
                "summary": "Recently, program synthesis driven by large language models (LLMs) has become\nincreasingly popular. However, program synthesis for machine learning (ML)\ntasks still poses significant challenges. This paper explores a novel form of\nprogram synthesis, targeting ML programs, by combining LLMs and automated\nmachine learning (autoML). Specifically, our goal is to fully automate the\ngeneration and optimization of the code of the entire ML workflow, from data\npreparation to modeling and post-processing, utilizing only textual\ndescriptions of the ML tasks. To manage the length and diversity of ML\nprograms, we propose to break each ML program into smaller, manageable parts.\nEach part is generated separately by the LLM, with careful consideration of\ntheir compatibilities. To ensure compatibilities, we design a testing technique\nfor ML programs. Unlike traditional program synthesis, which typically relies\non binary evaluations (i.e., correct or incorrect), evaluating ML programs\nnecessitates more than just binary judgments. Our approach automates the\nnumerical evaluation and optimization of these programs, selecting the best\ncandidates through autoML techniques. In experiments across various ML tasks,\nour method outperforms existing methods in 10 out of 12 tasks for generating ML\nprograms. In addition, autoML significantly improves the performance of the\ngenerated ML programs. In experiments, given the textual task description, our\nmethod, Text-to-ML, generates the complete and optimized ML program in a fully\nautonomous process. The implementation of our method is available at\nhttps://github.com/JLX0/llm-automl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, program synthesis driven by large language models (LLMs) has become\nincreasingly popular. However, program synthesis for machine learning (ML)\ntasks still poses significant challenges. This paper explores a novel form of\nprogram synthesis, targeting ML programs, by combining LLMs and automated\nmachine learning (autoML). Specifically, our goal is to fully automate the\ngeneration and optimization of the code of the entire ML workflow, from data\npreparation to modeling and post-processing, utilizing only textual\ndescriptions of the ML tasks. To manage the length and diversity of ML\nprograms, we propose to break each ML program into smaller, manageable parts.\nEach part is generated separately by the LLM, with careful consideration of\ntheir compatibilities. To ensure compatibilities, we design a testing technique\nfor ML programs. Unlike traditional program synthesis, which typically relies\non binary evaluations (i.e., correct or incorrect), evaluating ML programs\nnecessitates more than just binary judgments. Our approach automates the\nnumerical evaluation and optimization of these programs, selecting the best\ncandidates through autoML techniques. In experiments across various ML tasks,\nour method outperforms existing methods in 10 out of 12 tasks for generating ML\nprograms. In addition, autoML significantly improves the performance of the\ngenerated ML programs. In experiments, given the textual task description, our\nmethod, Text-to-ML, generates the complete and optimized ML program in a fully\nautonomous process. The implementation of our method is available at\nhttps://github.com/JLX0/llm-automl."
                },
                "authors": [
                    {
                        "name": "Jinglue Xu"
                    },
                    {
                        "name": "Jialong Li"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Nagar Anthel Venkatesh Suryanarayanan"
                    },
                    {
                        "name": "Guoyuan Zhou"
                    },
                    {
                        "name": "Jia Guo"
                    },
                    {
                        "name": "Hitoshi Iba"
                    },
                    {
                        "name": "Kenji Tei"
                    }
                ],
                "author_detail": {
                    "name": "Kenji Tei"
                },
                "author": "Kenji Tei",
                "arxiv_comment": "published at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.11500v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.11500v4",
                "updated": "2024-09-09T14:52:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    52,
                    15,
                    0,
                    253,
                    0
                ],
                "published": "2023-09-20T17:59:32Z",
                "published_parsed": [
                    2023,
                    9,
                    20,
                    17,
                    59,
                    32,
                    2,
                    263,
                    0
                ],
                "title": "Auto-ACD: A Large-scale Dataset for Audio-Language Representation\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-ACD: A Large-scale Dataset for Audio-Language Representation\n  Learning"
                },
                "summary": "Recently, the AI community has made significant strides in developing\npowerful foundation models, driven by large-scale multimodal datasets. However,\nfor audio representation learning, existing datasets suffer from limitations in\nthe following aspects: insufficient volume, simplistic content, and arduous\ncollection procedures. To establish an audio dataset with high-quality\ncaptions, we propose an innovative, automatic approach leveraging multimodal\ninputs, such as video frames, audio streams. Specifically, we construct a\nlarge-scale, high-quality, audio-language dataset, named as Auto-ACD,\ncomprising over 1.5M audio-text pairs. We exploit a series of pre-trained\nmodels or APIs, to determine audio-visual synchronisation, generate image\ncaptions, object detection, or audio tags for specific videos. Subsequently, we\nemploy LLM to paraphrase a congruent caption for each audio, guided by the\nextracted multi-modality clues. To demonstrate the effectiveness of the\nproposed dataset, we train widely used models on our dataset and show\nperformance improvement on various downstream tasks, for example,\naudio-language retrieval, audio captioning, zero-shot classification. In\naddition, we establish a novel benchmark with environmental information and\nprovide a benchmark for audio-text tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the AI community has made significant strides in developing\npowerful foundation models, driven by large-scale multimodal datasets. However,\nfor audio representation learning, existing datasets suffer from limitations in\nthe following aspects: insufficient volume, simplistic content, and arduous\ncollection procedures. To establish an audio dataset with high-quality\ncaptions, we propose an innovative, automatic approach leveraging multimodal\ninputs, such as video frames, audio streams. Specifically, we construct a\nlarge-scale, high-quality, audio-language dataset, named as Auto-ACD,\ncomprising over 1.5M audio-text pairs. We exploit a series of pre-trained\nmodels or APIs, to determine audio-visual synchronisation, generate image\ncaptions, object detection, or audio tags for specific videos. Subsequently, we\nemploy LLM to paraphrase a congruent caption for each audio, guided by the\nextracted multi-modality clues. To demonstrate the effectiveness of the\nproposed dataset, we train widely used models on our dataset and show\nperformance improvement on various downstream tasks, for example,\naudio-language retrieval, audio captioning, zero-shot classification. In\naddition, we establish a novel benchmark with environmental information and\nprovide a benchmark for audio-text tasks."
                },
                "authors": [
                    {
                        "name": "Luoyi Sun"
                    },
                    {
                        "name": "Xuenan Xu"
                    },
                    {
                        "name": "Mengyue Wu"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "arxiv_comment": "Accepted by ACM MM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.11500v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.11500v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19636v2",
                "updated": "2024-09-09T14:48:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    48,
                    32,
                    0,
                    253,
                    0
                ],
                "published": "2024-04-30T15:38:01Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    15,
                    38,
                    1,
                    1,
                    121,
                    0
                ],
                "title": "Bayesian calibration of bubble size dynamics applied to CO2 gas\n  fermenters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian calibration of bubble size dynamics applied to CO2 gas\n  fermenters"
                },
                "summary": "To accelerate the scale-up of gaseous CO2 fermentation reactors,\ncomputational models need to predict gas-to-liquid mass transfer which requires\ncapturing the bubble size dynamics, i.e. bubble breakup and coalescence.\nHowever, the applicability of existing models beyond air-water mixtures remains\nto be established. Here, an inverse modeling approach, accelerated with a\nneural network surrogate, calibrates the breakup and coalescence closure\nmodels, that are used in the Multiple-Size-Group (MUSIG) population balance\nmodeling (PBM). The calibration is performed based on experimental results\nobtained in a CO2-air-water-coflowing bubble column reactor. Bayesian inference\nis used to account for noise in the experimental dataset and bias in the\nsimulation results. To accurately capture gas holdup and interphase mass\ntransfer, the results show that the breakage rate needs to be increased by one\norder of magnitude. The inferred model parameters are then used on a separate\nconfiguration and shown to also improve bubble size distribution predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To accelerate the scale-up of gaseous CO2 fermentation reactors,\ncomputational models need to predict gas-to-liquid mass transfer which requires\ncapturing the bubble size dynamics, i.e. bubble breakup and coalescence.\nHowever, the applicability of existing models beyond air-water mixtures remains\nto be established. Here, an inverse modeling approach, accelerated with a\nneural network surrogate, calibrates the breakup and coalescence closure\nmodels, that are used in the Multiple-Size-Group (MUSIG) population balance\nmodeling (PBM). The calibration is performed based on experimental results\nobtained in a CO2-air-water-coflowing bubble column reactor. Bayesian inference\nis used to account for noise in the experimental dataset and bias in the\nsimulation results. To accurately capture gas holdup and interphase mass\ntransfer, the results show that the breakage rate needs to be increased by one\norder of magnitude. The inferred model parameters are then used on a separate\nconfiguration and shown to also improve bubble size distribution predictions."
                },
                "authors": [
                    {
                        "name": "Malik Hassanaly"
                    },
                    {
                        "name": "John M. Parra-Alvarez"
                    },
                    {
                        "name": "Mohammad J. Rahimi"
                    },
                    {
                        "name": "Hariswaran Sitaraman"
                    }
                ],
                "author_detail": {
                    "name": "Hariswaran Sitaraman"
                },
                "author": "Hariswaran Sitaraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05672v1",
                "updated": "2024-09-09T14:41:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    41,
                    24,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T14:41:24Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    41,
                    24,
                    0,
                    253,
                    0
                ],
                "title": "Zero-shot Outlier Detection via Prior-data Fitted Networks: Model\n  Selection Bygone!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Outlier Detection via Prior-data Fitted Networks: Model\n  Selection Bygone!"
                },
                "summary": "Outlier detection (OD) has a vast literature as it finds numerous\napplications in environmental monitoring, cybersecurity, finance, and medicine\nto name a few. Being an inherently unsupervised task, model selection is a key\nbottleneck for OD (both algorithm and hyperparameter selection) without label\nsupervision. There is a long list of techniques to choose from -- both\nclassical algorithms and deep neural architectures -- and while several studies\nreport their hyperparameter sensitivity, the literature is quite slim on\nunsupervised model selection -- limiting the effective use of OD in practice.\nIn this paper we present FoMo-0D, for zero/0-shot OD exploring a transformative\nnew direction that bypasses the hurdle of model selection altogether (!), thus\nbreaking new ground. The fundamental idea behind FoMo-0D is the Prior-data\nFitted Networks, recently introduced by Muller et al.(2022), which trains a\nTransformer model on a large body of synthetically generated data from a prior\ndata distribution. In essence, FoMo-0D is a pretrained Foundation Model for\nzero/0-shot OD on tabular data, which can directly predict the (outlier/inlier)\nlabel of any test data at inference time, by merely a single forward pass --\nmaking obsolete the need for choosing an algorithm/architecture, tuning its\nassociated hyperparameters, and even training any model parameters when given a\nnew OD dataset. Extensive experiments on 57 public benchmark datasets against\n26 baseline methods show that FoMo-0D performs statistically no different from\nthe top 2nd baseline, while significantly outperforming the majority of the\nbaselines, with an average inference time of 7.7 ms per test sample.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outlier detection (OD) has a vast literature as it finds numerous\napplications in environmental monitoring, cybersecurity, finance, and medicine\nto name a few. Being an inherently unsupervised task, model selection is a key\nbottleneck for OD (both algorithm and hyperparameter selection) without label\nsupervision. There is a long list of techniques to choose from -- both\nclassical algorithms and deep neural architectures -- and while several studies\nreport their hyperparameter sensitivity, the literature is quite slim on\nunsupervised model selection -- limiting the effective use of OD in practice.\nIn this paper we present FoMo-0D, for zero/0-shot OD exploring a transformative\nnew direction that bypasses the hurdle of model selection altogether (!), thus\nbreaking new ground. The fundamental idea behind FoMo-0D is the Prior-data\nFitted Networks, recently introduced by Muller et al.(2022), which trains a\nTransformer model on a large body of synthetically generated data from a prior\ndata distribution. In essence, FoMo-0D is a pretrained Foundation Model for\nzero/0-shot OD on tabular data, which can directly predict the (outlier/inlier)\nlabel of any test data at inference time, by merely a single forward pass --\nmaking obsolete the need for choosing an algorithm/architecture, tuning its\nassociated hyperparameters, and even training any model parameters when given a\nnew OD dataset. Extensive experiments on 57 public benchmark datasets against\n26 baseline methods show that FoMo-0D performs statistically no different from\nthe top 2nd baseline, while significantly outperforming the majority of the\nbaselines, with an average inference time of 7.7 ms per test sample."
                },
                "authors": [
                    {
                        "name": "Yuchen Shen"
                    },
                    {
                        "name": "Haomin Wen"
                    },
                    {
                        "name": "Leman Akoglu"
                    }
                ],
                "author_detail": {
                    "name": "Leman Akoglu"
                },
                "author": "Leman Akoglu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05671v1",
                "updated": "2024-09-09T14:40:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    40,
                    21,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T14:40:21Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    40,
                    21,
                    0,
                    253,
                    0
                ],
                "title": "HyperSteiner: Computing Heuristic Hyperbolic Steiner Minimal Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperSteiner: Computing Heuristic Hyperbolic Steiner Minimal Trees"
                },
                "summary": "We propose HyperSteiner -- an efficient heuristic algorithm for computing\nSteiner minimal trees in the hyperbolic space. HyperSteiner extends the\nEuclidean Smith-Lee-Liebman algorithm, which is grounded in a\ndivide-and-conquer approach involving the Delaunay triangulation. The central\nidea is rephrasing Steiner tree problems with three terminals as a system of\nequations in the Klein-Beltrami model. Motivated by the fact that hyperbolic\ngeometry is well-suited for representing hierarchies, we explore applications\nto hierarchy discovery in data. Results show that HyperSteiner infers more\nrealistic hierarchies than the Minimum Spanning Tree and is more scalable to\nlarge datasets than Neighbor Joining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose HyperSteiner -- an efficient heuristic algorithm for computing\nSteiner minimal trees in the hyperbolic space. HyperSteiner extends the\nEuclidean Smith-Lee-Liebman algorithm, which is grounded in a\ndivide-and-conquer approach involving the Delaunay triangulation. The central\nidea is rephrasing Steiner tree problems with three terminals as a system of\nequations in the Klein-Beltrami model. Motivated by the fact that hyperbolic\ngeometry is well-suited for representing hierarchies, we explore applications\nto hierarchy discovery in data. Results show that HyperSteiner infers more\nrealistic hierarchies than the Minimum Spanning Tree and is more scalable to\nlarge datasets than Neighbor Joining."
                },
                "authors": [
                    {
                        "name": "Alejandro García-Castellanos"
                    },
                    {
                        "name": "Aniss Aiman Medbouhi"
                    },
                    {
                        "name": "Giovanni Luca Marchetti"
                    },
                    {
                        "name": "Erik J. Bekkers"
                    },
                    {
                        "name": "Danica Kragic"
                    }
                ],
                "author_detail": {
                    "name": "Danica Kragic"
                },
                "author": "Danica Kragic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05668v1",
                "updated": "2024-09-09T14:38:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    38,
                    31,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T14:38:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    38,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "Unlearning or Concealment? A Critical Analysis and Evaluation Metrics\n  for Unlearning in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning or Concealment? A Critical Analysis and Evaluation Metrics\n  for Unlearning in Diffusion Models"
                },
                "summary": "Recent research has seen significant interest in methods for concept removal\nand targeted forgetting in diffusion models. In this paper, we conduct a\ncomprehensive white-box analysis to expose significant vulnerabilities in\nexisting diffusion model unlearning methods. We show that the objective\nfunctions used for unlearning in the existing methods lead to decoupling of the\ntargeted concepts (meant to be forgotten) for the corresponding prompts. This\nis concealment and not actual unlearning, which was the original goal. The\nineffectiveness of current methods stems primarily from their narrow focus on\nreducing generation probabilities for specific prompt sets, neglecting the\ndiverse modalities of intermediate guidance employed during the inference\nprocess. The paper presents a rigorous theoretical and empirical examination of\nfour commonly used techniques for unlearning in diffusion models. We introduce\ntwo new evaluation metrics: Concept Retrieval Score (CRS) and Concept\nConfidence Score (CCS). These metrics are based on a successful adversarial\nattack setup that can recover forgotten concepts from unlearned diffusion\nmodels. The CRS measures the similarity between the latent representations of\nthe unlearned and fully trained models after unlearning. It reports the extent\nof retrieval of the forgotten concepts with increasing amount of guidance. The\nCCS quantifies the confidence of the model in assigning the target concept to\nthe manipulated data. It reports the probability of the unlearned model's\ngenerations to be aligned with the original domain knowledge with increasing\namount of guidance. Evaluating existing unlearning methods with our proposed\nstringent metrics for diffusion models reveals significant shortcomings in\ntheir ability to truly unlearn concepts. Source Code:\nhttps://respailab.github.io/unlearning-or-concealment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has seen significant interest in methods for concept removal\nand targeted forgetting in diffusion models. In this paper, we conduct a\ncomprehensive white-box analysis to expose significant vulnerabilities in\nexisting diffusion model unlearning methods. We show that the objective\nfunctions used for unlearning in the existing methods lead to decoupling of the\ntargeted concepts (meant to be forgotten) for the corresponding prompts. This\nis concealment and not actual unlearning, which was the original goal. The\nineffectiveness of current methods stems primarily from their narrow focus on\nreducing generation probabilities for specific prompt sets, neglecting the\ndiverse modalities of intermediate guidance employed during the inference\nprocess. The paper presents a rigorous theoretical and empirical examination of\nfour commonly used techniques for unlearning in diffusion models. We introduce\ntwo new evaluation metrics: Concept Retrieval Score (CRS) and Concept\nConfidence Score (CCS). These metrics are based on a successful adversarial\nattack setup that can recover forgotten concepts from unlearned diffusion\nmodels. The CRS measures the similarity between the latent representations of\nthe unlearned and fully trained models after unlearning. It reports the extent\nof retrieval of the forgotten concepts with increasing amount of guidance. The\nCCS quantifies the confidence of the model in assigning the target concept to\nthe manipulated data. It reports the probability of the unlearned model's\ngenerations to be aligned with the original domain knowledge with increasing\namount of guidance. Evaluating existing unlearning methods with our proposed\nstringent metrics for diffusion models reveals significant shortcomings in\ntheir ability to truly unlearn concepts. Source Code:\nhttps://respailab.github.io/unlearning-or-concealment"
                },
                "authors": [
                    {
                        "name": "Aakash Sen Sharma"
                    },
                    {
                        "name": "Niladri Sarkar"
                    },
                    {
                        "name": "Vikram Chundawat"
                    },
                    {
                        "name": "Ankur A Mali"
                    },
                    {
                        "name": "Murari Mandal"
                    }
                ],
                "author_detail": {
                    "name": "Murari Mandal"
                },
                "author": "Murari Mandal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05665v1",
                "updated": "2024-09-09T14:36:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    36,
                    33,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T14:36:33Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    36,
                    33,
                    0,
                    253,
                    0
                ],
                "title": "K-Fold Causal BART for CATE Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "K-Fold Causal BART for CATE Estimation"
                },
                "summary": "This research aims to propose and evaluate a novel model named K-Fold Causal\nBayesian Additive Regression Trees (K-Fold Causal BART) for improved estimation\nof Average Treatment Effects (ATE) and Conditional Average Treatment Effects\n(CATE). The study employs synthetic and semi-synthetic datasets, including the\nwidely recognized Infant Health and Development Program (IHDP) benchmark\ndataset, to validate the model's performance. Despite promising results in\nsynthetic scenarios, the IHDP dataset reveals that the proposed model is not\nstate-of-the-art for ATE and CATE estimation. Nonetheless, the research\nprovides several novel insights: 1. The ps-BART model is likely the preferred\nchoice for CATE and ATE estimation due to better generalization compared to the\nother benchmark models - including the Bayesian Causal Forest (BCF) model,\nwhich is considered by many the current best model for CATE estimation, 2. The\nBCF model's performance deteriorates significantly with increasing treatment\neffect heterogeneity, while the ps-BART model remains robust, 3. Models tend to\nbe overconfident in CATE uncertainty quantification when treatment effect\nheterogeneity is low, 4. A second K-Fold method is unnecessary for avoiding\noverfitting in CATE estimation, as it adds computational costs without\nimproving performance, 5. Detailed analysis reveals the importance of\nunderstanding dataset characteristics and using nuanced evaluation methods, 6.\nThe conclusion of Curth et al. (2021) that indirect strategies for CATE\nestimation are superior for the IHDP dataset is contradicted by the results of\nthis research. These findings challenge existing assumptions and suggest\ndirections for future research to enhance causal inference methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research aims to propose and evaluate a novel model named K-Fold Causal\nBayesian Additive Regression Trees (K-Fold Causal BART) for improved estimation\nof Average Treatment Effects (ATE) and Conditional Average Treatment Effects\n(CATE). The study employs synthetic and semi-synthetic datasets, including the\nwidely recognized Infant Health and Development Program (IHDP) benchmark\ndataset, to validate the model's performance. Despite promising results in\nsynthetic scenarios, the IHDP dataset reveals that the proposed model is not\nstate-of-the-art for ATE and CATE estimation. Nonetheless, the research\nprovides several novel insights: 1. The ps-BART model is likely the preferred\nchoice for CATE and ATE estimation due to better generalization compared to the\nother benchmark models - including the Bayesian Causal Forest (BCF) model,\nwhich is considered by many the current best model for CATE estimation, 2. The\nBCF model's performance deteriorates significantly with increasing treatment\neffect heterogeneity, while the ps-BART model remains robust, 3. Models tend to\nbe overconfident in CATE uncertainty quantification when treatment effect\nheterogeneity is low, 4. A second K-Fold method is unnecessary for avoiding\noverfitting in CATE estimation, as it adds computational costs without\nimproving performance, 5. Detailed analysis reveals the importance of\nunderstanding dataset characteristics and using nuanced evaluation methods, 6.\nThe conclusion of Curth et al. (2021) that indirect strategies for CATE\nestimation are superior for the IHDP dataset is contradicted by the results of\nthis research. These findings challenge existing assumptions and suggest\ndirections for future research to enhance causal inference methodologies."
                },
                "authors": [
                    {
                        "name": "Hugo Gobato Souto"
                    },
                    {
                        "name": "Francisco Louzada Neto"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Louzada Neto"
                },
                "author": "Francisco Louzada Neto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16528v3",
                "updated": "2024-09-09T14:31:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    31,
                    26,
                    0,
                    253,
                    0
                ],
                "published": "2024-05-26T11:29:57Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    11,
                    29,
                    57,
                    6,
                    147,
                    0
                ],
                "title": "LoQT: Low-Rank Adapters for Quantized Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoQT: Low-Rank Adapters for Quantized Pre-Training"
                },
                "summary": "Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware."
                },
                "authors": [
                    {
                        "name": "Sebastian Loeschcke"
                    },
                    {
                        "name": "Mads Toftrup"
                    },
                    {
                        "name": "Michael J. Kastoryano"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Vésteinn Snæbjarnarson"
                    }
                ],
                "author_detail": {
                    "name": "Vésteinn Snæbjarnarson"
                },
                "author": "Vésteinn Snæbjarnarson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.17336v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.17336v3",
                "updated": "2024-09-09T14:24:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    24,
                    54,
                    0,
                    253,
                    0
                ],
                "published": "2023-12-28T19:28:23Z",
                "published_parsed": [
                    2023,
                    12,
                    28,
                    19,
                    28,
                    23,
                    3,
                    362,
                    0
                ],
                "title": "PINN surrogate of Li-ion battery models for parameter inference. Part\n  II: Regularization and application of the pseudo-2D model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINN surrogate of Li-ion battery models for parameter inference. Part\n  II: Regularization and application of the pseudo-2D model"
                },
                "summary": "Bayesian parameter inference is useful to improve Li-ion battery diagnostics\nand can help formulate battery aging models. However, it is computationally\nintensive and cannot be easily repeated for multiple cycles, multiple operating\nconditions, or multiple replicate cells. To reduce the computational cost of\nBayesian calibration, numerical solvers for physics-based models can be\nreplaced with faster surrogates. A physics-informed neural network (PINN) is\ndeveloped as a surrogate for the pseudo-2D (P2D) battery model calibration. For\nthe P2D surrogate, additional training regularization was needed as compared to\nthe PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and\nP2D surrogate models are exercised for parameter inference and compared to data\nobtained from a direct numerical solution of the governing equations. A\nparameter inference study highlights the ability to use these PINNs to\ncalibrate scaling parameters for the cathode Li diffusion and the anode\nexchange current density. By realizing computational speed-ups of 2250x for the\nP2D model, as compared to using standard integrating methods, the PINN\nsurrogates enable rapid state-of-health diagnostics. In the low-data\navailability scenario, the testing error was estimated to 2mV for the SPM\nsurrogate and 10mV for the P2D surrogate which could be mitigated with\nadditional data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian parameter inference is useful to improve Li-ion battery diagnostics\nand can help formulate battery aging models. However, it is computationally\nintensive and cannot be easily repeated for multiple cycles, multiple operating\nconditions, or multiple replicate cells. To reduce the computational cost of\nBayesian calibration, numerical solvers for physics-based models can be\nreplaced with faster surrogates. A physics-informed neural network (PINN) is\ndeveloped as a surrogate for the pseudo-2D (P2D) battery model calibration. For\nthe P2D surrogate, additional training regularization was needed as compared to\nthe PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and\nP2D surrogate models are exercised for parameter inference and compared to data\nobtained from a direct numerical solution of the governing equations. A\nparameter inference study highlights the ability to use these PINNs to\ncalibrate scaling parameters for the cathode Li diffusion and the anode\nexchange current density. By realizing computational speed-ups of 2250x for the\nP2D model, as compared to using standard integrating methods, the PINN\nsurrogates enable rapid state-of-health diagnostics. In the low-data\navailability scenario, the testing error was estimated to 2mV for the SPM\nsurrogate and 10mV for the P2D surrogate which could be mitigated with\nadditional data."
                },
                "authors": [
                    {
                        "name": "Malik Hassanaly"
                    },
                    {
                        "name": "Peter J. Weddle"
                    },
                    {
                        "name": "Ryan N. King"
                    },
                    {
                        "name": "Subhayan De"
                    },
                    {
                        "name": "Alireza Doostan"
                    },
                    {
                        "name": "Corey R. Randall"
                    },
                    {
                        "name": "Eric J. Dufek"
                    },
                    {
                        "name": "Andrew M. Colclasure"
                    },
                    {
                        "name": "Kandler Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kandler Smith"
                },
                "author": "Kandler Smith",
                "arxiv_journal_ref": "Journal of Energy Storage, Volume 98, Part B, 2024, 113104",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.17336v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.17336v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05657v1",
                "updated": "2024-09-09T14:23:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    23,
                    19,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T14:23:19Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    23,
                    19,
                    0,
                    253,
                    0
                ],
                "title": "Adversarial Attacks on Data Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Attacks on Data Attribution"
                },
                "summary": "Data attribution aims to quantify the contribution of individual training\ndata points to the outputs of an AI model, which has been used to measure the\nvalue of training data and compensate data providers. Given the impact on\nfinancial decisions and compensation mechanisms, a critical question arises\nconcerning the adversarial robustness of data attribution methods. However,\nthere has been little to no systematic research addressing this issue. In this\nwork, we aim to bridge this gap by detailing a threat model with clear\nassumptions about the adversary's goal and capabilities, and by proposing\nprincipled adversarial attack methods on data attribution. We present two such\nmethods, Shadow Attack and Outlier Attack, both of which generate manipulated\ndatasets to adversarially inflate the compensation. The Shadow Attack leverages\nknowledge about the data distribution in the AI applications, and derives\nadversarial perturbations through \"shadow training\", a technique commonly used\nin membership inference attacks. In contrast, the Outlier Attack does not\nassume any knowledge about the data distribution and relies solely on black-box\nqueries to the target model's predictions. It exploits an inductive bias\npresent in many data attribution methods - outlier data points are more likely\nto be influential - and employs adversarial examples to generate manipulated\ndatasets. Empirically, in image classification and text generation tasks, the\nShadow Attack can inflate the data-attribution-based compensation by at least\n200%, while the Outlier Attack achieves compensation inflation ranging from\n185% to as much as 643%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data attribution aims to quantify the contribution of individual training\ndata points to the outputs of an AI model, which has been used to measure the\nvalue of training data and compensate data providers. Given the impact on\nfinancial decisions and compensation mechanisms, a critical question arises\nconcerning the adversarial robustness of data attribution methods. However,\nthere has been little to no systematic research addressing this issue. In this\nwork, we aim to bridge this gap by detailing a threat model with clear\nassumptions about the adversary's goal and capabilities, and by proposing\nprincipled adversarial attack methods on data attribution. We present two such\nmethods, Shadow Attack and Outlier Attack, both of which generate manipulated\ndatasets to adversarially inflate the compensation. The Shadow Attack leverages\nknowledge about the data distribution in the AI applications, and derives\nadversarial perturbations through \"shadow training\", a technique commonly used\nin membership inference attacks. In contrast, the Outlier Attack does not\nassume any knowledge about the data distribution and relies solely on black-box\nqueries to the target model's predictions. It exploits an inductive bias\npresent in many data attribution methods - outlier data points are more likely\nto be influential - and employs adversarial examples to generate manipulated\ndatasets. Empirically, in image classification and text generation tasks, the\nShadow Attack can inflate the data-attribution-based compensation by at least\n200%, while the Outlier Attack achieves compensation inflation ranging from\n185% to as much as 643%."
                },
                "authors": [
                    {
                        "name": "Xinhe Wang"
                    },
                    {
                        "name": "Pingbang Hu"
                    },
                    {
                        "name": "Junwei Deng"
                    },
                    {
                        "name": "Jiaqi W. Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi W. Ma"
                },
                "author": "Jiaqi W. Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05630v1",
                "updated": "2024-09-09T14:00:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    0,
                    35,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T14:00:35Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    0,
                    35,
                    0,
                    253,
                    0
                ],
                "title": "Multilevel testing of constraints induced by structural equation\n  modeling in fMRI effective connectivity analysis: A proof of concept",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilevel testing of constraints induced by structural equation\n  modeling in fMRI effective connectivity analysis: A proof of concept"
                },
                "summary": "In functional MRI (fMRI), effective connectivity analysis aims at inferring\nthe causal influences that brain regions exert on one another. A common method\nfor this type of analysis is structural equation modeling (SEM). We here\npropose a novel method to test the validity of a given model of structural\nequation. Given a structural model in the form of a directed graph, the method\nextracts the set of all constraints of conditional independence induced by the\nabsence of links between pairs of regions in the model and tests for their\nvalidity in a Bayesian framework, either individually (constraint by\nconstraint), jointly (e.g., by gathering all constraints associated with a\ngiven missing link), or globally (i.e., all constraints associated with the\nstructural model). This approach has two main advantages. First, it only tests\nwhat is testable from observational data and does allow for false causal\ninterpretation. Second, it makes it possible to test each constraint (or group\nof constraints) separately and, therefore, quantify in what measure each\nconstraint (or, e..g., missing link) is respected in the data. We validate our\napproach using a simulation study and illustrate its potential benefits through\nthe reanalysis of published data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In functional MRI (fMRI), effective connectivity analysis aims at inferring\nthe causal influences that brain regions exert on one another. A common method\nfor this type of analysis is structural equation modeling (SEM). We here\npropose a novel method to test the validity of a given model of structural\nequation. Given a structural model in the form of a directed graph, the method\nextracts the set of all constraints of conditional independence induced by the\nabsence of links between pairs of regions in the model and tests for their\nvalidity in a Bayesian framework, either individually (constraint by\nconstraint), jointly (e.g., by gathering all constraints associated with a\ngiven missing link), or globally (i.e., all constraints associated with the\nstructural model). This approach has two main advantages. First, it only tests\nwhat is testable from observational data and does allow for false causal\ninterpretation. Second, it makes it possible to test each constraint (or group\nof constraints) separately and, therefore, quantify in what measure each\nconstraint (or, e..g., missing link) is respected in the data. We validate our\napproach using a simulation study and illustrate its potential benefits through\nthe reanalysis of published data."
                },
                "authors": [
                    {
                        "name": "G. Marrelec"
                    },
                    {
                        "name": "A. Giron"
                    }
                ],
                "author_detail": {
                    "name": "A. Giron"
                },
                "author": "A. Giron",
                "arxiv_doi": "10.1016/j.mri.2024.01.010",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.mri.2024.01.010",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.05630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Magnetic Resonance Imaging 109, 294-303 (2024)",
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05617v1",
                "updated": "2024-09-09T13:52:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    52,
                    58,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T13:52:58Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    52,
                    58,
                    0,
                    253,
                    0
                ],
                "title": "G-NeLF: Memory- and Data-Efficient Hybrid Neural Light Field for Novel\n  View Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-NeLF: Memory- and Data-Efficient Hybrid Neural Light Field for Novel\n  View Synthesis"
                },
                "summary": "Following the burgeoning interest in implicit neural representation, Neural\nLight Field (NeLF) has been introduced to predict the color of a ray directly.\nUnlike Neural Radiance Field (NeRF), NeLF does not create a point-wise\nrepresentation by predicting color and volume density for each point in space.\nHowever, the current NeLF methods face a challenge as they need to train a NeRF\nmodel first and then synthesize over 10K views to train NeLF for improved\nperformance. Additionally, the rendering quality of NeLF methods is lower\ncompared to NeRF methods. In this paper, we propose G-NeLF, a versatile\ngrid-based NeLF approach that utilizes spatial-aware features to unleash the\npotential of the neural network's inference capability, and consequently\novercome the difficulties of NeLF training. Specifically, we employ a\nspatial-aware feature sequence derived from a meticulously crafted grid as the\nray's representation. Drawing from our empirical studies on the adaptability of\nmulti-resolution hash tables, we introduce a novel grid-based ray\nrepresentation for NeLF that can represent the entire space with a very limited\nnumber of parameters. To better utilize the sequence feature, we design a\nlightweight ray color decoder that simulates the ray propagation process,\nenabling a more efficient inference of the ray's color. G-NeLF can be trained\nwithout necessitating significant storage overhead and with the model size of\nonly 0.95 MB to surpass previous state-of-the-art NeLF. Moreover, compared with\ngrid-based NeRF methods, e.g., Instant-NGP, we only utilize one-tenth of its\nparameters to achieve higher performance. Our code will be released upon\nacceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the burgeoning interest in implicit neural representation, Neural\nLight Field (NeLF) has been introduced to predict the color of a ray directly.\nUnlike Neural Radiance Field (NeRF), NeLF does not create a point-wise\nrepresentation by predicting color and volume density for each point in space.\nHowever, the current NeLF methods face a challenge as they need to train a NeRF\nmodel first and then synthesize over 10K views to train NeLF for improved\nperformance. Additionally, the rendering quality of NeLF methods is lower\ncompared to NeRF methods. In this paper, we propose G-NeLF, a versatile\ngrid-based NeLF approach that utilizes spatial-aware features to unleash the\npotential of the neural network's inference capability, and consequently\novercome the difficulties of NeLF training. Specifically, we employ a\nspatial-aware feature sequence derived from a meticulously crafted grid as the\nray's representation. Drawing from our empirical studies on the adaptability of\nmulti-resolution hash tables, we introduce a novel grid-based ray\nrepresentation for NeLF that can represent the entire space with a very limited\nnumber of parameters. To better utilize the sequence feature, we design a\nlightweight ray color decoder that simulates the ray propagation process,\nenabling a more efficient inference of the ray's color. G-NeLF can be trained\nwithout necessitating significant storage overhead and with the model size of\nonly 0.95 MB to surpass previous state-of-the-art NeLF. Moreover, compared with\ngrid-based NeRF methods, e.g., Instant-NGP, we only utilize one-tenth of its\nparameters to achieve higher performance. Our code will be released upon\nacceptance."
                },
                "authors": [
                    {
                        "name": "Lutao Jiang"
                    },
                    {
                        "name": "Lin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Wang"
                },
                "author": "Lin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00369v3",
                "updated": "2024-09-09T13:50:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    50,
                    30,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-31T07:10:16Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    7,
                    10,
                    16,
                    5,
                    244,
                    0
                ],
                "title": "An Empirical Study on Information Extraction using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Information Extraction using Large Language Models"
                },
                "summary": "Human-like large language models (LLMs), especially the most powerful and\npopular ones in OpenAI's GPT family, have proven to be very helpful for many\nnatural language processing (NLP) related tasks. Therefore, various attempts\nhave been made to apply LLMs to information extraction (IE), which is a\nfundamental NLP task that involves extracting information from unstructured\nplain text. To demonstrate the latest representative progress in LLMs'\ninformation extraction ability, we assess the information extraction ability of\nGPT-4 (the latest version of GPT at the time of writing this paper) from four\nperspectives: Performance, Evaluation Criteria, Robustness, and Error Types.\nOur results suggest a visible performance gap between GPT-4 and\nstate-of-the-art (SOTA) IE methods. To alleviate this problem, considering the\nLLMs' human-like characteristics, we propose and analyze the effects of a\nseries of simple prompt-based methods, which can be generalized to other LLMs\nand NLP tasks. Rich experiments show our methods' effectiveness and some of\ntheir remaining issues in improving GPT-4's information extraction ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-like large language models (LLMs), especially the most powerful and\npopular ones in OpenAI's GPT family, have proven to be very helpful for many\nnatural language processing (NLP) related tasks. Therefore, various attempts\nhave been made to apply LLMs to information extraction (IE), which is a\nfundamental NLP task that involves extracting information from unstructured\nplain text. To demonstrate the latest representative progress in LLMs'\ninformation extraction ability, we assess the information extraction ability of\nGPT-4 (the latest version of GPT at the time of writing this paper) from four\nperspectives: Performance, Evaluation Criteria, Robustness, and Error Types.\nOur results suggest a visible performance gap between GPT-4 and\nstate-of-the-art (SOTA) IE methods. To alleviate this problem, considering the\nLLMs' human-like characteristics, we propose and analyze the effects of a\nseries of simple prompt-based methods, which can be generalized to other LLMs\nand NLP tasks. Rich experiments show our methods' effectiveness and some of\ntheir remaining issues in improving GPT-4's information extraction ability."
                },
                "authors": [
                    {
                        "name": "Ridong Han"
                    },
                    {
                        "name": "Chaohao Yang"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Prayag Tiwari"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Lu Liu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "Need to submit this paper as the replacement of arXiv:2305.14450",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01199v2",
                "updated": "2024-09-09T13:49:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    49,
                    53,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-02T12:20:42Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    12,
                    20,
                    42,
                    0,
                    246,
                    0
                ],
                "title": "OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video\n  Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OD-VAE: An Omni-dimensional Video Compressor for Improving Latent Video\n  Diffusion Model"
                },
                "summary": "Variational Autoencoder (VAE), compressing videos into latent\nrepresentations, is a crucial preceding component of Latent Video Diffusion\nModels (LVDMs). With the same reconstruction quality, the more sufficient the\nVAE's compression for videos is, the more efficient the LVDMs are. However,\nmost LVDMs utilize 2D image VAE, whose compression for videos is only in the\nspatial dimension and often ignored in the temporal dimension. How to conduct\ntemporal compression for videos in a VAE to obtain more concise latent\nrepresentations while promising accurate reconstruction is seldom explored. To\nfill this gap, we propose an omni-dimension compression VAE, named OD-VAE,\nwhich can temporally and spatially compress videos. Although OD-VAE's more\nsufficient compression brings a great challenge to video reconstruction, it can\nstill achieve high reconstructed accuracy by our fine design. To obtain a\nbetter trade-off between video reconstruction quality and compression speed,\nfour variants of OD-VAE are introduced and analyzed. In addition, a novel tail\ninitialization is designed to train OD-VAE more efficiently, and a novel\ninference strategy is proposed to enable OD-VAE to handle videos of arbitrary\nlength with limited GPU memory. Comprehensive experiments on video\nreconstruction and LVDM-based video generation demonstrate the effectiveness\nand efficiency of our proposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Autoencoder (VAE), compressing videos into latent\nrepresentations, is a crucial preceding component of Latent Video Diffusion\nModels (LVDMs). With the same reconstruction quality, the more sufficient the\nVAE's compression for videos is, the more efficient the LVDMs are. However,\nmost LVDMs utilize 2D image VAE, whose compression for videos is only in the\nspatial dimension and often ignored in the temporal dimension. How to conduct\ntemporal compression for videos in a VAE to obtain more concise latent\nrepresentations while promising accurate reconstruction is seldom explored. To\nfill this gap, we propose an omni-dimension compression VAE, named OD-VAE,\nwhich can temporally and spatially compress videos. Although OD-VAE's more\nsufficient compression brings a great challenge to video reconstruction, it can\nstill achieve high reconstructed accuracy by our fine design. To obtain a\nbetter trade-off between video reconstruction quality and compression speed,\nfour variants of OD-VAE are introduced and analyzed. In addition, a novel tail\ninitialization is designed to train OD-VAE more efficiently, and a novel\ninference strategy is proposed to enable OD-VAE to handle videos of arbitrary\nlength with limited GPU memory. Comprehensive experiments on video\nreconstruction and LVDM-based video generation demonstrate the effectiveness\nand efficiency of our proposed methods."
                },
                "authors": [
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Bin Zhu"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Xing Zhou"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "https://github.com/PKU-YuanGroup/Open-Sora-Plan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.17022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.17022v2",
                "updated": "2024-09-09T13:34:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    34,
                    39,
                    0,
                    253,
                    0
                ],
                "published": "2023-06-29T15:14:20Z",
                "published_parsed": [
                    2023,
                    6,
                    29,
                    15,
                    14,
                    20,
                    3,
                    180,
                    0
                ],
                "title": "Footprints of Axion-Like Particle in Pulsar Timing Array Data and James\n  Webb Space Telescope Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Footprints of Axion-Like Particle in Pulsar Timing Array Data and James\n  Webb Space Telescope Observations"
                },
                "summary": "Several Pulsar Timing Array (PTA) collaborations have recently reported the\nevidence for a stochastic gravitational-wave background (SGWB), which can\nunveil the formation of primordial seeds of inhomogeneities in the early\nuniverse. With the SGWB parameters inferred from PTAs data, we can make a\nprediction of the seeds for early galaxy formation from the domain walls in the\naxion-like particles (ALPs) field distribution. This also naturally provides a\nsolution to the observation of high redshifts by the James Webb Space\nTelescope. The predicted photon coupling of the ALP is within the reach of\nfuture experimental searches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several Pulsar Timing Array (PTA) collaborations have recently reported the\nevidence for a stochastic gravitational-wave background (SGWB), which can\nunveil the formation of primordial seeds of inhomogeneities in the early\nuniverse. With the SGWB parameters inferred from PTAs data, we can make a\nprediction of the seeds for early galaxy formation from the domain walls in the\naxion-like particles (ALPs) field distribution. This also naturally provides a\nsolution to the observation of high redshifts by the James Webb Space\nTelescope. The predicted photon coupling of the ALP is within the reach of\nfuture experimental searches."
                },
                "authors": [
                    {
                        "name": "Shu-Yuan Guo"
                    },
                    {
                        "name": "Maxim Khlopov"
                    },
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Lei Wu"
                    },
                    {
                        "name": "Yongcheng Wu"
                    },
                    {
                        "name": "Bin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Zhu"
                },
                "author": "Bin Zhu",
                "arxiv_comment": "9 pages, 4 figures. version published in Sci. China-Phys. Mech.\n  Astron. November 2024 Vol.67 No.11: 111011",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.17022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.17022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v2",
                "updated": "2024-09-10T02:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    1,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge\n  Discovery"
                },
                "summary": "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access\nexternal databases, thereby enhancing the generation quality of large language\nmodels (LLMs) through optimized context. However, the existing retrieval\nmethods are constrained inherently, as they can only perform relevance matching\nbetween explicitly stated queries and well-formed knowledge, but unable to\nhandle tasks involving ambiguous information needs or unstructured knowledge.\nConsequently, existing RAG systems are primarily effective for straightforward\nquestion-answering tasks. In this work, we propose MemoRAG, a novel\nretrieval-augmented generation paradigm empowered by long-term memory. MemoRAG\nadopts a dual-system architecture. On the one hand, it employs a light but\nlong-range LLM to form the global memory of database. Once a task is presented,\nit generates draft answers, cluing the retrieval tools to locate useful\ninformation within the database. On the other hand, it leverages an expensive\nbut expressive LLM, which generates the ultimate answer based on the retrieved\ninformation. Building on this general framework, we further optimize MemoRAG's\nperformance by enhancing its cluing mechanism and memorization capacity. In our\nexperiment, MemoRAG achieves superior performance across a variety of\nevaluation tasks, including both complex ones where conventional RAG fails and\nstraightforward ones where RAG is commonly applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access\nexternal databases, thereby enhancing the generation quality of large language\nmodels (LLMs) through optimized context. However, the existing retrieval\nmethods are constrained inherently, as they can only perform relevance matching\nbetween explicitly stated queries and well-formed knowledge, but unable to\nhandle tasks involving ambiguous information needs or unstructured knowledge.\nConsequently, existing RAG systems are primarily effective for straightforward\nquestion-answering tasks. In this work, we propose MemoRAG, a novel\nretrieval-augmented generation paradigm empowered by long-term memory. MemoRAG\nadopts a dual-system architecture. On the one hand, it employs a light but\nlong-range LLM to form the global memory of database. Once a task is presented,\nit generates draft answers, cluing the retrieval tools to locate useful\ninformation within the database. On the other hand, it leverages an expensive\nbut expressive LLM, which generates the ultimate answer based on the retrieved\ninformation. Building on this general framework, we further optimize MemoRAG's\nperformance by enhancing its cluing mechanism and memorization capacity. In our\nexperiment, MemoRAG achieves superior performance across a variety of\nevaluation tasks, including both complex ones where conventional RAG fails and\nstraightforward ones where RAG is commonly applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Technical Report. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12325v2",
                "updated": "2024-09-09T13:10:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    10,
                    50,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-22T12:00:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    0,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators"
                },
                "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Hallucination Mitigation in LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05564v1",
                "updated": "2024-09-09T12:43:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    43,
                    25,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T12:43:25Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    43,
                    25,
                    0,
                    253,
                    0
                ],
                "title": "LEROjD: Lidar Extended Radar-Only Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEROjD: Lidar Extended Radar-Only Object Detection"
                },
                "summary": "Accurate 3D object detection is vital for automated driving. While lidar\nsensors are well suited for this task, they are expensive and have limitations\nin adverse weather conditions. 3+1D imaging radar sensors offer a\ncost-effective, robust alternative but face challenges due to their low\nresolution and high measurement noise. Existing 3+1D imaging radar datasets\ninclude radar and lidar data, enabling cross-modal model improvements. Although\nlidar should not be used during inference, it can aid the training of\nradar-only object detectors. We explore two strategies to transfer knowledge\nfrom the lidar to the radar domain and radar-only object detectors: 1.\nmulti-stage training with sequential lidar point cloud thin-out, and 2.\ncross-modal knowledge distillation. In the multi-stage process, three thin-out\nmethods are examined. Our results show significant performance gains of up to\n4.2 percentage points in mean Average Precision with multi-stage training and\nup to 3.9 percentage points with knowledge distillation by initializing the\nstudent with the teacher's weights. The main benefit of these approaches is\ntheir applicability to other 3D object detection networks without altering\ntheir architecture, as we show by analyzing it on two different object\ndetectors. Our code is available at https://github.com/rst-tu-dortmund/lerojd",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate 3D object detection is vital for automated driving. While lidar\nsensors are well suited for this task, they are expensive and have limitations\nin adverse weather conditions. 3+1D imaging radar sensors offer a\ncost-effective, robust alternative but face challenges due to their low\nresolution and high measurement noise. Existing 3+1D imaging radar datasets\ninclude radar and lidar data, enabling cross-modal model improvements. Although\nlidar should not be used during inference, it can aid the training of\nradar-only object detectors. We explore two strategies to transfer knowledge\nfrom the lidar to the radar domain and radar-only object detectors: 1.\nmulti-stage training with sequential lidar point cloud thin-out, and 2.\ncross-modal knowledge distillation. In the multi-stage process, three thin-out\nmethods are examined. Our results show significant performance gains of up to\n4.2 percentage points in mean Average Precision with multi-stage training and\nup to 3.9 percentage points with knowledge distillation by initializing the\nstudent with the teacher's weights. The main benefit of these approaches is\ntheir applicability to other 3D object detection networks without altering\ntheir architecture, as we show by analyzing it on two different object\ndetectors. Our code is available at https://github.com/rst-tu-dortmund/lerojd"
                },
                "authors": [
                    {
                        "name": "Patrick Palmer"
                    },
                    {
                        "name": "Martin Krüger"
                    },
                    {
                        "name": "Stefan Schütte"
                    },
                    {
                        "name": "Richard Altendorfer"
                    },
                    {
                        "name": "Ganesh Adam"
                    },
                    {
                        "name": "Torsten Bertram"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Bertram"
                },
                "author": "Torsten Bertram",
                "arxiv_comment": "Accepted for publication as ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11944v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11944v3",
                "updated": "2024-09-09T12:38:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    38,
                    11,
                    0,
                    253,
                    0
                ],
                "published": "2024-01-22T13:34:34Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    13,
                    34,
                    34,
                    0,
                    22,
                    0
                ],
                "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark"
                },
                "summary": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts."
                },
                "authors": [
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Tongxu Luo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Kang Zhu"
                    },
                    {
                        "name": "Yuyang Cheng"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Yudong Liu"
                    },
                    {
                        "name": "Yu-Hsuan Tsai"
                    },
                    {
                        "name": "Fengji Zhang"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11944v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11944v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.11724v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.11724v3",
                "updated": "2024-09-09T12:37:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    37,
                    44,
                    0,
                    253,
                    0
                ],
                "published": "2023-10-18T05:43:49Z",
                "published_parsed": [
                    2023,
                    10,
                    18,
                    5,
                    43,
                    49,
                    2,
                    291,
                    0
                ],
                "title": "Self-convolved Bootstrap for M-regression under Complex Temporal\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-convolved Bootstrap for M-regression under Complex Temporal\n  Dynamics"
                },
                "summary": "The paper considers simultaneous nonparametric inference for a wide class of\nM-regression models with time-varying coefficients. The covariates and errors\nof the regression model are tackled as a general class of nonstationary time\nseries and are allowed to be cross-dependent. A novel and easy-to-implement\nself-convolved bootstrap procedure is proposed. With only one tuning parameter,\nthe bootstrap facilitates a $\\sqrt{n}$-consistent inference of the cumulative\nregression function for the M-estimators under complex temporal dynamics, even\nunder the possible presence of breakpoints in time series. Our methodology\nleads to a unified framework to conduct general classes of Exact Function\nTests, Lack-of-fit Tests, and Qualitative Tests for the time-varying\ncoefficients. These tests enable one to, among many others, conduct variable\nselection, check for constancy and linearity, as well as verify shape\nassumptions, including monotonicity and convexity. As applications, our method\nis utilized to study the time-varying properties of global climate data and\nMicrosoft stock return, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper considers simultaneous nonparametric inference for a wide class of\nM-regression models with time-varying coefficients. The covariates and errors\nof the regression model are tackled as a general class of nonstationary time\nseries and are allowed to be cross-dependent. A novel and easy-to-implement\nself-convolved bootstrap procedure is proposed. With only one tuning parameter,\nthe bootstrap facilitates a $\\sqrt{n}$-consistent inference of the cumulative\nregression function for the M-estimators under complex temporal dynamics, even\nunder the possible presence of breakpoints in time series. Our methodology\nleads to a unified framework to conduct general classes of Exact Function\nTests, Lack-of-fit Tests, and Qualitative Tests for the time-varying\ncoefficients. These tests enable one to, among many others, conduct variable\nselection, check for constancy and linearity, as well as verify shape\nassumptions, including monotonicity and convexity. As applications, our method\nis utilized to study the time-varying properties of global climate data and\nMicrosoft stock return, respectively."
                },
                "authors": [
                    {
                        "name": "Miaoshiqi Liu"
                    },
                    {
                        "name": "Zhou Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhou"
                },
                "author": "Zhou Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.11724v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.11724v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11654v2",
                "updated": "2024-09-09T12:36:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    36,
                    29,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-16T12:21:29Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    12,
                    21,
                    29,
                    1,
                    198,
                    0
                ],
                "title": "R-SFLLM: Jamming Resilient Framework for Split Federated Learning with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-SFLLM: Jamming Resilient Framework for Split Federated Learning with\n  Large Language Models"
                },
                "summary": "Split federated learning (SFL) is a compute-efficient paradigm in distributed\nmachine learning (ML), where components of large ML models are outsourced to\nremote servers. A significant challenge in SFL, particularly when deployed over\nwireless channels, is the susceptibility of transmitted model parameters to\nadversarial jamming that could jeopardize the learning process. This is\nparticularly pronounced for word embedding parameters in large language models\n(LLMs), which are crucial for language understanding. In this paper, rigorous\ninsights are provided into the influence of jamming LLM word embeddings in SFL\nby deriving an expression for the ML training loss divergence and showing that\nit is upper-bounded by the mean squared error (MSE). Based on this analysis, a\nphysical layer framework is developed for resilient SFL with LLMs (R-SFLLM)\nover wireless networks. R-SFLLM leverages wireless sensing data to gather\ninformation on the jamming directions-of-arrival (DoAs) for the purpose of\ndevising a novel, sensing-assisted anti-jamming strategy while jointly\noptimizing beamforming, user scheduling, and resource allocation. Extensive\nexperiments using BERT and RoBERTa models demonstrate R-SFLLM's effectiveness,\nachieving close-to-baseline performance across various natural language\nprocessing (NLP) tasks and datasets. The proposed methodology further\nintroduces an adversarial training component, where controlled noise exposure\nsignificantly enhances the LLM's resilience to perturbed parameters during\ntraining. The results show that more noise-sensitive models, such as RoBERTa,\nbenefit from this feature, especially when resource allocation is unfair. It is\nalso shown that worst-case jamming in particular translates into worst-case\nmodel outcomes, thereby necessitating the need for jamming-resilient SFL\nprotocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split federated learning (SFL) is a compute-efficient paradigm in distributed\nmachine learning (ML), where components of large ML models are outsourced to\nremote servers. A significant challenge in SFL, particularly when deployed over\nwireless channels, is the susceptibility of transmitted model parameters to\nadversarial jamming that could jeopardize the learning process. This is\nparticularly pronounced for word embedding parameters in large language models\n(LLMs), which are crucial for language understanding. In this paper, rigorous\ninsights are provided into the influence of jamming LLM word embeddings in SFL\nby deriving an expression for the ML training loss divergence and showing that\nit is upper-bounded by the mean squared error (MSE). Based on this analysis, a\nphysical layer framework is developed for resilient SFL with LLMs (R-SFLLM)\nover wireless networks. R-SFLLM leverages wireless sensing data to gather\ninformation on the jamming directions-of-arrival (DoAs) for the purpose of\ndevising a novel, sensing-assisted anti-jamming strategy while jointly\noptimizing beamforming, user scheduling, and resource allocation. Extensive\nexperiments using BERT and RoBERTa models demonstrate R-SFLLM's effectiveness,\nachieving close-to-baseline performance across various natural language\nprocessing (NLP) tasks and datasets. The proposed methodology further\nintroduces an adversarial training component, where controlled noise exposure\nsignificantly enhances the LLM's resilience to perturbed parameters during\ntraining. The results show that more noise-sensitive models, such as RoBERTa,\nbenefit from this feature, especially when resource allocation is unfair. It is\nalso shown that worst-case jamming in particular translates into worst-case\nmodel outcomes, thereby necessitating the need for jamming-resilient SFL\nprotocols."
                },
                "authors": [
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Vlad C. Andrei"
                    },
                    {
                        "name": "Xinyang Li"
                    },
                    {
                        "name": "Ullrich J. Mönich"
                    },
                    {
                        "name": "Holger Boche"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05559v1",
                "updated": "2024-09-09T12:30:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    30,
                    43,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T12:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    30,
                    43,
                    0,
                    253,
                    0
                ],
                "title": "CauseJudger: Identifying the Cause with LLMs for Abductive Logical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CauseJudger: Identifying the Cause with LLMs for Abductive Logical\n  Reasoning"
                },
                "summary": "Large language models (LLMs) have been utilized in solving diverse reasoning\ntasks, encompassing common sense, arithmetic and deduction tasks. However, with\ndifficulties of reversing thinking patterns and irrelevant premises, how to\ndetermine the authenticity of the cause in abductive logical reasoning remains\nunderexplored. Inspired by hypothesis and verification method and\nidentification of irrelevant information in human thinking process, we propose\na new framework for LLMs abductive logical reasoning called CauseJudger (CJ),\nwhich identifies the authenticity of possible cause by transforming thinking\nfrom reverse to forward and removing irrelevant information. In addition, we\nconstruct an abductive logical reasoning dataset for decision task called\nCauseLogics, which contains 200,000 tasks of varying reasoning lengths. Our\nexperiments show the efficiency of CJ with overall experiments and ablation\nexperiments as well as case studies on our dataset and reconstructed public\ndataset. Notably, CJ's implementation is efficient, requiring only two calls to\nLLM. Its impact is profound: when using gpt-3.5, CJ achieves a maximum\ncorrectness improvement of 41% compared to Zero-Shot-CoT. Moreover, with gpt-4,\nCJ attains an accuracy exceeding 90% across all datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been utilized in solving diverse reasoning\ntasks, encompassing common sense, arithmetic and deduction tasks. However, with\ndifficulties of reversing thinking patterns and irrelevant premises, how to\ndetermine the authenticity of the cause in abductive logical reasoning remains\nunderexplored. Inspired by hypothesis and verification method and\nidentification of irrelevant information in human thinking process, we propose\na new framework for LLMs abductive logical reasoning called CauseJudger (CJ),\nwhich identifies the authenticity of possible cause by transforming thinking\nfrom reverse to forward and removing irrelevant information. In addition, we\nconstruct an abductive logical reasoning dataset for decision task called\nCauseLogics, which contains 200,000 tasks of varying reasoning lengths. Our\nexperiments show the efficiency of CJ with overall experiments and ablation\nexperiments as well as case studies on our dataset and reconstructed public\ndataset. Notably, CJ's implementation is efficient, requiring only two calls to\nLLM. Its impact is profound: when using gpt-3.5, CJ achieves a maximum\ncorrectness improvement of 41% compared to Zero-Shot-CoT. Moreover, with gpt-4,\nCJ attains an accuracy exceeding 90% across all datasets."
                },
                "authors": [
                    {
                        "name": "Jinwei He"
                    },
                    {
                        "name": "Feng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lu"
                },
                "author": "Feng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05556v1",
                "updated": "2024-09-09T12:25:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    25,
                    10,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T12:25:10Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    25,
                    10,
                    0,
                    253,
                    0
                ],
                "title": "SciAgents: Automating scientific discovery through multi-agent\n  intelligent graph reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciAgents: Automating scientific discovery through multi-agent\n  intelligent graph reasoning"
                },
                "summary": "A key challenge in artificial intelligence is the creation of systems capable\nof autonomously advancing scientific understanding by exploring novel domains,\nidentifying complex patterns, and uncovering previously unseen connections in\nvast scientific data. In this work, we present SciAgents, an approach that\nleverages three core concepts: (1) the use of large-scale ontological knowledge\ngraphs to organize and interconnect diverse scientific concepts, (2) a suite of\nlarge language models (LLMs) and data retrieval tools, and (3) multi-agent\nsystems with in-situ learning capabilities. Applied to biologically inspired\nmaterials, SciAgents reveals hidden interdisciplinary relationships that were\npreviously considered unrelated, achieving a scale, precision, and exploratory\npower that surpasses traditional human-driven research methods. The framework\nautonomously generates and refines research hypotheses, elucidating underlying\nmechanisms, design principles, and unexpected material properties. By\nintegrating these capabilities in a modular fashion, the intelligent system\nyields material discoveries, critique and improve existing hypotheses, retrieve\nup-to-date data about existing research, and highlights their strengths and\nlimitations. Our case studies demonstrate scalable capabilities to combine\ngenerative AI, ontological representations, and multi-agent modeling,\nharnessing a `swarm of intelligence' similar to biological systems. This\nprovides new avenues for materials discovery and accelerates the development of\nadvanced materials by unlocking Nature's design principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in artificial intelligence is the creation of systems capable\nof autonomously advancing scientific understanding by exploring novel domains,\nidentifying complex patterns, and uncovering previously unseen connections in\nvast scientific data. In this work, we present SciAgents, an approach that\nleverages three core concepts: (1) the use of large-scale ontological knowledge\ngraphs to organize and interconnect diverse scientific concepts, (2) a suite of\nlarge language models (LLMs) and data retrieval tools, and (3) multi-agent\nsystems with in-situ learning capabilities. Applied to biologically inspired\nmaterials, SciAgents reveals hidden interdisciplinary relationships that were\npreviously considered unrelated, achieving a scale, precision, and exploratory\npower that surpasses traditional human-driven research methods. The framework\nautonomously generates and refines research hypotheses, elucidating underlying\nmechanisms, design principles, and unexpected material properties. By\nintegrating these capabilities in a modular fashion, the intelligent system\nyields material discoveries, critique and improve existing hypotheses, retrieve\nup-to-date data about existing research, and highlights their strengths and\nlimitations. Our case studies demonstrate scalable capabilities to combine\ngenerative AI, ontological representations, and multi-agent modeling,\nharnessing a `swarm of intelligence' similar to biological systems. This\nprovides new avenues for materials discovery and accelerates the development of\nadvanced materials by unlocking Nature's design principles."
                },
                "authors": [
                    {
                        "name": "Alireza Ghafarollahi"
                    },
                    {
                        "name": "Markus J. Buehler"
                    }
                ],
                "author_detail": {
                    "name": "Markus J. Buehler"
                },
                "author": "Markus J. Buehler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05555v1",
                "updated": "2024-09-09T12:22:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    22,
                    38,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T12:22:38Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    22,
                    38,
                    0,
                    253,
                    0
                ],
                "title": "Dynamics in the nonequilibrium energy landscape of a frustrated Mott\n  insulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamics in the nonequilibrium energy landscape of a frustrated Mott\n  insulator"
                },
                "summary": "In a Mott insulator, a laser pulse with frequency tuned to the gap scale can\ncreate a holon-doublon plasma, suppressing the magnetic moment ${\\vec m}_i$ and\ndestroying magnetic order. While this disruptive effect is well established\nexperimentally on a square lattice, we investigate the effect of laser pumping\non the triangular lattice, where geometric frustration leads to a richer set of\nordering possibilities. We work with the Mott-Hubbard problem at a coupling\nwhere $120^{\\circ}$ order is just stable and employ spatio-temporal mean field\ndynamics to study the pump response. Moderate pump amplitude just leads to the\nreduction of $120^{\\circ}$ order, but at larger amplitude the suppression of\n$120^{\\circ}$ order is followed by the appearance of `spiral order'. On the\nelectronic side the density of `excited carriers' $n_{exc}$ in the upper\nHubbard band increases monotonically with pump amplitude. We show that the long\ntime ordering possibilities in the pumped system, e.g., the emergence of spiral\norder, can be inferred from a nonequilibrium `energy landscape'. We analyse the\ngrowth of spiral order by using an exact diagonalisation based Langevin\nequation on large lattices and discover that the new order can take $\\sim\n10^3-10^4$ times the electronic timescale to appear. The threefold combination,\nof mean field dynamics, landscape construction, and Langevin dynamics, readily\ngeneralises to the search for pump induced `hidden order' in other gapped\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a Mott insulator, a laser pulse with frequency tuned to the gap scale can\ncreate a holon-doublon plasma, suppressing the magnetic moment ${\\vec m}_i$ and\ndestroying magnetic order. While this disruptive effect is well established\nexperimentally on a square lattice, we investigate the effect of laser pumping\non the triangular lattice, where geometric frustration leads to a richer set of\nordering possibilities. We work with the Mott-Hubbard problem at a coupling\nwhere $120^{\\circ}$ order is just stable and employ spatio-temporal mean field\ndynamics to study the pump response. Moderate pump amplitude just leads to the\nreduction of $120^{\\circ}$ order, but at larger amplitude the suppression of\n$120^{\\circ}$ order is followed by the appearance of `spiral order'. On the\nelectronic side the density of `excited carriers' $n_{exc}$ in the upper\nHubbard band increases monotonically with pump amplitude. We show that the long\ntime ordering possibilities in the pumped system, e.g., the emergence of spiral\norder, can be inferred from a nonequilibrium `energy landscape'. We analyse the\ngrowth of spiral order by using an exact diagonalisation based Langevin\nequation on large lattices and discover that the new order can take $\\sim\n10^3-10^4$ times the electronic timescale to appear. The threefold combination,\nof mean field dynamics, landscape construction, and Langevin dynamics, readily\ngeneralises to the search for pump induced `hidden order' in other gapped\nsystems."
                },
                "authors": [
                    {
                        "name": "Sankha Subhra Bakshi"
                    },
                    {
                        "name": "Tanmoy Mondal"
                    },
                    {
                        "name": "Pinaki Majumdar"
                    }
                ],
                "author_detail": {
                    "name": "Pinaki Majumdar"
                },
                "author": "Pinaki Majumdar",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02202v2",
                "updated": "2024-09-09T12:04:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    4,
                    27,
                    0,
                    253,
                    0
                ],
                "published": "2024-06-04T10:57:59Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    10,
                    57,
                    59,
                    1,
                    156,
                    0
                ],
                "title": "No Captions, No Problem: Captionless 3D-CLIP Alignment with Hard\n  Negatives via CLIP Knowledge and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Captions, No Problem: Captionless 3D-CLIP Alignment with Hard\n  Negatives via CLIP Knowledge and LLMs"
                },
                "summary": "In this study, we explore an alternative approach to enhance contrastive\ntext-image-3D alignment in the absence of textual descriptions for 3D objects.\nWe introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP\nknowledge about textual and 2D data to compute the neural perceived similarity\nbetween two 3D samples. We employ the proposed methods to mine 3D hard\nnegatives, establishing a multimodal contrastive pipeline with hard negative\nweighting via a custom loss function. We train on different configurations of\nthe proposed hard negative mining approach, and we evaluate the accuracy of our\nmodels in 3D classification and on the cross-modal retrieval benchmark, testing\nimage-to-shape and shape-to-image retrieval. Results demonstrate that our\napproach, even without explicit text alignment, achieves comparable or superior\nperformance on zero-shot and standard 3D classification, while significantly\nimproving both image-to-shape and shape-to-image retrieval compared to previous\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we explore an alternative approach to enhance contrastive\ntext-image-3D alignment in the absence of textual descriptions for 3D objects.\nWe introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP\nknowledge about textual and 2D data to compute the neural perceived similarity\nbetween two 3D samples. We employ the proposed methods to mine 3D hard\nnegatives, establishing a multimodal contrastive pipeline with hard negative\nweighting via a custom loss function. We train on different configurations of\nthe proposed hard negative mining approach, and we evaluate the accuracy of our\nmodels in 3D classification and on the cross-modal retrieval benchmark, testing\nimage-to-shape and shape-to-image retrieval. Results demonstrate that our\napproach, even without explicit text alignment, achieves comparable or superior\nperformance on zero-shot and standard 3D classification, while significantly\nimproving both image-to-shape and shape-to-image retrieval compared to previous\nmethods."
                },
                "authors": [
                    {
                        "name": "Cristian Sbrolli"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "to be published in BMVC 2024 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08174v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08174v4",
                "updated": "2024-09-09T12:00:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    0,
                    58,
                    0,
                    253,
                    0
                ],
                "published": "2023-12-13T14:34:12Z",
                "published_parsed": [
                    2023,
                    12,
                    13,
                    14,
                    34,
                    12,
                    2,
                    347,
                    0
                ],
                "title": "Double Machine Learning for Static Panel Models with Fixed Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double Machine Learning for Static Panel Models with Fixed Effects"
                },
                "summary": "Recent advances in causal inference have seen the development of methods\nwhich make use of the predictive power of machine learning algorithms. In this\npaper, we use these algorithms to approximate high-dimensional and non-linear\nnuisance functions of the confounders and double machine learning (DML) to make\ninferences about the effects of policy interventions from panel data. We\npropose new estimators by extending correlated random effects, within-group and\nfirst-difference estimation for linear models to an extension of Robinson\n(1988)'s partially linear regression model to static panel data models with\nindividual fixed effects and unspecified non-linear confounding effects. We\nprovide an illustrative example of DML for observational panel data showing the\nimpact of the introduction of the minimum wage on voting behaviour in the UK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in causal inference have seen the development of methods\nwhich make use of the predictive power of machine learning algorithms. In this\npaper, we use these algorithms to approximate high-dimensional and non-linear\nnuisance functions of the confounders and double machine learning (DML) to make\ninferences about the effects of policy interventions from panel data. We\npropose new estimators by extending correlated random effects, within-group and\nfirst-difference estimation for linear models to an extension of Robinson\n(1988)'s partially linear regression model to static panel data models with\nindividual fixed effects and unspecified non-linear confounding effects. We\nprovide an illustrative example of DML for observational panel data showing the\nimpact of the introduction of the minimum wage on voting behaviour in the UK."
                },
                "authors": [
                    {
                        "name": "Paul Clarke"
                    },
                    {
                        "name": "Annalivia Polselli"
                    }
                ],
                "author_detail": {
                    "name": "Annalivia Polselli"
                },
                "author": "Annalivia Polselli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08174v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08174v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06227v2",
                "updated": "2024-09-09T11:38:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    38,
                    24,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-06T14:49:46Z",
                "published_parsed": [
                    2024,
                    7,
                    6,
                    14,
                    49,
                    46,
                    5,
                    188,
                    0
                ],
                "title": "Communication and Control Co-Design in 6G: Sequential Decision-Making\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication and Control Co-Design in 6G: Sequential Decision-Making\n  with LLMs"
                },
                "summary": "This article investigates a control system within the context of\nsix-generation wireless networks. The control performance optimization\nconfronts the technical challenges that arise from the intricate interactions\nbetween communication and control sub-systems, asking for a co-design.\nAccounting for the system dynamics, we formulate the sequential co-design\ndecision-makings of communication and control over the discrete time horizon as\na Markov decision process, for which a practical offline learning framework is\nproposed. Our proposed framework integrates large language models into the\nelements of reinforcement learning. We present a case study on the age of\nsemantics-aware communication and control co-design to showcase the potentials\nfrom our proposed learning framework. Furthermore, we discuss the open issues\nremaining to make our proposed offline learning framework feasible for\nreal-world implementations, and highlight the research directions for future\nexplorations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article investigates a control system within the context of\nsix-generation wireless networks. The control performance optimization\nconfronts the technical challenges that arise from the intricate interactions\nbetween communication and control sub-systems, asking for a co-design.\nAccounting for the system dynamics, we formulate the sequential co-design\ndecision-makings of communication and control over the discrete time horizon as\na Markov decision process, for which a practical offline learning framework is\nproposed. Our proposed framework integrates large language models into the\nelements of reinforcement learning. We present a case study on the age of\nsemantics-aware communication and control co-design to showcase the potentials\nfrom our proposed learning framework. Furthermore, we discuss the open issues\nremaining to make our proposed offline learning framework feasible for\nreal-world implementations, and highlight the research directions for future\nexplorations."
                },
                "authors": [
                    {
                        "name": "Xianfu Chen"
                    },
                    {
                        "name": "Celimuge Wu"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yusheng Ji"
                    },
                    {
                        "name": "Tsutomu Yoshinaga"
                    },
                    {
                        "name": "Qiang Ni"
                    },
                    {
                        "name": "Charilaos C. Zarakovitis"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05529v1",
                "updated": "2024-09-09T11:37:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    37,
                    32,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T11:37:32Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    37,
                    32,
                    0,
                    253,
                    0
                ],
                "title": "Bootstrapping Estimators based on the Block Maxima Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Estimators based on the Block Maxima Method"
                },
                "summary": "The block maxima method is a standard approach for analyzing the extremal\nbehavior of a potentially multivariate time series. It has recently been found\nthat the classical approach based on disjoint block maxima may be universally\nimproved by considering sliding block maxima instead. However, the asymptotic\nvariance formula for estimators based on sliding block maxima involves an\nintegral over the covariance of a certain family of multivariate extreme value\ndistributions, which makes its estimation, and inference in general, an\nintricate problem. As an alternative, one may rely on bootstrap approximations:\nwe show that naive block-bootstrap approaches from time series analysis are\ninconsistent even in i.i.d.\\ situations, and provide a consistent alternative\nbased on resampling circular block maxima. As a by-product, we show consistency\nof the classical resampling bootstrap for disjoint block maxima, and that\nestimators based on circular block maxima have the same asymptotic variance as\ntheir sliding block maxima counterparts. The finite sample properties are\nillustrated by Monte Carlo experiments, and the methods are demonstrated by a\ncase study of precipitation extremes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The block maxima method is a standard approach for analyzing the extremal\nbehavior of a potentially multivariate time series. It has recently been found\nthat the classical approach based on disjoint block maxima may be universally\nimproved by considering sliding block maxima instead. However, the asymptotic\nvariance formula for estimators based on sliding block maxima involves an\nintegral over the covariance of a certain family of multivariate extreme value\ndistributions, which makes its estimation, and inference in general, an\nintricate problem. As an alternative, one may rely on bootstrap approximations:\nwe show that naive block-bootstrap approaches from time series analysis are\ninconsistent even in i.i.d.\\ situations, and provide a consistent alternative\nbased on resampling circular block maxima. As a by-product, we show consistency\nof the classical resampling bootstrap for disjoint block maxima, and that\nestimators based on circular block maxima have the same asymptotic variance as\ntheir sliding block maxima counterparts. The finite sample properties are\nillustrated by Monte Carlo experiments, and the methods are demonstrated by a\ncase study of precipitation extremes."
                },
                "authors": [
                    {
                        "name": "Axel Bücher"
                    },
                    {
                        "name": "Torben Staud"
                    }
                ],
                "author_detail": {
                    "name": "Torben Staud"
                },
                "author": "Torben Staud",
                "arxiv_comment": "42 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62F40, 62G32, Secondary 62E20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04073v2",
                "updated": "2024-09-09T11:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    33,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-06T07:29:01Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    29,
                    1,
                    4,
                    250,
                    0
                ],
                "title": "AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language\n  Model"
                },
                "summary": "Entity matching (EM) is the problem of determining whether two records refer\nto same real-world entity, which is crucial in data integration, e.g., for\nproduct catalogs or address databases. A major drawback of many EM approaches\nis their dependence on labelled examples. We thus focus on the challenging\nsetting of zero-shot entity matching where no labelled examples are available\nfor an unseen target dataset. Recently, large language models (LLMs) have shown\npromising results for zero-shot EM, but their low throughput and high\ndeployment cost limit their applicability and scalability.\n  We revisit the zero-shot EM problem with AnyMatch, a small language model\nfine-tuned in a transfer learning setup. We propose several novel data\nselection techniques to generate fine-tuning data for our model, e.g., by\nselecting difficult pairs to match via an AutoML filter, by generating\nadditional attribute-level examples, and by controlling label imbalance in the\ndata.\n  We conduct an extensive evaluation of the prediction quality and deployment\ncost of our model, in a comparison to thirteen baselines on nine benchmark\ndatasets. We find that AnyMatch provides competitive prediction quality despite\nits small parameter size: it achieves the second-highest F1 score overall, and\noutperforms several other approaches that employ models with hundreds of\nbillions of parameters. Furthermore, our approach exhibits major cost benefits:\nthe average prediction quality of AnyMatch is within 4.4% of the\nstate-of-the-art method MatchGPT with the proprietary trillion-parameter model\nGPT-4, yet AnyMatch requires four orders of magnitude less parameters and\nincurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity matching (EM) is the problem of determining whether two records refer\nto same real-world entity, which is crucial in data integration, e.g., for\nproduct catalogs or address databases. A major drawback of many EM approaches\nis their dependence on labelled examples. We thus focus on the challenging\nsetting of zero-shot entity matching where no labelled examples are available\nfor an unseen target dataset. Recently, large language models (LLMs) have shown\npromising results for zero-shot EM, but their low throughput and high\ndeployment cost limit their applicability and scalability.\n  We revisit the zero-shot EM problem with AnyMatch, a small language model\nfine-tuned in a transfer learning setup. We propose several novel data\nselection techniques to generate fine-tuning data for our model, e.g., by\nselecting difficult pairs to match via an AutoML filter, by generating\nadditional attribute-level examples, and by controlling label imbalance in the\ndata.\n  We conduct an extensive evaluation of the prediction quality and deployment\ncost of our model, in a comparison to thirteen baselines on nine benchmark\ndatasets. We find that AnyMatch provides competitive prediction quality despite\nits small parameter size: it achieves the second-highest F1 score overall, and\noutperforms several other approaches that employ models with hundreds of\nbillions of parameters. Furthermore, our approach exhibits major cost benefits:\nthe average prediction quality of AnyMatch is within 4.4% of the\nstate-of-the-art method MatchGPT with the proprietary trillion-parameter model\nGPT-4, yet AnyMatch requires four orders of magnitude less parameters and\nincurs a 3,899 times lower inference cost (in dollars per 1,000 tokens)."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Paul Groth"
                    },
                    {
                        "name": "Iacer Calixto"
                    },
                    {
                        "name": "Sebastian Schelter"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Schelter"
                },
                "author": "Sebastian Schelter",
                "arxiv_comment": "12 pages excluding references, 3 figures, and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05521v1",
                "updated": "2024-09-09T11:28:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    28,
                    2,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T11:28:02Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    28,
                    2,
                    0,
                    253,
                    0
                ],
                "title": "Harmonic Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmonic Reasoning in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are becoming very popular and are used for many\ndifferent purposes, including creative tasks in the arts. However, these models\nsometimes have trouble with specific reasoning tasks, especially those that\ninvolve logical thinking and counting. This paper looks at how well LLMs\nunderstand and reason when dealing with musical tasks like figuring out notes\nfrom intervals and identifying chords and scales. We tested GPT-3.5 and GPT-4o\nto see how they handle these tasks. Our results show that while LLMs do well\nwith note intervals, they struggle with more complicated tasks like recognizing\nchords and scales. This points out clear limits in current LLM abilities and\nshows where we need to make them better, which could help improve how they\nthink and work in both artistic and other complex areas. We also provide an\nautomatically generated benchmark data set for the described tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming very popular and are used for many\ndifferent purposes, including creative tasks in the arts. However, these models\nsometimes have trouble with specific reasoning tasks, especially those that\ninvolve logical thinking and counting. This paper looks at how well LLMs\nunderstand and reason when dealing with musical tasks like figuring out notes\nfrom intervals and identifying chords and scales. We tested GPT-3.5 and GPT-4o\nto see how they handle these tasks. Our results show that while LLMs do well\nwith note intervals, they struggle with more complicated tasks like recognizing\nchords and scales. This points out clear limits in current LLM abilities and\nshows where we need to make them better, which could help improve how they\nthink and work in both artistic and other complex areas. We also provide an\nautomatically generated benchmark data set for the described tasks."
                },
                "authors": [
                    {
                        "name": "Anna Kruspe"
                    }
                ],
                "author_detail": {
                    "name": "Anna Kruspe"
                },
                "author": "Anna Kruspe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11381v2",
                "updated": "2024-09-09T11:18:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    18,
                    16,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-21T07:20:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    20,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "RAGLAB: A Modular and Research-Oriented Unified Framework for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGLAB: A Modular and Research-Oriented Unified Framework for\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) demonstrate human-level capabilities in\ndialogue, reasoning, and knowledge retention. However, even the most advanced\nLLMs face challenges such as hallucinations and real-time updating of their\nknowledge. Current research addresses this bottleneck by equipping LLMs with\nexternal knowledge, a technique known as Retrieval Augmented Generation (RAG).\nHowever, two key issues constrained the development of RAG. First, there is a\ngrowing lack of comprehensive and fair comparisons between novel RAG\nalgorithms. Second, open-source tools such as LlamaIndex and LangChain employ\nhigh-level abstractions, which results in a lack of transparency and limits the\nability to develop novel algorithms and evaluation metrics. To close this gap,\nwe introduce RAGLAB, a modular and research-oriented open-source library.\nRAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem\nfor investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair\ncomparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers\ncan efficiently compare the performance of various algorithms and develop novel\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate human-level capabilities in\ndialogue, reasoning, and knowledge retention. However, even the most advanced\nLLMs face challenges such as hallucinations and real-time updating of their\nknowledge. Current research addresses this bottleneck by equipping LLMs with\nexternal knowledge, a technique known as Retrieval Augmented Generation (RAG).\nHowever, two key issues constrained the development of RAG. First, there is a\ngrowing lack of comprehensive and fair comparisons between novel RAG\nalgorithms. Second, open-source tools such as LlamaIndex and LangChain employ\nhigh-level abstractions, which results in a lack of transparency and limits the\nability to develop novel algorithms and evaluation metrics. To close this gap,\nwe introduce RAGLAB, a modular and research-oriented open-source library.\nRAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem\nfor investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair\ncomparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers\ncan efficiently compare the performance of various algorithms and develop novel\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Xuanwang Zhang"
                    },
                    {
                        "name": "Yunze Song"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Shuyun Tang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Zhengran Zeng"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Wenyuan Xu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xinyu Dai"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05511v1",
                "updated": "2024-09-09T11:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    9,
                    28,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T11:09:28Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    9,
                    28,
                    0,
                    253,
                    0
                ],
                "title": "Enhancing Critical Thinking in Education by means of a Socratic Chatbot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Critical Thinking in Education by means of a Socratic Chatbot"
                },
                "summary": "While large language models (LLMs) are increasingly playing a pivotal role in\neducation by providing instantaneous, adaptive responses, their potential to\npromote critical thinking remains understudied. In this paper, we fill such a\ngap and present an innovative educational chatbot designed to foster critical\nthinking through Socratic questioning. Unlike traditional intelligent tutoring\nsystems, including educational chatbots, that tend to offer direct answers, the\nproposed Socratic tutor encourages students to explore various perspectives and\nengage in self-reflection by posing structured, thought-provoking questions.\nOur Socratic questioning is implemented by fine and prompt-tuning the\nopen-source pretrained LLM with a specialized dataset that stimulates critical\nthinking and offers multiple viewpoints. In an effort to democratize access and\nto protect the students' privacy, the proposed tutor is based on small LLMs\n(Llama2 7B and 13B-parameter models) that are able to run locally on\noff-the-shelf hardware. We validate our approach in a battery of experiments\nconsisting of interactions between a simulated student and the chatbot to\nevaluate its effectiveness in enhancing critical thinking skills. Results\nindicate that the Socratic tutor supports the development of reflection and\ncritical thinking significantly better than standard chatbots. Our approach\nopens the door for improving educational outcomes by cultivating active\nlearning and encouraging intellectual autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly playing a pivotal role in\neducation by providing instantaneous, adaptive responses, their potential to\npromote critical thinking remains understudied. In this paper, we fill such a\ngap and present an innovative educational chatbot designed to foster critical\nthinking through Socratic questioning. Unlike traditional intelligent tutoring\nsystems, including educational chatbots, that tend to offer direct answers, the\nproposed Socratic tutor encourages students to explore various perspectives and\nengage in self-reflection by posing structured, thought-provoking questions.\nOur Socratic questioning is implemented by fine and prompt-tuning the\nopen-source pretrained LLM with a specialized dataset that stimulates critical\nthinking and offers multiple viewpoints. In an effort to democratize access and\nto protect the students' privacy, the proposed tutor is based on small LLMs\n(Llama2 7B and 13B-parameter models) that are able to run locally on\noff-the-shelf hardware. We validate our approach in a battery of experiments\nconsisting of interactions between a simulated student and the chatbot to\nevaluate its effectiveness in enhancing critical thinking skills. Results\nindicate that the Socratic tutor supports the development of reflection and\ncritical thinking significantly better than standard chatbots. Our approach\nopens the door for improving educational outcomes by cultivating active\nlearning and encouraging intellectual autonomy."
                },
                "authors": [
                    {
                        "name": "Lucile Favero"
                    },
                    {
                        "name": "Juan Antonio Pérez-Ortiz"
                    },
                    {
                        "name": "Tanja Käser"
                    },
                    {
                        "name": "Nuria Oliver"
                    }
                ],
                "author_detail": {
                    "name": "Nuria Oliver"
                },
                "author": "Nuria Oliver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05468v1",
                "updated": "2024-09-09T09:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    50,
                    13,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T09:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    50,
                    13,
                    0,
                    253,
                    0
                ],
                "title": "Modeling the Spatial Distributions of Macro Base Stations with\n  Homogeneous Density: Theory and Application to Real Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling the Spatial Distributions of Macro Base Stations with\n  Homogeneous Density: Theory and Application to Real Networks"
                },
                "summary": "Stochastic geometry is a highly studied field in telecommunications as in\nmany other scientific fields. In the last ten years in particular, theoretical\nknowledge has evolved a lot, whether for the calculation of metrics to\ncharacterize interference, coverage, energy or spectral efficiency, or exposure\nto electromagnetic fields. Many spatial point process models have been\ndeveloped but are often left aside because of their unfamiliarity, their lack\nof tractability in favor of the Poisson point process or the regular lattice,\neasier to use. This article is intended to be a short guide presenting a\ncomplete and simple methodology to follow to infer a real stationary macro\nantenna network using tractable spatial models. The focus is mainly on\nrepulsive point processes and in particular on determinantal point processes\nwhich are among the most tractable repulsive point processes. This methodology\nis applied on Belgian and French cell towers. The results show that for all\nstationary distributions in France and Belgium, the best inference model is the\n$\\beta$-Ginibre point process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic geometry is a highly studied field in telecommunications as in\nmany other scientific fields. In the last ten years in particular, theoretical\nknowledge has evolved a lot, whether for the calculation of metrics to\ncharacterize interference, coverage, energy or spectral efficiency, or exposure\nto electromagnetic fields. Many spatial point process models have been\ndeveloped but are often left aside because of their unfamiliarity, their lack\nof tractability in favor of the Poisson point process or the regular lattice,\neasier to use. This article is intended to be a short guide presenting a\ncomplete and simple methodology to follow to infer a real stationary macro\nantenna network using tractable spatial models. The focus is mainly on\nrepulsive point processes and in particular on determinantal point processes\nwhich are among the most tractable repulsive point processes. This methodology\nis applied on Belgian and French cell towers. The results show that for all\nstationary distributions in France and Belgium, the best inference model is the\n$\\beta$-Ginibre point process."
                },
                "authors": [
                    {
                        "name": "Q. Gontier"
                    },
                    {
                        "name": "C. Tsigros"
                    },
                    {
                        "name": "F. Horlin"
                    },
                    {
                        "name": "J. Wiart"
                    },
                    {
                        "name": "C. Oestges"
                    },
                    {
                        "name": "P. De Doncker"
                    }
                ],
                "author_detail": {
                    "name": "P. De Doncker"
                },
                "author": "P. De Doncker",
                "arxiv_comment": "EURO-COST CA20120 TD(22)0174",
                "arxiv_journal_ref": "\"Modeling the spatial distributions of macro base stations with\n  homogeneous density: Theory and application to real networks,\" in Proc. of\n  the European Cooperation in Science and Technology (EURO-COST CA20120 TD (22)\n  0174), Feb. 2022",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15419v2",
                "updated": "2024-09-09T09:46:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    46,
                    4,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-27T21:50:11Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    21,
                    50,
                    11,
                    1,
                    240,
                    0
                ],
                "title": "Bayesian Inference General Procedures for A Single-subject Test Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Inference General Procedures for A Single-subject Test Study"
                },
                "summary": "Abnormality detection in the identification of a single-subject which\ndeviates from the majority of the dataset that comes from a control group is a\ncritical problem. A common approach is to assume that the control group can be\ncharacterised in terms of standard Normal statistics and the detection of\nsingle abnormal subject is in that context. But in many situations the control\ngroup can not be described in terms of Gaussian statistics and the use of\nstandard statistics is inappropriate. This paper presents a Bayesian Inference\nGeneral Procedures for A Single-Subject Test (BIGPAST), designed to mitigate\nthe effects of skewness under the assumption that the dataset of control group\ncomes from the skewed Student's \\( t \\) distribution. BIGPAST operates under\nthe null hypothesis that the single-subject follows the same distribution as\nthe control group. We assess BIGPAST's performance against other methods\nthrough a series of simulation studies. The results demonstrate that BIGPAST is\nrobust against deviations from normality and outperforms the existing\napproaches in terms of accuracy. This is because BIGPAST can effectively reduce\nmodel misspecification errors under the skewed Student's \\( t \\) assumption. We\napply BIGPAST to a MEG dataset consisting of an individual with mild traumatic\nbrain injury and an age and gender-matched control group, demonstrating its\neffectiveness in detecting abnormalities in the single-subject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abnormality detection in the identification of a single-subject which\ndeviates from the majority of the dataset that comes from a control group is a\ncritical problem. A common approach is to assume that the control group can be\ncharacterised in terms of standard Normal statistics and the detection of\nsingle abnormal subject is in that context. But in many situations the control\ngroup can not be described in terms of Gaussian statistics and the use of\nstandard statistics is inappropriate. This paper presents a Bayesian Inference\nGeneral Procedures for A Single-Subject Test (BIGPAST), designed to mitigate\nthe effects of skewness under the assumption that the dataset of control group\ncomes from the skewed Student's \\( t \\) distribution. BIGPAST operates under\nthe null hypothesis that the single-subject follows the same distribution as\nthe control group. We assess BIGPAST's performance against other methods\nthrough a series of simulation studies. The results demonstrate that BIGPAST is\nrobust against deviations from normality and outperforms the existing\napproaches in terms of accuracy. This is because BIGPAST can effectively reduce\nmodel misspecification errors under the skewed Student's \\( t \\) assumption. We\napply BIGPAST to a MEG dataset consisting of an individual with mild traumatic\nbrain injury and an age and gender-matched control group, demonstrating its\neffectiveness in detecting abnormalities in the single-subject."
                },
                "authors": [
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Gary Green"
                    },
                    {
                        "name": "Sarah J. A. Carr"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "arxiv_comment": "35 pages, 12 figures and 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05462v1",
                "updated": "2024-09-09T09:42:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    42,
                    46,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T09:42:46Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    42,
                    46,
                    0,
                    253,
                    0
                ],
                "title": "Federated Transfer Learning Based Cooperative Wideband Spectrum Sensing\n  with Model Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Transfer Learning Based Cooperative Wideband Spectrum Sensing\n  with Model Pruning"
                },
                "summary": "For ultra-wideband and high-rate wireless communication systems, wideband\nspectrum sensing (WSS) is critical, since it empowers secondary users (SUs) to\ncapture the spectrum holes for opportunistic transmission. However, WSS\nencounters challenges such as excessive costs of hardware and computation due\nto the high sampling rate, as well as robustness issues arising from scenario\nmismatch. In this paper, a WSS neural network (WSSNet) is proposed by\nexploiting multicoset preprocessing to enable the sub-Nyquist sampling, with\nthe two dimensional convolution design specifically tailored to work with the\npreprocessed samples. A federated transfer learning (FTL) based framework\nmobilizing multiple SUs is further developed to achieve a robust model\nadaptable to various scenarios, which is paved by the selective weight pruning\nfor the fast model adaptation and inference. Simulation results demonstrate\nthat the proposed FTL-WSSNet achieves the fairly good performance in different\ntarget scenarios even without local adaptation samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For ultra-wideband and high-rate wireless communication systems, wideband\nspectrum sensing (WSS) is critical, since it empowers secondary users (SUs) to\ncapture the spectrum holes for opportunistic transmission. However, WSS\nencounters challenges such as excessive costs of hardware and computation due\nto the high sampling rate, as well as robustness issues arising from scenario\nmismatch. In this paper, a WSS neural network (WSSNet) is proposed by\nexploiting multicoset preprocessing to enable the sub-Nyquist sampling, with\nthe two dimensional convolution design specifically tailored to work with the\npreprocessed samples. A federated transfer learning (FTL) based framework\nmobilizing multiple SUs is further developed to achieve a robust model\nadaptable to various scenarios, which is paved by the selective weight pruning\nfor the fast model adaptation and inference. Simulation results demonstrate\nthat the proposed FTL-WSSNet achieves the fairly good performance in different\ntarget scenarios even without local adaptation samples."
                },
                "authors": [
                    {
                        "name": "Jibin Jia"
                    },
                    {
                        "name": "Peihao Dong"
                    },
                    {
                        "name": "Fuhui Zhou"
                    },
                    {
                        "name": "Qihui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qihui Wu"
                },
                "author": "Qihui Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05459v1",
                "updated": "2024-09-09T09:39:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    39,
                    47,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T09:39:47Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    39,
                    47,
                    0,
                    253,
                    0
                ],
                "title": "Beyond Flatland: A Geometric Take on Matching Methods for Treatment\n  Effect Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Flatland: A Geometric Take on Matching Methods for Treatment\n  Effect Estimation"
                },
                "summary": "Matching is a popular approach in causal inference to estimate treatment\neffects by pairing treated and control units that are most similar in terms of\ntheir covariate information. However, classic matching methods completely\nignore the geometry of the data manifold, which is crucial to define a\nmeaningful distance for matching, and struggle when covariates are noisy and\nhigh-dimensional. In this work, we propose GeoMatching, a matching method to\nestimate treatment effects that takes into account the intrinsic data geometry\ninduced by existing causal mechanisms among the confounding variables. First,\nwe learn a low-dimensional, latent Riemannian manifold that accounts for\nuncertainty and geometry of the original input data. Second, we estimate\ntreatment effects via matching in the latent space based on the learned latent\nRiemannian metric. We provide theoretical insights and empirical results in\nsynthetic and real-world scenarios, demonstrating that GeoMatching yields more\neffective treatment effect estimators, even as we increase input\ndimensionality, in the presence of outliers, or in semi-supervised scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matching is a popular approach in causal inference to estimate treatment\neffects by pairing treated and control units that are most similar in terms of\ntheir covariate information. However, classic matching methods completely\nignore the geometry of the data manifold, which is crucial to define a\nmeaningful distance for matching, and struggle when covariates are noisy and\nhigh-dimensional. In this work, we propose GeoMatching, a matching method to\nestimate treatment effects that takes into account the intrinsic data geometry\ninduced by existing causal mechanisms among the confounding variables. First,\nwe learn a low-dimensional, latent Riemannian manifold that accounts for\nuncertainty and geometry of the original input data. Second, we estimate\ntreatment effects via matching in the latent space based on the learned latent\nRiemannian metric. We provide theoretical insights and empirical results in\nsynthetic and real-world scenarios, demonstrating that GeoMatching yields more\neffective treatment effect estimators, even as we increase input\ndimensionality, in the presence of outliers, or in semi-supervised scenarios."
                },
                "authors": [
                    {
                        "name": "Melanie F. Pradier"
                    },
                    {
                        "name": "Javier González"
                    }
                ],
                "author_detail": {
                    "name": "Javier González"
                },
                "author": "Javier González",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02795v3",
                "updated": "2024-09-09T09:31:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    31,
                    30,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-04T15:11:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    11,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Helan Hu"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Houfeng Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05448v1",
                "updated": "2024-09-09T09:04:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    4,
                    56,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T09:04:56Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    4,
                    56,
                    0,
                    253,
                    0
                ],
                "title": "Representational Analysis of Binding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representational Analysis of Binding in Large Language Models"
                },
                "summary": "Entity tracking is essential for complex reasoning. To perform in-context\nentity tracking, language models (LMs) must bind an entity to its attribute\n(e.g., bind a container to its content) to recall attribute for a given entity.\nFor example, given a context mentioning ``The coffee is in Box Z, the stone is\nin Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later,\nLMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs,\nFeng and Steinhardt (2023) introduce a Binding ID mechanism and state that LMs\nuse a abstract concept called Binding ID (BI) to internally mark\nentity-attribute pairs. However, they have not directly captured the BI\ndeterminant information from entity activations. In this work, we provide a\nnovel view of the Binding ID mechanism by localizing the prototype of BI\ninformation. Specifically, we discover that there exists a low-rank subspace in\nthe hidden state (or activation) of LMs, that primarily encodes the order of\nentity and attribute and which is used as the prototype of BI to causally\ndetermine the binding. To identify this subspace, we choose principle component\nanalysis as our first attempt and it is empirically proven to be effective.\nMoreover, we also discover that when editing representations along directions\nin the subspace, LMs tend to bind a given entity to other attributes\naccordingly. For example, by patching activations along the BI encoding\ndirection we can make the LM to infer ``Box Z contains the stone'' and ``Box Z\ncontains the map''.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity tracking is essential for complex reasoning. To perform in-context\nentity tracking, language models (LMs) must bind an entity to its attribute\n(e.g., bind a container to its content) to recall attribute for a given entity.\nFor example, given a context mentioning ``The coffee is in Box Z, the stone is\nin Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later,\nLMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs,\nFeng and Steinhardt (2023) introduce a Binding ID mechanism and state that LMs\nuse a abstract concept called Binding ID (BI) to internally mark\nentity-attribute pairs. However, they have not directly captured the BI\ndeterminant information from entity activations. In this work, we provide a\nnovel view of the Binding ID mechanism by localizing the prototype of BI\ninformation. Specifically, we discover that there exists a low-rank subspace in\nthe hidden state (or activation) of LMs, that primarily encodes the order of\nentity and attribute and which is used as the prototype of BI to causally\ndetermine the binding. To identify this subspace, we choose principle component\nanalysis as our first attempt and it is empirically proven to be effective.\nMoreover, we also discover that when editing representations along directions\nin the subspace, LMs tend to bind a given entity to other attributes\naccordingly. For example, by patching activations along the BI encoding\ndirection we can make the LM to infer ``Box Z contains the stone'' and ``Box Z\ncontains the map''."
                },
                "authors": [
                    {
                        "name": "Qin Dai"
                    },
                    {
                        "name": "Benjamin Heinzerling"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.13610v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.13610v4",
                "updated": "2024-09-09T08:58:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    58,
                    17,
                    0,
                    253,
                    0
                ],
                "published": "2022-11-24T13:53:15Z",
                "published_parsed": [
                    2022,
                    11,
                    24,
                    13,
                    53,
                    15,
                    3,
                    328,
                    0
                ],
                "title": "Cross-Sectional Dynamics Under Network Structure: Theory and\n  Macroeconomic Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Sectional Dynamics Under Network Structure: Theory and\n  Macroeconomic Applications"
                },
                "summary": "Many environments in economics feature a cross-section of units linked by\nbilateral ties. I develop a framework for studying dynamics of cross-sectional\nvariables that exploits this network structure. The Network-VAR (NVAR) is a\nvector autoregression in which innovations transmit cross-sectionally via\nbilateral links and which can accommodate rich patterns of how network effects\nof higher order accumulate over time. It can be used to estimate dynamic\nnetwork effects, with the network given or inferred from dynamic\ncross-correlations in the data. It also offers a dimensionality-reduction\ntechnique for modeling high-dimensional (cross-sectional) processes, owing to\nnetworks' ability to summarize complex relations among variables (units) by\nrelatively few bilateral links. In a first application, consistent with an RBC\neconomy with lagged input-output conversion, I estimate how sectoral\nproductivity shocks transmit along supply chains and affect sectoral prices in\nthe US economy. In a second application, I forecast monthly industrial\nproduction growth across 44 countries by assuming and estimating a network\nunderlying the dynamics. This reduces out-of-sample mean squared errors by up\nto 23% relative to a factor model, consistent with an equivalence result I\nderive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many environments in economics feature a cross-section of units linked by\nbilateral ties. I develop a framework for studying dynamics of cross-sectional\nvariables that exploits this network structure. The Network-VAR (NVAR) is a\nvector autoregression in which innovations transmit cross-sectionally via\nbilateral links and which can accommodate rich patterns of how network effects\nof higher order accumulate over time. It can be used to estimate dynamic\nnetwork effects, with the network given or inferred from dynamic\ncross-correlations in the data. It also offers a dimensionality-reduction\ntechnique for modeling high-dimensional (cross-sectional) processes, owing to\nnetworks' ability to summarize complex relations among variables (units) by\nrelatively few bilateral links. In a first application, consistent with an RBC\neconomy with lagged input-output conversion, I estimate how sectoral\nproductivity shocks transmit along supply chains and affect sectoral prices in\nthe US economy. In a second application, I forecast monthly industrial\nproduction growth across 44 countries by assuming and estimating a network\nunderlying the dynamics. This reduces out-of-sample mean squared errors by up\nto 23% relative to a factor model, consistent with an equivalence result I\nderive."
                },
                "authors": [
                    {
                        "name": "Marko Mlikota"
                    }
                ],
                "author_detail": {
                    "name": "Marko Mlikota"
                },
                "author": "Marko Mlikota",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.13610v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.13610v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07601v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07601v4",
                "updated": "2024-09-09T08:56:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    56,
                    31,
                    0,
                    253,
                    0
                ],
                "published": "2023-11-11T02:13:32Z",
                "published_parsed": [
                    2023,
                    11,
                    11,
                    2,
                    13,
                    32,
                    5,
                    315,
                    0
                ],
                "title": "Online Advertisements with LLMs: Opportunities and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Advertisements with LLMs: Opportunities and Challenges"
                },
                "summary": "This paper explores the potential for leveraging Large Language Models (LLM)\nin the realm of online advertising systems. We introduce a general framework\nfor LLM advertisement, consisting of modification, bidding, prediction, and\nauction modules. Different design considerations for each module are presented.\nThese design choices are evaluated and discussed based on essential desiderata\nrequired to maintain a sustainable system. Further fundamental questions\nregarding practicality, efficiency, and implementation challenges are raised\nfor future research. Finally, we exposit how recent approaches on mechanism\ndesign for LLM can be framed in our unified perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential for leveraging Large Language Models (LLM)\nin the realm of online advertising systems. We introduce a general framework\nfor LLM advertisement, consisting of modification, bidding, prediction, and\nauction modules. Different design considerations for each module are presented.\nThese design choices are evaluated and discussed based on essential desiderata\nrequired to maintain a sustainable system. Further fundamental questions\nregarding practicality, efficiency, and implementation challenges are raised\nfor future research. Finally, we exposit how recent approaches on mechanism\ndesign for LLM can be framed in our unified perspective."
                },
                "authors": [
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "MohammadTaghi Hajiaghayi"
                    },
                    {
                        "name": "Keivan Rezaei"
                    },
                    {
                        "name": "Suho Shin"
                    }
                ],
                "author_detail": {
                    "name": "Suho Shin"
                },
                "author": "Suho Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07601v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07601v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04102v2",
                "updated": "2024-09-09T08:55:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    55,
                    22,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-06T08:08:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    8,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "Intelligent tutoring systems by Bayesian nets with noisy gates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent tutoring systems by Bayesian nets with noisy gates"
                },
                "summary": "Directed graphical models such as Bayesian nets are often used to implement\nintelligent tutoring systems able to interact in real-time with learners in a\npurely automatic way. When coping with such models, keeping a bound on the\nnumber of parameters might be important for multiple reasons. First, as these\nmodels are typically based on expert knowledge, a huge number of parameters to\nelicit might discourage practitioners from adopting them. Moreover, the number\nof model parameters affects the complexity of the inferences, while a fast\ncomputation of the queries is needed for real-time feedback. We advocate\nlogical gates with uncertainty for a compact parametrization of the conditional\nprobability tables in the underlying Bayesian net used by tutoring systems. We\ndiscuss the semantics of the model parameters to elicit and the assumptions\nrequired to apply such approach in this domain. We also derive a dedicated\ninference scheme to speed up computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed graphical models such as Bayesian nets are often used to implement\nintelligent tutoring systems able to interact in real-time with learners in a\npurely automatic way. When coping with such models, keeping a bound on the\nnumber of parameters might be important for multiple reasons. First, as these\nmodels are typically based on expert knowledge, a huge number of parameters to\nelicit might discourage practitioners from adopting them. Moreover, the number\nof model parameters affects the complexity of the inferences, while a fast\ncomputation of the queries is needed for real-time feedback. We advocate\nlogical gates with uncertainty for a compact parametrization of the conditional\nprobability tables in the underlying Bayesian net used by tutoring systems. We\ndiscuss the semantics of the model parameters to elicit and the assumptions\nrequired to apply such approach in this domain. We also derive a dedicated\ninference scheme to speed up computations."
                },
                "authors": [
                    {
                        "name": "Alessandro Antonucci"
                    },
                    {
                        "name": "Francesca Mangili"
                    },
                    {
                        "name": "Claudio Bonesana"
                    },
                    {
                        "name": "Giorgia Adorni"
                    }
                ],
                "author_detail": {
                    "name": "Giorgia Adorni"
                },
                "author": "Giorgia Adorni",
                "arxiv_doi": "10.32473/flairs.v35i.130692",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.32473/flairs.v35i.130692",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.04102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The International FLAIRS Conference Proceedings 35 (2022)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05554v2",
                "updated": "2024-09-09T08:39:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    39,
                    53,
                    0,
                    253,
                    0
                ],
                "published": "2024-04-08T14:22:12Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    14,
                    22,
                    12,
                    0,
                    99,
                    0
                ],
                "title": "Maximum likelihood estimation in the ergodic Volterra Ornstein-Uhlenbeck\n  process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum likelihood estimation in the ergodic Volterra Ornstein-Uhlenbeck\n  process"
                },
                "summary": "We study statistical inference of the drift parameters for the Volterra\nOrnstein-Uhlenbeck process on R in the ergodic regime. For continuous-time\nobservations, we derive the corresponding maximum likelihood estimators and\nshow that they are strongly consistent and asymptotically normal locally\nuniformly in the parameters. For the case of discrete high-frequency\nobservations, we prove similar results by discretization of the continuous-time\nmaximum likelihood estimator. Finally, for discrete low-frequency observations,\nwe show that the method of moments is consistent. Our proofs are crucially\nbased on the law of large numbers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study statistical inference of the drift parameters for the Volterra\nOrnstein-Uhlenbeck process on R in the ergodic regime. For continuous-time\nobservations, we derive the corresponding maximum likelihood estimators and\nshow that they are strongly consistent and asymptotically normal locally\nuniformly in the parameters. For the case of discrete high-frequency\nobservations, we prove similar results by discretization of the continuous-time\nmaximum likelihood estimator. Finally, for discrete low-frequency observations,\nwe show that the method of moments is consistent. Our proofs are crucially\nbased on the law of large numbers."
                },
                "authors": [
                    {
                        "name": "Mohamed Ben Alaya"
                    },
                    {
                        "name": "Martin Friesen"
                    },
                    {
                        "name": "Jonas Kremer"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Kremer"
                },
                "author": "Jonas Kremer",
                "arxiv_comment": "The old version has been split into two separate articles. This\n  identifier contains part one on the Volterra Ornstein-Uhlenbeck process,\n  while the other one focuses on the Volterra Cox-Ingersoll-Ross process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05422v1",
                "updated": "2024-09-09T08:23:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    23,
                    13,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T08:23:13Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    23,
                    13,
                    0,
                    253,
                    0
                ],
                "title": "Density and speed of sound of (iodobenzene + n-alkane) liquid mixtures\n  at $T$ = (288.15 to 308.15) K. Application of the Prigogine-Flory-Patterson\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Density and speed of sound of (iodobenzene + n-alkane) liquid mixtures\n  at $T$ = (288.15 to 308.15) K. Application of the Prigogine-Flory-Patterson\n  model"
                },
                "summary": "(Iodobenzene + n-alkane) liquid mixtures have been studied experimentally, in\nterms of densities and speeds of sound at a pressure $p$ = 0.1 MPa and in the\ntemperature range $T$ = (288.15 to 308.15) K, and theoretically, by the\napplication of the Prigogine-Flory-Patterson (PFP) model. The n-alkanes\nconsidered are n-heptane, n-decane, n-dodecane, and n-tetradecane. Excess molar\nvolumes ($V_{\\text{m}}^{\\text{E}}$) and excess isentropic compressibilities\n($\\kappa_S^{\\text{E}}$) have been calculated and correlated by Redlich-Kister\npolynomials. ${(\\partial{V_{\\text{m}}^{\\text{E}}}/\\partial T)}_p$ curves at the\nsame (p,T) conditions have been obtained from correlated\n$V_{\\text{m}}^{\\text{E}}$ values. From these experimental results and the\nknowledge of the excess molar enthalpies and volumes of mixtures containing\nfluorobenzene, chlorobenzene or bromobenzene with n-alkanes, we have inferred:\n(i) the presence of structural effects, especially important for the n-heptane\nmixture and less relevant for volumetric properties as the length of the\nn-alkane increases; and (ii) that the interactional effects on\n$V_{\\text{m}}^{\\text{E}}$ do not vary appreciably with the length of the\nn-alkane, so the observed $V_{\\text{m}}^{\\text{E}}$ variation is fundamentally\ndetermined by the corresponding variation of the contribution from structural\neffects. The application of the PFP model supports this interpretation,\nproviding free volume contributions to $V_{\\text{m}}^{\\text{E}}$ that vary\nparallelly to $V_{\\text{m}}^{\\text{E}}$ with the length of the n-alkane, and\ninteractional contributions that rest approximately constant independently of\nthe n-alkane size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(Iodobenzene + n-alkane) liquid mixtures have been studied experimentally, in\nterms of densities and speeds of sound at a pressure $p$ = 0.1 MPa and in the\ntemperature range $T$ = (288.15 to 308.15) K, and theoretically, by the\napplication of the Prigogine-Flory-Patterson (PFP) model. The n-alkanes\nconsidered are n-heptane, n-decane, n-dodecane, and n-tetradecane. Excess molar\nvolumes ($V_{\\text{m}}^{\\text{E}}$) and excess isentropic compressibilities\n($\\kappa_S^{\\text{E}}$) have been calculated and correlated by Redlich-Kister\npolynomials. ${(\\partial{V_{\\text{m}}^{\\text{E}}}/\\partial T)}_p$ curves at the\nsame (p,T) conditions have been obtained from correlated\n$V_{\\text{m}}^{\\text{E}}$ values. From these experimental results and the\nknowledge of the excess molar enthalpies and volumes of mixtures containing\nfluorobenzene, chlorobenzene or bromobenzene with n-alkanes, we have inferred:\n(i) the presence of structural effects, especially important for the n-heptane\nmixture and less relevant for volumetric properties as the length of the\nn-alkane increases; and (ii) that the interactional effects on\n$V_{\\text{m}}^{\\text{E}}$ do not vary appreciably with the length of the\nn-alkane, so the observed $V_{\\text{m}}^{\\text{E}}$ variation is fundamentally\ndetermined by the corresponding variation of the contribution from structural\neffects. The application of the PFP model supports this interpretation,\nproviding free volume contributions to $V_{\\text{m}}^{\\text{E}}$ that vary\nparallelly to $V_{\\text{m}}^{\\text{E}}$ with the length of the n-alkane, and\ninteractional contributions that rest approximately constant independently of\nthe n-alkane size."
                },
                "authors": [
                    {
                        "name": "Fernando Hevia"
                    },
                    {
                        "name": "Daniel Lozano-Martín"
                    },
                    {
                        "name": "Juan Antonio González"
                    },
                    {
                        "name": "Luis Felipe Sanz"
                    },
                    {
                        "name": "Isaías García de la Fuente"
                    },
                    {
                        "name": "José Carlos Cobos"
                    }
                ],
                "author_detail": {
                    "name": "José Carlos Cobos"
                },
                "author": "José Carlos Cobos",
                "arxiv_doi": "10.1016/j.fluid.2023.114017",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fluid.2023.114017",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.05422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Fluid Phase Equilib. 578 (2024) 114017",
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05966v2",
                "updated": "2024-09-09T08:21:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    21,
                    13,
                    0,
                    253,
                    0
                ],
                "published": "2024-05-09T17:59:32Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    17,
                    59,
                    32,
                    3,
                    130,
                    0
                ],
                "title": "Natural Language Processing RELIES on Linguistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing RELIES on Linguistics"
                },
                "summary": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage."
                },
                "authors": [
                    {
                        "name": "Juri Opitz"
                    },
                    {
                        "name": "Shira Wein"
                    },
                    {
                        "name": "Nathan Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Schneider"
                },
                "author": "Nathan Schneider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18154v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18154v3",
                "updated": "2024-09-09T08:19:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    19,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-06-26T08:12:34Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    8,
                    12,
                    34,
                    2,
                    178,
                    0
                ],
                "title": "Errors-In-Variables Model Fitting for Partially Unpaired Data Utilizing\n  Mixture Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Errors-In-Variables Model Fitting for Partially Unpaired Data Utilizing\n  Mixture Models"
                },
                "summary": "We introduce a general framework for regression in the errors-in-variables\nregime, allowing for full flexibility about the dimensionality of the data,\nobservational error probability density types, the (nonlinear) model type and\nthe avoidance of ad-hoc definitions of loss functions. In this framework, we\nintroduce model fitting for partially unpaired data, i.e. for given data groups\nthe pairing information of input and output is lost (semi-supervised). This is\nachieved by constructing mixture model densities, which directly model the loss\nof pairing information allowing inference. In a numerical simulation study\nlinear and nonlinear model fits are illustrated as well as a real data study is\npresented based on life expectancy data from the world bank utilizing a\nmultiple linear regression model. These results show that high quality model\nfitting is possible with partially unpaired data, which opens the possibility\nfor new applications with unfortunate or deliberate loss of pairing information\nin data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a general framework for regression in the errors-in-variables\nregime, allowing for full flexibility about the dimensionality of the data,\nobservational error probability density types, the (nonlinear) model type and\nthe avoidance of ad-hoc definitions of loss functions. In this framework, we\nintroduce model fitting for partially unpaired data, i.e. for given data groups\nthe pairing information of input and output is lost (semi-supervised). This is\nachieved by constructing mixture model densities, which directly model the loss\nof pairing information allowing inference. In a numerical simulation study\nlinear and nonlinear model fits are illustrated as well as a real data study is\npresented based on life expectancy data from the world bank utilizing a\nmultiple linear regression model. These results show that high quality model\nfitting is possible with partially unpaired data, which opens the possibility\nfor new applications with unfortunate or deliberate loss of pairing information\nin data."
                },
                "authors": [
                    {
                        "name": "Wolfgang Hoegele"
                    },
                    {
                        "name": "Sarah Brockhaus"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Brockhaus"
                },
                "author": "Sarah Brockhaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18154v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18154v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05414v1",
                "updated": "2024-09-09T08:16:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    16,
                    17,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T08:16:17Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    16,
                    17,
                    0,
                    253,
                    0
                ],
                "title": "CipherDM: Secure Three-Party Inference for Diffusion Model Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CipherDM: Secure Three-Party Inference for Diffusion Model Sampling"
                },
                "summary": "Diffusion Models (DMs) achieve state-of-the-art synthesis results in image\ngeneration and have been applied to various fields. However, DMs sometimes\nseriously violate user privacy during usage, making the protection of privacy\nan urgent issue. Using traditional privacy computing schemes like Secure\nMulti-Party Computation (MPC) directly in DMs faces significant computation and\ncommunication challenges. To address these issues, we propose CipherDM, the\nfirst novel, versatile and universal framework applying MPC technology to DMs\nfor secure sampling, which can be widely implemented on multiple DM based\ntasks. We thoroughly analyze sampling latency breakdown, find time-consuming\nparts and design corresponding secure MPC protocols for computing nonlinear\nactivations including SoftMax, SiLU and Mish. CipherDM is evaluated on popular\narchitectures (DDPM, DDIM) using MNIST dataset and on SD deployed by diffusers.\nCompared to direct implementation on SPU, our approach improves running time by\napproximately 1.084\\times \\sim 2.328\\times, and reduces communication costs by\napproximately 1.212\\times \\sim 1.791\\times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models (DMs) achieve state-of-the-art synthesis results in image\ngeneration and have been applied to various fields. However, DMs sometimes\nseriously violate user privacy during usage, making the protection of privacy\nan urgent issue. Using traditional privacy computing schemes like Secure\nMulti-Party Computation (MPC) directly in DMs faces significant computation and\ncommunication challenges. To address these issues, we propose CipherDM, the\nfirst novel, versatile and universal framework applying MPC technology to DMs\nfor secure sampling, which can be widely implemented on multiple DM based\ntasks. We thoroughly analyze sampling latency breakdown, find time-consuming\nparts and design corresponding secure MPC protocols for computing nonlinear\nactivations including SoftMax, SiLU and Mish. CipherDM is evaluated on popular\narchitectures (DDPM, DDIM) using MNIST dataset and on SD deployed by diffusers.\nCompared to direct implementation on SPU, our approach improves running time by\napproximately 1.084\\times \\sim 2.328\\times, and reduces communication costs by\napproximately 1.212\\times \\sim 1.791\\times."
                },
                "authors": [
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Xudong Chen"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Tingyu Fan"
                    },
                    {
                        "name": "Zhendong Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Zhao"
                },
                "author": "Zhendong Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00523v2",
                "updated": "2024-09-09T08:09:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    9,
                    14,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-01T12:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    12,
                    54,
                    46,
                    3,
                    214,
                    0
                ],
                "title": "Jailbreaking Text-to-Image Models with LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Text-to-Image Models with LLM-Based Agents"
                },
                "summary": "Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving their potential for addressing generative AI safety tasks\nlargely unexplored. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework targeting generative AI models, specifically focusing on\njailbreak attacks against text-to-image (T2I) models with built-in safety\nfilters. Atlas consists of two agents, namely the mutation agent and the\nselection agent, each comprising four key modules: a vision-language model\n(VLM) or LLM brain, planning, memory, and tool usage. The mutation agent uses\nits VLM brain to determine whether a prompt triggers the T2I model's safety\nfilter. It then collaborates iteratively with the LLM brain of the selection\nagent to generate new candidate jailbreak prompts with the highest potential to\nbypass the filter. In addition to multi-agent communication, we leverage\nin-context learning (ICL) memory mechanisms and the chain-of-thought (COT)\napproach to learn from past successes and failures, thereby enhancing Atlas's\nperformance. Our evaluation demonstrates that Atlas successfully jailbreaks\nseveral state-of-the-art T2I models equipped with multi-modal safety filters in\na black-box setting. Additionally, Atlas outperforms existing methods in both\nquery efficiency and the quality of generated images. This work convincingly\ndemonstrates the successful application of LLM-based agents in studying the\nsafety vulnerabilities of popular text-to-image generation models. We urge the\ncommunity to consider advanced techniques like ours in response to the rapidly\nevolving text-to-image generation field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving their potential for addressing generative AI safety tasks\nlargely unexplored. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework targeting generative AI models, specifically focusing on\njailbreak attacks against text-to-image (T2I) models with built-in safety\nfilters. Atlas consists of two agents, namely the mutation agent and the\nselection agent, each comprising four key modules: a vision-language model\n(VLM) or LLM brain, planning, memory, and tool usage. The mutation agent uses\nits VLM brain to determine whether a prompt triggers the T2I model's safety\nfilter. It then collaborates iteratively with the LLM brain of the selection\nagent to generate new candidate jailbreak prompts with the highest potential to\nbypass the filter. In addition to multi-agent communication, we leverage\nin-context learning (ICL) memory mechanisms and the chain-of-thought (COT)\napproach to learn from past successes and failures, thereby enhancing Atlas's\nperformance. Our evaluation demonstrates that Atlas successfully jailbreaks\nseveral state-of-the-art T2I models equipped with multi-modal safety filters in\na black-box setting. Additionally, Atlas outperforms existing methods in both\nquery efficiency and the quality of generated images. This work convincingly\ndemonstrates the successful application of LLM-based agents in studying the\nsafety vulnerabilities of popular text-to-image generation models. We urge the\ncommunity to consider advanced techniques like ours in response to the rapidly\nevolving text-to-image generation field."
                },
                "authors": [
                    {
                        "name": "Yingkai Dong"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Xiangtao Meng"
                    },
                    {
                        "name": "Ning Yu"
                    },
                    {
                        "name": "Shanqing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Shanqing Guo"
                },
                "author": "Shanqing Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05407v1",
                "updated": "2024-09-09T08:08:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    8,
                    5,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T08:08:05Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    8,
                    5,
                    0,
                    253,
                    0
                ],
                "title": "KRONC: Keypoint-based Robust Camera Optimization for 3D Car\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KRONC: Keypoint-based Robust Camera Optimization for 3D Car\n  Reconstruction"
                },
                "summary": "The three-dimensional representation of objects or scenes starting from a set\nof images has been a widely discussed topic for years and has gained additional\nattention after the diffusion of NeRF-based approaches. However, an\nunderestimated prerequisite is the knowledge of camera poses or, more\nspecifically, the estimation of the extrinsic calibration parameters. Although\nexcellent general-purpose Structure-from-Motion methods are available as a\npre-processing step, their computational load is high and they require a lot of\nframes to guarantee sufficient overlapping among the views. This paper\nintroduces KRONC, a novel approach aimed at inferring view poses by leveraging\nprior knowledge about the object to reconstruct and its representation through\nsemantic keypoints. With a focus on vehicle scenes, KRONC is able to estimate\nthe position of the views as a solution to a light optimization problem\ntargeting the convergence of keypoints' back-projections to a singular point.\nTo validate the method, a specific dataset of real-world car scenes has been\ncollected. Experiments confirm KRONC's ability to generate excellent estimates\nof camera poses starting from very coarse initialization. Results are\ncomparable with Structure-from-Motion methods with huge savings in computation.\nCode and data will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The three-dimensional representation of objects or scenes starting from a set\nof images has been a widely discussed topic for years and has gained additional\nattention after the diffusion of NeRF-based approaches. However, an\nunderestimated prerequisite is the knowledge of camera poses or, more\nspecifically, the estimation of the extrinsic calibration parameters. Although\nexcellent general-purpose Structure-from-Motion methods are available as a\npre-processing step, their computational load is high and they require a lot of\nframes to guarantee sufficient overlapping among the views. This paper\nintroduces KRONC, a novel approach aimed at inferring view poses by leveraging\nprior knowledge about the object to reconstruct and its representation through\nsemantic keypoints. With a focus on vehicle scenes, KRONC is able to estimate\nthe position of the views as a solution to a light optimization problem\ntargeting the convergence of keypoints' back-projections to a singular point.\nTo validate the method, a specific dataset of real-world car scenes has been\ncollected. Experiments confirm KRONC's ability to generate excellent estimates\nof camera poses starting from very coarse initialization. Results are\ncomparable with Structure-from-Motion methods with huge savings in computation.\nCode and data will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Davide Di Nucci"
                    },
                    {
                        "name": "Alessandro Simoni"
                    },
                    {
                        "name": "Matteo Tomei"
                    },
                    {
                        "name": "Luca Ciuffreda"
                    },
                    {
                        "name": "Roberto Vezzani"
                    },
                    {
                        "name": "Rita Cucchiara"
                    }
                ],
                "author_detail": {
                    "name": "Rita Cucchiara"
                },
                "author": "Rita Cucchiara",
                "arxiv_comment": "Accepted at ECCVW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15910v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15910v5",
                "updated": "2024-09-09T08:07:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    7,
                    43,
                    0,
                    253,
                    0
                ],
                "published": "2023-12-26T07:04:39Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    7,
                    4,
                    39,
                    1,
                    360,
                    0
                ],
                "title": "Reinforcement Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Unlearning"
                },
                "summary": "Machine unlearning refers to the process of mitigating the influence of\nspecific training data on machine learning models based on removal requests\nfrom data owners. However, one important area that has been largely overlooked\nin the research of unlearning is reinforcement learning. Reinforcement learning\nfocuses on training an agent to make optimal decisions within an environment to\nmaximize its cumulative rewards. During the training, the agent tends to\nmemorize the features of the environment, which raises a significant concern\nabout privacy. As per data protection regulations, the owner of the environment\nholds the right to revoke access to the agent's training data, thus\nnecessitating the development of a novel and pressing research field, known as\n\\emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking\nentire environments rather than individual data samples. This unique\ncharacteristic presents three distinct challenges: 1) how to propose unlearning\nschemes for environments; 2) how to avoid degrading the agent's performance in\nremaining environments; and 3) how to evaluate the effectiveness of unlearning.\nTo tackle these challenges, we propose two reinforcement unlearning methods.\nThe first method is based on decremental reinforcement learning, which aims to\nerase the agent's previously acquired knowledge gradually. The second method\nleverages environment poisoning attacks, which encourage the agent to learn\nnew, albeit incorrect, knowledge to remove the unlearning environment.\nParticularly, to tackle the third challenge, we introduce the concept of\n``environment inference attack'' to evaluate the unlearning outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning refers to the process of mitigating the influence of\nspecific training data on machine learning models based on removal requests\nfrom data owners. However, one important area that has been largely overlooked\nin the research of unlearning is reinforcement learning. Reinforcement learning\nfocuses on training an agent to make optimal decisions within an environment to\nmaximize its cumulative rewards. During the training, the agent tends to\nmemorize the features of the environment, which raises a significant concern\nabout privacy. As per data protection regulations, the owner of the environment\nholds the right to revoke access to the agent's training data, thus\nnecessitating the development of a novel and pressing research field, known as\n\\emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking\nentire environments rather than individual data samples. This unique\ncharacteristic presents three distinct challenges: 1) how to propose unlearning\nschemes for environments; 2) how to avoid degrading the agent's performance in\nremaining environments; and 3) how to evaluate the effectiveness of unlearning.\nTo tackle these challenges, we propose two reinforcement unlearning methods.\nThe first method is based on decremental reinforcement learning, which aims to\nerase the agent's previously acquired knowledge gradually. The second method\nleverages environment poisoning attacks, which encourage the agent to learn\nnew, albeit incorrect, knowledge to remove the unlearning environment.\nParticularly, to tackle the third challenge, we introduce the concept of\n``environment inference attack'' to evaluate the unlearning outcomes."
                },
                "authors": [
                    {
                        "name": "Dayong Ye"
                    },
                    {
                        "name": "Tianqing Zhu"
                    },
                    {
                        "name": "Congcong Zhu"
                    },
                    {
                        "name": "Derui Wang"
                    },
                    {
                        "name": "Kun Gao"
                    },
                    {
                        "name": "Zewei Shi"
                    },
                    {
                        "name": "Sheng Shen"
                    },
                    {
                        "name": "Wanlei Zhou"
                    },
                    {
                        "name": "Minhui Xue"
                    }
                ],
                "author_detail": {
                    "name": "Minhui Xue"
                },
                "author": "Minhui Xue",
                "arxiv_comment": "Accepted by NDSS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15910v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15910v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05404v1",
                "updated": "2024-09-09T08:05:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    5,
                    43,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T08:05:43Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    5,
                    43,
                    0,
                    253,
                    0
                ],
                "title": "DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid\n  Interconnects"
                },
                "summary": "Emerging interconnects, such as CXL and NVLink, have been integrated into the\nintra-host topology to scale more accelerators and facilitate efficient\ncommunication between them, such as GPUs. To keep pace with the accelerator's\ngrowing computing throughput, the interconnect has seen substantial enhancement\nin link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet\nand InfiniBand network links by an order of magnitude or more. Consequently,\nwhen data-intensive jobs, such as LLM training, scale across multiple hosts\nbeyond the reach limit of the interconnect, the performance is significantly\nhindered by the limiting bandwidth of the network infrastructure. We address\nthe problem by proposing DFabric, a two-tier interconnect architecture. We\naddress the problem by proposing DFabric, a two-tier interconnect architecture.\nFirst, DFabric disaggregates rack's computing units with an interconnect\nfabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy\nintra-rack efficient interconnecting. Second, DFabric disaggregates NICs from\nhosts, and consolidates them to form a NIC pool with CXL fabric. By providing\nsufficient aggregated capacity comparable to interconnect bandwidth, the NIC\npool bridges efficient communication across racks or beyond the reach limit of\ninterconnect fabric. However, the local memory accessing becomes the bottleneck\nwhen enabling each host to utilize the NIC pool efficiently. To the end,\nDFabric builds a memory pool with sufficient bandwidth by disaggregating host\nlocal memory and adding more memory devices. We have implemented a prototype of\nDFabric that can run applications transparently. We validated its performance\ngain by running various microbenchmarks and compute-intensive applications such\nas DNN and graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging interconnects, such as CXL and NVLink, have been integrated into the\nintra-host topology to scale more accelerators and facilitate efficient\ncommunication between them, such as GPUs. To keep pace with the accelerator's\ngrowing computing throughput, the interconnect has seen substantial enhancement\nin link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet\nand InfiniBand network links by an order of magnitude or more. Consequently,\nwhen data-intensive jobs, such as LLM training, scale across multiple hosts\nbeyond the reach limit of the interconnect, the performance is significantly\nhindered by the limiting bandwidth of the network infrastructure. We address\nthe problem by proposing DFabric, a two-tier interconnect architecture. We\naddress the problem by proposing DFabric, a two-tier interconnect architecture.\nFirst, DFabric disaggregates rack's computing units with an interconnect\nfabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy\nintra-rack efficient interconnecting. Second, DFabric disaggregates NICs from\nhosts, and consolidates them to form a NIC pool with CXL fabric. By providing\nsufficient aggregated capacity comparable to interconnect bandwidth, the NIC\npool bridges efficient communication across racks or beyond the reach limit of\ninterconnect fabric. However, the local memory accessing becomes the bottleneck\nwhen enabling each host to utilize the NIC pool efficiently. To the end,\nDFabric builds a memory pool with sufficient bandwidth by disaggregating host\nlocal memory and adding more memory devices. We have implemented a prototype of\nDFabric that can run applications transparently. We validated its performance\ngain by running various microbenchmarks and compute-intensive applications such\nas DNN and graph."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Yisong Chang"
                    },
                    {
                        "name": "Hui Yuan"
                    },
                    {
                        "name": "Xiaolong Zheng"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Mingyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Chen"
                },
                "author": "Mingyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05399v1",
                "updated": "2024-09-09T07:55:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    55,
                    59,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T07:55:59Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    55,
                    59,
                    0,
                    253,
                    0
                ],
                "title": "Sequential Posterior Sampling with Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Posterior Sampling with Diffusion Models"
                },
                "summary": "Diffusion models have quickly risen in popularity for their ability to model\ncomplex distributions and perform effective posterior sampling. Unfortunately,\nthe iterative nature of these generative models makes them computationally\nexpensive and unsuitable for real-time sequential inverse problems such as\nultrasound imaging. Considering the strong temporal structure across sequences\nof frames, we propose a novel approach that models the transition dynamics to\nimprove the efficiency of sequential diffusion posterior sampling in\nconditional image synthesis. Through modeling sequence data using a video\nvision transformer (ViViT) transition model based on previous diffusion\noutputs, we can initialize the reverse diffusion trajectory at a lower noise\nscale, greatly reducing the number of iterations required for convergence. We\ndemonstrate the effectiveness of our approach on a real-world dataset of high\nframe rate cardiac ultrasound images and show that it achieves the same\nperformance as a full diffusion trajectory while accelerating inference\n25$\\times$, enabling real-time posterior sampling. Furthermore, we show that\nthe addition of a transition model improves the PSNR up to 8\\% in cases with\nsevere motion. Our method opens up new possibilities for real-time applications\nof diffusion models in imaging and other domains requiring real-time inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have quickly risen in popularity for their ability to model\ncomplex distributions and perform effective posterior sampling. Unfortunately,\nthe iterative nature of these generative models makes them computationally\nexpensive and unsuitable for real-time sequential inverse problems such as\nultrasound imaging. Considering the strong temporal structure across sequences\nof frames, we propose a novel approach that models the transition dynamics to\nimprove the efficiency of sequential diffusion posterior sampling in\nconditional image synthesis. Through modeling sequence data using a video\nvision transformer (ViViT) transition model based on previous diffusion\noutputs, we can initialize the reverse diffusion trajectory at a lower noise\nscale, greatly reducing the number of iterations required for convergence. We\ndemonstrate the effectiveness of our approach on a real-world dataset of high\nframe rate cardiac ultrasound images and show that it achieves the same\nperformance as a full diffusion trajectory while accelerating inference\n25$\\times$, enabling real-time posterior sampling. Furthermore, we show that\nthe addition of a transition model improves the PSNR up to 8\\% in cases with\nsevere motion. Our method opens up new possibilities for real-time applications\nof diffusion models in imaging and other domains requiring real-time inference."
                },
                "authors": [
                    {
                        "name": "Tristan S. W. Stevens"
                    },
                    {
                        "name": "Oisín Nolan"
                    },
                    {
                        "name": "Jean-Luc Robert"
                    },
                    {
                        "name": "Ruud J. G. van Sloun"
                    }
                ],
                "author_detail": {
                    "name": "Ruud J. G. van Sloun"
                },
                "author": "Ruud J. G. van Sloun",
                "arxiv_comment": "5 pages, 4 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05385v2",
                "updated": "2024-09-10T06:11:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    11,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T07:32:30Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    32,
                    30,
                    0,
                    253,
                    0
                ],
                "title": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models"
                },
                "summary": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability."
                },
                "authors": [
                    {
                        "name": "Hong Xingyun Hong"
                    },
                    {
                        "name": "Shao Yan Shao"
                    },
                    {
                        "name": "Wang Zhilin Wang"
                    },
                    {
                        "name": "Duan Manni Duan"
                    },
                    {
                        "name": "Jin Xiongnan"
                    }
                ],
                "author_detail": {
                    "name": "Jin Xiongnan"
                },
                "author": "Jin Xiongnan",
                "arxiv_comment": "This paper has been accepted by NLPCC-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03488v3",
                "updated": "2024-09-09T07:31:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    31,
                    36,
                    0,
                    253,
                    0
                ],
                "published": "2024-06-05T17:50:03Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    50,
                    3,
                    2,
                    157,
                    0
                ],
                "title": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large\n  Language Model Training"
                },
                "summary": "The emergence of large language models (LLMs) relies heavily on distributed\ntraining strategies, among which pipeline parallelism plays a crucial role. As\nLLMs' training sequence length extends to 32k or even 128k, the current\npipeline parallel methods face severe bottlenecks, including high memory\nfootprints and substantial pipeline bubbles, greatly hindering model\nscalability and training throughput. To enhance memory efficiency and training\nthroughput, in this work, we introduce an efficient sequence-level\none-forward-one-backward (1F1B) pipeline scheduling method tailored for\ntraining LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level\nschedulable units into finer sequence-level units, reducing bubble size and\nmemory footprint. Considering that Seq1F1B may produce slight extra bubbles if\nsequences are split evenly, we design a computation-wise strategy to partition\ninput sequences and mitigate this side effect. Compared to competitive pipeline\nbaseline methods such as Megatron 1F1B pipeline parallelism, our method\nachieves higher training throughput with less memory footprint. Notably,\nSeq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k\nusing 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable\nwith existing methods. Our source code is based on Megatron-LM, and now is\navaiable at: https://github.com/MayDomine/Seq1F1B.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) relies heavily on distributed\ntraining strategies, among which pipeline parallelism plays a crucial role. As\nLLMs' training sequence length extends to 32k or even 128k, the current\npipeline parallel methods face severe bottlenecks, including high memory\nfootprints and substantial pipeline bubbles, greatly hindering model\nscalability and training throughput. To enhance memory efficiency and training\nthroughput, in this work, we introduce an efficient sequence-level\none-forward-one-backward (1F1B) pipeline scheduling method tailored for\ntraining LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level\nschedulable units into finer sequence-level units, reducing bubble size and\nmemory footprint. Considering that Seq1F1B may produce slight extra bubbles if\nsequences are split evenly, we design a computation-wise strategy to partition\ninput sequences and mitigate this side effect. Compared to competitive pipeline\nbaseline methods such as Megatron 1F1B pipeline parallelism, our method\nachieves higher training throughput with less memory footprint. Notably,\nSeq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k\nusing 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable\nwith existing methods. Our source code is based on Megatron-LM, and now is\navaiable at: https://github.com/MayDomine/Seq1F1B.git."
                },
                "authors": [
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Xinrong Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "12 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07487v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07487v3",
                "updated": "2024-09-09T07:23:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    23,
                    36,
                    0,
                    253,
                    0
                ],
                "published": "2024-06-11T17:27:23Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    27,
                    23,
                    1,
                    163,
                    0
                ],
                "title": "GLAD: Towards Better Reconstruction with Global and Local Adaptive\n  Diffusion Models for Unsupervised Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLAD: Towards Better Reconstruction with Global and Local Adaptive\n  Diffusion Models for Unsupervised Anomaly Detection"
                },
                "summary": "Diffusion models have shown superior performance on unsupervised anomaly\ndetection tasks. Since trained with normal data only, diffusion models tend to\nreconstruct normal counterparts of test images with certain noises added.\nHowever, these methods treat all potential anomalies equally, which may cause\ntwo main problems. From the global perspective, the difficulty of\nreconstructing images with different anomalies is uneven. Therefore, instead of\nutilizing the same setting for all samples, we propose to predict a particular\ndenoising step for each sample by evaluating the difference between image\ncontents and the priors extracted from diffusion models. From the local\nperspective, reconstructing abnormal regions differs from normal areas even in\nthe same image. Theoretically, the diffusion model predicts a noise for each\nstep, typically following a standard Gaussian distribution. However, due to the\ndifference between the anomaly and its potential normal counterpart, the\npredicted noise in abnormal regions will inevitably deviate from the standard\nGaussian distribution. To this end, we propose introducing synthetic abnormal\nsamples in training to encourage the diffusion models to break through the\nlimitation of standard Gaussian distribution, and a spatial-adaptive feature\nfusion scheme is utilized during inference. With the above modifications, we\npropose a global and local adaptive diffusion model (abbreviated to GLAD) for\nunsupervised anomaly detection, which introduces appealing flexibility and\nachieves anomaly-free reconstruction while retaining as much normal information\nas possible. Extensive experiments are conducted on three commonly used anomaly\ndetection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board\ndataset (PCB-Bank) we integrated, showing the effectiveness of the proposed\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown superior performance on unsupervised anomaly\ndetection tasks. Since trained with normal data only, diffusion models tend to\nreconstruct normal counterparts of test images with certain noises added.\nHowever, these methods treat all potential anomalies equally, which may cause\ntwo main problems. From the global perspective, the difficulty of\nreconstructing images with different anomalies is uneven. Therefore, instead of\nutilizing the same setting for all samples, we propose to predict a particular\ndenoising step for each sample by evaluating the difference between image\ncontents and the priors extracted from diffusion models. From the local\nperspective, reconstructing abnormal regions differs from normal areas even in\nthe same image. Theoretically, the diffusion model predicts a noise for each\nstep, typically following a standard Gaussian distribution. However, due to the\ndifference between the anomaly and its potential normal counterpart, the\npredicted noise in abnormal regions will inevitably deviate from the standard\nGaussian distribution. To this end, we propose introducing synthetic abnormal\nsamples in training to encourage the diffusion models to break through the\nlimitation of standard Gaussian distribution, and a spatial-adaptive feature\nfusion scheme is utilized during inference. With the above modifications, we\npropose a global and local adaptive diffusion model (abbreviated to GLAD) for\nunsupervised anomaly detection, which introduces appealing flexibility and\nachieves anomaly-free reconstruction while retaining as much normal information\nas possible. Extensive experiments are conducted on three commonly used anomaly\ndetection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board\ndataset (PCB-Bank) we integrated, showing the effectiveness of the proposed\nmethod."
                },
                "authors": [
                    {
                        "name": "Hang Yao"
                    },
                    {
                        "name": "Ming Liu"
                    },
                    {
                        "name": "Haolin Wang"
                    },
                    {
                        "name": "Zhicun Yin"
                    },
                    {
                        "name": "Zifei Yan"
                    },
                    {
                        "name": "Xiaopeng Hong"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "arxiv_comment": "Accepted by ECCV 2024, code and models:\n  https://github.com/hyao1/GLAD. Due to the limitation \"The abstract field\n  cannot be longer than 1,920 characters\", the abstract here is shorter than\n  that in the PDF file",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07487v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07487v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08872v2",
                "updated": "2024-09-09T07:17:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    17,
                    31,
                    0,
                    253,
                    0
                ],
                "published": "2024-03-13T18:00:11Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    18,
                    0,
                    11,
                    2,
                    73,
                    0
                ],
                "title": "Galaxy Build-up in the first 1.5 Gyr of Cosmic History: Insights from\n  the Stellar Mass Function at $z\\sim4-9$ from JWST NIRCam Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy Build-up in the first 1.5 Gyr of Cosmic History: Insights from\n  the Stellar Mass Function at $z\\sim4-9$ from JWST NIRCam Observations"
                },
                "summary": "Combining the public JWST/NIRCam imaging programs CEERS, PRIMER and JADES,\nspanning a total area of $\\sim500\\,{\\rm arcmin}^2$, we obtain a sample of\n$>$30,000 galaxies at $z_{\\rm phot}\\sim4-9$ that allows us to perform a\ncomplete, rest-optical selected census of the galaxy population at $z>3$.\nComparing the stellar mass $M_*$ and the UV-slope $\\beta$ distributions between\nJWST- and HST-selected samples, we generally find very good agreement and no\nsignificant biases. Nevertheless, JWST enables us to probe a new population of\nUV-red galaxies that was missing from previous HST-based Lyman Break Galaxy\n(LBG) samples. We measure galaxy stellar mass functions (SMFs) at $z\\sim4-9$\ndown to limiting masses of $10^{7.5}-10^{8.5}\\,{\\rm M_\\odot}$, finding steep\nlow mass slopes over the entire redshift range, reaching values of\n$\\alpha\\approx-2$ at $z\\gtrsim6$. At the high-mass end, UV-red galaxies\ndominate at least out to $z\\sim6$. The implied redshift evolution of the SMF\nsuggests a rapid build-up of massive dust-obscured or quiescent galaxies from\n$z\\sim6$ to $z\\sim4$ as well as an enhanced efficiency of star formation\ntowards earlier times ($z\\gtrsim6$). Finally, we show that the galaxy mass\ndensity grows by a factor $\\sim20\\times$ from $z\\sim9$ to $z\\sim4$. Our results\nemphasize the importance of rest-frame optically-selected samples in inferring\naccurate distributions of physical properties and studying the mass build-up of\ngalaxies in the first 1.5 Gyr of cosmic history.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining the public JWST/NIRCam imaging programs CEERS, PRIMER and JADES,\nspanning a total area of $\\sim500\\,{\\rm arcmin}^2$, we obtain a sample of\n$>$30,000 galaxies at $z_{\\rm phot}\\sim4-9$ that allows us to perform a\ncomplete, rest-optical selected census of the galaxy population at $z>3$.\nComparing the stellar mass $M_*$ and the UV-slope $\\beta$ distributions between\nJWST- and HST-selected samples, we generally find very good agreement and no\nsignificant biases. Nevertheless, JWST enables us to probe a new population of\nUV-red galaxies that was missing from previous HST-based Lyman Break Galaxy\n(LBG) samples. We measure galaxy stellar mass functions (SMFs) at $z\\sim4-9$\ndown to limiting masses of $10^{7.5}-10^{8.5}\\,{\\rm M_\\odot}$, finding steep\nlow mass slopes over the entire redshift range, reaching values of\n$\\alpha\\approx-2$ at $z\\gtrsim6$. At the high-mass end, UV-red galaxies\ndominate at least out to $z\\sim6$. The implied redshift evolution of the SMF\nsuggests a rapid build-up of massive dust-obscured or quiescent galaxies from\n$z\\sim6$ to $z\\sim4$ as well as an enhanced efficiency of star formation\ntowards earlier times ($z\\gtrsim6$). Finally, we show that the galaxy mass\ndensity grows by a factor $\\sim20\\times$ from $z\\sim9$ to $z\\sim4$. Our results\nemphasize the importance of rest-frame optically-selected samples in inferring\naccurate distributions of physical properties and studying the mass build-up of\ngalaxies in the first 1.5 Gyr of cosmic history."
                },
                "authors": [
                    {
                        "name": "Andrea Weibel"
                    },
                    {
                        "name": "Pascal A. Oesch"
                    },
                    {
                        "name": "Laia Barrufet"
                    },
                    {
                        "name": "Rashmi Gottumukkala"
                    },
                    {
                        "name": "Richard S. Ellis"
                    },
                    {
                        "name": "Paola Santini"
                    },
                    {
                        "name": "John R. Weaver"
                    },
                    {
                        "name": "Natalie Allen"
                    },
                    {
                        "name": "Rychard Bouwens"
                    },
                    {
                        "name": "Rebecca A. A. Bowler"
                    },
                    {
                        "name": "Gabe Brammer"
                    },
                    {
                        "name": "Adam C. Carnall"
                    },
                    {
                        "name": "Fergus Cullen"
                    },
                    {
                        "name": "Pratika Dayal"
                    },
                    {
                        "name": "Callum T. Donnan"
                    },
                    {
                        "name": "James S. Dunlop"
                    },
                    {
                        "name": "Mauro Giavalisco"
                    },
                    {
                        "name": "Norman A. Grogin"
                    },
                    {
                        "name": "Garth D. Illingworth"
                    },
                    {
                        "name": "Anton M. Koekemoer"
                    },
                    {
                        "name": "Ivo Labbe"
                    },
                    {
                        "name": "Danilo Marchesini"
                    },
                    {
                        "name": "Derek J. McLeod"
                    },
                    {
                        "name": "Ross J. McLure"
                    },
                    {
                        "name": "Rohan P. Naidu"
                    },
                    {
                        "name": "Marko Shuntov"
                    },
                    {
                        "name": "Mauro Stefanon"
                    },
                    {
                        "name": "Sune Toft"
                    },
                    {
                        "name": "Mengyuan Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Xiao"
                },
                "author": "Mengyuan Xiao",
                "arxiv_doi": "10.1093/mnras/stae1891",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae1891",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.08872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "31 pages, 17 figures, published in MNRAS",
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society, Volume 533\n  (2024), Issue 2, pp.1808-1838",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04363v2",
                "updated": "2024-09-09T07:01:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    1,
                    56,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-05T09:06:47Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    9,
                    6,
                    47,
                    4,
                    187,
                    0
                ],
                "title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents"
                },
                "summary": "Advancements in the capabilities of Large Language Models (LLMs) have created\na promising foundation for developing autonomous agents. With the right tools,\nthese agents could learn to solve tasks in new environments by accumulating and\nupdating their knowledge. Current LLM-based agents process past experiences\nusing a full history of observations, summarization, retrieval augmentation.\nHowever, these unstructured memory representations do not facilitate the\nreasoning and planning essential for complex decision-making. In our study, we\nintroduce AriGraph, a novel method wherein the agent constructs and updates a\nmemory graph that integrates semantic and episodic memories while exploring the\nenvironment. We demonstrate that our Ariadne LLM agent, consisting of the\nproposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks within interactive text game environments\ndifficult even for human players. Results show that our approach markedly\noutperforms other established memory methods and strong RL baselines in a range\nof problems of varying complexity. Additionally, AriGraph demonstrates\ncompetitive performance compared to dedicated knowledge graph-based methods in\nstatic multi-hop question-answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in the capabilities of Large Language Models (LLMs) have created\na promising foundation for developing autonomous agents. With the right tools,\nthese agents could learn to solve tasks in new environments by accumulating and\nupdating their knowledge. Current LLM-based agents process past experiences\nusing a full history of observations, summarization, retrieval augmentation.\nHowever, these unstructured memory representations do not facilitate the\nreasoning and planning essential for complex decision-making. In our study, we\nintroduce AriGraph, a novel method wherein the agent constructs and updates a\nmemory graph that integrates semantic and episodic memories while exploring the\nenvironment. We demonstrate that our Ariadne LLM agent, consisting of the\nproposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks within interactive text game environments\ndifficult even for human players. Results show that our approach markedly\noutperforms other established memory methods and strong RL baselines in a range\nof problems of varying complexity. Additionally, AriGraph demonstrates\ncompetitive performance compared to dedicated knowledge graph-based methods in\nstatic multi-hop question-answering."
                },
                "authors": [
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Nikita Semenov"
                    },
                    {
                        "name": "Artyom Sorokin"
                    },
                    {
                        "name": "Dmitry Evseev"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Burnaev"
                },
                "author": "Evgeny Burnaev",
                "arxiv_comment": "Code for this work is avaliable at\n  https://github.com/AIRI-Institute/AriGraph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05370v1",
                "updated": "2024-09-09T06:57:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    57,
                    22,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T06:57:22Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    57,
                    22,
                    0,
                    253,
                    0
                ],
                "title": "KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using\n  Large Language Models"
                },
                "summary": "Harnessing the robust capabilities of Large Language Models (LLMs) for\nnarrative generation, logical reasoning, and common-sense knowledge\nintegration, this study delves into utilizing LLMs to enhance automated\nradiology report generation (R2Gen). Despite the wealth of knowledge within\nLLMs, efficiently triggering relevant knowledge within these large models for\nspecific tasks like R2Gen poses a critical research challenge. This paper\npresents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration\nframework based on LLMs. Utilizing a frozen LLM to generate reports, the\nframework integrates a knowledge graph to unlock chest disease-related\nknowledge within the LLM to enhance the clinical utility of generated reports.\nThis is achieved by leveraging the knowledge graph to distill disease-related\nfeatures in a designed way. Since a radiology report encompasses both normal\nand disease-related findings, the extracted graph-enhanced disease-related\nfeatures are integrated with regional image features, attending to both\naspects. We explore two fusion methods to automatically prioritize and select\nthe most relevant features. The fused features are employed by LLM to generate\nreports that are more sensitive to diseases and of improved quality. Our\napproach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the robust capabilities of Large Language Models (LLMs) for\nnarrative generation, logical reasoning, and common-sense knowledge\nintegration, this study delves into utilizing LLMs to enhance automated\nradiology report generation (R2Gen). Despite the wealth of knowledge within\nLLMs, efficiently triggering relevant knowledge within these large models for\nspecific tasks like R2Gen poses a critical research challenge. This paper\npresents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration\nframework based on LLMs. Utilizing a frozen LLM to generate reports, the\nframework integrates a knowledge graph to unlock chest disease-related\nknowledge within the LLM to enhance the clinical utility of generated reports.\nThis is achieved by leveraging the knowledge graph to distill disease-related\nfeatures in a designed way. Since a radiology report encompasses both normal\nand disease-related findings, the extracted graph-enhanced disease-related\nfeatures are integrated with regional image features, attending to both\naspects. We explore two fusion methods to automatically prioritize and select\nthe most relevant features. The fused features are employed by LLM to generate\nreports that are more sensitive to diseases and of improved quality. Our\napproach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets."
                },
                "authors": [
                    {
                        "name": "Yingshu Li"
                    },
                    {
                        "name": "Zhanyu Wang"
                    },
                    {
                        "name": "Yunyi Liu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Lingqiao Liu"
                    },
                    {
                        "name": "Luping Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Luping Zhou"
                },
                "author": "Luping Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05368v1",
                "updated": "2024-09-09T06:55:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    55,
                    38,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T06:55:38Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    55,
                    38,
                    0,
                    253,
                    0
                ],
                "title": "Application Specific Compression of Deep Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application Specific Compression of Deep Learning Models"
                },
                "summary": "Large Deep Learning models are compressed and deployed for specific\napplications. However, current Deep Learning model compression methods do not\nutilize the information about the target application. As a result, the\ncompressed models are application agnostic. Our goal is to customize the model\ncompression process to create a compressed model that will perform better for\nthe target application. Our method, Application Specific Compression (ASC),\nidentifies and prunes components of the large Deep Learning model that are\nredundant specifically for the given target application. The intuition of our\nwork is to prune the parts of the network that do not contribute significantly\nto updating the data representation for the given application. We have\nexperimented with the BERT family of models for three applications: Extractive\nQA, Natural Language Inference, and Paraphrase Identification. We observe that\ncustomized compressed models created using ASC method perform better than\nexisting model compression methods and off-the-shelf compressed models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Deep Learning models are compressed and deployed for specific\napplications. However, current Deep Learning model compression methods do not\nutilize the information about the target application. As a result, the\ncompressed models are application agnostic. Our goal is to customize the model\ncompression process to create a compressed model that will perform better for\nthe target application. Our method, Application Specific Compression (ASC),\nidentifies and prunes components of the large Deep Learning model that are\nredundant specifically for the given target application. The intuition of our\nwork is to prune the parts of the network that do not contribute significantly\nto updating the data representation for the given application. We have\nexperimented with the BERT family of models for three applications: Extractive\nQA, Natural Language Inference, and Paraphrase Identification. We observe that\ncustomized compressed models created using ASC method perform better than\nexisting model compression methods and off-the-shelf compressed models."
                },
                "authors": [
                    {
                        "name": "Rohit Raj Rai"
                    },
                    {
                        "name": "Angana Borah"
                    },
                    {
                        "name": "Amit Awekar"
                    }
                ],
                "author_detail": {
                    "name": "Amit Awekar"
                },
                "author": "Amit Awekar",
                "arxiv_comment": "Accepted in the Proceedings of the 8th Joint International Conference\n  on Data Science & Management of Data (12th ACM IKDD CODS and 30th COMAD) for\n  the Short Research Paper track, 5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05367v1",
                "updated": "2024-09-09T06:55:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    55,
                    37,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T06:55:37Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    55,
                    37,
                    0,
                    253,
                    0
                ],
                "title": "Diagnostic Reasoning in Natural Language: Computational Model and\n  Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Reasoning in Natural Language: Computational Model and\n  Application"
                },
                "summary": "Diagnostic reasoning is a key component of expert work in many domains. It is\na hard, time-consuming activity that requires expertise, and AI research has\ninvestigated the ways automated systems can support this process. Yet, due to\nthe complexity of natural language, the applications of AI for diagnostic\nreasoning to language-related tasks are lacking. To close this gap, we\ninvestigate diagnostic abductive reasoning (DAR) in the context of\nlanguage-grounded tasks (NL-DAR). We propose a novel modeling framework for\nNL-DAR based on Pearl's structural causal models and instantiate it in a\ncomprehensive study of scientific paper assessment in the biomedical domain. We\nuse the resulting dataset to investigate the human decision-making process in\nNL-DAR and determine the potential of LLMs to support structured\ndecision-making over text. Our framework, open resources and tools lay the\ngroundwork for the empirical study of collaborative diagnostic reasoning in the\nage of LLMs, in the scholarly domain and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic reasoning is a key component of expert work in many domains. It is\na hard, time-consuming activity that requires expertise, and AI research has\ninvestigated the ways automated systems can support this process. Yet, due to\nthe complexity of natural language, the applications of AI for diagnostic\nreasoning to language-related tasks are lacking. To close this gap, we\ninvestigate diagnostic abductive reasoning (DAR) in the context of\nlanguage-grounded tasks (NL-DAR). We propose a novel modeling framework for\nNL-DAR based on Pearl's structural causal models and instantiate it in a\ncomprehensive study of scientific paper assessment in the biomedical domain. We\nuse the resulting dataset to investigate the human decision-making process in\nNL-DAR and determine the potential of LLMs to support structured\ndecision-making over text. Our framework, open resources and tools lay the\ngroundwork for the empirical study of collaborative diagnostic reasoning in the\nage of LLMs, in the scholarly domain and beyond."
                },
                "authors": [
                    {
                        "name": "Nils Dycke"
                    },
                    {
                        "name": "Matej Zečević"
                    },
                    {
                        "name": "Ilia Kuznetsov"
                    },
                    {
                        "name": "Beatrix Suess"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14846v2",
                "updated": "2024-09-09T06:50:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    50,
                    18,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-27T07:57:58Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    7,
                    57,
                    58,
                    1,
                    240,
                    0
                ],
                "title": "Diffusion-Occ: 3D Point Cloud Completion via Occupancy Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-Occ: 3D Point Cloud Completion via Occupancy Diffusion"
                },
                "summary": "Point clouds are crucial for capturing three-dimensional data but often\nsuffer from incompleteness due to limitations such as resolution and occlusion.\nTraditional methods typically rely on point-based approaches within\ndiscriminative frameworks for point cloud completion. In this paper, we\nintroduce \\textbf{Diffusion-Occ}, a novel framework for Diffusion Point Cloud\nCompletion. Diffusion-Occ utilizes a two-stage coarse-to-fine approach. In the\nfirst stage, the Coarse Density Voxel Prediction Network (CDNet) processes\npartial points to predict coarse density voxels, streamlining global feature\nextraction through voxel classification, as opposed to previous\nregression-based methods. In the second stage, we introduce the Occupancy\nGeneration Network (OccGen), a conditional occupancy diffusion model based on a\ntransformer architecture and enhanced by our Point-Voxel Fuse (PVF) block. This\nblock integrates coarse density voxels with partial points to leverage both\nglobal and local features for comprehensive completion. By thresholding the\noccupancy field, we convert it into a complete point cloud. Additionally, our\nmethod employs diverse training mixtures and efficient diffusion\nparameterization to enable effective one-step sampling during both training and\ninference. Experimental results demonstrate that Diffusion-Occ outperforms\nexisting discriminative and generative methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point clouds are crucial for capturing three-dimensional data but often\nsuffer from incompleteness due to limitations such as resolution and occlusion.\nTraditional methods typically rely on point-based approaches within\ndiscriminative frameworks for point cloud completion. In this paper, we\nintroduce \\textbf{Diffusion-Occ}, a novel framework for Diffusion Point Cloud\nCompletion. Diffusion-Occ utilizes a two-stage coarse-to-fine approach. In the\nfirst stage, the Coarse Density Voxel Prediction Network (CDNet) processes\npartial points to predict coarse density voxels, streamlining global feature\nextraction through voxel classification, as opposed to previous\nregression-based methods. In the second stage, we introduce the Occupancy\nGeneration Network (OccGen), a conditional occupancy diffusion model based on a\ntransformer architecture and enhanced by our Point-Voxel Fuse (PVF) block. This\nblock integrates coarse density voxels with partial points to leverage both\nglobal and local features for comprehensive completion. By thresholding the\noccupancy field, we convert it into a complete point cloud. Additionally, our\nmethod employs diverse training mixtures and efficient diffusion\nparameterization to enable effective one-step sampling during both training and\ninference. Experimental results demonstrate that Diffusion-Occ outperforms\nexisting discriminative and generative methods."
                },
                "authors": [
                    {
                        "name": "Guoqing Zhang"
                    },
                    {
                        "name": "Jian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Liu"
                },
                "author": "Jian Liu",
                "arxiv_comment": "After a closer examination of our work, we've determined that our\n  experiments are not thorough and robust enough, possibly impacting the\n  accuracy of our conclusions. Hence, we've decided to withdraw our article\n  and, after refining our experiments, intend to resubmit the paper once\n  significant improvements have been made",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04392v3",
                "updated": "2024-09-09T06:25:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    25,
                    33,
                    0,
                    253,
                    0
                ],
                "published": "2024-04-05T20:31:45Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    20,
                    31,
                    45,
                    4,
                    96,
                    0
                ],
                "title": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes"
                },
                "summary": "Large Language Models (LLMs) have gained widespread adoption across various\ndomains, including chatbots and auto-task completion agents. However, these\nmodels are susceptible to safety vulnerabilities such as jailbreaking, prompt\ninjection, and privacy leakage attacks. These vulnerabilities can lead to the\ngeneration of malicious content, unauthorized actions, or the disclosure of\nconfidential information. While foundational LLMs undergo alignment training\nand incorporate safety measures, they are often subject to fine-tuning, or\ndoing quantization resource-constrained environments. This study investigates\nthe impact of these modifications on LLM safety, a critical consideration for\nbuilding reliable and secure AI systems. We evaluate foundational models\nincluding Mistral, Llama series, Qwen, and MosaicML, along with their\nfine-tuned variants. Our comprehensive analysis reveals that fine-tuning\ngenerally increases the success rates of jailbreak attacks, while quantization\nhas variable effects on attack success rates. Importantly, we find that\nproperly implemented guardrails significantly enhance resistance to jailbreak\nattempts. These findings contribute to our understanding of LLM vulnerabilities\nand provide insights for developing more robust safety strategies in the\ndeployment of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained widespread adoption across various\ndomains, including chatbots and auto-task completion agents. However, these\nmodels are susceptible to safety vulnerabilities such as jailbreaking, prompt\ninjection, and privacy leakage attacks. These vulnerabilities can lead to the\ngeneration of malicious content, unauthorized actions, or the disclosure of\nconfidential information. While foundational LLMs undergo alignment training\nand incorporate safety measures, they are often subject to fine-tuning, or\ndoing quantization resource-constrained environments. This study investigates\nthe impact of these modifications on LLM safety, a critical consideration for\nbuilding reliable and secure AI systems. We evaluate foundational models\nincluding Mistral, Llama series, Qwen, and MosaicML, along with their\nfine-tuned variants. Our comprehensive analysis reveals that fine-tuning\ngenerally increases the success rates of jailbreak attacks, while quantization\nhas variable effects on attack success rates. Importantly, we find that\nproperly implemented guardrails significantly enhance resistance to jailbreak\nattempts. These findings contribute to our understanding of LLM vulnerabilities\nand provide insights for developing more robust safety strategies in the\ndeployment of language models."
                },
                "authors": [
                    {
                        "name": "Divyanshu Kumar"
                    },
                    {
                        "name": "Anurakt Kumar"
                    },
                    {
                        "name": "Sahil Agarwal"
                    },
                    {
                        "name": "Prashanth Harshangi"
                    }
                ],
                "author_detail": {
                    "name": "Prashanth Harshangi"
                },
                "author": "Prashanth Harshangi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14482v2",
                "updated": "2024-09-09T06:19:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    19,
                    7,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-19T17:35:47Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    35,
                    47,
                    4,
                    201,
                    0
                ],
                "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities"
                },
                "summary": "In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and leading\nproprietary models (e.g., GPT-4-Turbo) in long-context understanding and\nretrieval-augmented generation (RAG) capabilities. These two capabilities are\nessential for LLMs to process large volumes of information that cannot fit into\na single prompt and are complementary to each other, depending on the\ndownstream tasks and computational budgets. We present a detailed continued\ntraining recipe to extend the context window of Llama3-70B-base from 8K to 128K\ntokens, along with a three-stage instruction tuning process to enhance the\nmodel's instruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\noutperforms most existing state-of-the-art models, including\nGPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on\nultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only\na 4K context window, showing the strong long context capability across varying\nsequence lengths. We further provide extensive comparisons between direct\nlong-context and RAG solutions using the same state-of-the-art long-context\nLLMs. Interestingly, we find that the performance of strong long-context LLMs\nusing RAG improves when retrieving a larger number of chunks. With a large set\nof top-k chunks, RAG consistently outperforms direct long-context solution\nusing the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B\nand Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To\nadvance research in this field, we open-sourced the model weights, training\ndata, and the evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and leading\nproprietary models (e.g., GPT-4-Turbo) in long-context understanding and\nretrieval-augmented generation (RAG) capabilities. These two capabilities are\nessential for LLMs to process large volumes of information that cannot fit into\na single prompt and are complementary to each other, depending on the\ndownstream tasks and computational budgets. We present a detailed continued\ntraining recipe to extend the context window of Llama3-70B-base from 8K to 128K\ntokens, along with a three-stage instruction tuning process to enhance the\nmodel's instruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\noutperforms most existing state-of-the-art models, including\nGPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on\nultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only\na 4K context window, showing the strong long context capability across varying\nsequence lengths. We further provide extensive comparisons between direct\nlong-context and RAG solutions using the same state-of-the-art long-context\nLLMs. Interestingly, we find that the performance of strong long-context LLMs\nusing RAG improves when retrieving a larger number of chunks. With a large set\nof top-k chunks, RAG consistently outperforms direct long-context solution\nusing the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B\nand Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To\nadvance research in this field, we open-sourced the model weights, training\ndata, and the evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/"
                },
                "authors": [
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Wei Ping"
                    },
                    {
                        "name": "Xianchao Wu"
                    },
                    {
                        "name": "Chejian Xu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "arxiv_comment": "v2: major update with significantly improved results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15186v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15186v3",
                "updated": "2024-09-09T06:17:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    17,
                    21,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-21T14:48:23Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    48,
                    23,
                    6,
                    203,
                    0
                ],
                "title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Employing Large Language Models for Text-to-SQL Tasks"
                },
                "summary": "The increasing volume of data stored in relational databases has led to the\nneed for efficient querying and utilization of this data in various sectors.\nHowever, writing SQL queries requires specialized knowledge, which poses a\nchallenge for non-professional users trying to access and query databases.\nText-to-SQL parsing solves this issue by converting natural language queries\ninto SQL queries, thus making database access more accessible for non-expert\nusers. To take advantage of the recent developments in Large Language Models\n(LLMs), a range of new methods have emerged, with a primary focus on prompt\nengineering and fine-tuning. This survey provides a comprehensive overview of\nLLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering,\nfine-tuning methods, and future research directions. We hope this review will\nenable readers to gain a broader understanding of the recent advances in this\nfield and offer some insights into its future trajectory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing volume of data stored in relational databases has led to the\nneed for efficient querying and utilization of this data in various sectors.\nHowever, writing SQL queries requires specialized knowledge, which poses a\nchallenge for non-professional users trying to access and query databases.\nText-to-SQL parsing solves this issue by converting natural language queries\ninto SQL queries, thus making database access more accessible for non-expert\nusers. To take advantage of the recent developments in Large Language Models\n(LLMs), a range of new methods have emerged, with a primary focus on prompt\nengineering and fine-tuning. This survey provides a comprehensive overview of\nLLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering,\nfine-tuning methods, and future research directions. We hope this review will\nenable readers to gain a broader understanding of the recent advances in this\nfield and offer some insights into its future trajectory."
                },
                "authors": [
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Zhengju Tang"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Xiaotong Zhang"
                    },
                    {
                        "name": "Zhi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yang"
                },
                "author": "Zhi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15186v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15186v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11802v2",
                "updated": "2024-09-09T06:17:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    17,
                    15,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-16T14:53:35Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    14,
                    53,
                    35,
                    1,
                    198,
                    0
                ],
                "title": "Invariant Causal Knowledge Distillation in Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariant Causal Knowledge Distillation in Neural Networks"
                },
                "summary": "Knowledge distillation (KD) involves transferring the knowledge from one\nneural network to another, often from a larger, well-trained model (teacher) to\na smaller, more efficient model (student). Traditional KD methods minimize the\nKullback-Leibler (KL) divergence between the probabilistic outputs of the\nteacher and student networks. However, this approach often overlooks crucial\nstructural knowledge embedded within the teacher's network. In this paper, we\nintroduce Invariant Consistency Distillation (ICD), a novel methodology\ndesigned to enhance KD by ensuring that the student model's representations are\nboth discriminative and invariant with respect to the teacher's outputs. Our\napproach is based on causal inference principles and combines contrastive\nlearning with an explicit invariance penalty, capturing significantly more\ninformation from the teacher's representation. ICD uses an efficient,\nparameter-free approach for flexible teacher-student alignment. We provide a\ntheoretical foundation for ICD and demonstrate its effectiveness through\nextensive experiments. Our results on CIFAR-100 and ImageNet ILSVRC-2012 show\nthat ICD outperforms traditional KD techniques and surpasses state-of-the-art\nmethods. In some cases, the student model even exceeds the teacher model in\nterms of accuracy. Furthermore, we successfully apply our method to other\ndatasets, such as Tiny ImageNet and STL-10, demonstrating superior\ncross-dataset generalization. Code is available at\nhttps://github.com/giakoumoglou/distillers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) involves transferring the knowledge from one\nneural network to another, often from a larger, well-trained model (teacher) to\na smaller, more efficient model (student). Traditional KD methods minimize the\nKullback-Leibler (KL) divergence between the probabilistic outputs of the\nteacher and student networks. However, this approach often overlooks crucial\nstructural knowledge embedded within the teacher's network. In this paper, we\nintroduce Invariant Consistency Distillation (ICD), a novel methodology\ndesigned to enhance KD by ensuring that the student model's representations are\nboth discriminative and invariant with respect to the teacher's outputs. Our\napproach is based on causal inference principles and combines contrastive\nlearning with an explicit invariance penalty, capturing significantly more\ninformation from the teacher's representation. ICD uses an efficient,\nparameter-free approach for flexible teacher-student alignment. We provide a\ntheoretical foundation for ICD and demonstrate its effectiveness through\nextensive experiments. Our results on CIFAR-100 and ImageNet ILSVRC-2012 show\nthat ICD outperforms traditional KD techniques and surpasses state-of-the-art\nmethods. In some cases, the student model even exceeds the teacher model in\nterms of accuracy. Furthermore, we successfully apply our method to other\ndatasets, such as Tiny ImageNet and STL-10, demonstrating superior\ncross-dataset generalization. Code is available at\nhttps://github.com/giakoumoglou/distillers."
                },
                "authors": [
                    {
                        "name": "Nikolaos Giakoumoglou"
                    },
                    {
                        "name": "Tania Stathaki"
                    }
                ],
                "author_detail": {
                    "name": "Tania Stathaki"
                },
                "author": "Tania Stathaki",
                "arxiv_comment": "8 pages, 2 figures, 4 tables. The paper's title has been changed to\n  better emphasize its theoretical foundation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05346v1",
                "updated": "2024-09-09T06:04:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    4,
                    41,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T06:04:41Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    4,
                    41,
                    0,
                    253,
                    0
                ],
                "title": "GDFlow: Anomaly Detection with NCDE-based Normalizing Flow for Advanced\n  Driver Assistance System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GDFlow: Anomaly Detection with NCDE-based Normalizing Flow for Advanced\n  Driver Assistance System"
                },
                "summary": "For electric vehicles, the Adaptive Cruise Control (ACC) in Advanced Driver\nAssistance Systems (ADAS) is designed to assist braking based on driving\nconditions, road inclines, predefined deceleration strengths, and user braking\npatterns. However, the driving data collected during the development of ADAS\nare generally limited and lack diversity. This deficiency leads to late or\naggressive braking for different users. Crucially, it is necessary to\neffectively identify anomalies, such as unexpected or inconsistent braking\npatterns in ADAS, especially given the challenge of working with unlabelled,\nlimited, and noisy datasets from real-world electric vehicles. In order to\ntackle the aforementioned challenges in ADAS, we propose Graph Neural\nControlled Differential Equation Normalizing Flow (GDFlow), a model that\nleverages Normalizing Flow (NF) with Neural Controlled Differential Equations\n(NCDE) to learn the distribution of normal driving patterns continuously.\nCompared to the traditional clustering or anomaly detection algorithms, our\napproach effectively captures the spatio-temporal information from different\nsensor data and more accurately models continuous changes in driving patterns.\nAdditionally, we introduce a quantile-based maximum likelihood objective to\nimprove the likelihood estimate of the normal data near the boundary of the\ndistribution, enhancing the model's ability to distinguish between normal and\nanomalous patterns. We validate GDFlow using real-world electric vehicle\ndriving data that we collected from Hyundai IONIQ5 and GV80EV, achieving\nstate-of-the-art performance compared to six baselines across four dataset\nconfigurations of different vehicle types and drivers. Furthermore, our model\noutperforms the latest anomaly detection methods across four time series\nbenchmark datasets. Our approach demonstrates superior efficiency in inference\ntime compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For electric vehicles, the Adaptive Cruise Control (ACC) in Advanced Driver\nAssistance Systems (ADAS) is designed to assist braking based on driving\nconditions, road inclines, predefined deceleration strengths, and user braking\npatterns. However, the driving data collected during the development of ADAS\nare generally limited and lack diversity. This deficiency leads to late or\naggressive braking for different users. Crucially, it is necessary to\neffectively identify anomalies, such as unexpected or inconsistent braking\npatterns in ADAS, especially given the challenge of working with unlabelled,\nlimited, and noisy datasets from real-world electric vehicles. In order to\ntackle the aforementioned challenges in ADAS, we propose Graph Neural\nControlled Differential Equation Normalizing Flow (GDFlow), a model that\nleverages Normalizing Flow (NF) with Neural Controlled Differential Equations\n(NCDE) to learn the distribution of normal driving patterns continuously.\nCompared to the traditional clustering or anomaly detection algorithms, our\napproach effectively captures the spatio-temporal information from different\nsensor data and more accurately models continuous changes in driving patterns.\nAdditionally, we introduce a quantile-based maximum likelihood objective to\nimprove the likelihood estimate of the normal data near the boundary of the\ndistribution, enhancing the model's ability to distinguish between normal and\nanomalous patterns. We validate GDFlow using real-world electric vehicle\ndriving data that we collected from Hyundai IONIQ5 and GV80EV, achieving\nstate-of-the-art performance compared to six baselines across four dataset\nconfigurations of different vehicle types and drivers. Furthermore, our model\noutperforms the latest anomaly detection methods across four time series\nbenchmark datasets. Our approach demonstrates superior efficiency in inference\ntime compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Kangjun Lee"
                    },
                    {
                        "name": "Minha Kim"
                    },
                    {
                        "name": "Youngho Jun"
                    },
                    {
                        "name": "Simon S. Woo"
                    }
                ],
                "author_detail": {
                    "name": "Simon S. Woo"
                },
                "author": "Simon S. Woo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05344v1",
                "updated": "2024-09-09T06:02:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    2,
                    17,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T06:02:17Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    2,
                    17,
                    0,
                    253,
                    0
                ],
                "title": "GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep\n  Reinforcement Learning"
                },
                "summary": "Robotic object packing has broad practical applications in the logistics and\nautomation industry, often formulated by researchers as the online 3D Bin\nPacking Problem (3D-BPP). However, existing DRL-based methods primarily focus\non enhancing performance in limited packing environments while neglecting the\nability to generalize across multiple environments characterized by different\nbin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin\nPacking approach via Transformer-based deep reinforcement learning (DRL).\nFirst, we design a Placement Generator module to yield finite subspaces as\nplacement candidates and the representation of the bin. Second, we propose a\nPacking Transformer, which fuses the features of the items and bin, to identify\nthe spatial correlation between the item to be packed and available sub-spaces\nwithin the bin. Coupling these two components enables GOPT's ability to perform\ninference on bins of varying dimensions. We conduct extensive experiments and\ndemonstrate that GOPT not only achieves superior performance against the\nbaselines, but also exhibits excellent generalization capabilities.\nFurthermore, the deployment with a robot showcases the practical applicability\nof our method in the real world. The source code will be publicly available at\nhttps://github.com/Xiong5Heng/GOPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic object packing has broad practical applications in the logistics and\nautomation industry, often formulated by researchers as the online 3D Bin\nPacking Problem (3D-BPP). However, existing DRL-based methods primarily focus\non enhancing performance in limited packing environments while neglecting the\nability to generalize across multiple environments characterized by different\nbin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin\nPacking approach via Transformer-based deep reinforcement learning (DRL).\nFirst, we design a Placement Generator module to yield finite subspaces as\nplacement candidates and the representation of the bin. Second, we propose a\nPacking Transformer, which fuses the features of the items and bin, to identify\nthe spatial correlation between the item to be packed and available sub-spaces\nwithin the bin. Coupling these two components enables GOPT's ability to perform\ninference on bins of varying dimensions. We conduct extensive experiments and\ndemonstrate that GOPT not only achieves superior performance against the\nbaselines, but also exhibits excellent generalization capabilities.\nFurthermore, the deployment with a robot showcases the practical applicability\nof our method in the real world. The source code will be publicly available at\nhttps://github.com/Xiong5Heng/GOPT."
                },
                "authors": [
                    {
                        "name": "Heng Xiong"
                    },
                    {
                        "name": "Changrong Guo"
                    },
                    {
                        "name": "Jian Peng"
                    },
                    {
                        "name": "Kai Ding"
                    },
                    {
                        "name": "Xuchong Qiu"
                    },
                    {
                        "name": "Long Bai"
                    },
                    {
                        "name": "Jianfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Xu"
                },
                "author": "Jianfeng Xu",
                "arxiv_comment": "8 pages, 6 figures. This paper has been accepted by IEEE Robotics and\n  Automation Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03420v2",
                "updated": "2024-09-09T05:36:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    5,
                    36,
                    27,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T11:09:00Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    11,
                    9,
                    0,
                    3,
                    249,
                    0
                ],
                "title": "mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page\n  Document Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page\n  Document Understanding"
                },
                "summary": "Multimodel Large Language Models(MLLMs) have achieved promising OCR-free\nDocument Understanding performance by increasing the supported resolution of\ndocument images. However, this comes at the cost of generating thousands of\nvisual tokens for a single document image, leading to excessive GPU memory and\nslower inference times, particularly in multi-page document comprehension. In\nthis work, to address these challenges, we propose a High-resolution\nDocCompressor module to compress each high-resolution document image into 324\ntokens, guided by low-resolution global visual features. With this compression\nmodule, to strengthen multi-page document comprehension ability and balance\nboth token efficiency and question-answering performance, we develop the\nDocOwl2 under a three-stage training framework: Single-image Pretraining,\nMulti-image Continue-pretraining, and Multi-task Finetuning. DocOwl2 sets a new\nstate-of-the-art across multi-page document understanding benchmarks and\nreduces first token latency by more than 50%, demonstrating advanced\ncapabilities in multi-page questioning answering, explanation with evidence\npages, and cross-page structure understanding. Additionally, compared to\nsingle-image MLLMs trained on similar data, our DocOwl2 achieves comparable\nsingle-page understanding performance with less than 20% of the visual tokens.\nOur codes, models, and data are publicly available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodel Large Language Models(MLLMs) have achieved promising OCR-free\nDocument Understanding performance by increasing the supported resolution of\ndocument images. However, this comes at the cost of generating thousands of\nvisual tokens for a single document image, leading to excessive GPU memory and\nslower inference times, particularly in multi-page document comprehension. In\nthis work, to address these challenges, we propose a High-resolution\nDocCompressor module to compress each high-resolution document image into 324\ntokens, guided by low-resolution global visual features. With this compression\nmodule, to strengthen multi-page document comprehension ability and balance\nboth token efficiency and question-answering performance, we develop the\nDocOwl2 under a three-stage training framework: Single-image Pretraining,\nMulti-image Continue-pretraining, and Multi-task Finetuning. DocOwl2 sets a new\nstate-of-the-art across multi-page document understanding benchmarks and\nreduces first token latency by more than 50%, demonstrating advanced\ncapabilities in multi-page questioning answering, explanation with evidence\npages, and cross-page structure understanding. Additionally, compared to\nsingle-image MLLMs trained on similar data, our DocOwl2 achieves comparable\nsingle-page understanding performance with less than 20% of the visual tokens.\nOur codes, models, and data are publicly available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2."
                },
                "authors": [
                    {
                        "name": "Anwen Hu"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Jiabo Ye"
                    },
                    {
                        "name": "Ming Yan"
                    },
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Qin Jin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05336v1",
                "updated": "2024-09-09T05:29:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    5,
                    29,
                    38,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T05:29:38Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    5,
                    29,
                    38,
                    0,
                    253,
                    0
                ],
                "title": "Early-exit Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-exit Convolutional Neural Networks"
                },
                "summary": "This paper is aimed at developing a method that reduces the computational\ncost of convolutional neural networks (CNN) during inference. Conventionally,\nthe input data pass through a fixed neural network architecture. However, easy\nexamples can be classified at early stages of processing and conventional\nnetworks do not take this into account. In this paper, we introduce 'Early-exit\nCNNs', EENets for short, which adapt their computational cost based on the\ninput by stopping the inference process at certain exit locations. In EENets,\nthere are a number of exit blocks each of which consists of a confidence branch\nand a softmax branch. The confidence branch computes the confidence score of\nexiting (i.e. stopping the inference process) at that location; while the\nsoftmax branch outputs a classification probability vector. Both branches are\nlearnable and their parameters are separate. During training of EENets, in\naddition to the classical classification loss, the computational cost of\ninference is taken into account as well. As a result, the network adapts its\nmany confidence branches to the inputs so that less computation is spent for\neasy examples. Inference works as in conventional feed-forward networks,\nhowever, when the output of a confidence branch is larger than a certain\nthreshold, the inference stops for that specific example. The idea of EENets is\napplicable to available CNN architectures such as ResNets. Through\ncomprehensive experiments on MNIST, SVHN, CIFAR10 and Tiny-ImageNet datasets,\nwe show that early-exit (EE) ResNets achieve similar accuracy with their non-EE\nversions while reducing the computational cost to 20% of the original. Code is\navailable at https://github.com/eksuas/eenets.pytorch",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is aimed at developing a method that reduces the computational\ncost of convolutional neural networks (CNN) during inference. Conventionally,\nthe input data pass through a fixed neural network architecture. However, easy\nexamples can be classified at early stages of processing and conventional\nnetworks do not take this into account. In this paper, we introduce 'Early-exit\nCNNs', EENets for short, which adapt their computational cost based on the\ninput by stopping the inference process at certain exit locations. In EENets,\nthere are a number of exit blocks each of which consists of a confidence branch\nand a softmax branch. The confidence branch computes the confidence score of\nexiting (i.e. stopping the inference process) at that location; while the\nsoftmax branch outputs a classification probability vector. Both branches are\nlearnable and their parameters are separate. During training of EENets, in\naddition to the classical classification loss, the computational cost of\ninference is taken into account as well. As a result, the network adapts its\nmany confidence branches to the inputs so that less computation is spent for\neasy examples. Inference works as in conventional feed-forward networks,\nhowever, when the output of a confidence branch is larger than a certain\nthreshold, the inference stops for that specific example. The idea of EENets is\napplicable to available CNN architectures such as ResNets. Through\ncomprehensive experiments on MNIST, SVHN, CIFAR10 and Tiny-ImageNet datasets,\nwe show that early-exit (EE) ResNets achieve similar accuracy with their non-EE\nversions while reducing the computational cost to 20% of the original. Code is\navailable at https://github.com/eksuas/eenets.pytorch"
                },
                "authors": [
                    {
                        "name": "Edanur Demir"
                    },
                    {
                        "name": "Emre Akbas"
                    }
                ],
                "author_detail": {
                    "name": "Emre Akbas"
                },
                "author": "Emre Akbas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01688v2",
                "updated": "2024-09-09T05:24:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    5,
                    24,
                    27,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-03T07:02:01Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    7,
                    2,
                    1,
                    5,
                    216,
                    0
                ],
                "title": "SiamMo: Siamese Motion-Centric 3D Object Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiamMo: Siamese Motion-Centric 3D Object Tracking"
                },
                "summary": "Current 3D single object tracking methods primarily rely on the Siamese\nmatching-based paradigm, which struggles with textureless and incomplete LiDAR\npoint clouds. Conversely, the motion-centric paradigm avoids appearance\nmatching, thus overcoming these issues. However, its complex multi-stage\npipeline and the limited temporal modeling capability of a single-stream\narchitecture constrain its potential. In this paper, we introduce SiamMo, a\nnovel and simple Siamese motion-centric tracking approach. Unlike the\ntraditional single-stream architecture, we employ Siamese feature extraction\nfor motion-centric tracking. This decouples feature extraction from temporal\nfusion, significantly enhancing tracking performance. Additionally, we design a\nSpatio-Temporal Feature Aggregation module to integrate Siamese features at\nmultiple scales, capturing motion information effectively. We also introduce a\nBox-aware Feature Encoding module to encode object size priors into motion\nestimation. SiamMo is a purely motion-centric tracker that eliminates the need\nfor additional processes like segmentation and box refinement. Without whistles\nand bells, SiamMo not only surpasses state-of-the-art methods across multiple\nbenchmarks but also demonstrates exceptional robustness in challenging\nscenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\\%\nprecision while maintaining a high inference speed of 108 FPS. The code will be\nreleased at https://github.com/HDU-VRLab/SiamMo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current 3D single object tracking methods primarily rely on the Siamese\nmatching-based paradigm, which struggles with textureless and incomplete LiDAR\npoint clouds. Conversely, the motion-centric paradigm avoids appearance\nmatching, thus overcoming these issues. However, its complex multi-stage\npipeline and the limited temporal modeling capability of a single-stream\narchitecture constrain its potential. In this paper, we introduce SiamMo, a\nnovel and simple Siamese motion-centric tracking approach. Unlike the\ntraditional single-stream architecture, we employ Siamese feature extraction\nfor motion-centric tracking. This decouples feature extraction from temporal\nfusion, significantly enhancing tracking performance. Additionally, we design a\nSpatio-Temporal Feature Aggregation module to integrate Siamese features at\nmultiple scales, capturing motion information effectively. We also introduce a\nBox-aware Feature Encoding module to encode object size priors into motion\nestimation. SiamMo is a purely motion-centric tracker that eliminates the need\nfor additional processes like segmentation and box refinement. Without whistles\nand bells, SiamMo not only surpasses state-of-the-art methods across multiple\nbenchmarks but also demonstrates exceptional robustness in challenging\nscenarios. SiamMo sets a new record on the KITTI tracking benchmark with 90.1\\%\nprecision while maintaining a high inference speed of 108 FPS. The code will be\nreleased at https://github.com/HDU-VRLab/SiamMo."
                },
                "authors": [
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yingqi Deng"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Hongjie Gu"
                    },
                    {
                        "name": "Zhekang Dong"
                    }
                ],
                "author_detail": {
                    "name": "Zhekang Dong"
                },
                "author": "Zhekang Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02647v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02647v3",
                "updated": "2024-09-09T05:19:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    5,
                    19,
                    48,
                    0,
                    253,
                    0
                ],
                "published": "2023-12-05T10:39:37Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    10,
                    39,
                    37,
                    1,
                    339,
                    0
                ],
                "title": "TPA3D: Triplane Attention for Fast Text-to-3D Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPA3D: Triplane Attention for Fast Text-to-3D Generation"
                },
                "summary": "Due to the lack of large-scale text-3D correspondence data, recent text-to-3D\ngeneration works mainly rely on utilizing 2D diffusion models for synthesizing\n3D data. Since diffusion-based methods typically require significant\noptimization time for both training and inference, the use of GAN-based models\nwould still be desirable for fast 3D generation. In this work, we propose\nTriplane Attention for text-guided 3D generation (TPA3D), an end-to-end\ntrainable GAN-based deep learning model for fast text-to-3D generation. With\nonly 3D shape data and their rendered 2D images observed during training, our\nTPA3D is designed to retrieve detailed visual descriptions for synthesizing the\ncorresponding 3D mesh data. This is achieved by the proposed attention\nmechanisms on the extracted sentence and word-level text features. In our\nexperiments, we show that TPA3D generates high-quality 3D textured shapes\naligned with fine-grained descriptions, while impressive computation efficiency\ncan be observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the lack of large-scale text-3D correspondence data, recent text-to-3D\ngeneration works mainly rely on utilizing 2D diffusion models for synthesizing\n3D data. Since diffusion-based methods typically require significant\noptimization time for both training and inference, the use of GAN-based models\nwould still be desirable for fast 3D generation. In this work, we propose\nTriplane Attention for text-guided 3D generation (TPA3D), an end-to-end\ntrainable GAN-based deep learning model for fast text-to-3D generation. With\nonly 3D shape data and their rendered 2D images observed during training, our\nTPA3D is designed to retrieve detailed visual descriptions for synthesizing the\ncorresponding 3D mesh data. This is achieved by the proposed attention\nmechanisms on the extracted sentence and word-level text features. In our\nexperiments, we show that TPA3D generates high-quality 3D textured shapes\naligned with fine-grained descriptions, while impressive computation efficiency\ncan be observed."
                },
                "authors": [
                    {
                        "name": "Bin-Shih Wu"
                    },
                    {
                        "name": "Hong-En Chen"
                    },
                    {
                        "name": "Sheng-Yu Huang"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Chiang Frank Wang"
                },
                "author": "Yu-Chiang Frank Wang",
                "arxiv_comment": "ECCV2024, Project Page: https://redxouls.github.io/TPA3D/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02647v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02647v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14795v3",
                "updated": "2024-09-09T04:38:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    4,
                    38,
                    16,
                    0,
                    253,
                    0
                ],
                "published": "2023-05-24T06:48:41Z",
                "published_parsed": [
                    2023,
                    5,
                    24,
                    6,
                    48,
                    41,
                    2,
                    144,
                    0
                ],
                "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop\n  Questions"
                },
                "summary": "The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark,\nMQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs\n(e.g., OpenAI GPT-3.5-turbo) and outperforms previous model editors by a large\nmargin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark,\nMQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs\n(e.g., OpenAI GPT-3.5-turbo) and outperforms previous model editors by a large\nmargin."
                },
                "authors": [
                    {
                        "name": "Zexuan Zhong"
                    },
                    {
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "name": "Christopher D. Manning"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "EMNLP 2023. Our code and datasets are available at\n  https://github.com/princeton-nlp/MQuAKE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05326v1",
                "updated": "2024-09-09T04:36:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    4,
                    36,
                    7,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T04:36:07Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    4,
                    36,
                    7,
                    0,
                    253,
                    0
                ],
                "title": "SCALE at Scale: Cosmological applications of small-scale CMB lensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCALE at Scale: Cosmological applications of small-scale CMB lensing"
                },
                "summary": "The Small-Correlated-Against-Large Estimator (SCALE) for small-scale lensing\nof the cosmic microwave background (CMB) provides a novel method for measuring\nthe amplitude of CMB lensing power without the need for reconstruction of the\nlensing field. In our previous study, we showed that the SCALE method can\noutperform existing reconstruction methods to detect the presence of lensing at\nsmall scales ($\\ell \\gg 3000$). Here we develop a procedure to include\ninformation from SCALE in cosmological parameter inference. We construct a\nprecise neural network emulator to quickly map cosmological parameters to\ndesired CMB observables such as temperature and lensing power spectra and SCALE\ncross spectra. We also outline a method to apply SCALE to full-sky maps of the\nCMB temperature field, and construct a likelihood for the application of SCALE\nin parameter estimation. SCALE supplements conventional observables such as the\nCMB power spectra and baryon acoustic oscillations in constraining parameters\nthat are sensitive to the small-scale lensing amplitude such as the neutrino\nmass $m_\\nu$. We show that including estimates of the small-scale lensing\namplitude from SCALE in such an analysis provides enough constraining\ninformation to measure the minimum neutrino mass at $4\\sigma$ significance in\nthe scenario of minimal mass, and higher significance for higher mass. Finally,\nwe show that SCALE will play a powerful role in constraining models of\nclustering that generate scale-dependent modulation to the distribution of\nmatter and the lensing power spectrum, as predicted by models of warm or fuzzy\ndark matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Small-Correlated-Against-Large Estimator (SCALE) for small-scale lensing\nof the cosmic microwave background (CMB) provides a novel method for measuring\nthe amplitude of CMB lensing power without the need for reconstruction of the\nlensing field. In our previous study, we showed that the SCALE method can\noutperform existing reconstruction methods to detect the presence of lensing at\nsmall scales ($\\ell \\gg 3000$). Here we develop a procedure to include\ninformation from SCALE in cosmological parameter inference. We construct a\nprecise neural network emulator to quickly map cosmological parameters to\ndesired CMB observables such as temperature and lensing power spectra and SCALE\ncross spectra. We also outline a method to apply SCALE to full-sky maps of the\nCMB temperature field, and construct a likelihood for the application of SCALE\nin parameter estimation. SCALE supplements conventional observables such as the\nCMB power spectra and baryon acoustic oscillations in constraining parameters\nthat are sensitive to the small-scale lensing amplitude such as the neutrino\nmass $m_\\nu$. We show that including estimates of the small-scale lensing\namplitude from SCALE in such an analysis provides enough constraining\ninformation to measure the minimum neutrino mass at $4\\sigma$ significance in\nthe scenario of minimal mass, and higher significance for higher mass. Finally,\nwe show that SCALE will play a powerful role in constraining models of\nclustering that generate scale-dependent modulation to the distribution of\nmatter and the lensing power spectrum, as predicted by models of warm or fuzzy\ndark matter."
                },
                "authors": [
                    {
                        "name": "Victor C. Chan"
                    },
                    {
                        "name": "Renée Hložek"
                    },
                    {
                        "name": "Joel Meyers"
                    },
                    {
                        "name": "Alexander van Engelen"
                    }
                ],
                "author_detail": {
                    "name": "Alexander van Engelen"
                },
                "author": "Alexander van Engelen",
                "arxiv_comment": "18 pages, 8 figures, 6 tables, submitting to Physical Review D in a\n  few days",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05325v1",
                "updated": "2024-09-09T04:36:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    4,
                    36,
                    6,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T04:36:06Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    4,
                    36,
                    6,
                    0,
                    253,
                    0
                ],
                "title": "Sample-Efficient Bayesian Optimization with Transfer Learning for\n  Heterogeneous Search Spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample-Efficient Bayesian Optimization with Transfer Learning for\n  Heterogeneous Search Spaces"
                },
                "summary": "Bayesian optimization (BO) is a powerful approach to sample-efficient\noptimization of black-box functions. However, in settings with very few\nfunction evaluations, a successful application of BO may require transferring\ninformation from historical experiments. These related experiments may not have\nexactly the same tunable parameters (search spaces), motivating the need for BO\nwith transfer learning for heterogeneous search spaces. In this paper, we\npropose two methods for this setting. The first approach leverages a Gaussian\nprocess (GP) model with a conditional kernel to transfer information between\ndifferent search spaces. Our second approach treats the missing parameters as\nhyperparameters of the GP model that can be inferred jointly with the other GP\nhyperparameters or set to fixed values. We show that these two methods perform\nwell on several benchmark problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian optimization (BO) is a powerful approach to sample-efficient\noptimization of black-box functions. However, in settings with very few\nfunction evaluations, a successful application of BO may require transferring\ninformation from historical experiments. These related experiments may not have\nexactly the same tunable parameters (search spaces), motivating the need for BO\nwith transfer learning for heterogeneous search spaces. In this paper, we\npropose two methods for this setting. The first approach leverages a Gaussian\nprocess (GP) model with a conditional kernel to transfer information between\ndifferent search spaces. Our second approach treats the missing parameters as\nhyperparameters of the GP model that can be inferred jointly with the other GP\nhyperparameters or set to fixed values. We show that these two methods perform\nwell on several benchmark problems."
                },
                "authors": [
                    {
                        "name": "Aryan Deshwal"
                    },
                    {
                        "name": "Sait Cakmak"
                    },
                    {
                        "name": "Yuhou Xia"
                    },
                    {
                        "name": "David Eriksson"
                    }
                ],
                "author_detail": {
                    "name": "David Eriksson"
                },
                "author": "David Eriksson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10042v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10042v3",
                "updated": "2024-09-09T04:16:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    4,
                    16,
                    9,
                    0,
                    253,
                    0
                ],
                "published": "2022-12-20T07:28:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    7,
                    28,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Guarantees for Comprehensive Simulation Assessment of Statistical\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guarantees for Comprehensive Simulation Assessment of Statistical\n  Methods"
                },
                "summary": "Simulation can evaluate a statistical method for properties such as Type I\nError, FDR, or bias on a grid of hypothesized parameter values. But what about\nthe gaps between the grid-points? Continuous Simulation Extension (CSE) is a\nproof-by-simulation framework which can supplement simulations with (1)\nconfidence bands valid over regions of parameter space or (2) calibration of\nrejection thresholds to provide rigorous proof of strong Type I Error control.\nCSE extends simulation estimates at grid-points into bounds over nearby space\nusing a model shift bound related to the Renyi divergence, which we analyze for\nmodels in exponential family or canonical GLM form. CSE can work with adaptive\nsampling, nuisance parameters, administrative censoring, multiple arms,\nmultiple testing, Bayesian randomization, Bayesian decision-making, and\ninference algorithms of arbitrary complexity. As a case study, we calibrate for\nstrong Type I Error control a Phase II/III Bayesian selection design with 4\nunknown statistical parameters. Potential applications include calibration of\nnew statistical procedures or streamlining regulatory review of adaptive trial\ndesigns. Our open-source software implementation imprint is available\nathttps://github.com/Confirm-Solutions/imprint",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation can evaluate a statistical method for properties such as Type I\nError, FDR, or bias on a grid of hypothesized parameter values. But what about\nthe gaps between the grid-points? Continuous Simulation Extension (CSE) is a\nproof-by-simulation framework which can supplement simulations with (1)\nconfidence bands valid over regions of parameter space or (2) calibration of\nrejection thresholds to provide rigorous proof of strong Type I Error control.\nCSE extends simulation estimates at grid-points into bounds over nearby space\nusing a model shift bound related to the Renyi divergence, which we analyze for\nmodels in exponential family or canonical GLM form. CSE can work with adaptive\nsampling, nuisance parameters, administrative censoring, multiple arms,\nmultiple testing, Bayesian randomization, Bayesian decision-making, and\ninference algorithms of arbitrary complexity. As a case study, we calibrate for\nstrong Type I Error control a Phase II/III Bayesian selection design with 4\nunknown statistical parameters. Potential applications include calibration of\nnew statistical procedures or streamlining regulatory review of adaptive trial\ndesigns. Our open-source software implementation imprint is available\nathttps://github.com/Confirm-Solutions/imprint"
                },
                "authors": [
                    {
                        "name": "James Yang"
                    },
                    {
                        "name": "T. Ben Thompson"
                    },
                    {
                        "name": "Michael Sklar"
                    }
                ],
                "author_detail": {
                    "name": "Michael Sklar"
                },
                "author": "Michael Sklar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10042v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10042v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.05865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05865v1",
                "updated": "2024-09-09T17:59:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    50,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:50Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    50,
                    0,
                    253,
                    0
                ],
                "title": "Robot Utility Models: General Policies for Zero-Shot Deployment in New\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot Utility Models: General Policies for Zero-Shot Deployment in New\n  Environments"
                },
                "summary": "Robot models, particularly those trained with large amounts of data, have\nrecently shown a plethora of real-world manipulation and navigation\ncapabilities. Several independent efforts have shown that given sufficient\ntraining data in an environment, robot policies can generalize to demonstrated\nvariations in that environment. However, needing to finetune robot models to\nevery new environment stands in stark contrast to models in language or vision\nthat can be deployed zero-shot for open-world problems. In this work, we\npresent Robot Utility Models (RUMs), a framework for training and deploying\nzero-shot robot policies that can directly generalize to new environments\nwithout any finetuning. To create RUMs efficiently, we develop new tools to\nquickly collect data for mobile manipulation tasks, integrate such data into a\npolicy with multi-modal imitation learning, and deploy policies on-device on\nHello Robot Stretch, a cheap commodity robot, with an external mLLM verifier\nfor retrying. We train five such utility models for opening cabinet doors,\nopening drawers, picking up napkins, picking up paper bags, and reorienting\nfallen objects. Our system, on average, achieves 90% success rate in unseen,\nnovel environments interacting with unseen objects. Moreover, the utility\nmodels can also succeed in different robot and camera set-ups with no further\ndata, training, or fine-tuning. Primary among our lessons are the importance of\ntraining data over training algorithm and policy class, guidance about data\nscaling, necessity for diverse yet high-quality demonstrations, and a recipe\nfor robot introspection and retrying to improve performance on individual\nenvironments. Our code, data, models, hardware designs, as well as our\nexperiment and deployment videos are open sourced and can be found on our\nproject website: https://robotutilitymodels.com",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot models, particularly those trained with large amounts of data, have\nrecently shown a plethora of real-world manipulation and navigation\ncapabilities. Several independent efforts have shown that given sufficient\ntraining data in an environment, robot policies can generalize to demonstrated\nvariations in that environment. However, needing to finetune robot models to\nevery new environment stands in stark contrast to models in language or vision\nthat can be deployed zero-shot for open-world problems. In this work, we\npresent Robot Utility Models (RUMs), a framework for training and deploying\nzero-shot robot policies that can directly generalize to new environments\nwithout any finetuning. To create RUMs efficiently, we develop new tools to\nquickly collect data for mobile manipulation tasks, integrate such data into a\npolicy with multi-modal imitation learning, and deploy policies on-device on\nHello Robot Stretch, a cheap commodity robot, with an external mLLM verifier\nfor retrying. We train five such utility models for opening cabinet doors,\nopening drawers, picking up napkins, picking up paper bags, and reorienting\nfallen objects. Our system, on average, achieves 90% success rate in unseen,\nnovel environments interacting with unseen objects. Moreover, the utility\nmodels can also succeed in different robot and camera set-ups with no further\ndata, training, or fine-tuning. Primary among our lessons are the importance of\ntraining data over training algorithm and policy class, guidance about data\nscaling, necessity for diverse yet high-quality demonstrations, and a recipe\nfor robot introspection and retrying to improve performance on individual\nenvironments. Our code, data, models, hardware designs, as well as our\nexperiment and deployment videos are open sourced and can be found on our\nproject website: https://robotutilitymodels.com"
                },
                "authors": [
                    {
                        "name": "Haritheja Etukuru"
                    },
                    {
                        "name": "Norihito Naka"
                    },
                    {
                        "name": "Zijin Hu"
                    },
                    {
                        "name": "Seungjae Lee"
                    },
                    {
                        "name": "Julian Mehu"
                    },
                    {
                        "name": "Aaron Edsinger"
                    },
                    {
                        "name": "Chris Paxton"
                    },
                    {
                        "name": "Soumith Chintala"
                    },
                    {
                        "name": "Lerrel Pinto"
                    },
                    {
                        "name": "Nur Muhammad Mahi Shafiullah"
                    }
                ],
                "author_detail": {
                    "name": "Nur Muhammad Mahi Shafiullah"
                },
                "author": "Nur Muhammad Mahi Shafiullah",
                "arxiv_comment": "Project website https://robotutilitymodels.com",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05864v1",
                "updated": "2024-09-09T17:59:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    45,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:45Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    45,
                    0,
                    253,
                    0
                ],
                "title": "Neural MP: A Generalist Neural Motion Planner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural MP: A Generalist Neural Motion Planner"
                },
                "summary": "The current paradigm for motion planning generates solutions from scratch for\nevery new problem, which consumes significant amounts of time and computational\nresources. For complex, cluttered scenes, motion planning approaches can often\ntake minutes to produce a solution, while humans are able to accurately and\nsafely reach any goal in seconds by leveraging their prior experience. We seek\nto do the same by applying data-driven learning at scale to the problem of\nmotion planning. Our approach builds a large number of complex scenes in\nsimulation, collects expert data from a motion planner, then distills it into a\nreactive generalist policy. We then combine this with lightweight optimization\nto obtain a safe path for real world deployment. We perform a thorough\nevaluation of our method on 64 motion planning tasks across four diverse\nenvironments with randomized poses, scenes and obstacles, in the real world,\ndemonstrating an improvement of 23%, 17% and 79% motion planning success rate\nover state of the art sampling, optimization and learning based planning\nmethods. Video results available at mihdalal.github.io/neuralmotionplanner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current paradigm for motion planning generates solutions from scratch for\nevery new problem, which consumes significant amounts of time and computational\nresources. For complex, cluttered scenes, motion planning approaches can often\ntake minutes to produce a solution, while humans are able to accurately and\nsafely reach any goal in seconds by leveraging their prior experience. We seek\nto do the same by applying data-driven learning at scale to the problem of\nmotion planning. Our approach builds a large number of complex scenes in\nsimulation, collects expert data from a motion planner, then distills it into a\nreactive generalist policy. We then combine this with lightweight optimization\nto obtain a safe path for real world deployment. We perform a thorough\nevaluation of our method on 64 motion planning tasks across four diverse\nenvironments with randomized poses, scenes and obstacles, in the real world,\ndemonstrating an improvement of 23%, 17% and 79% motion planning success rate\nover state of the art sampling, optimization and learning based planning\nmethods. Video results available at mihdalal.github.io/neuralmotionplanner"
                },
                "authors": [
                    {
                        "name": "Murtaza Dalal"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Russell Mendonca"
                    },
                    {
                        "name": "Youssef Khaky"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Deepak Pathak"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Pathak"
                },
                "author": "Deepak Pathak",
                "arxiv_comment": "Website at mihdalal.github.io/neuralmotionplanner. Main paper: 7\n  pages, 4 figures, 2 tables. Appendix: 9 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14770v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14770v5",
                "updated": "2024-09-09T17:37:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    37,
                    16,
                    0,
                    253,
                    0
                ],
                "published": "2023-05-24T06:19:14Z",
                "published_parsed": [
                    2023,
                    5,
                    24,
                    6,
                    19,
                    14,
                    2,
                    144,
                    0
                ],
                "title": "Using Natural Language Explanations to Rescale Human Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Natural Language Explanations to Rescale Human Judgments"
                },
                "summary": "The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover human judgments. However, annotators' judgments for subjective tasks can\ndiffer in many ways: they may reflect different qualitative judgments about an\nexample, and they may be mapped to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover human judgments. However, annotators' judgments for subjective tasks can\ndiffer in many ways: they may reflect different qualitative judgments about an\nexample, and they may be mapped to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric."
                },
                "authors": [
                    {
                        "name": "Manya Wadhwa"
                    },
                    {
                        "name": "Jifan Chen"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "arxiv_comment": "Data available at\n  https://github.com/ManyaWadhwa/explanation_based_rescaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14770v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14770v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05824v1",
                "updated": "2024-09-09T17:30:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    30,
                    20,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:30:20Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    30,
                    20,
                    0,
                    253,
                    0
                ],
                "title": "Are Large Language Models a Threat to Programming Platforms? An\n  Exploratory Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models a Threat to Programming Platforms? An\n  Exploratory Study"
                },
                "summary": "Competitive programming platforms like LeetCode, Codeforces, and HackerRank\nevaluate programming skills, often used by recruiters for screening. With the\nrise of advanced Large Language Models (LLMs) such as ChatGPT, Gemini, and Meta\nAI, their problem-solving ability on these platforms needs assessment. This\nstudy explores LLMs' ability to tackle diverse programming challenges across\nplatforms with varying difficulty, offering insights into their real-time and\noffline performance and comparing them with human programmers.\n  We tested 98 problems from LeetCode, 126 from Codeforces, covering 15\ncategories. Nine online contests from Codeforces and LeetCode were conducted,\nalong with two certification tests on HackerRank, to assess real-time\nperformance. Prompts and feedback mechanisms were used to guide LLMs, and\ncorrelations were explored across different scenarios.\n  LLMs, like ChatGPT (71.43% success on LeetCode), excelled in LeetCode and\nHackerRank certifications but struggled in virtual contests, particularly on\nCodeforces. They performed better than users in LeetCode archives, excelling in\ntime and memory efficiency but underperforming in harder Codeforces contests.\nWhile not immediately threatening, LLMs performance on these platforms is\nconcerning, and future improvements will need addressing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive programming platforms like LeetCode, Codeforces, and HackerRank\nevaluate programming skills, often used by recruiters for screening. With the\nrise of advanced Large Language Models (LLMs) such as ChatGPT, Gemini, and Meta\nAI, their problem-solving ability on these platforms needs assessment. This\nstudy explores LLMs' ability to tackle diverse programming challenges across\nplatforms with varying difficulty, offering insights into their real-time and\noffline performance and comparing them with human programmers.\n  We tested 98 problems from LeetCode, 126 from Codeforces, covering 15\ncategories. Nine online contests from Codeforces and LeetCode were conducted,\nalong with two certification tests on HackerRank, to assess real-time\nperformance. Prompts and feedback mechanisms were used to guide LLMs, and\ncorrelations were explored across different scenarios.\n  LLMs, like ChatGPT (71.43% success on LeetCode), excelled in LeetCode and\nHackerRank certifications but struggled in virtual contests, particularly on\nCodeforces. They performed better than users in LeetCode archives, excelling in\ntime and memory efficiency but underperforming in harder Codeforces contests.\nWhile not immediately threatening, LLMs performance on these platforms is\nconcerning, and future improvements will need addressing."
                },
                "authors": [
                    {
                        "name": "Md Mustakim Billah"
                    },
                    {
                        "name": "Palash Ranjan Roy"
                    },
                    {
                        "name": "Zadia Codabux"
                    },
                    {
                        "name": "Banani Roy"
                    }
                ],
                "author_detail": {
                    "name": "Banani Roy"
                },
                "author": "Banani Roy",
                "arxiv_doi": "10.1145/3674805.3686689",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3674805.3686689",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.05824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in ESEM 2024",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05816v1",
                "updated": "2024-09-09T17:23:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    23,
                    29,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:23:29Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    23,
                    29,
                    0,
                    253,
                    0
                ],
                "title": "Improving Pretraining Data Using Perplexity Correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Pretraining Data Using Perplexity Correlations"
                },
                "summary": "Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality pretraining data is often seen as the key to high-performance\nlanguage models. However, progress in understanding pretraining data has been\nslow due to the costly pretraining runs required for data selection\nexperiments. We present a framework that avoids these costs and selects\nhigh-quality pretraining data without any LLM training of our own. Our work is\nbased on a simple observation: LLM losses on many pretraining texts are\ncorrelated with downstream benchmark performance, and selecting\nhigh-correlation documents is an effective pretraining data selection method.\nWe build a new statistical framework for data selection centered around\nestimates of perplexity-benchmark correlations and perform data selection using\na sample of 90 LLMs taken from the Open LLM Leaderboard on texts from tens of\nthousands of web domains. In controlled pretraining experiments at the 160M\nparameter scale on 8 benchmarks, our approach outperforms DSIR on every\nbenchmark, while matching the best data selector found in DataComp-LM, a\nhand-engineered bigram classifier."
                },
                "authors": [
                    {
                        "name": "Tristan Thrush"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05806v1",
                "updated": "2024-09-09T17:11:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    11,
                    51,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:11:51Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    11,
                    51,
                    0,
                    253,
                    0
                ],
                "title": "Benchmarking Chinese Knowledge Rectification in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Chinese Knowledge Rectification in Large Language Models"
                },
                "summary": "While Large Language Models (LLMs) exhibit remarkable generative\ncapabilities, they are not without flaws, particularly in the form of\nhallucinations. This issue is even more pronounced when LLMs are applied to\nspecific languages and domains. For example, LLMs may generate nonsense\ninformation when handling Chinese ancient poetry, proverbs, or idioms, owing to\nthe lack of specific knowledge. To this end, this paper introduces a benchmark\nfor rectifying Chinese knowledge in LLMs via knowledge editing. Specifically,\nwe introduce a new Chinese dataset, CKnowEdit, by collecting seven type of\nknowledge from various sources, including classical texts, idioms, and content\nfrom Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony,\nantithesis, and logical constructs inherent in the Chinese language. Through\nthe analysis of this dataset, we uncover the challenges faced by current LLMs\nin mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques on this dataset unveil the substantial scope for advancement\nin the rectification of Chinese knowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) exhibit remarkable generative\ncapabilities, they are not without flaws, particularly in the form of\nhallucinations. This issue is even more pronounced when LLMs are applied to\nspecific languages and domains. For example, LLMs may generate nonsense\ninformation when handling Chinese ancient poetry, proverbs, or idioms, owing to\nthe lack of specific knowledge. To this end, this paper introduces a benchmark\nfor rectifying Chinese knowledge in LLMs via knowledge editing. Specifically,\nwe introduce a new Chinese dataset, CKnowEdit, by collecting seven type of\nknowledge from various sources, including classical texts, idioms, and content\nfrom Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony,\nantithesis, and logical constructs inherent in the Chinese language. Through\nthe analysis of this dataset, we uncover the challenges faced by current LLMs\nin mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques on this dataset unveil the substantial scope for advancement\nin the rectification of Chinese knowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit."
                },
                "authors": [
                    {
                        "name": "Tianhe Lu"
                    },
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Ongoing work; code and dataset are available at\n  https://github.com/zjunlp/EasyEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05782v1",
                "updated": "2024-09-09T16:45:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    45,
                    26,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:45:26Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    45,
                    26,
                    0,
                    253,
                    0
                ],
                "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Neural Network Scaling Laws and Scale-time Equivalence"
                },
                "summary": "As neural networks continue to grow in size but datasets might not, it is\nvital to understand how much performance improvement can be expected: is it\nmore important to scale network size or data volume? Thus, neural network\nscaling laws, which characterize how test error varies with network size and\ndata volume, have become increasingly important. However, existing scaling laws\nare often applicable only in limited regimes and often do not incorporate or\npredict well-known phenomena such as double descent. Here, we present a novel\ntheoretical characterization of how three factors -- model size, training time,\nand data volume -- interact to determine the performance of deep neural\nnetworks. We first establish a theoretical and empirical equivalence between\nscaling the size of a neural network and increasing its training time\nproportionally. Scale-time equivalence challenges the current practice, wherein\nlarge models are trained for small durations, and suggests that smaller models\ntrained over extended periods could match their efficacy. It also leads to a\nnovel method for predicting the performance of large-scale networks from\nsmall-scale networks trained for extended epochs, and vice versa. We next\ncombine scale-time equivalence with a linear model analysis of double descent\nto obtain a unified theoretical scaling law, which we confirm with experiments\nacross vision benchmarks and network architectures. These laws explain several\npreviously unexplained phenomena: reduced data requirements for generalization\nin larger models, heightened sensitivity to label noise in overparameterized\nmodels, and instances where increasing model scale does not necessarily enhance\nperformance. Our findings hold significant implications for the practical\ndeployment of neural networks, offering a more accessible and efficient path to\ntraining and fine-tuning large models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As neural networks continue to grow in size but datasets might not, it is\nvital to understand how much performance improvement can be expected: is it\nmore important to scale network size or data volume? Thus, neural network\nscaling laws, which characterize how test error varies with network size and\ndata volume, have become increasingly important. However, existing scaling laws\nare often applicable only in limited regimes and often do not incorporate or\npredict well-known phenomena such as double descent. Here, we present a novel\ntheoretical characterization of how three factors -- model size, training time,\nand data volume -- interact to determine the performance of deep neural\nnetworks. We first establish a theoretical and empirical equivalence between\nscaling the size of a neural network and increasing its training time\nproportionally. Scale-time equivalence challenges the current practice, wherein\nlarge models are trained for small durations, and suggests that smaller models\ntrained over extended periods could match their efficacy. It also leads to a\nnovel method for predicting the performance of large-scale networks from\nsmall-scale networks trained for extended epochs, and vice versa. We next\ncombine scale-time equivalence with a linear model analysis of double descent\nto obtain a unified theoretical scaling law, which we confirm with experiments\nacross vision benchmarks and network architectures. These laws explain several\npreviously unexplained phenomena: reduced data requirements for generalization\nin larger models, heightened sensitivity to label noise in overparameterized\nmodels, and instances where increasing model scale does not necessarily enhance\nperformance. Our findings hold significant implications for the practical\ndeployment of neural networks, offering a more accessible and efficient path to\ntraining and fine-tuning large models."
                },
                "authors": [
                    {
                        "name": "Akhilan Boopathy"
                    },
                    {
                        "name": "Ila Fiete"
                    }
                ],
                "author_detail": {
                    "name": "Ila Fiete"
                },
                "author": "Ila Fiete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14774v2",
                "updated": "2024-09-09T16:41:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    41,
                    36,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-27T04:31:58Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    4,
                    31,
                    58,
                    1,
                    240,
                    0
                ],
                "title": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning"
                },
                "summary": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just $4$K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in $20\\%$ of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings."
                },
                "authors": [
                    {
                        "name": "Simran Kaur"
                    },
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Anirudh Goyal"
                    },
                    {
                        "name": "Sanjeev Arora"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Arora"
                },
                "author": "Sanjeev Arora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14417v2",
                "updated": "2024-09-09T16:34:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    34,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-19T15:42:49Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    15,
                    42,
                    49,
                    4,
                    201,
                    0
                ],
                "title": "Mixture of Experts with Mixture of Precisions for Tuning Quality of\n  Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts with Mixture of Precisions for Tuning Quality of\n  Service"
                },
                "summary": "The increasing demand for deploying large Mixture-of-Experts (MoE) models in\nresource-constrained environments necessitates efficient approaches to address\ntheir high memory and computational requirements challenges. Moreover, given\nthat tasks come in different user-defined constraints and the available\nresources change over time in multi-tenant environments, it is necessary to\ndesign an approach which provides a flexible configuration space. This paper\npresents an adaptive serving approach for the efficient deployment of MoE\nmodels, capitalizing on partial quantization of the experts. By dynamically\ndetermining the number of quantized experts and their distribution across CPU\nand GPU, our approach explores the Pareto frontier and offers a fine-grained\nrange of configurations for tuning throughput and model quality. Our evaluation\non an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three language\nmodelling benchmarks demonstrates that the throughput of token generation can\nbe adjusted from 0.63 to 13.00 token per second. This enhancement comes with a\nmarginal perplexity increase of 3.81 to 4.00, 13.59 to 14.17, and 7.24 to 7.40\nfor WikiText2, PTB, and C4 datasets respectively under maximum quantization.\nThese results highlight the practical applicability of our approach in dynamic\nand accuracy-sensitive applications where both memory usage and output quality\nare important.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for deploying large Mixture-of-Experts (MoE) models in\nresource-constrained environments necessitates efficient approaches to address\ntheir high memory and computational requirements challenges. Moreover, given\nthat tasks come in different user-defined constraints and the available\nresources change over time in multi-tenant environments, it is necessary to\ndesign an approach which provides a flexible configuration space. This paper\npresents an adaptive serving approach for the efficient deployment of MoE\nmodels, capitalizing on partial quantization of the experts. By dynamically\ndetermining the number of quantized experts and their distribution across CPU\nand GPU, our approach explores the Pareto frontier and offers a fine-grained\nrange of configurations for tuning throughput and model quality. Our evaluation\non an NVIDIA A100 GPU using a Mixtral 8x7B MoE model for three language\nmodelling benchmarks demonstrates that the throughput of token generation can\nbe adjusted from 0.63 to 13.00 token per second. This enhancement comes with a\nmarginal perplexity increase of 3.81 to 4.00, 13.59 to 14.17, and 7.24 to 7.40\nfor WikiText2, PTB, and C4 datasets respectively under maximum quantization.\nThese results highlight the practical applicability of our approach in dynamic\nand accuracy-sensitive applications where both memory usage and output quality\nare important."
                },
                "authors": [
                    {
                        "name": "HamidReza Imani"
                    },
                    {
                        "name": "Abdolah Amirany"
                    },
                    {
                        "name": "Tarek El-Ghazawi"
                    }
                ],
                "author_detail": {
                    "name": "Tarek El-Ghazawi"
                },
                "author": "Tarek El-Ghazawi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05771v1",
                "updated": "2024-09-09T16:33:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    33,
                    16,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:33:16Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    33,
                    16,
                    0,
                    253,
                    0
                ],
                "title": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence from fMRI Supports a Two-Phase Abstraction Process in Language\n  Models"
                },
                "summary": "Research has repeatedly demonstrated that intermediate hidden states\nextracted from large language models are able to predict measured brain\nresponse to natural language stimuli. Yet, very little is known about the\nrepresentation properties that enable this high prediction performance. Why is\nit the intermediate layers, and not the output layers, that are most capable\nfor this unique and highly general transfer task? In this work, we show that\nevidence from language encoding models in fMRI supports the existence of a\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\nshow that this abstraction process naturally arises over the course of training\na language model and that the first \"composition\" phase of this abstraction\nprocess is compressed into fewer layers as training continues. Finally, we\ndemonstrate a strong correspondence between layerwise encoding performance and\nthe intrinsic dimensionality of representations from LLMs. We give initial\nevidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research has repeatedly demonstrated that intermediate hidden states\nextracted from large language models are able to predict measured brain\nresponse to natural language stimuli. Yet, very little is known about the\nrepresentation properties that enable this high prediction performance. Why is\nit the intermediate layers, and not the output layers, that are most capable\nfor this unique and highly general transfer task? In this work, we show that\nevidence from language encoding models in fMRI supports the existence of a\ntwo-phase abstraction process within LLMs. We use manifold learning methods to\nshow that this abstraction process naturally arises over the course of training\na language model and that the first \"composition\" phase of this abstraction\nprocess is compressed into fewer layers as training continues. Finally, we\ndemonstrate a strong correspondence between layerwise encoding performance and\nthe intrinsic dimensionality of representations from LLMs. We give initial\nevidence that this correspondence primarily derives from the inherent\ncompositionality of LLMs and not their next-word prediction properties."
                },
                "authors": [
                    {
                        "name": "Emily Cheng"
                    },
                    {
                        "name": "Richard J. Antonello"
                    }
                ],
                "author_detail": {
                    "name": "Richard J. Antonello"
                },
                "author": "Richard J. Antonello",
                "arxiv_comment": "Equal contribution from both authors. Submitted to NeurIPS NeuroAI\n  workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05768v1",
                "updated": "2024-09-09T16:32:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    32,
                    14,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:32:14Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    32,
                    14,
                    0,
                    253,
                    0
                ],
                "title": "Model Input Verification of Large Scale Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Input Verification of Large Scale Simulations"
                },
                "summary": "Reliable simulations are critical for analyzing and understanding complex\nsystems, but their accuracy depends on correct input data. Incorrect inputs\nsuch as invalid or out-of-range values, missing data, and format\ninconsistencies can cause simulation crashes or unnoticed result distortions,\nultimately undermining the validity of the conclusions. This paper presents a\nmethodology for verifying the validity of input data in simulations, a process\nwe term model input verification (MIV). We implement this approach in FabGuard,\na toolset that uses established data schema and validation tools for the\nspecific needs of simulation modeling. We introduce a formalism for\ncategorizing MIV patterns and offer a streamlined verification pipeline that\nintegrates into existing simulation workflows. FabGuard's applicability is\ndemonstrated across three diverse domains: conflict-driven migration, disaster\nevacuation, and disease spread models. We also explore the use of Large\nLanguage Models (LLMs) for automating constraint generation and inference. In a\ncase study with a migration simulation, LLMs not only correctly inferred 22 out\nof 23 developer-defined constraints, but also identified errors in existing\nconstraints and proposed new, valid constraints. Our evaluation demonstrates\nthat MIV is feasible on large datasets, with FabGuard efficiently processing\n12,000 input files in 140 seconds and maintaining consistent performance across\nvarying file sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable simulations are critical for analyzing and understanding complex\nsystems, but their accuracy depends on correct input data. Incorrect inputs\nsuch as invalid or out-of-range values, missing data, and format\ninconsistencies can cause simulation crashes or unnoticed result distortions,\nultimately undermining the validity of the conclusions. This paper presents a\nmethodology for verifying the validity of input data in simulations, a process\nwe term model input verification (MIV). We implement this approach in FabGuard,\na toolset that uses established data schema and validation tools for the\nspecific needs of simulation modeling. We introduce a formalism for\ncategorizing MIV patterns and offer a streamlined verification pipeline that\nintegrates into existing simulation workflows. FabGuard's applicability is\ndemonstrated across three diverse domains: conflict-driven migration, disaster\nevacuation, and disease spread models. We also explore the use of Large\nLanguage Models (LLMs) for automating constraint generation and inference. In a\ncase study with a migration simulation, LLMs not only correctly inferred 22 out\nof 23 developer-defined constraints, but also identified errors in existing\nconstraints and proposed new, valid constraints. Our evaluation demonstrates\nthat MIV is feasible on large datasets, with FabGuard efficiently processing\n12,000 input files in 140 seconds and maintaining consistent performance across\nvarying file sizes."
                },
                "authors": [
                    {
                        "name": "Rumyana Neykova"
                    },
                    {
                        "name": "Derek Groen"
                    }
                ],
                "author_detail": {
                    "name": "Derek Groen"
                },
                "author": "Derek Groen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10999v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10999v3",
                "updated": "2024-09-09T16:28:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    28,
                    9,
                    0,
                    253,
                    0
                ],
                "published": "2024-06-16T16:25:22Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    16,
                    25,
                    22,
                    6,
                    168,
                    0
                ],
                "title": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions"
                },
                "summary": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. We show that certain cognitive biases when properly\nbalanced, can enhance decision-making efficiency through rational deviations\nand heuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications."
                },
                "authors": [
                    {
                        "name": "Liman Wang"
                    },
                    {
                        "name": "Hanyang Zhong"
                    },
                    {
                        "name": "Wenting Cao"
                    },
                    {
                        "name": "Zeyuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zeyuan Sun"
                },
                "author": "Zeyuan Sun",
                "arxiv_comment": "This article is currently under review. All data will be open on\n  GitHub once the review is complete.\n  https://github.com/limanwang/Balancing-Rigor-and-Utility",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10999v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10999v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05747v1",
                "updated": "2024-09-09T16:02:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    2,
                    27,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:02:27Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    2,
                    27,
                    0,
                    253,
                    0
                ],
                "title": "A Novel Idea Generation Tool using a Structured Conversational AI (CAI)\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Idea Generation Tool using a Structured Conversational AI (CAI)\n  System"
                },
                "summary": "This paper presents a novel conversational AI-enabled active ideation\ninterface as a creative idea-generation tool to assist novice designers in\nmitigating the initial latency and ideation bottlenecks that are commonly\nobserved. It is a dynamic, interactive, and contextually responsive approach,\nactively involving a large language model (LLM) from the domain of natural\nlanguage processing (NLP) in artificial intelligence (AI) to produce multiple\nstatements of potential ideas for different design problems. Integrating such\nAI models with ideation creates what we refer to as an Active Ideation\nscenario, which helps foster continuous dialogue-based interaction,\ncontext-sensitive conversation, and prolific idea generation. A pilot study was\nconducted with thirty novice designers to generate ideas for given problems\nusing traditional methods and the new CAI-based interface. The key parameters\nof fluency, novelty, and variety were used to compare the outcomes\nqualitatively by a panel of experts. The findings demonstrated the\neffectiveness of the proposed tool for generating prolific, diverse and novel\nideas. The interface was enhanced by incorporating a prompt-engineered\nstructured dialogue style for each ideation stage to make it uniform and more\nconvenient for the designers. The resulting responses of such a structured CAI\ninterface were found to be more succinct and aligned towards the subsequent\ndesign stage, namely conceptualization. The paper thus established the rich\npotential of using Generative AI (Gen-AI) for the early ill-structured phase of\nthe creative product design process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel conversational AI-enabled active ideation\ninterface as a creative idea-generation tool to assist novice designers in\nmitigating the initial latency and ideation bottlenecks that are commonly\nobserved. It is a dynamic, interactive, and contextually responsive approach,\nactively involving a large language model (LLM) from the domain of natural\nlanguage processing (NLP) in artificial intelligence (AI) to produce multiple\nstatements of potential ideas for different design problems. Integrating such\nAI models with ideation creates what we refer to as an Active Ideation\nscenario, which helps foster continuous dialogue-based interaction,\ncontext-sensitive conversation, and prolific idea generation. A pilot study was\nconducted with thirty novice designers to generate ideas for given problems\nusing traditional methods and the new CAI-based interface. The key parameters\nof fluency, novelty, and variety were used to compare the outcomes\nqualitatively by a panel of experts. The findings demonstrated the\neffectiveness of the proposed tool for generating prolific, diverse and novel\nideas. The interface was enhanced by incorporating a prompt-engineered\nstructured dialogue style for each ideation stage to make it uniform and more\nconvenient for the designers. The resulting responses of such a structured CAI\ninterface were found to be more succinct and aligned towards the subsequent\ndesign stage, namely conceptualization. The paper thus established the rich\npotential of using Generative AI (Gen-AI) for the early ill-structured phase of\nthe creative product design process."
                },
                "authors": [
                    {
                        "name": "B. Sankar"
                    },
                    {
                        "name": "Dibakar Sen"
                    }
                ],
                "author_detail": {
                    "name": "Dibakar Sen"
                },
                "author": "Dibakar Sen",
                "arxiv_comment": "21 pages, 16 figures, AIEDAM Journal Article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; J.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05746v1",
                "updated": "2024-09-09T16:01:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    1,
                    58,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T16:01:58Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    1,
                    58,
                    0,
                    253,
                    0
                ],
                "title": "LLMs Will Always Hallucinate, and We Need to Live With This",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Will Always Hallucinate, and We Need to Live With This"
                },
                "summary": "As Large Language Models become more ubiquitous across domains, it becomes\nimportant to examine their inherent limitations critically. This work argues\nthat hallucinations in language models are not just occasional errors but an\ninevitable feature of these systems. We demonstrate that hallucinations stem\nfrom the fundamental mathematical and logical structure of LLMs. It is,\ntherefore, impossible to eliminate them through architectural improvements,\ndataset enhancements, or fact-checking mechanisms. Our analysis draws on\ncomputational theory and Godel's First Incompleteness Theorem, which references\nthe undecidability of problems like the Halting, Emptiness, and Acceptance\nProblems. We demonstrate that every stage of the LLM process-from training data\ncompilation to fact retrieval, intent classification, and text generation-will\nhave a non-zero probability of producing hallucinations. This work introduces\nthe concept of Structural Hallucination as an intrinsic nature of these\nsystems. By establishing the mathematical certainty of hallucinations, we\nchallenge the prevailing notion that they can be fully mitigated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models become more ubiquitous across domains, it becomes\nimportant to examine their inherent limitations critically. This work argues\nthat hallucinations in language models are not just occasional errors but an\ninevitable feature of these systems. We demonstrate that hallucinations stem\nfrom the fundamental mathematical and logical structure of LLMs. It is,\ntherefore, impossible to eliminate them through architectural improvements,\ndataset enhancements, or fact-checking mechanisms. Our analysis draws on\ncomputational theory and Godel's First Incompleteness Theorem, which references\nthe undecidability of problems like the Halting, Emptiness, and Acceptance\nProblems. We demonstrate that every stage of the LLM process-from training data\ncompilation to fact retrieval, intent classification, and text generation-will\nhave a non-zero probability of producing hallucinations. This work introduces\nthe concept of Structural Hallucination as an intrinsic nature of these\nsystems. By establishing the mathematical certainty of hallucinations, we\nchallenge the prevailing notion that they can be fully mitigated."
                },
                "authors": [
                    {
                        "name": "Sourav Banerjee"
                    },
                    {
                        "name": "Ayushi Agarwal"
                    },
                    {
                        "name": "Saloni Singla"
                    }
                ],
                "author_detail": {
                    "name": "Saloni Singla"
                },
                "author": "Saloni Singla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.18799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.18799v2",
                "updated": "2024-09-09T16:00:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    16,
                    0,
                    4,
                    0,
                    253,
                    0
                ],
                "published": "2023-11-30T18:43:51Z",
                "published_parsed": [
                    2023,
                    11,
                    30,
                    18,
                    43,
                    51,
                    3,
                    334,
                    0
                ],
                "title": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\n  representations to LLMs and Emergent Cross-modal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\n  representations to LLMs and Emergent Cross-modal Reasoning"
                },
                "summary": "Recent research has achieved significant advancements in visual reasoning\ntasks through learning image-to-language projections and leveraging the\nimpressive reasoning abilities of Large Language Models (LLMs). This paper\nintroduces an efficient and effective framework that integrates multiple\nmodalities (images, 3D, audio and video) to a frozen LLM and demonstrates an\nemergent ability for cross-modal reasoning (2+ modality inputs). Our approach\nexplores two distinct projection mechanisms: Q-Formers and Linear Projections\n(LPs). Through extensive experimentation across all four modalities on 16\nbenchmarks, we explore both methods and assess their adaptability in integrated\nand separate cross-modal reasoning. The Q-Former projection demonstrates\nsuperior performance in single modality scenarios and adaptability in joint\nversus discriminative reasoning involving two or more modalities. However, it\nexhibits lower generalization capabilities than linear projection in contexts\nwhere task-modality data are limited. To enable this framework, we devise a\nscalable pipeline that automatically generates high-quality, instruction-tuning\ndatasets from readily available captioning data across different modalities,\nand contribute 24K QA data for audio and 250K QA data for 3D. To facilitate\nfurther research in cross-modal reasoning, we introduce the DisCRn\n(Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA\nsamples and 28K image-3D QA samples that require the model to reason\ndiscriminatively across disparate input modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has achieved significant advancements in visual reasoning\ntasks through learning image-to-language projections and leveraging the\nimpressive reasoning abilities of Large Language Models (LLMs). This paper\nintroduces an efficient and effective framework that integrates multiple\nmodalities (images, 3D, audio and video) to a frozen LLM and demonstrates an\nemergent ability for cross-modal reasoning (2+ modality inputs). Our approach\nexplores two distinct projection mechanisms: Q-Formers and Linear Projections\n(LPs). Through extensive experimentation across all four modalities on 16\nbenchmarks, we explore both methods and assess their adaptability in integrated\nand separate cross-modal reasoning. The Q-Former projection demonstrates\nsuperior performance in single modality scenarios and adaptability in joint\nversus discriminative reasoning involving two or more modalities. However, it\nexhibits lower generalization capabilities than linear projection in contexts\nwhere task-modality data are limited. To enable this framework, we devise a\nscalable pipeline that automatically generates high-quality, instruction-tuning\ndatasets from readily available captioning data across different modalities,\nand contribute 24K QA data for audio and 250K QA data for 3D. To facilitate\nfurther research in cross-modal reasoning, we introduce the DisCRn\n(Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA\nsamples and 28K image-3D QA samples that require the model to reason\ndiscriminatively across disparate input modalities."
                },
                "authors": [
                    {
                        "name": "Artemis Panagopoulou"
                    },
                    {
                        "name": "Le Xue"
                    },
                    {
                        "name": "Ning Yu"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Niebles"
                },
                "author": "Juan Carlos Niebles",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.18799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.18799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05735v1",
                "updated": "2024-09-09T15:44:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    44,
                    39,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:44:39Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    44,
                    39,
                    0,
                    253,
                    0
                ],
                "title": "A System and Benchmark for LLM-based Q\\&A on Heterogeneous Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System and Benchmark for LLM-based Q\\&A on Heterogeneous Data"
                },
                "summary": "In many industrial settings, users wish to ask questions whose answers may be\nfound in structured data sources such as a spreadsheets, databases, APIs, or\ncombinations thereof. Often, the user doesn't know how to identify or access\nthe right data source. This problem is compounded even further if multiple (and\npotentially siloed) data sources must be assembled to derive the answer.\nRecently, various Text-to-SQL applications that leverage Large Language Models\n(LLMs) have addressed some of these problems by enabling users to ask questions\nin natural language. However, these applications remain impractical in\nrealistic industrial settings because they fail to cope with the data source\nheterogeneity that typifies such environments. In this paper, we address\nheterogeneity by introducing the siwarex platform, which enables seamless\nnatural language access to both databases and APIs. To demonstrate the\neffectiveness of siwarex, we extend the popular Spider dataset and benchmark by\nreplacing some of its tables by data retrieval APIs. We find that siwarex does\na good job of coping with data source heterogeneity. Our modified Spider\nbenchmark will soon be available to the research community",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many industrial settings, users wish to ask questions whose answers may be\nfound in structured data sources such as a spreadsheets, databases, APIs, or\ncombinations thereof. Often, the user doesn't know how to identify or access\nthe right data source. This problem is compounded even further if multiple (and\npotentially siloed) data sources must be assembled to derive the answer.\nRecently, various Text-to-SQL applications that leverage Large Language Models\n(LLMs) have addressed some of these problems by enabling users to ask questions\nin natural language. However, these applications remain impractical in\nrealistic industrial settings because they fail to cope with the data source\nheterogeneity that typifies such environments. In this paper, we address\nheterogeneity by introducing the siwarex platform, which enables seamless\nnatural language access to both databases and APIs. To demonstrate the\neffectiveness of siwarex, we extend the popular Spider dataset and benchmark by\nreplacing some of its tables by data retrieval APIs. We find that siwarex does\na good job of coping with data source heterogeneity. Our modified Spider\nbenchmark will soon be available to the research community"
                },
                "authors": [
                    {
                        "name": "Achille Fokoue"
                    },
                    {
                        "name": "Srideepika Jayaraman"
                    },
                    {
                        "name": "Elham Khabiri"
                    },
                    {
                        "name": "Jeffrey O. Kephart"
                    },
                    {
                        "name": "Yingjie Li"
                    },
                    {
                        "name": "Dhruv Shah"
                    },
                    {
                        "name": "Youssef Drissi"
                    },
                    {
                        "name": "Fenno F. Heath III"
                    },
                    {
                        "name": "Anu Bhamidipaty"
                    },
                    {
                        "name": "Fateh A. Tipu"
                    },
                    {
                        "name": "Robert J. Baseman"
                    }
                ],
                "author_detail": {
                    "name": "Robert J. Baseman"
                },
                "author": "Robert J. Baseman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05732v1",
                "updated": "2024-09-09T15:42:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    42,
                    19,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:42:19Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    42,
                    19,
                    0,
                    253,
                    0
                ],
                "title": "Towards Democratizing Multilingual Large Language Models For Medicine\n  Through A Two-Stage Instruction Fine-tuning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Democratizing Multilingual Large Language Models For Medicine\n  Through A Two-Stage Instruction Fine-tuning Approach"
                },
                "summary": "Open-source, multilingual medical large language models (LLMs) have the\npotential to serve linguistically diverse populations across different regions.\nAdapting generic LLMs for healthcare often requires continual pretraining, but\nthis approach is computationally expensive and sometimes impractical.\nInstruction fine-tuning on a specific task may not always guarantee optimal\nperformance due to the lack of broader domain knowledge that the model needs to\nunderstand and reason effectively in diverse scenarios. To address these\nchallenges, we introduce two multilingual instruction fine-tuning datasets,\nMMed-IFT and MMed-IFT-MC, containing over 200k high-quality medical samples in\nsix languages. We propose a two-stage training paradigm: the first stage\ninjects general medical knowledge using MMed-IFT, while the second stage\nfine-tunes task-specific multiple-choice questions with MMed-IFT-MC. Our method\nachieves competitive results on both English and multilingual benchmarks,\nstriking a balance between computational efficiency and performance. We plan to\nmake our dataset and model weights public at\n\\url{https://github.com/SpassMed/Med-Llama3} in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source, multilingual medical large language models (LLMs) have the\npotential to serve linguistically diverse populations across different regions.\nAdapting generic LLMs for healthcare often requires continual pretraining, but\nthis approach is computationally expensive and sometimes impractical.\nInstruction fine-tuning on a specific task may not always guarantee optimal\nperformance due to the lack of broader domain knowledge that the model needs to\nunderstand and reason effectively in diverse scenarios. To address these\nchallenges, we introduce two multilingual instruction fine-tuning datasets,\nMMed-IFT and MMed-IFT-MC, containing over 200k high-quality medical samples in\nsix languages. We propose a two-stage training paradigm: the first stage\ninjects general medical knowledge using MMed-IFT, while the second stage\nfine-tunes task-specific multiple-choice questions with MMed-IFT-MC. Our method\nachieves competitive results on both English and multilingual benchmarks,\nstriking a balance between computational efficiency and performance. We plan to\nmake our dataset and model weights public at\n\\url{https://github.com/SpassMed/Med-Llama3} in the future."
                },
                "authors": [
                    {
                        "name": "Meng Zhou"
                    },
                    {
                        "name": "Surajsinh Parmar"
                    },
                    {
                        "name": "Anubhav Bhatti"
                    }
                ],
                "author_detail": {
                    "name": "Anubhav Bhatti"
                },
                "author": "Anubhav Bhatti",
                "arxiv_comment": "Technical Report v1, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05731v1",
                "updated": "2024-09-09T15:41:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    41,
                    53,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:41:53Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    41,
                    53,
                    0,
                    253,
                    0
                ],
                "title": "What Did My Car Say? Autonomous Vehicle Explanation Errors, Context, and\n  Personal Traits Impact Comfort, Reliance, Satisfaction, and Driving\n  Confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Did My Car Say? Autonomous Vehicle Explanation Errors, Context, and\n  Personal Traits Impact Comfort, Reliance, Satisfaction, and Driving\n  Confidence"
                },
                "summary": "Explanations for autonomous vehicle (AV) decisions may build trust, however,\nexplanations can contain errors. In a simulated driving study (n = 232), we\ntested how AV explanation errors, driving context characteristics (perceived\nharm and driving difficulty), and personal traits (prior trust and expertise)\naffected a passenger's comfort in relying on an AV, preference for control,\nconfidence in the AV's ability, and explanation satisfaction. Errors negatively\naffected all outcomes. Surprisingly, despite identical driving, explanation\nerrors reduced ratings of the AV's driving ability. Severity and potential harm\namplified the negative impact of errors. Contextual harm and driving difficulty\ndirectly impacted outcome ratings and influenced the relationship between\nerrors and outcomes. Prior trust and expertise were positively associated with\noutcome ratings. Results emphasize the need for accurate, contextually\nadaptive, and personalized AV explanations to foster trust, reliance,\nsatisfaction, and confidence. We conclude with design, research, and deployment\nrecommendations for trustworthy AV explanation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations for autonomous vehicle (AV) decisions may build trust, however,\nexplanations can contain errors. In a simulated driving study (n = 232), we\ntested how AV explanation errors, driving context characteristics (perceived\nharm and driving difficulty), and personal traits (prior trust and expertise)\naffected a passenger's comfort in relying on an AV, preference for control,\nconfidence in the AV's ability, and explanation satisfaction. Errors negatively\naffected all outcomes. Surprisingly, despite identical driving, explanation\nerrors reduced ratings of the AV's driving ability. Severity and potential harm\namplified the negative impact of errors. Contextual harm and driving difficulty\ndirectly impacted outcome ratings and influenced the relationship between\nerrors and outcomes. Prior trust and expertise were positively associated with\noutcome ratings. Results emphasize the need for accurate, contextually\nadaptive, and personalized AV explanations to foster trust, reliance,\nsatisfaction, and confidence. We conclude with design, research, and deployment\nrecommendations for trustworthy AV explanation systems."
                },
                "authors": [
                    {
                        "name": "Robert Kaufman"
                    },
                    {
                        "name": "Aaron Broukhim"
                    },
                    {
                        "name": "David Kirsh"
                    },
                    {
                        "name": "Nadir Weibel"
                    }
                ],
                "author_detail": {
                    "name": "Nadir Weibel"
                },
                "author": "Nadir Weibel",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05703v1",
                "updated": "2024-09-09T15:14:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    14,
                    31,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T15:14:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    14,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "The Influence of Task and Group Disparities over Users' Attitudes Toward\n  Using Large Language Models for Psychotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Influence of Task and Group Disparities over Users' Attitudes Toward\n  Using Large Language Models for Psychotherapy"
                },
                "summary": "The population suffering from mental health disorders has kept increasing in\nrecent years. With the advancements in large language models (LLMs) in diverse\nfields, LLM-based psychotherapy has also attracted increasingly more attention.\nHowever, the factors influencing users' attitudes to LLM-based psychotherapy\nhave rarely been explored. As the first attempt, this paper investigated the\ninfluence of task and group disparities on user attitudes toward LLM-based\npsychotherapy tools. Utilizing the Technology Acceptance Model (TAM) and\nAutomation Acceptance Model (AAM), based on an online survey, we collected and\nanalyzed responses from 222 LLM-based psychotherapy users in mainland China.\nThe results revealed that group disparity (i.e., mental health conditions) can\ninfluence users' attitudes toward LLM tools. Further, one of the typical task\ndisparities, i.e., the privacy concern, was not found to have a significant\neffect on trust and usage intention. These findings can guide the design of\nfuture LLM-based psychotherapy services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The population suffering from mental health disorders has kept increasing in\nrecent years. With the advancements in large language models (LLMs) in diverse\nfields, LLM-based psychotherapy has also attracted increasingly more attention.\nHowever, the factors influencing users' attitudes to LLM-based psychotherapy\nhave rarely been explored. As the first attempt, this paper investigated the\ninfluence of task and group disparities on user attitudes toward LLM-based\npsychotherapy tools. Utilizing the Technology Acceptance Model (TAM) and\nAutomation Acceptance Model (AAM), based on an online survey, we collected and\nanalyzed responses from 222 LLM-based psychotherapy users in mainland China.\nThe results revealed that group disparity (i.e., mental health conditions) can\ninfluence users' attitudes toward LLM tools. Further, one of the typical task\ndisparities, i.e., the privacy concern, was not found to have a significant\neffect on trust and usage intention. These findings can guide the design of\nfuture LLM-based psychotherapy services."
                },
                "authors": [
                    {
                        "name": "Qihang He"
                    },
                    {
                        "name": "Jiyao Wang"
                    },
                    {
                        "name": "Dengbo He"
                    }
                ],
                "author_detail": {
                    "name": "Dengbo He"
                },
                "author": "Dengbo He",
                "arxiv_comment": "Accepted by HFES 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03727v3",
                "updated": "2024-09-09T15:04:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    15,
                    4,
                    15,
                    0,
                    253,
                    0
                ],
                "published": "2024-05-06T08:09:46Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    8,
                    9,
                    46,
                    0,
                    127,
                    0
                ],
                "title": "Large Language Models Synergize with Automated Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Synergize with Automated Machine Learning"
                },
                "summary": "Recently, program synthesis driven by large language models (LLMs) has become\nincreasingly popular. However, program synthesis for machine learning (ML)\ntasks still poses significant challenges. This paper explores a novel form of\nprogram synthesis, targeting ML programs, by combining LLMs and automated\nmachine learning (autoML). Specifically, our goal is to fully automate the\ngeneration and optimization of the code of the entire ML workflow, from data\npreparation to modeling and post-processing, utilizing only textual\ndescriptions of the ML tasks. To manage the length and diversity of ML\nprograms, we propose to break each ML program into smaller, manageable parts.\nEach part is generated separately by the LLM, with careful consideration of\ntheir compatibilities. To ensure compatibilities, we design a testing technique\nfor ML programs. Unlike traditional program synthesis, which typically relies\non binary evaluations (i.e., correct or incorrect), evaluating ML programs\nnecessitates more than just binary judgments. Our approach automates the\nnumerical evaluation and optimization of these programs, selecting the best\ncandidates through autoML techniques. In experiments across various ML tasks,\nour method outperforms existing methods in 10 out of 12 tasks for generating ML\nprograms. In addition, autoML significantly improves the performance of the\ngenerated ML programs. In experiments, given the textual task description, our\nmethod, Text-to-ML, generates the complete and optimized ML program in a fully\nautonomous process. The implementation of our method is available at\nhttps://github.com/JLX0/llm-automl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, program synthesis driven by large language models (LLMs) has become\nincreasingly popular. However, program synthesis for machine learning (ML)\ntasks still poses significant challenges. This paper explores a novel form of\nprogram synthesis, targeting ML programs, by combining LLMs and automated\nmachine learning (autoML). Specifically, our goal is to fully automate the\ngeneration and optimization of the code of the entire ML workflow, from data\npreparation to modeling and post-processing, utilizing only textual\ndescriptions of the ML tasks. To manage the length and diversity of ML\nprograms, we propose to break each ML program into smaller, manageable parts.\nEach part is generated separately by the LLM, with careful consideration of\ntheir compatibilities. To ensure compatibilities, we design a testing technique\nfor ML programs. Unlike traditional program synthesis, which typically relies\non binary evaluations (i.e., correct or incorrect), evaluating ML programs\nnecessitates more than just binary judgments. Our approach automates the\nnumerical evaluation and optimization of these programs, selecting the best\ncandidates through autoML techniques. In experiments across various ML tasks,\nour method outperforms existing methods in 10 out of 12 tasks for generating ML\nprograms. In addition, autoML significantly improves the performance of the\ngenerated ML programs. In experiments, given the textual task description, our\nmethod, Text-to-ML, generates the complete and optimized ML program in a fully\nautonomous process. The implementation of our method is available at\nhttps://github.com/JLX0/llm-automl."
                },
                "authors": [
                    {
                        "name": "Jinglue Xu"
                    },
                    {
                        "name": "Jialong Li"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Nagar Anthel Venkatesh Suryanarayanan"
                    },
                    {
                        "name": "Guoyuan Zhou"
                    },
                    {
                        "name": "Jia Guo"
                    },
                    {
                        "name": "Hitoshi Iba"
                    },
                    {
                        "name": "Kenji Tei"
                    }
                ],
                "author_detail": {
                    "name": "Kenji Tei"
                },
                "author": "Kenji Tei",
                "arxiv_comment": "published at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.11500v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.11500v4",
                "updated": "2024-09-09T14:52:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    52,
                    15,
                    0,
                    253,
                    0
                ],
                "published": "2023-09-20T17:59:32Z",
                "published_parsed": [
                    2023,
                    9,
                    20,
                    17,
                    59,
                    32,
                    2,
                    263,
                    0
                ],
                "title": "Auto-ACD: A Large-scale Dataset for Audio-Language Representation\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-ACD: A Large-scale Dataset for Audio-Language Representation\n  Learning"
                },
                "summary": "Recently, the AI community has made significant strides in developing\npowerful foundation models, driven by large-scale multimodal datasets. However,\nfor audio representation learning, existing datasets suffer from limitations in\nthe following aspects: insufficient volume, simplistic content, and arduous\ncollection procedures. To establish an audio dataset with high-quality\ncaptions, we propose an innovative, automatic approach leveraging multimodal\ninputs, such as video frames, audio streams. Specifically, we construct a\nlarge-scale, high-quality, audio-language dataset, named as Auto-ACD,\ncomprising over 1.5M audio-text pairs. We exploit a series of pre-trained\nmodels or APIs, to determine audio-visual synchronisation, generate image\ncaptions, object detection, or audio tags for specific videos. Subsequently, we\nemploy LLM to paraphrase a congruent caption for each audio, guided by the\nextracted multi-modality clues. To demonstrate the effectiveness of the\nproposed dataset, we train widely used models on our dataset and show\nperformance improvement on various downstream tasks, for example,\naudio-language retrieval, audio captioning, zero-shot classification. In\naddition, we establish a novel benchmark with environmental information and\nprovide a benchmark for audio-text tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the AI community has made significant strides in developing\npowerful foundation models, driven by large-scale multimodal datasets. However,\nfor audio representation learning, existing datasets suffer from limitations in\nthe following aspects: insufficient volume, simplistic content, and arduous\ncollection procedures. To establish an audio dataset with high-quality\ncaptions, we propose an innovative, automatic approach leveraging multimodal\ninputs, such as video frames, audio streams. Specifically, we construct a\nlarge-scale, high-quality, audio-language dataset, named as Auto-ACD,\ncomprising over 1.5M audio-text pairs. We exploit a series of pre-trained\nmodels or APIs, to determine audio-visual synchronisation, generate image\ncaptions, object detection, or audio tags for specific videos. Subsequently, we\nemploy LLM to paraphrase a congruent caption for each audio, guided by the\nextracted multi-modality clues. To demonstrate the effectiveness of the\nproposed dataset, we train widely used models on our dataset and show\nperformance improvement on various downstream tasks, for example,\naudio-language retrieval, audio captioning, zero-shot classification. In\naddition, we establish a novel benchmark with environmental information and\nprovide a benchmark for audio-text tasks."
                },
                "authors": [
                    {
                        "name": "Luoyi Sun"
                    },
                    {
                        "name": "Xuenan Xu"
                    },
                    {
                        "name": "Mengyue Wu"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "arxiv_comment": "Accepted by ACM MM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.11500v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.11500v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16528v3",
                "updated": "2024-09-09T14:31:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    14,
                    31,
                    26,
                    0,
                    253,
                    0
                ],
                "published": "2024-05-26T11:29:57Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    11,
                    29,
                    57,
                    6,
                    147,
                    0
                ],
                "title": "LoQT: Low-Rank Adapters for Quantized Pre-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoQT: Low-Rank Adapters for Quantized Pre-Training"
                },
                "summary": "Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training of large neural networks requires significant computational\nresources. Despite advances using low-rank adapters and quantization,\npretraining of models such as LLMs on consumer hardware has not been possible\nwithout model sharding, offloading during training, or per-layer gradient\nupdates. To address these limitations, we propose LoQT, a method for\nefficiently training quantized models. LoQT uses gradient-based tensor\nfactorization to initialize low-rank trainable weight matrices that are\nperiodically merged into quantized full-rank weight matrices. Our approach is\nsuitable for both pretraining and fine-tuning of models, which we demonstrate\nexperimentally for language modeling and downstream task adaptation. We find\nthat LoQT enables efficient training of models up to 7B parameters on a\nconsumer-grade 24GB GPU. We also demonstrate the feasibility of training a 13B\nparameter model using per-layer gradient updates on the same hardware."
                },
                "authors": [
                    {
                        "name": "Sebastian Loeschcke"
                    },
                    {
                        "name": "Mads Toftrup"
                    },
                    {
                        "name": "Michael J. Kastoryano"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Vésteinn Snæbjarnarson"
                    }
                ],
                "author_detail": {
                    "name": "Vésteinn Snæbjarnarson"
                },
                "author": "Vésteinn Snæbjarnarson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05627v1",
                "updated": "2024-09-09T13:59:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    59,
                    22,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T13:59:22Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    59,
                    22,
                    0,
                    253,
                    0
                ],
                "title": "ECG Biometric Authentication Using Self-Supervised Learning for IoT Edge\n  Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECG Biometric Authentication Using Self-Supervised Learning for IoT Edge\n  Sensors"
                },
                "summary": "Wearable Internet of Things (IoT) devices are gaining ground for continuous\nphysiological data acquisition and health monitoring. These physiological\nsignals can be used for security applications to achieve continuous\nauthentication and user convenience due to passive data acquisition. This paper\ninvestigates an electrocardiogram (ECG) based biometric user authentication\nsystem using features derived from the Convolutional Neural Network (CNN) and\nself-supervised contrastive learning. Contrastive learning enables us to use\nlarge unlabeled datasets to train the model and establish its generalizability.\nWe propose approaches enabling the CNN encoder to extract appropriate features\nthat distinguish the user from other subjects. When evaluated using the PTB ECG\ndatabase with 290 subjects, the proposed technique achieved an authentication\naccuracy of 99.15%. To test its generalizability, we applied the model to two\nnew datasets, the MIT-BIH Arrhythmia Database and the ECG-ID Database,\nachieving over 98.5% accuracy without any modifications. Furthermore, we show\nthat repeating the authentication step three times can increase accuracy to\nnearly 100% for both PTBDB and ECGIDDB. This paper also presents model\noptimizations for embedded device deployment, which makes the system more\nrelevant to real-world scenarios. To deploy our model in IoT edge sensors, we\noptimized the model complexity by applying quantization and pruning. The\noptimized model achieves 98.67% accuracy on PTBDB, with 0.48% accuracy loss and\n62.6% CPU cycles compared to the unoptimized model. An\naccuracy-vs-time-complexity tradeoff analysis is performed, and results are\npresented for different optimization levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable Internet of Things (IoT) devices are gaining ground for continuous\nphysiological data acquisition and health monitoring. These physiological\nsignals can be used for security applications to achieve continuous\nauthentication and user convenience due to passive data acquisition. This paper\ninvestigates an electrocardiogram (ECG) based biometric user authentication\nsystem using features derived from the Convolutional Neural Network (CNN) and\nself-supervised contrastive learning. Contrastive learning enables us to use\nlarge unlabeled datasets to train the model and establish its generalizability.\nWe propose approaches enabling the CNN encoder to extract appropriate features\nthat distinguish the user from other subjects. When evaluated using the PTB ECG\ndatabase with 290 subjects, the proposed technique achieved an authentication\naccuracy of 99.15%. To test its generalizability, we applied the model to two\nnew datasets, the MIT-BIH Arrhythmia Database and the ECG-ID Database,\nachieving over 98.5% accuracy without any modifications. Furthermore, we show\nthat repeating the authentication step three times can increase accuracy to\nnearly 100% for both PTBDB and ECGIDDB. This paper also presents model\noptimizations for embedded device deployment, which makes the system more\nrelevant to real-world scenarios. To deploy our model in IoT edge sensors, we\noptimized the model complexity by applying quantization and pruning. The\noptimized model achieves 98.67% accuracy on PTBDB, with 0.48% accuracy loss and\n62.6% CPU cycles compared to the unoptimized model. An\naccuracy-vs-time-complexity tradeoff analysis is performed, and results are\npresented for different optimization levels."
                },
                "authors": [
                    {
                        "name": "Guoxin Wang"
                    },
                    {
                        "name": "Shreejith Shanker"
                    },
                    {
                        "name": "Avishek Nag"
                    },
                    {
                        "name": "Yong Lian"
                    },
                    {
                        "name": "Deepu John"
                    }
                ],
                "author_detail": {
                    "name": "Deepu John"
                },
                "author": "Deepu John",
                "arxiv_doi": "10.1109/JBHI.2024.3455803",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JBHI.2024.3455803",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.05627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00369v3",
                "updated": "2024-09-09T13:50:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    50,
                    30,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-31T07:10:16Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    7,
                    10,
                    16,
                    5,
                    244,
                    0
                ],
                "title": "An Empirical Study on Information Extraction using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Information Extraction using Large Language Models"
                },
                "summary": "Human-like large language models (LLMs), especially the most powerful and\npopular ones in OpenAI's GPT family, have proven to be very helpful for many\nnatural language processing (NLP) related tasks. Therefore, various attempts\nhave been made to apply LLMs to information extraction (IE), which is a\nfundamental NLP task that involves extracting information from unstructured\nplain text. To demonstrate the latest representative progress in LLMs'\ninformation extraction ability, we assess the information extraction ability of\nGPT-4 (the latest version of GPT at the time of writing this paper) from four\nperspectives: Performance, Evaluation Criteria, Robustness, and Error Types.\nOur results suggest a visible performance gap between GPT-4 and\nstate-of-the-art (SOTA) IE methods. To alleviate this problem, considering the\nLLMs' human-like characteristics, we propose and analyze the effects of a\nseries of simple prompt-based methods, which can be generalized to other LLMs\nand NLP tasks. Rich experiments show our methods' effectiveness and some of\ntheir remaining issues in improving GPT-4's information extraction ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-like large language models (LLMs), especially the most powerful and\npopular ones in OpenAI's GPT family, have proven to be very helpful for many\nnatural language processing (NLP) related tasks. Therefore, various attempts\nhave been made to apply LLMs to information extraction (IE), which is a\nfundamental NLP task that involves extracting information from unstructured\nplain text. To demonstrate the latest representative progress in LLMs'\ninformation extraction ability, we assess the information extraction ability of\nGPT-4 (the latest version of GPT at the time of writing this paper) from four\nperspectives: Performance, Evaluation Criteria, Robustness, and Error Types.\nOur results suggest a visible performance gap between GPT-4 and\nstate-of-the-art (SOTA) IE methods. To alleviate this problem, considering the\nLLMs' human-like characteristics, we propose and analyze the effects of a\nseries of simple prompt-based methods, which can be generalized to other LLMs\nand NLP tasks. Rich experiments show our methods' effectiveness and some of\ntheir remaining issues in improving GPT-4's information extraction ability."
                },
                "authors": [
                    {
                        "name": "Ridong Han"
                    },
                    {
                        "name": "Chaohao Yang"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Prayag Tiwari"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Lu Liu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "arxiv_comment": "Need to submit this paper as the replacement of arXiv:2305.14450",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v2",
                "updated": "2024-09-10T02:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    2,
                    1,
                    43,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Moving towards Next-Gen RAG Via Memory-Inspired Knowledge\n  Discovery"
                },
                "summary": "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access\nexternal databases, thereby enhancing the generation quality of large language\nmodels (LLMs) through optimized context. However, the existing retrieval\nmethods are constrained inherently, as they can only perform relevance matching\nbetween explicitly stated queries and well-formed knowledge, but unable to\nhandle tasks involving ambiguous information needs or unstructured knowledge.\nConsequently, existing RAG systems are primarily effective for straightforward\nquestion-answering tasks. In this work, we propose MemoRAG, a novel\nretrieval-augmented generation paradigm empowered by long-term memory. MemoRAG\nadopts a dual-system architecture. On the one hand, it employs a light but\nlong-range LLM to form the global memory of database. Once a task is presented,\nit generates draft answers, cluing the retrieval tools to locate useful\ninformation within the database. On the other hand, it leverages an expensive\nbut expressive LLM, which generates the ultimate answer based on the retrieved\ninformation. Building on this general framework, we further optimize MemoRAG's\nperformance by enhancing its cluing mechanism and memorization capacity. In our\nexperiment, MemoRAG achieves superior performance across a variety of\nevaluation tasks, including both complex ones where conventional RAG fails and\nstraightforward ones where RAG is commonly applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) leverages retrieval tools to access\nexternal databases, thereby enhancing the generation quality of large language\nmodels (LLMs) through optimized context. However, the existing retrieval\nmethods are constrained inherently, as they can only perform relevance matching\nbetween explicitly stated queries and well-formed knowledge, but unable to\nhandle tasks involving ambiguous information needs or unstructured knowledge.\nConsequently, existing RAG systems are primarily effective for straightforward\nquestion-answering tasks. In this work, we propose MemoRAG, a novel\nretrieval-augmented generation paradigm empowered by long-term memory. MemoRAG\nadopts a dual-system architecture. On the one hand, it employs a light but\nlong-range LLM to form the global memory of database. Once a task is presented,\nit generates draft answers, cluing the retrieval tools to locate useful\ninformation within the database. On the other hand, it leverages an expensive\nbut expressive LLM, which generates the ultimate answer based on the retrieved\ninformation. Building on this general framework, we further optimize MemoRAG's\nperformance by enhancing its cluing mechanism and memorization capacity. In our\nexperiment, MemoRAG achieves superior performance across a variety of\nevaluation tasks, including both complex ones where conventional RAG fails and\nstraightforward ones where RAG is commonly applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Technical Report. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12325v2",
                "updated": "2024-09-09T13:10:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    10,
                    50,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-22T12:00:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    0,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators"
                },
                "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Hallucination Mitigation in LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11944v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11944v3",
                "updated": "2024-09-09T12:38:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    38,
                    11,
                    0,
                    253,
                    0
                ],
                "published": "2024-01-22T13:34:34Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    13,
                    34,
                    34,
                    0,
                    22,
                    0
                ],
                "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark"
                },
                "summary": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU. CMMMU includes\n12k manually collected multimodal questions from college exams, quizzes, and\ntextbooks, covering six core disciplines: Art & Design, Business, Science,\nHealth & Medicine, Humanities & Social Science, and Tech & Engineering, like\nits companion, MMMU. These questions span 30 subjects and comprise 39 highly\nheterogeneous image types, such as charts, diagrams, maps, tables, music\nsheets, and chemical structures. CMMMU focuses on complex perception and\nreasoning with domain-specific knowledge in the Chinese context. We evaluate 11\nopen-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves\naccuracies of 42%, indicating a large space for improvement. CMMMU will boost\nthe community to build the next-generation LMMs towards expert artificial\nintelligence and promote the democratization of LMMs by providing diverse\nlanguage contexts."
                },
                "authors": [
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "Yiming Liang"
                    },
                    {
                        "name": "Tongxu Luo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Kang Zhu"
                    },
                    {
                        "name": "Yuyang Cheng"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Yudong Liu"
                    },
                    {
                        "name": "Yu-Hsuan Tsai"
                    },
                    {
                        "name": "Fengji Zhang"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11944v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11944v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11654v2",
                "updated": "2024-09-09T12:36:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    36,
                    29,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-16T12:21:29Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    12,
                    21,
                    29,
                    1,
                    198,
                    0
                ],
                "title": "R-SFLLM: Jamming Resilient Framework for Split Federated Learning with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-SFLLM: Jamming Resilient Framework for Split Federated Learning with\n  Large Language Models"
                },
                "summary": "Split federated learning (SFL) is a compute-efficient paradigm in distributed\nmachine learning (ML), where components of large ML models are outsourced to\nremote servers. A significant challenge in SFL, particularly when deployed over\nwireless channels, is the susceptibility of transmitted model parameters to\nadversarial jamming that could jeopardize the learning process. This is\nparticularly pronounced for word embedding parameters in large language models\n(LLMs), which are crucial for language understanding. In this paper, rigorous\ninsights are provided into the influence of jamming LLM word embeddings in SFL\nby deriving an expression for the ML training loss divergence and showing that\nit is upper-bounded by the mean squared error (MSE). Based on this analysis, a\nphysical layer framework is developed for resilient SFL with LLMs (R-SFLLM)\nover wireless networks. R-SFLLM leverages wireless sensing data to gather\ninformation on the jamming directions-of-arrival (DoAs) for the purpose of\ndevising a novel, sensing-assisted anti-jamming strategy while jointly\noptimizing beamforming, user scheduling, and resource allocation. Extensive\nexperiments using BERT and RoBERTa models demonstrate R-SFLLM's effectiveness,\nachieving close-to-baseline performance across various natural language\nprocessing (NLP) tasks and datasets. The proposed methodology further\nintroduces an adversarial training component, where controlled noise exposure\nsignificantly enhances the LLM's resilience to perturbed parameters during\ntraining. The results show that more noise-sensitive models, such as RoBERTa,\nbenefit from this feature, especially when resource allocation is unfair. It is\nalso shown that worst-case jamming in particular translates into worst-case\nmodel outcomes, thereby necessitating the need for jamming-resilient SFL\nprotocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split federated learning (SFL) is a compute-efficient paradigm in distributed\nmachine learning (ML), where components of large ML models are outsourced to\nremote servers. A significant challenge in SFL, particularly when deployed over\nwireless channels, is the susceptibility of transmitted model parameters to\nadversarial jamming that could jeopardize the learning process. This is\nparticularly pronounced for word embedding parameters in large language models\n(LLMs), which are crucial for language understanding. In this paper, rigorous\ninsights are provided into the influence of jamming LLM word embeddings in SFL\nby deriving an expression for the ML training loss divergence and showing that\nit is upper-bounded by the mean squared error (MSE). Based on this analysis, a\nphysical layer framework is developed for resilient SFL with LLMs (R-SFLLM)\nover wireless networks. R-SFLLM leverages wireless sensing data to gather\ninformation on the jamming directions-of-arrival (DoAs) for the purpose of\ndevising a novel, sensing-assisted anti-jamming strategy while jointly\noptimizing beamforming, user scheduling, and resource allocation. Extensive\nexperiments using BERT and RoBERTa models demonstrate R-SFLLM's effectiveness,\nachieving close-to-baseline performance across various natural language\nprocessing (NLP) tasks and datasets. The proposed methodology further\nintroduces an adversarial training component, where controlled noise exposure\nsignificantly enhances the LLM's resilience to perturbed parameters during\ntraining. The results show that more noise-sensitive models, such as RoBERTa,\nbenefit from this feature, especially when resource allocation is unfair. It is\nalso shown that worst-case jamming in particular translates into worst-case\nmodel outcomes, thereby necessitating the need for jamming-resilient SFL\nprotocols."
                },
                "authors": [
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Vlad C. Andrei"
                    },
                    {
                        "name": "Xinyang Li"
                    },
                    {
                        "name": "Ullrich J. Mönich"
                    },
                    {
                        "name": "Holger Boche"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05559v1",
                "updated": "2024-09-09T12:30:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    30,
                    43,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T12:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    30,
                    43,
                    0,
                    253,
                    0
                ],
                "title": "CauseJudger: Identifying the Cause with LLMs for Abductive Logical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CauseJudger: Identifying the Cause with LLMs for Abductive Logical\n  Reasoning"
                },
                "summary": "Large language models (LLMs) have been utilized in solving diverse reasoning\ntasks, encompassing common sense, arithmetic and deduction tasks. However, with\ndifficulties of reversing thinking patterns and irrelevant premises, how to\ndetermine the authenticity of the cause in abductive logical reasoning remains\nunderexplored. Inspired by hypothesis and verification method and\nidentification of irrelevant information in human thinking process, we propose\na new framework for LLMs abductive logical reasoning called CauseJudger (CJ),\nwhich identifies the authenticity of possible cause by transforming thinking\nfrom reverse to forward and removing irrelevant information. In addition, we\nconstruct an abductive logical reasoning dataset for decision task called\nCauseLogics, which contains 200,000 tasks of varying reasoning lengths. Our\nexperiments show the efficiency of CJ with overall experiments and ablation\nexperiments as well as case studies on our dataset and reconstructed public\ndataset. Notably, CJ's implementation is efficient, requiring only two calls to\nLLM. Its impact is profound: when using gpt-3.5, CJ achieves a maximum\ncorrectness improvement of 41% compared to Zero-Shot-CoT. Moreover, with gpt-4,\nCJ attains an accuracy exceeding 90% across all datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been utilized in solving diverse reasoning\ntasks, encompassing common sense, arithmetic and deduction tasks. However, with\ndifficulties of reversing thinking patterns and irrelevant premises, how to\ndetermine the authenticity of the cause in abductive logical reasoning remains\nunderexplored. Inspired by hypothesis and verification method and\nidentification of irrelevant information in human thinking process, we propose\na new framework for LLMs abductive logical reasoning called CauseJudger (CJ),\nwhich identifies the authenticity of possible cause by transforming thinking\nfrom reverse to forward and removing irrelevant information. In addition, we\nconstruct an abductive logical reasoning dataset for decision task called\nCauseLogics, which contains 200,000 tasks of varying reasoning lengths. Our\nexperiments show the efficiency of CJ with overall experiments and ablation\nexperiments as well as case studies on our dataset and reconstructed public\ndataset. Notably, CJ's implementation is efficient, requiring only two calls to\nLLM. Its impact is profound: when using gpt-3.5, CJ achieves a maximum\ncorrectness improvement of 41% compared to Zero-Shot-CoT. Moreover, with gpt-4,\nCJ attains an accuracy exceeding 90% across all datasets."
                },
                "authors": [
                    {
                        "name": "Jinwei He"
                    },
                    {
                        "name": "Feng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lu"
                },
                "author": "Feng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05556v1",
                "updated": "2024-09-09T12:25:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    25,
                    10,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T12:25:10Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    25,
                    10,
                    0,
                    253,
                    0
                ],
                "title": "SciAgents: Automating scientific discovery through multi-agent\n  intelligent graph reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciAgents: Automating scientific discovery through multi-agent\n  intelligent graph reasoning"
                },
                "summary": "A key challenge in artificial intelligence is the creation of systems capable\nof autonomously advancing scientific understanding by exploring novel domains,\nidentifying complex patterns, and uncovering previously unseen connections in\nvast scientific data. In this work, we present SciAgents, an approach that\nleverages three core concepts: (1) the use of large-scale ontological knowledge\ngraphs to organize and interconnect diverse scientific concepts, (2) a suite of\nlarge language models (LLMs) and data retrieval tools, and (3) multi-agent\nsystems with in-situ learning capabilities. Applied to biologically inspired\nmaterials, SciAgents reveals hidden interdisciplinary relationships that were\npreviously considered unrelated, achieving a scale, precision, and exploratory\npower that surpasses traditional human-driven research methods. The framework\nautonomously generates and refines research hypotheses, elucidating underlying\nmechanisms, design principles, and unexpected material properties. By\nintegrating these capabilities in a modular fashion, the intelligent system\nyields material discoveries, critique and improve existing hypotheses, retrieve\nup-to-date data about existing research, and highlights their strengths and\nlimitations. Our case studies demonstrate scalable capabilities to combine\ngenerative AI, ontological representations, and multi-agent modeling,\nharnessing a `swarm of intelligence' similar to biological systems. This\nprovides new avenues for materials discovery and accelerates the development of\nadvanced materials by unlocking Nature's design principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in artificial intelligence is the creation of systems capable\nof autonomously advancing scientific understanding by exploring novel domains,\nidentifying complex patterns, and uncovering previously unseen connections in\nvast scientific data. In this work, we present SciAgents, an approach that\nleverages three core concepts: (1) the use of large-scale ontological knowledge\ngraphs to organize and interconnect diverse scientific concepts, (2) a suite of\nlarge language models (LLMs) and data retrieval tools, and (3) multi-agent\nsystems with in-situ learning capabilities. Applied to biologically inspired\nmaterials, SciAgents reveals hidden interdisciplinary relationships that were\npreviously considered unrelated, achieving a scale, precision, and exploratory\npower that surpasses traditional human-driven research methods. The framework\nautonomously generates and refines research hypotheses, elucidating underlying\nmechanisms, design principles, and unexpected material properties. By\nintegrating these capabilities in a modular fashion, the intelligent system\nyields material discoveries, critique and improve existing hypotheses, retrieve\nup-to-date data about existing research, and highlights their strengths and\nlimitations. Our case studies demonstrate scalable capabilities to combine\ngenerative AI, ontological representations, and multi-agent modeling,\nharnessing a `swarm of intelligence' similar to biological systems. This\nprovides new avenues for materials discovery and accelerates the development of\nadvanced materials by unlocking Nature's design principles."
                },
                "authors": [
                    {
                        "name": "Alireza Ghafarollahi"
                    },
                    {
                        "name": "Markus J. Buehler"
                    }
                ],
                "author_detail": {
                    "name": "Markus J. Buehler"
                },
                "author": "Markus J. Buehler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02202v2",
                "updated": "2024-09-09T12:04:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    12,
                    4,
                    27,
                    0,
                    253,
                    0
                ],
                "published": "2024-06-04T10:57:59Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    10,
                    57,
                    59,
                    1,
                    156,
                    0
                ],
                "title": "No Captions, No Problem: Captionless 3D-CLIP Alignment with Hard\n  Negatives via CLIP Knowledge and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Captions, No Problem: Captionless 3D-CLIP Alignment with Hard\n  Negatives via CLIP Knowledge and LLMs"
                },
                "summary": "In this study, we explore an alternative approach to enhance contrastive\ntext-image-3D alignment in the absence of textual descriptions for 3D objects.\nWe introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP\nknowledge about textual and 2D data to compute the neural perceived similarity\nbetween two 3D samples. We employ the proposed methods to mine 3D hard\nnegatives, establishing a multimodal contrastive pipeline with hard negative\nweighting via a custom loss function. We train on different configurations of\nthe proposed hard negative mining approach, and we evaluate the accuracy of our\nmodels in 3D classification and on the cross-modal retrieval benchmark, testing\nimage-to-shape and shape-to-image retrieval. Results demonstrate that our\napproach, even without explicit text alignment, achieves comparable or superior\nperformance on zero-shot and standard 3D classification, while significantly\nimproving both image-to-shape and shape-to-image retrieval compared to previous\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we explore an alternative approach to enhance contrastive\ntext-image-3D alignment in the absence of textual descriptions for 3D objects.\nWe introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP\nknowledge about textual and 2D data to compute the neural perceived similarity\nbetween two 3D samples. We employ the proposed methods to mine 3D hard\nnegatives, establishing a multimodal contrastive pipeline with hard negative\nweighting via a custom loss function. We train on different configurations of\nthe proposed hard negative mining approach, and we evaluate the accuracy of our\nmodels in 3D classification and on the cross-modal retrieval benchmark, testing\nimage-to-shape and shape-to-image retrieval. Results demonstrate that our\napproach, even without explicit text alignment, achieves comparable or superior\nperformance on zero-shot and standard 3D classification, while significantly\nimproving both image-to-shape and shape-to-image retrieval compared to previous\nmethods."
                },
                "authors": [
                    {
                        "name": "Cristian Sbrolli"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "to be published in BMVC 2024 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06227v2",
                "updated": "2024-09-09T11:38:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    38,
                    24,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-06T14:49:46Z",
                "published_parsed": [
                    2024,
                    7,
                    6,
                    14,
                    49,
                    46,
                    5,
                    188,
                    0
                ],
                "title": "Communication and Control Co-Design in 6G: Sequential Decision-Making\n  with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication and Control Co-Design in 6G: Sequential Decision-Making\n  with LLMs"
                },
                "summary": "This article investigates a control system within the context of\nsix-generation wireless networks. The control performance optimization\nconfronts the technical challenges that arise from the intricate interactions\nbetween communication and control sub-systems, asking for a co-design.\nAccounting for the system dynamics, we formulate the sequential co-design\ndecision-makings of communication and control over the discrete time horizon as\na Markov decision process, for which a practical offline learning framework is\nproposed. Our proposed framework integrates large language models into the\nelements of reinforcement learning. We present a case study on the age of\nsemantics-aware communication and control co-design to showcase the potentials\nfrom our proposed learning framework. Furthermore, we discuss the open issues\nremaining to make our proposed offline learning framework feasible for\nreal-world implementations, and highlight the research directions for future\nexplorations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article investigates a control system within the context of\nsix-generation wireless networks. The control performance optimization\nconfronts the technical challenges that arise from the intricate interactions\nbetween communication and control sub-systems, asking for a co-design.\nAccounting for the system dynamics, we formulate the sequential co-design\ndecision-makings of communication and control over the discrete time horizon as\na Markov decision process, for which a practical offline learning framework is\nproposed. Our proposed framework integrates large language models into the\nelements of reinforcement learning. We present a case study on the age of\nsemantics-aware communication and control co-design to showcase the potentials\nfrom our proposed learning framework. Furthermore, we discuss the open issues\nremaining to make our proposed offline learning framework feasible for\nreal-world implementations, and highlight the research directions for future\nexplorations."
                },
                "authors": [
                    {
                        "name": "Xianfu Chen"
                    },
                    {
                        "name": "Celimuge Wu"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yusheng Ji"
                    },
                    {
                        "name": "Tsutomu Yoshinaga"
                    },
                    {
                        "name": "Qiang Ni"
                    },
                    {
                        "name": "Charilaos C. Zarakovitis"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04073v2",
                "updated": "2024-09-09T11:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    33,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-06T07:29:01Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    7,
                    29,
                    1,
                    4,
                    250,
                    0
                ],
                "title": "AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language\n  Model"
                },
                "summary": "Entity matching (EM) is the problem of determining whether two records refer\nto same real-world entity, which is crucial in data integration, e.g., for\nproduct catalogs or address databases. A major drawback of many EM approaches\nis their dependence on labelled examples. We thus focus on the challenging\nsetting of zero-shot entity matching where no labelled examples are available\nfor an unseen target dataset. Recently, large language models (LLMs) have shown\npromising results for zero-shot EM, but their low throughput and high\ndeployment cost limit their applicability and scalability.\n  We revisit the zero-shot EM problem with AnyMatch, a small language model\nfine-tuned in a transfer learning setup. We propose several novel data\nselection techniques to generate fine-tuning data for our model, e.g., by\nselecting difficult pairs to match via an AutoML filter, by generating\nadditional attribute-level examples, and by controlling label imbalance in the\ndata.\n  We conduct an extensive evaluation of the prediction quality and deployment\ncost of our model, in a comparison to thirteen baselines on nine benchmark\ndatasets. We find that AnyMatch provides competitive prediction quality despite\nits small parameter size: it achieves the second-highest F1 score overall, and\noutperforms several other approaches that employ models with hundreds of\nbillions of parameters. Furthermore, our approach exhibits major cost benefits:\nthe average prediction quality of AnyMatch is within 4.4% of the\nstate-of-the-art method MatchGPT with the proprietary trillion-parameter model\nGPT-4, yet AnyMatch requires four orders of magnitude less parameters and\nincurs a 3,899 times lower inference cost (in dollars per 1,000 tokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity matching (EM) is the problem of determining whether two records refer\nto same real-world entity, which is crucial in data integration, e.g., for\nproduct catalogs or address databases. A major drawback of many EM approaches\nis their dependence on labelled examples. We thus focus on the challenging\nsetting of zero-shot entity matching where no labelled examples are available\nfor an unseen target dataset. Recently, large language models (LLMs) have shown\npromising results for zero-shot EM, but their low throughput and high\ndeployment cost limit their applicability and scalability.\n  We revisit the zero-shot EM problem with AnyMatch, a small language model\nfine-tuned in a transfer learning setup. We propose several novel data\nselection techniques to generate fine-tuning data for our model, e.g., by\nselecting difficult pairs to match via an AutoML filter, by generating\nadditional attribute-level examples, and by controlling label imbalance in the\ndata.\n  We conduct an extensive evaluation of the prediction quality and deployment\ncost of our model, in a comparison to thirteen baselines on nine benchmark\ndatasets. We find that AnyMatch provides competitive prediction quality despite\nits small parameter size: it achieves the second-highest F1 score overall, and\noutperforms several other approaches that employ models with hundreds of\nbillions of parameters. Furthermore, our approach exhibits major cost benefits:\nthe average prediction quality of AnyMatch is within 4.4% of the\nstate-of-the-art method MatchGPT with the proprietary trillion-parameter model\nGPT-4, yet AnyMatch requires four orders of magnitude less parameters and\nincurs a 3,899 times lower inference cost (in dollars per 1,000 tokens)."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Paul Groth"
                    },
                    {
                        "name": "Iacer Calixto"
                    },
                    {
                        "name": "Sebastian Schelter"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Schelter"
                },
                "author": "Sebastian Schelter",
                "arxiv_comment": "12 pages excluding references, 3 figures, and 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05521v1",
                "updated": "2024-09-09T11:28:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    28,
                    2,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T11:28:02Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    28,
                    2,
                    0,
                    253,
                    0
                ],
                "title": "Harmonic Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmonic Reasoning in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are becoming very popular and are used for many\ndifferent purposes, including creative tasks in the arts. However, these models\nsometimes have trouble with specific reasoning tasks, especially those that\ninvolve logical thinking and counting. This paper looks at how well LLMs\nunderstand and reason when dealing with musical tasks like figuring out notes\nfrom intervals and identifying chords and scales. We tested GPT-3.5 and GPT-4o\nto see how they handle these tasks. Our results show that while LLMs do well\nwith note intervals, they struggle with more complicated tasks like recognizing\nchords and scales. This points out clear limits in current LLM abilities and\nshows where we need to make them better, which could help improve how they\nthink and work in both artistic and other complex areas. We also provide an\nautomatically generated benchmark data set for the described tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are becoming very popular and are used for many\ndifferent purposes, including creative tasks in the arts. However, these models\nsometimes have trouble with specific reasoning tasks, especially those that\ninvolve logical thinking and counting. This paper looks at how well LLMs\nunderstand and reason when dealing with musical tasks like figuring out notes\nfrom intervals and identifying chords and scales. We tested GPT-3.5 and GPT-4o\nto see how they handle these tasks. Our results show that while LLMs do well\nwith note intervals, they struggle with more complicated tasks like recognizing\nchords and scales. This points out clear limits in current LLM abilities and\nshows where we need to make them better, which could help improve how they\nthink and work in both artistic and other complex areas. We also provide an\nautomatically generated benchmark data set for the described tasks."
                },
                "authors": [
                    {
                        "name": "Anna Kruspe"
                    }
                ],
                "author_detail": {
                    "name": "Anna Kruspe"
                },
                "author": "Anna Kruspe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05059v2",
                "updated": "2024-09-09T11:19:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    19,
                    26,
                    0,
                    253,
                    0
                ],
                "published": "2024-05-08T13:54:16Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    13,
                    54,
                    16,
                    2,
                    129,
                    0
                ],
                "title": "G-Loc: Tightly-coupled Graph Localization with Prior Topo-metric\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Loc: Tightly-coupled Graph Localization with Prior Topo-metric\n  Information"
                },
                "summary": "Localization in already mapped environments is a critical component in many\nrobotics and automotive applications, where previously acquired information can\nbe exploited along with sensor fusion to provide robust and accurate\nlocalization estimates. In this work, we offer a new perspective on map-based\nlocalization by reusing prior topological and metric information. Thus, we\nreformulate this long-studied problem to go beyond the mere use of metric maps.\nOur framework seamlessly integrates LiDAR, inertial and GNSS measurements, and\ncloud-to-map registrations in a sliding window graph fashion, which allows to\naccommodate the uncertainty of each observation. The modularity of our\nframework allows it to work with different sensor configurations (e.g., LiDAR\nresolutions, GNSS denial) and environmental conditions (e.g., mapless regions,\nlarge environments). We have conducted several validation experiments,\nincluding the deployment in a real-world automotive application, demonstrating\nthe accuracy, efficiency, and versatility of our system in online localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localization in already mapped environments is a critical component in many\nrobotics and automotive applications, where previously acquired information can\nbe exploited along with sensor fusion to provide robust and accurate\nlocalization estimates. In this work, we offer a new perspective on map-based\nlocalization by reusing prior topological and metric information. Thus, we\nreformulate this long-studied problem to go beyond the mere use of metric maps.\nOur framework seamlessly integrates LiDAR, inertial and GNSS measurements, and\ncloud-to-map registrations in a sliding window graph fashion, which allows to\naccommodate the uncertainty of each observation. The modularity of our\nframework allows it to work with different sensor configurations (e.g., LiDAR\nresolutions, GNSS denial) and environmental conditions (e.g., mapless regions,\nlarge environments). We have conducted several validation experiments,\nincluding the deployment in a real-world automotive application, demonstrating\nthe accuracy, efficiency, and versatility of our system in online localization."
                },
                "authors": [
                    {
                        "name": "Lorenzo Montano-Oliván"
                    },
                    {
                        "name": "Julio A. Placed"
                    },
                    {
                        "name": "Luis Montano"
                    },
                    {
                        "name": "María T. Lázaro"
                    }
                ],
                "author_detail": {
                    "name": "María T. Lázaro"
                },
                "author": "María T. Lázaro",
                "arxiv_doi": "10.1109/LRA.2024.3457383",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3457383",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11381v2",
                "updated": "2024-09-09T11:18:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    18,
                    16,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-21T07:20:48Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    20,
                    48,
                    2,
                    234,
                    0
                ],
                "title": "RAGLAB: A Modular and Research-Oriented Unified Framework for\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAGLAB: A Modular and Research-Oriented Unified Framework for\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) demonstrate human-level capabilities in\ndialogue, reasoning, and knowledge retention. However, even the most advanced\nLLMs face challenges such as hallucinations and real-time updating of their\nknowledge. Current research addresses this bottleneck by equipping LLMs with\nexternal knowledge, a technique known as Retrieval Augmented Generation (RAG).\nHowever, two key issues constrained the development of RAG. First, there is a\ngrowing lack of comprehensive and fair comparisons between novel RAG\nalgorithms. Second, open-source tools such as LlamaIndex and LangChain employ\nhigh-level abstractions, which results in a lack of transparency and limits the\nability to develop novel algorithms and evaluation metrics. To close this gap,\nwe introduce RAGLAB, a modular and research-oriented open-source library.\nRAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem\nfor investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair\ncomparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers\ncan efficiently compare the performance of various algorithms and develop novel\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate human-level capabilities in\ndialogue, reasoning, and knowledge retention. However, even the most advanced\nLLMs face challenges such as hallucinations and real-time updating of their\nknowledge. Current research addresses this bottleneck by equipping LLMs with\nexternal knowledge, a technique known as Retrieval Augmented Generation (RAG).\nHowever, two key issues constrained the development of RAG. First, there is a\ngrowing lack of comprehensive and fair comparisons between novel RAG\nalgorithms. Second, open-source tools such as LlamaIndex and LangChain employ\nhigh-level abstractions, which results in a lack of transparency and limits the\nability to develop novel algorithms and evaluation metrics. To close this gap,\nwe introduce RAGLAB, a modular and research-oriented open-source library.\nRAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem\nfor investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair\ncomparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers\ncan efficiently compare the performance of various algorithms and develop novel\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Xuanwang Zhang"
                    },
                    {
                        "name": "Yunze Song"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Shuyun Tang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Zhengran Zeng"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Wenyuan Xu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Xinyu Dai"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05511v1",
                "updated": "2024-09-09T11:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    9,
                    28,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T11:09:28Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    11,
                    9,
                    28,
                    0,
                    253,
                    0
                ],
                "title": "Enhancing Critical Thinking in Education by means of a Socratic Chatbot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Critical Thinking in Education by means of a Socratic Chatbot"
                },
                "summary": "While large language models (LLMs) are increasingly playing a pivotal role in\neducation by providing instantaneous, adaptive responses, their potential to\npromote critical thinking remains understudied. In this paper, we fill such a\ngap and present an innovative educational chatbot designed to foster critical\nthinking through Socratic questioning. Unlike traditional intelligent tutoring\nsystems, including educational chatbots, that tend to offer direct answers, the\nproposed Socratic tutor encourages students to explore various perspectives and\nengage in self-reflection by posing structured, thought-provoking questions.\nOur Socratic questioning is implemented by fine and prompt-tuning the\nopen-source pretrained LLM with a specialized dataset that stimulates critical\nthinking and offers multiple viewpoints. In an effort to democratize access and\nto protect the students' privacy, the proposed tutor is based on small LLMs\n(Llama2 7B and 13B-parameter models) that are able to run locally on\noff-the-shelf hardware. We validate our approach in a battery of experiments\nconsisting of interactions between a simulated student and the chatbot to\nevaluate its effectiveness in enhancing critical thinking skills. Results\nindicate that the Socratic tutor supports the development of reflection and\ncritical thinking significantly better than standard chatbots. Our approach\nopens the door for improving educational outcomes by cultivating active\nlearning and encouraging intellectual autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly playing a pivotal role in\neducation by providing instantaneous, adaptive responses, their potential to\npromote critical thinking remains understudied. In this paper, we fill such a\ngap and present an innovative educational chatbot designed to foster critical\nthinking through Socratic questioning. Unlike traditional intelligent tutoring\nsystems, including educational chatbots, that tend to offer direct answers, the\nproposed Socratic tutor encourages students to explore various perspectives and\nengage in self-reflection by posing structured, thought-provoking questions.\nOur Socratic questioning is implemented by fine and prompt-tuning the\nopen-source pretrained LLM with a specialized dataset that stimulates critical\nthinking and offers multiple viewpoints. In an effort to democratize access and\nto protect the students' privacy, the proposed tutor is based on small LLMs\n(Llama2 7B and 13B-parameter models) that are able to run locally on\noff-the-shelf hardware. We validate our approach in a battery of experiments\nconsisting of interactions between a simulated student and the chatbot to\nevaluate its effectiveness in enhancing critical thinking skills. Results\nindicate that the Socratic tutor supports the development of reflection and\ncritical thinking significantly better than standard chatbots. Our approach\nopens the door for improving educational outcomes by cultivating active\nlearning and encouraging intellectual autonomy."
                },
                "authors": [
                    {
                        "name": "Lucile Favero"
                    },
                    {
                        "name": "Juan Antonio Pérez-Ortiz"
                    },
                    {
                        "name": "Tanja Käser"
                    },
                    {
                        "name": "Nuria Oliver"
                    }
                ],
                "author_detail": {
                    "name": "Nuria Oliver"
                },
                "author": "Nuria Oliver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05493v1",
                "updated": "2024-09-09T10:43:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    43,
                    18,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T10:43:18Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    43,
                    18,
                    0,
                    253,
                    0
                ],
                "title": "DexDiff: Towards Extrinsic Dexterity Manipulation of Ungraspable Objects\n  in Unrestricted Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DexDiff: Towards Extrinsic Dexterity Manipulation of Ungraspable Objects\n  in Unrestricted Environments"
                },
                "summary": "Grasping large and flat objects (e.g. a book or a pan) is often regarded as\nan ungraspable task, which poses significant challenges due to the unreachable\ngrasping poses. Previous works leverage Extrinsic Dexterity like walls or table\nedges to grasp such objects. However, they are limited to task-specific\npolicies and lack task planning to find pre-grasp conditions. This makes it\ndifficult to adapt to various environments and extrinsic dexterity constraints.\nTherefore, we present DexDiff, a robust robotic manipulation method for\nlong-horizon planning with extrinsic dexterity. Specifically, we utilize a\nvision-language model (VLM) to perceive the environmental state and generate\nhigh-level task plans, followed by a goal-conditioned action diffusion (GCAD)\nmodel to predict the sequence of low-level actions. This model learns the\nlow-level policy from offline data with the cumulative reward guided by\nhigh-level planning as the goal condition, which allows for improved prediction\nof robot actions. Experimental results demonstrate that our method not only\neffectively performs ungraspable tasks but also generalizes to previously\nunseen objects. It outperforms baselines by a 47% higher success rate in\nsimulation and facilitates efficient deployment and manipulation in real-world\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasping large and flat objects (e.g. a book or a pan) is often regarded as\nan ungraspable task, which poses significant challenges due to the unreachable\ngrasping poses. Previous works leverage Extrinsic Dexterity like walls or table\nedges to grasp such objects. However, they are limited to task-specific\npolicies and lack task planning to find pre-grasp conditions. This makes it\ndifficult to adapt to various environments and extrinsic dexterity constraints.\nTherefore, we present DexDiff, a robust robotic manipulation method for\nlong-horizon planning with extrinsic dexterity. Specifically, we utilize a\nvision-language model (VLM) to perceive the environmental state and generate\nhigh-level task plans, followed by a goal-conditioned action diffusion (GCAD)\nmodel to predict the sequence of low-level actions. This model learns the\nlow-level policy from offline data with the cumulative reward guided by\nhigh-level planning as the goal condition, which allows for improved prediction\nof robot actions. Experimental results demonstrate that our method not only\neffectively performs ungraspable tasks but also generalizes to previously\nunseen objects. It outperforms baselines by a 47% higher success rate in\nsimulation and facilitates efficient deployment and manipulation in real-world\nscenarios."
                },
                "authors": [
                    {
                        "name": "Chengzhong Ma"
                    },
                    {
                        "name": "Houxue Yang"
                    },
                    {
                        "name": "Hanbo Zhang"
                    },
                    {
                        "name": "Zeyang Liu"
                    },
                    {
                        "name": "Chao Zhao"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Xuguang Lan"
                    },
                    {
                        "name": "Nanning Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Nanning Zheng"
                },
                "author": "Nanning Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05480v1",
                "updated": "2024-09-09T10:14:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    14,
                    30,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T10:14:30Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    14,
                    30,
                    0,
                    253,
                    0
                ],
                "title": "Adaptive Multi-Layer Deployment for A Digital Twin Empowered\n  Satellite-Terrestrial Integrated Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Layer Deployment for A Digital Twin Empowered\n  Satellite-Terrestrial Integrated Network"
                },
                "summary": "With the development of satellite communication technology,\nsatellite-terrestrial integrated networks (STIN), which integrate satellite\nnetworks and ground networks, can realize seamless global coverage of\ncommunication services. Confronting the intricacies of network dynamics, the\ndiversity of resource heterogeneity, and the unpredictability of user mobility,\ndynamic resource allocation within networks faces formidable challenges.\nDigital twin (DT), as a new technique, can reflect a physical network to a\nvirtual network to monitor, analyze, and optimize the physical network.\nNevertheless, in the process of constructing the DT model, the deployment\nlocation and resource allocation of DTs may adversely affect its performance.\nTherefore, we propose a STIN model, which alleviates the problem of\ninsufficient single-layer deployment flexibility of the traditional edge\nnetwork by deploying DTs in multi-layer nodes in a STIN. To address the\nchallenge of deploying DTs in the network, we propose multi-layer DT deployment\nin a STIN to reduce system delay. Then we adopt a multi-agent reinforcement\nlearning (MARL) scheme to explore the optimal strategy of the DT multi-layer\ndeployment problem. The implemented scheme demonstrates a notable reduction in\nsystem delay, as evidenced by simulation outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of satellite communication technology,\nsatellite-terrestrial integrated networks (STIN), which integrate satellite\nnetworks and ground networks, can realize seamless global coverage of\ncommunication services. Confronting the intricacies of network dynamics, the\ndiversity of resource heterogeneity, and the unpredictability of user mobility,\ndynamic resource allocation within networks faces formidable challenges.\nDigital twin (DT), as a new technique, can reflect a physical network to a\nvirtual network to monitor, analyze, and optimize the physical network.\nNevertheless, in the process of constructing the DT model, the deployment\nlocation and resource allocation of DTs may adversely affect its performance.\nTherefore, we propose a STIN model, which alleviates the problem of\ninsufficient single-layer deployment flexibility of the traditional edge\nnetwork by deploying DTs in multi-layer nodes in a STIN. To address the\nchallenge of deploying DTs in the network, we propose multi-layer DT deployment\nin a STIN to reduce system delay. Then we adopt a multi-agent reinforcement\nlearning (MARL) scheme to explore the optimal strategy of the DT multi-layer\ndeployment problem. The implemented scheme demonstrates a notable reduction in\nsystem delay, as evidenced by simulation outcomes."
                },
                "authors": [
                    {
                        "name": "Yihong Tao"
                    },
                    {
                        "name": "Bo Lei"
                    },
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Jingkai Chen"
                    },
                    {
                        "name": "Xing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xing Zhang"
                },
                "author": "Xing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09681v2",
                "updated": "2024-09-09T09:49:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    49,
                    29,
                    0,
                    253,
                    0
                ],
                "published": "2024-04-15T11:35:15Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    11,
                    35,
                    15,
                    0,
                    106,
                    0
                ],
                "title": "Dissecting Open Edge Computing Platforms: Ecosystem, Usage, and Security\n  Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Open Edge Computing Platforms: Ecosystem, Usage, and Security\n  Risks"
                },
                "summary": "Emerging in recent years, open edge computing platforms (OECPs) claim\nlarge-scale edge nodes, the extensive usage and adoption, as well as the\nopenness to any third parties to join as edge nodes. For instance,\nOneThingCloud, a major OECP operated in China, advertises 5 million edge nodes,\n70TB bandwidth, and 1,500PB storage. However, little information is publicly\navailable for such OECPs with regards to their technical mechanisms and\ninvolvement in edge computing activities. Furthermore, different from known\nedge computing paradigms, OECPs feature an open ecosystem wherein any third\nparty can participate as edge nodes and earn revenue for the contribution of\ncomputing and bandwidth resources, which, however, can introduce byzantine or\neven malicious edge nodes and thus break the traditional threat model for edge\ncomputing. In this study, we conduct the first empirical study on two\nrepresentative OECPs, which is made possible through the deployment of edge\nnodes across locations, the efficient and semi-automatic analysis of edge\ntraffic as well as the carefully designed security experiments. As the results,\na set of novel findings and insights have been distilled with regards to their\ntechnical mechanisms, the landscape of edge nodes, the usage and adoption, and\nthe practical security/privacy risks. Particularly, millions of daily active\nedge nodes have been observed, which feature a wide distribution in the network\nspace and the extensive adoption in content delivery towards end users of 16\npopular Internet services. Also, multiple practical and concerning security\nrisks have been identified along with acknowledgements received from relevant\nparties, e.g., the exposure of long-term and cross-edge-node credentials, the\nco-location with malicious activities of diverse categories, the failures of\nTLS certificate verification, the extensive information leakage against end\nusers, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging in recent years, open edge computing platforms (OECPs) claim\nlarge-scale edge nodes, the extensive usage and adoption, as well as the\nopenness to any third parties to join as edge nodes. For instance,\nOneThingCloud, a major OECP operated in China, advertises 5 million edge nodes,\n70TB bandwidth, and 1,500PB storage. However, little information is publicly\navailable for such OECPs with regards to their technical mechanisms and\ninvolvement in edge computing activities. Furthermore, different from known\nedge computing paradigms, OECPs feature an open ecosystem wherein any third\nparty can participate as edge nodes and earn revenue for the contribution of\ncomputing and bandwidth resources, which, however, can introduce byzantine or\neven malicious edge nodes and thus break the traditional threat model for edge\ncomputing. In this study, we conduct the first empirical study on two\nrepresentative OECPs, which is made possible through the deployment of edge\nnodes across locations, the efficient and semi-automatic analysis of edge\ntraffic as well as the carefully designed security experiments. As the results,\na set of novel findings and insights have been distilled with regards to their\ntechnical mechanisms, the landscape of edge nodes, the usage and adoption, and\nthe practical security/privacy risks. Particularly, millions of daily active\nedge nodes have been observed, which feature a wide distribution in the network\nspace and the extensive adoption in content delivery towards end users of 16\npopular Internet services. Also, multiple practical and concerning security\nrisks have been identified along with acknowledgements received from relevant\nparties, e.g., the exposure of long-term and cross-edge-node credentials, the\nco-location with malicious activities of diverse categories, the failures of\nTLS certificate verification, the extensive information leakage against end\nusers, etc."
                },
                "authors": [
                    {
                        "name": "Yu Bi"
                    },
                    {
                        "name": "Mingshuo Yang"
                    },
                    {
                        "name": "Yong Fang"
                    },
                    {
                        "name": "Xianghang Mi"
                    },
                    {
                        "name": "Shanqing Guo"
                    },
                    {
                        "name": "Shujun Tang"
                    },
                    {
                        "name": "Haixin Duan"
                    }
                ],
                "author_detail": {
                    "name": "Haixin Duan"
                },
                "author": "Haixin Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02795v3",
                "updated": "2024-09-09T09:31:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    31,
                    30,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-04T15:11:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    11,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Helan Hu"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Houfeng Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07601v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07601v4",
                "updated": "2024-09-09T08:56:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    56,
                    31,
                    0,
                    253,
                    0
                ],
                "published": "2023-11-11T02:13:32Z",
                "published_parsed": [
                    2023,
                    11,
                    11,
                    2,
                    13,
                    32,
                    5,
                    315,
                    0
                ],
                "title": "Online Advertisements with LLMs: Opportunities and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Advertisements with LLMs: Opportunities and Challenges"
                },
                "summary": "This paper explores the potential for leveraging Large Language Models (LLM)\nin the realm of online advertising systems. We introduce a general framework\nfor LLM advertisement, consisting of modification, bidding, prediction, and\nauction modules. Different design considerations for each module are presented.\nThese design choices are evaluated and discussed based on essential desiderata\nrequired to maintain a sustainable system. Further fundamental questions\nregarding practicality, efficiency, and implementation challenges are raised\nfor future research. Finally, we exposit how recent approaches on mechanism\ndesign for LLM can be framed in our unified perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential for leveraging Large Language Models (LLM)\nin the realm of online advertising systems. We introduce a general framework\nfor LLM advertisement, consisting of modification, bidding, prediction, and\nauction modules. Different design considerations for each module are presented.\nThese design choices are evaluated and discussed based on essential desiderata\nrequired to maintain a sustainable system. Further fundamental questions\nregarding practicality, efficiency, and implementation challenges are raised\nfor future research. Finally, we exposit how recent approaches on mechanism\ndesign for LLM can be framed in our unified perspective."
                },
                "authors": [
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "MohammadTaghi Hajiaghayi"
                    },
                    {
                        "name": "Keivan Rezaei"
                    },
                    {
                        "name": "Suho Shin"
                    }
                ],
                "author_detail": {
                    "name": "Suho Shin"
                },
                "author": "Suho Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07601v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07601v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05966v2",
                "updated": "2024-09-09T08:21:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    21,
                    13,
                    0,
                    253,
                    0
                ],
                "published": "2024-05-09T17:59:32Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    17,
                    59,
                    32,
                    3,
                    130,
                    0
                ],
                "title": "Natural Language Processing RELIES on Linguistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing RELIES on Linguistics"
                },
                "summary": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage."
                },
                "authors": [
                    {
                        "name": "Juri Opitz"
                    },
                    {
                        "name": "Shira Wein"
                    },
                    {
                        "name": "Nathan Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Schneider"
                },
                "author": "Nathan Schneider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00523v2",
                "updated": "2024-09-09T08:09:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    9,
                    14,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-01T12:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    12,
                    54,
                    46,
                    3,
                    214,
                    0
                ],
                "title": "Jailbreaking Text-to-Image Models with LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking Text-to-Image Models with LLM-Based Agents"
                },
                "summary": "Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving their potential for addressing generative AI safety tasks\nlargely unexplored. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework targeting generative AI models, specifically focusing on\njailbreak attacks against text-to-image (T2I) models with built-in safety\nfilters. Atlas consists of two agents, namely the mutation agent and the\nselection agent, each comprising four key modules: a vision-language model\n(VLM) or LLM brain, planning, memory, and tool usage. The mutation agent uses\nits VLM brain to determine whether a prompt triggers the T2I model's safety\nfilter. It then collaborates iteratively with the LLM brain of the selection\nagent to generate new candidate jailbreak prompts with the highest potential to\nbypass the filter. In addition to multi-agent communication, we leverage\nin-context learning (ICL) memory mechanisms and the chain-of-thought (COT)\napproach to learn from past successes and failures, thereby enhancing Atlas's\nperformance. Our evaluation demonstrates that Atlas successfully jailbreaks\nseveral state-of-the-art T2I models equipped with multi-modal safety filters in\na black-box setting. Additionally, Atlas outperforms existing methods in both\nquery efficiency and the quality of generated images. This work convincingly\ndemonstrates the successful application of LLM-based agents in studying the\nsafety vulnerabilities of popular text-to-image generation models. We urge the\ncommunity to consider advanced techniques like ours in response to the rapidly\nevolving text-to-image generation field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving their potential for addressing generative AI safety tasks\nlargely unexplored. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework targeting generative AI models, specifically focusing on\njailbreak attacks against text-to-image (T2I) models with built-in safety\nfilters. Atlas consists of two agents, namely the mutation agent and the\nselection agent, each comprising four key modules: a vision-language model\n(VLM) or LLM brain, planning, memory, and tool usage. The mutation agent uses\nits VLM brain to determine whether a prompt triggers the T2I model's safety\nfilter. It then collaborates iteratively with the LLM brain of the selection\nagent to generate new candidate jailbreak prompts with the highest potential to\nbypass the filter. In addition to multi-agent communication, we leverage\nin-context learning (ICL) memory mechanisms and the chain-of-thought (COT)\napproach to learn from past successes and failures, thereby enhancing Atlas's\nperformance. Our evaluation demonstrates that Atlas successfully jailbreaks\nseveral state-of-the-art T2I models equipped with multi-modal safety filters in\na black-box setting. Additionally, Atlas outperforms existing methods in both\nquery efficiency and the quality of generated images. This work convincingly\ndemonstrates the successful application of LLM-based agents in studying the\nsafety vulnerabilities of popular text-to-image generation models. We urge the\ncommunity to consider advanced techniques like ours in response to the rapidly\nevolving text-to-image generation field."
                },
                "authors": [
                    {
                        "name": "Yingkai Dong"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Xiangtao Meng"
                    },
                    {
                        "name": "Ning Yu"
                    },
                    {
                        "name": "Shanqing Guo"
                    }
                ],
                "author_detail": {
                    "name": "Shanqing Guo"
                },
                "author": "Shanqing Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05404v1",
                "updated": "2024-09-09T08:05:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    5,
                    43,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T08:05:43Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    5,
                    43,
                    0,
                    253,
                    0
                ],
                "title": "DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFabric: Scaling Out Data Parallel Applications with CXL-Ethernet Hybrid\n  Interconnects"
                },
                "summary": "Emerging interconnects, such as CXL and NVLink, have been integrated into the\nintra-host topology to scale more accelerators and facilitate efficient\ncommunication between them, such as GPUs. To keep pace with the accelerator's\ngrowing computing throughput, the interconnect has seen substantial enhancement\nin link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet\nand InfiniBand network links by an order of magnitude or more. Consequently,\nwhen data-intensive jobs, such as LLM training, scale across multiple hosts\nbeyond the reach limit of the interconnect, the performance is significantly\nhindered by the limiting bandwidth of the network infrastructure. We address\nthe problem by proposing DFabric, a two-tier interconnect architecture. We\naddress the problem by proposing DFabric, a two-tier interconnect architecture.\nFirst, DFabric disaggregates rack's computing units with an interconnect\nfabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy\nintra-rack efficient interconnecting. Second, DFabric disaggregates NICs from\nhosts, and consolidates them to form a NIC pool with CXL fabric. By providing\nsufficient aggregated capacity comparable to interconnect bandwidth, the NIC\npool bridges efficient communication across racks or beyond the reach limit of\ninterconnect fabric. However, the local memory accessing becomes the bottleneck\nwhen enabling each host to utilize the NIC pool efficiently. To the end,\nDFabric builds a memory pool with sufficient bandwidth by disaggregating host\nlocal memory and adding more memory devices. We have implemented a prototype of\nDFabric that can run applications transparently. We validated its performance\ngain by running various microbenchmarks and compute-intensive applications such\nas DNN and graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging interconnects, such as CXL and NVLink, have been integrated into the\nintra-host topology to scale more accelerators and facilitate efficient\ncommunication between them, such as GPUs. To keep pace with the accelerator's\ngrowing computing throughput, the interconnect has seen substantial enhancement\nin link bandwidth, e.g., 256GBps for CXL 3.0 links, which surpasses Ethernet\nand InfiniBand network links by an order of magnitude or more. Consequently,\nwhen data-intensive jobs, such as LLM training, scale across multiple hosts\nbeyond the reach limit of the interconnect, the performance is significantly\nhindered by the limiting bandwidth of the network infrastructure. We address\nthe problem by proposing DFabric, a two-tier interconnect architecture. We\naddress the problem by proposing DFabric, a two-tier interconnect architecture.\nFirst, DFabric disaggregates rack's computing units with an interconnect\nfabric, i.e., CXL fabric, which scales at rack-level, so that they can enjoy\nintra-rack efficient interconnecting. Second, DFabric disaggregates NICs from\nhosts, and consolidates them to form a NIC pool with CXL fabric. By providing\nsufficient aggregated capacity comparable to interconnect bandwidth, the NIC\npool bridges efficient communication across racks or beyond the reach limit of\ninterconnect fabric. However, the local memory accessing becomes the bottleneck\nwhen enabling each host to utilize the NIC pool efficiently. To the end,\nDFabric builds a memory pool with sufficient bandwidth by disaggregating host\nlocal memory and adding more memory devices. We have implemented a prototype of\nDFabric that can run applications transparently. We validated its performance\ngain by running various microbenchmarks and compute-intensive applications such\nas DNN and graph."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Yisong Chang"
                    },
                    {
                        "name": "Hui Yuan"
                    },
                    {
                        "name": "Xiaolong Zheng"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Mingyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Chen"
                },
                "author": "Mingyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05385v2",
                "updated": "2024-09-10T06:11:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    6,
                    11,
                    28,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-09T07:32:30Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    32,
                    30,
                    0,
                    253,
                    0
                ],
                "title": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Building a Robust Knowledge Intensive Question Answering Model\n  with Large Language Models"
                },
                "summary": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of LLMs has greatly enhanced the intelligence and fluency of\nquestion answering, while the emergence of retrieval enhancement has enabled\nmodels to better utilize external information. However, the presence of noise\nand errors in retrieved information poses challenges to the robustness of LLMs.\nIn this work, to evaluate the model's performance under multiple interferences,\nwe first construct a dataset based on machine reading comprehension datasets\nsimulating various scenarios, including critical information absence, noise,\nand conflicts. To address the issue of model accuracy decline caused by noisy\nexternal information, we propose a data augmentation-based fine-tuning method\nto enhance LLM's robustness against noise. Additionally, contrastive learning\napproach is utilized to preserve the model's discrimination capability of\nexternal information. We have conducted experiments on both existing LLMs and\nour approach, the results are evaluated by GPT-4, which indicates that our\nproposed methods improve model robustness while strengthening the model's\ndiscrimination capability."
                },
                "authors": [
                    {
                        "name": "Hong Xingyun Hong"
                    },
                    {
                        "name": "Shao Yan Shao"
                    },
                    {
                        "name": "Wang Zhilin Wang"
                    },
                    {
                        "name": "Duan Manni Duan"
                    },
                    {
                        "name": "Jin Xiongnan"
                    }
                ],
                "author_detail": {
                    "name": "Jin Xiongnan"
                },
                "author": "Jin Xiongnan",
                "arxiv_comment": "This paper has been accepted by NLPCC-2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03488v3",
                "updated": "2024-09-09T07:31:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    31,
                    36,
                    0,
                    253,
                    0
                ],
                "published": "2024-06-05T17:50:03Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    50,
                    3,
                    2,
                    157,
                    0
                ],
                "title": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large\n  Language Model Training"
                },
                "summary": "The emergence of large language models (LLMs) relies heavily on distributed\ntraining strategies, among which pipeline parallelism plays a crucial role. As\nLLMs' training sequence length extends to 32k or even 128k, the current\npipeline parallel methods face severe bottlenecks, including high memory\nfootprints and substantial pipeline bubbles, greatly hindering model\nscalability and training throughput. To enhance memory efficiency and training\nthroughput, in this work, we introduce an efficient sequence-level\none-forward-one-backward (1F1B) pipeline scheduling method tailored for\ntraining LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level\nschedulable units into finer sequence-level units, reducing bubble size and\nmemory footprint. Considering that Seq1F1B may produce slight extra bubbles if\nsequences are split evenly, we design a computation-wise strategy to partition\ninput sequences and mitigate this side effect. Compared to competitive pipeline\nbaseline methods such as Megatron 1F1B pipeline parallelism, our method\nachieves higher training throughput with less memory footprint. Notably,\nSeq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k\nusing 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable\nwith existing methods. Our source code is based on Megatron-LM, and now is\navaiable at: https://github.com/MayDomine/Seq1F1B.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) relies heavily on distributed\ntraining strategies, among which pipeline parallelism plays a crucial role. As\nLLMs' training sequence length extends to 32k or even 128k, the current\npipeline parallel methods face severe bottlenecks, including high memory\nfootprints and substantial pipeline bubbles, greatly hindering model\nscalability and training throughput. To enhance memory efficiency and training\nthroughput, in this work, we introduce an efficient sequence-level\none-forward-one-backward (1F1B) pipeline scheduling method tailored for\ntraining LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level\nschedulable units into finer sequence-level units, reducing bubble size and\nmemory footprint. Considering that Seq1F1B may produce slight extra bubbles if\nsequences are split evenly, we design a computation-wise strategy to partition\ninput sequences and mitigate this side effect. Compared to competitive pipeline\nbaseline methods such as Megatron 1F1B pipeline parallelism, our method\nachieves higher training throughput with less memory footprint. Notably,\nSeq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k\nusing 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable\nwith existing methods. Our source code is based on Megatron-LM, and now is\navaiable at: https://github.com/MayDomine/Seq1F1B.git."
                },
                "authors": [
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Xinrong Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "12 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04363v2",
                "updated": "2024-09-09T07:01:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    7,
                    1,
                    56,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-05T09:06:47Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    9,
                    6,
                    47,
                    4,
                    187,
                    0
                ],
                "title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents"
                },
                "summary": "Advancements in the capabilities of Large Language Models (LLMs) have created\na promising foundation for developing autonomous agents. With the right tools,\nthese agents could learn to solve tasks in new environments by accumulating and\nupdating their knowledge. Current LLM-based agents process past experiences\nusing a full history of observations, summarization, retrieval augmentation.\nHowever, these unstructured memory representations do not facilitate the\nreasoning and planning essential for complex decision-making. In our study, we\nintroduce AriGraph, a novel method wherein the agent constructs and updates a\nmemory graph that integrates semantic and episodic memories while exploring the\nenvironment. We demonstrate that our Ariadne LLM agent, consisting of the\nproposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks within interactive text game environments\ndifficult even for human players. Results show that our approach markedly\noutperforms other established memory methods and strong RL baselines in a range\nof problems of varying complexity. Additionally, AriGraph demonstrates\ncompetitive performance compared to dedicated knowledge graph-based methods in\nstatic multi-hop question-answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in the capabilities of Large Language Models (LLMs) have created\na promising foundation for developing autonomous agents. With the right tools,\nthese agents could learn to solve tasks in new environments by accumulating and\nupdating their knowledge. Current LLM-based agents process past experiences\nusing a full history of observations, summarization, retrieval augmentation.\nHowever, these unstructured memory representations do not facilitate the\nreasoning and planning essential for complex decision-making. In our study, we\nintroduce AriGraph, a novel method wherein the agent constructs and updates a\nmemory graph that integrates semantic and episodic memories while exploring the\nenvironment. We demonstrate that our Ariadne LLM agent, consisting of the\nproposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks within interactive text game environments\ndifficult even for human players. Results show that our approach markedly\noutperforms other established memory methods and strong RL baselines in a range\nof problems of varying complexity. Additionally, AriGraph demonstrates\ncompetitive performance compared to dedicated knowledge graph-based methods in\nstatic multi-hop question-answering."
                },
                "authors": [
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Nikita Semenov"
                    },
                    {
                        "name": "Artyom Sorokin"
                    },
                    {
                        "name": "Dmitry Evseev"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Burnaev"
                },
                "author": "Evgeny Burnaev",
                "arxiv_comment": "Code for this work is avaliable at\n  https://github.com/AIRI-Institute/AriGraph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05370v1",
                "updated": "2024-09-09T06:57:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    57,
                    22,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T06:57:22Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    57,
                    22,
                    0,
                    253,
                    0
                ],
                "title": "KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARGEN: Knowledge-enhanced Automated Radiology Report Generation Using\n  Large Language Models"
                },
                "summary": "Harnessing the robust capabilities of Large Language Models (LLMs) for\nnarrative generation, logical reasoning, and common-sense knowledge\nintegration, this study delves into utilizing LLMs to enhance automated\nradiology report generation (R2Gen). Despite the wealth of knowledge within\nLLMs, efficiently triggering relevant knowledge within these large models for\nspecific tasks like R2Gen poses a critical research challenge. This paper\npresents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration\nframework based on LLMs. Utilizing a frozen LLM to generate reports, the\nframework integrates a knowledge graph to unlock chest disease-related\nknowledge within the LLM to enhance the clinical utility of generated reports.\nThis is achieved by leveraging the knowledge graph to distill disease-related\nfeatures in a designed way. Since a radiology report encompasses both normal\nand disease-related findings, the extracted graph-enhanced disease-related\nfeatures are integrated with regional image features, attending to both\naspects. We explore two fusion methods to automatically prioritize and select\nthe most relevant features. The fused features are employed by LLM to generate\nreports that are more sensitive to diseases and of improved quality. Our\napproach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the robust capabilities of Large Language Models (LLMs) for\nnarrative generation, logical reasoning, and common-sense knowledge\nintegration, this study delves into utilizing LLMs to enhance automated\nradiology report generation (R2Gen). Despite the wealth of knowledge within\nLLMs, efficiently triggering relevant knowledge within these large models for\nspecific tasks like R2Gen poses a critical research challenge. This paper\npresents KARGEN, a Knowledge-enhanced Automated radiology Report GENeration\nframework based on LLMs. Utilizing a frozen LLM to generate reports, the\nframework integrates a knowledge graph to unlock chest disease-related\nknowledge within the LLM to enhance the clinical utility of generated reports.\nThis is achieved by leveraging the knowledge graph to distill disease-related\nfeatures in a designed way. Since a radiology report encompasses both normal\nand disease-related findings, the extracted graph-enhanced disease-related\nfeatures are integrated with regional image features, attending to both\naspects. We explore two fusion methods to automatically prioritize and select\nthe most relevant features. The fused features are employed by LLM to generate\nreports that are more sensitive to diseases and of improved quality. Our\napproach demonstrates promising results on the MIMIC-CXR and IU-Xray datasets."
                },
                "authors": [
                    {
                        "name": "Yingshu Li"
                    },
                    {
                        "name": "Zhanyu Wang"
                    },
                    {
                        "name": "Yunyi Liu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Lingqiao Liu"
                    },
                    {
                        "name": "Luping Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Luping Zhou"
                },
                "author": "Luping Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05367v1",
                "updated": "2024-09-09T06:55:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    55,
                    37,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T06:55:37Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    55,
                    37,
                    0,
                    253,
                    0
                ],
                "title": "Diagnostic Reasoning in Natural Language: Computational Model and\n  Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Reasoning in Natural Language: Computational Model and\n  Application"
                },
                "summary": "Diagnostic reasoning is a key component of expert work in many domains. It is\na hard, time-consuming activity that requires expertise, and AI research has\ninvestigated the ways automated systems can support this process. Yet, due to\nthe complexity of natural language, the applications of AI for diagnostic\nreasoning to language-related tasks are lacking. To close this gap, we\ninvestigate diagnostic abductive reasoning (DAR) in the context of\nlanguage-grounded tasks (NL-DAR). We propose a novel modeling framework for\nNL-DAR based on Pearl's structural causal models and instantiate it in a\ncomprehensive study of scientific paper assessment in the biomedical domain. We\nuse the resulting dataset to investigate the human decision-making process in\nNL-DAR and determine the potential of LLMs to support structured\ndecision-making over text. Our framework, open resources and tools lay the\ngroundwork for the empirical study of collaborative diagnostic reasoning in the\nage of LLMs, in the scholarly domain and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic reasoning is a key component of expert work in many domains. It is\na hard, time-consuming activity that requires expertise, and AI research has\ninvestigated the ways automated systems can support this process. Yet, due to\nthe complexity of natural language, the applications of AI for diagnostic\nreasoning to language-related tasks are lacking. To close this gap, we\ninvestigate diagnostic abductive reasoning (DAR) in the context of\nlanguage-grounded tasks (NL-DAR). We propose a novel modeling framework for\nNL-DAR based on Pearl's structural causal models and instantiate it in a\ncomprehensive study of scientific paper assessment in the biomedical domain. We\nuse the resulting dataset to investigate the human decision-making process in\nNL-DAR and determine the potential of LLMs to support structured\ndecision-making over text. Our framework, open resources and tools lay the\ngroundwork for the empirical study of collaborative diagnostic reasoning in the\nage of LLMs, in the scholarly domain and beyond."
                },
                "authors": [
                    {
                        "name": "Nils Dycke"
                    },
                    {
                        "name": "Matej Zečević"
                    },
                    {
                        "name": "Ilia Kuznetsov"
                    },
                    {
                        "name": "Beatrix Suess"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05847v2",
                "updated": "2024-09-09T06:54:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    54,
                    53,
                    0,
                    253,
                    0
                ],
                "published": "2024-03-09T09:15:37Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    9,
                    15,
                    37,
                    5,
                    69,
                    0
                ],
                "title": "iBA: Backdoor Attack on 3D Point Cloud via Reconstructing Itself",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iBA: Backdoor Attack on 3D Point Cloud via Reconstructing Itself"
                },
                "summary": "The widespread deployment of Deep Neural Networks (DNNs) for 3D point cloud\nprocessing starkly contrasts with their susceptibility to security breaches,\nnotably backdoor attacks. These attacks hijack DNNs during training, embedding\ntriggers in the data that, once activated, cause the network to make\npredetermined errors while maintaining normal performance on unaltered data.\nThis vulnerability poses significant risks, especially given the insufficient\nresearch on robust defense mechanisms for 3D point cloud networks against such\nsophisticated threats. Existing attacks either struggle to resist basic point\ncloud pre-processing methods, or rely on delicate manual design. Exploring\nsimple, effective, imperceptible, and difficult-to-defend triggers in 3D point\nclouds is still challenging.To address these challenges, we introduce\nMirrorAttack, a novel effective 3D backdoor attack method, which implants the\ntrigger by simply reconstructing a clean point cloud with an auto-encoder. The\ndata-driven nature of the MirrorAttack obviates the need for complex manual\ndesign. Minimizing the reconstruction loss automatically improves\nimperceptibility. Simultaneously, the reconstruction network endows the trigger\nwith pronounced nonlinearity and sample specificity, rendering traditional\npreprocessing techniques ineffective in eliminating it. A trigger smoothing\nmodule based on spherical harmonic transformation is also attached to regulate\nthe intensity of the attack.Both quantitive and qualitative results verify the\neffectiveness of our method. We achieve state-of-the-art ASR on different types\nof victim models with the intervention of defensive techniques. Moreover, the\nminimal perturbation introduced by our trigger, as assessed by various metrics,\nattests to the method's stealth, ensuring its imperceptibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of Deep Neural Networks (DNNs) for 3D point cloud\nprocessing starkly contrasts with their susceptibility to security breaches,\nnotably backdoor attacks. These attacks hijack DNNs during training, embedding\ntriggers in the data that, once activated, cause the network to make\npredetermined errors while maintaining normal performance on unaltered data.\nThis vulnerability poses significant risks, especially given the insufficient\nresearch on robust defense mechanisms for 3D point cloud networks against such\nsophisticated threats. Existing attacks either struggle to resist basic point\ncloud pre-processing methods, or rely on delicate manual design. Exploring\nsimple, effective, imperceptible, and difficult-to-defend triggers in 3D point\nclouds is still challenging.To address these challenges, we introduce\nMirrorAttack, a novel effective 3D backdoor attack method, which implants the\ntrigger by simply reconstructing a clean point cloud with an auto-encoder. The\ndata-driven nature of the MirrorAttack obviates the need for complex manual\ndesign. Minimizing the reconstruction loss automatically improves\nimperceptibility. Simultaneously, the reconstruction network endows the trigger\nwith pronounced nonlinearity and sample specificity, rendering traditional\npreprocessing techniques ineffective in eliminating it. A trigger smoothing\nmodule based on spherical harmonic transformation is also attached to regulate\nthe intensity of the attack.Both quantitive and qualitative results verify the\neffectiveness of our method. We achieve state-of-the-art ASR on different types\nof victim models with the intervention of defensive techniques. Moreover, the\nminimal perturbation introduced by our trigger, as assessed by various metrics,\nattests to the method's stealth, ensuring its imperceptibility."
                },
                "authors": [
                    {
                        "name": "Yuhao Bian"
                    },
                    {
                        "name": "Shengjing Tian"
                    },
                    {
                        "name": "Xiuping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiuping Liu"
                },
                "author": "Xiuping Liu",
                "arxiv_doi": "10.1109/TIFS.2024.3452630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TIFS.2024.3452630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.05847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages. in IEEE Transactions on Information Forensics and Security\n  (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04392v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04392v3",
                "updated": "2024-09-09T06:25:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    25,
                    33,
                    0,
                    253,
                    0
                ],
                "published": "2024-04-05T20:31:45Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    20,
                    31,
                    45,
                    4,
                    96,
                    0
                ],
                "title": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes"
                },
                "summary": "Large Language Models (LLMs) have gained widespread adoption across various\ndomains, including chatbots and auto-task completion agents. However, these\nmodels are susceptible to safety vulnerabilities such as jailbreaking, prompt\ninjection, and privacy leakage attacks. These vulnerabilities can lead to the\ngeneration of malicious content, unauthorized actions, or the disclosure of\nconfidential information. While foundational LLMs undergo alignment training\nand incorporate safety measures, they are often subject to fine-tuning, or\ndoing quantization resource-constrained environments. This study investigates\nthe impact of these modifications on LLM safety, a critical consideration for\nbuilding reliable and secure AI systems. We evaluate foundational models\nincluding Mistral, Llama series, Qwen, and MosaicML, along with their\nfine-tuned variants. Our comprehensive analysis reveals that fine-tuning\ngenerally increases the success rates of jailbreak attacks, while quantization\nhas variable effects on attack success rates. Importantly, we find that\nproperly implemented guardrails significantly enhance resistance to jailbreak\nattempts. These findings contribute to our understanding of LLM vulnerabilities\nand provide insights for developing more robust safety strategies in the\ndeployment of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained widespread adoption across various\ndomains, including chatbots and auto-task completion agents. However, these\nmodels are susceptible to safety vulnerabilities such as jailbreaking, prompt\ninjection, and privacy leakage attacks. These vulnerabilities can lead to the\ngeneration of malicious content, unauthorized actions, or the disclosure of\nconfidential information. While foundational LLMs undergo alignment training\nand incorporate safety measures, they are often subject to fine-tuning, or\ndoing quantization resource-constrained environments. This study investigates\nthe impact of these modifications on LLM safety, a critical consideration for\nbuilding reliable and secure AI systems. We evaluate foundational models\nincluding Mistral, Llama series, Qwen, and MosaicML, along with their\nfine-tuned variants. Our comprehensive analysis reveals that fine-tuning\ngenerally increases the success rates of jailbreak attacks, while quantization\nhas variable effects on attack success rates. Importantly, we find that\nproperly implemented guardrails significantly enhance resistance to jailbreak\nattempts. These findings contribute to our understanding of LLM vulnerabilities\nand provide insights for developing more robust safety strategies in the\ndeployment of language models."
                },
                "authors": [
                    {
                        "name": "Divyanshu Kumar"
                    },
                    {
                        "name": "Anurakt Kumar"
                    },
                    {
                        "name": "Sahil Agarwal"
                    },
                    {
                        "name": "Prashanth Harshangi"
                    }
                ],
                "author_detail": {
                    "name": "Prashanth Harshangi"
                },
                "author": "Prashanth Harshangi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04392v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04392v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14482v2",
                "updated": "2024-09-09T06:19:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    19,
                    7,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-19T17:35:47Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    17,
                    35,
                    47,
                    4,
                    201,
                    0
                ],
                "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities"
                },
                "summary": "In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and leading\nproprietary models (e.g., GPT-4-Turbo) in long-context understanding and\nretrieval-augmented generation (RAG) capabilities. These two capabilities are\nessential for LLMs to process large volumes of information that cannot fit into\na single prompt and are complementary to each other, depending on the\ndownstream tasks and computational budgets. We present a detailed continued\ntraining recipe to extend the context window of Llama3-70B-base from 8K to 128K\ntokens, along with a three-stage instruction tuning process to enhance the\nmodel's instruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\noutperforms most existing state-of-the-art models, including\nGPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on\nultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only\na 4K context window, showing the strong long context capability across varying\nsequence lengths. We further provide extensive comparisons between direct\nlong-context and RAG solutions using the same state-of-the-art long-context\nLLMs. Interestingly, we find that the performance of strong long-context LLMs\nusing RAG improves when retrieving a larger number of chunks. With a large set\nof top-k chunks, RAG consistently outperforms direct long-context solution\nusing the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B\nand Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To\nadvance research in this field, we open-sourced the model weights, training\ndata, and the evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and leading\nproprietary models (e.g., GPT-4-Turbo) in long-context understanding and\nretrieval-augmented generation (RAG) capabilities. These two capabilities are\nessential for LLMs to process large volumes of information that cannot fit into\na single prompt and are complementary to each other, depending on the\ndownstream tasks and computational budgets. We present a detailed continued\ntraining recipe to extend the context window of Llama3-70B-base from 8K to 128K\ntokens, along with a three-stage instruction tuning process to enhance the\nmodel's instruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\noutperforms most existing state-of-the-art models, including\nGPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on\nultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only\na 4K context window, showing the strong long context capability across varying\nsequence lengths. We further provide extensive comparisons between direct\nlong-context and RAG solutions using the same state-of-the-art long-context\nLLMs. Interestingly, we find that the performance of strong long-context LLMs\nusing RAG improves when retrieving a larger number of chunks. With a large set\nof top-k chunks, RAG consistently outperforms direct long-context solution\nusing the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B\nand Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To\nadvance research in this field, we open-sourced the model weights, training\ndata, and the evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/"
                },
                "authors": [
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Wei Ping"
                    },
                    {
                        "name": "Xianchao Wu"
                    },
                    {
                        "name": "Chejian Xu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "arxiv_comment": "v2: major update with significantly improved results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15186v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15186v3",
                "updated": "2024-09-09T06:17:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    17,
                    21,
                    0,
                    253,
                    0
                ],
                "published": "2024-07-21T14:48:23Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    48,
                    23,
                    6,
                    203,
                    0
                ],
                "title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Employing Large Language Models for Text-to-SQL Tasks"
                },
                "summary": "The increasing volume of data stored in relational databases has led to the\nneed for efficient querying and utilization of this data in various sectors.\nHowever, writing SQL queries requires specialized knowledge, which poses a\nchallenge for non-professional users trying to access and query databases.\nText-to-SQL parsing solves this issue by converting natural language queries\ninto SQL queries, thus making database access more accessible for non-expert\nusers. To take advantage of the recent developments in Large Language Models\n(LLMs), a range of new methods have emerged, with a primary focus on prompt\nengineering and fine-tuning. This survey provides a comprehensive overview of\nLLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering,\nfine-tuning methods, and future research directions. We hope this review will\nenable readers to gain a broader understanding of the recent advances in this\nfield and offer some insights into its future trajectory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing volume of data stored in relational databases has led to the\nneed for efficient querying and utilization of this data in various sectors.\nHowever, writing SQL queries requires specialized knowledge, which poses a\nchallenge for non-professional users trying to access and query databases.\nText-to-SQL parsing solves this issue by converting natural language queries\ninto SQL queries, thus making database access more accessible for non-expert\nusers. To take advantage of the recent developments in Large Language Models\n(LLMs), a range of new methods have emerged, with a primary focus on prompt\nengineering and fine-tuning. This survey provides a comprehensive overview of\nLLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering,\nfine-tuning methods, and future research directions. We hope this review will\nenable readers to gain a broader understanding of the recent advances in this\nfield and offer some insights into its future trajectory."
                },
                "authors": [
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Zhengju Tang"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Xiaotong Zhang"
                    },
                    {
                        "name": "Zhi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yang"
                },
                "author": "Zhi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15186v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15186v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05344v1",
                "updated": "2024-09-09T06:02:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    2,
                    17,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T06:02:17Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    2,
                    17,
                    0,
                    253,
                    0
                ],
                "title": "GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep\n  Reinforcement Learning"
                },
                "summary": "Robotic object packing has broad practical applications in the logistics and\nautomation industry, often formulated by researchers as the online 3D Bin\nPacking Problem (3D-BPP). However, existing DRL-based methods primarily focus\non enhancing performance in limited packing environments while neglecting the\nability to generalize across multiple environments characterized by different\nbin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin\nPacking approach via Transformer-based deep reinforcement learning (DRL).\nFirst, we design a Placement Generator module to yield finite subspaces as\nplacement candidates and the representation of the bin. Second, we propose a\nPacking Transformer, which fuses the features of the items and bin, to identify\nthe spatial correlation between the item to be packed and available sub-spaces\nwithin the bin. Coupling these two components enables GOPT's ability to perform\ninference on bins of varying dimensions. We conduct extensive experiments and\ndemonstrate that GOPT not only achieves superior performance against the\nbaselines, but also exhibits excellent generalization capabilities.\nFurthermore, the deployment with a robot showcases the practical applicability\nof our method in the real world. The source code will be publicly available at\nhttps://github.com/Xiong5Heng/GOPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic object packing has broad practical applications in the logistics and\nautomation industry, often formulated by researchers as the online 3D Bin\nPacking Problem (3D-BPP). However, existing DRL-based methods primarily focus\non enhancing performance in limited packing environments while neglecting the\nability to generalize across multiple environments characterized by different\nbin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin\nPacking approach via Transformer-based deep reinforcement learning (DRL).\nFirst, we design a Placement Generator module to yield finite subspaces as\nplacement candidates and the representation of the bin. Second, we propose a\nPacking Transformer, which fuses the features of the items and bin, to identify\nthe spatial correlation between the item to be packed and available sub-spaces\nwithin the bin. Coupling these two components enables GOPT's ability to perform\ninference on bins of varying dimensions. We conduct extensive experiments and\ndemonstrate that GOPT not only achieves superior performance against the\nbaselines, but also exhibits excellent generalization capabilities.\nFurthermore, the deployment with a robot showcases the practical applicability\nof our method in the real world. The source code will be publicly available at\nhttps://github.com/Xiong5Heng/GOPT."
                },
                "authors": [
                    {
                        "name": "Heng Xiong"
                    },
                    {
                        "name": "Changrong Guo"
                    },
                    {
                        "name": "Jian Peng"
                    },
                    {
                        "name": "Kai Ding"
                    },
                    {
                        "name": "Xuchong Qiu"
                    },
                    {
                        "name": "Long Bai"
                    },
                    {
                        "name": "Jianfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Xu"
                },
                "author": "Jianfeng Xu",
                "arxiv_comment": "8 pages, 6 figures. This paper has been accepted by IEEE Robotics and\n  Automation Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14795v3",
                "updated": "2024-09-09T04:38:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    4,
                    38,
                    16,
                    0,
                    253,
                    0
                ],
                "published": "2023-05-24T06:48:41Z",
                "published_parsed": [
                    2023,
                    5,
                    24,
                    6,
                    48,
                    41,
                    2,
                    144,
                    0
                ],
                "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop\n  Questions"
                },
                "summary": "The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark,\nMQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs\n(e.g., OpenAI GPT-3.5-turbo) and outperforms previous model editors by a large\nmargin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark,\nMQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs\n(e.g., OpenAI GPT-3.5-turbo) and outperforms previous model editors by a large\nmargin."
                },
                "authors": [
                    {
                        "name": "Zexuan Zhong"
                    },
                    {
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "name": "Christopher D. Manning"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "EMNLP 2023. Our code and datasets are available at\n  https://github.com/princeton-nlp/MQuAKE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05314v1",
                "updated": "2024-09-09T03:58:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    3,
                    58,
                    51,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T03:58:51Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    3,
                    58,
                    51,
                    0,
                    253,
                    0
                ],
                "title": "Tele-LLMs: A Series of Specialized Large Language Models for\n  Telecommunications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tele-LLMs: A Series of Specialized Large Language Models for\n  Telecommunications"
                },
                "summary": "The emergence of large language models (LLMs) has significantly impacted\nvarious fields, from natural language processing to sectors like medicine and\nfinance. However, despite their rapid proliferation, the applications of LLMs\nin telecommunications remain limited, often relying on general-purpose models\nthat lack domain-specific specialization. This lack of specialization results\nin underperformance, particularly when dealing with telecommunications-specific\ntechnical terminology and their associated mathematical representations. This\npaper addresses this gap by first creating and disseminating Tele-Data, a\ncomprehensive dataset of telecommunications material curated from relevant\nsources, and Tele-Eval, a large-scale question-and-answer dataset tailored to\nthe domain. Through extensive experiments, we explore the most effective\ntraining techniques for adapting LLMs to the telecommunications domain, ranging\nfrom examining the division of expertise across various telecommunications\naspects to employing parameter-efficient techniques. We also investigate how\nmodels of different sizes behave during adaptation and analyze the impact of\ntheir training data on this behavior. Leveraging these findings, we develop and\nopen-source Tele-LLMs, the first series of language models ranging from 1B to\n8B parameters, specifically tailored for telecommunications. Our evaluations\ndemonstrate that these models outperform their general-purpose counterparts on\nTele-Eval while retaining their previously acquired capabilities, thus avoiding\nthe catastrophic forgetting phenomenon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has significantly impacted\nvarious fields, from natural language processing to sectors like medicine and\nfinance. However, despite their rapid proliferation, the applications of LLMs\nin telecommunications remain limited, often relying on general-purpose models\nthat lack domain-specific specialization. This lack of specialization results\nin underperformance, particularly when dealing with telecommunications-specific\ntechnical terminology and their associated mathematical representations. This\npaper addresses this gap by first creating and disseminating Tele-Data, a\ncomprehensive dataset of telecommunications material curated from relevant\nsources, and Tele-Eval, a large-scale question-and-answer dataset tailored to\nthe domain. Through extensive experiments, we explore the most effective\ntraining techniques for adapting LLMs to the telecommunications domain, ranging\nfrom examining the division of expertise across various telecommunications\naspects to employing parameter-efficient techniques. We also investigate how\nmodels of different sizes behave during adaptation and analyze the impact of\ntheir training data on this behavior. Leveraging these findings, we develop and\nopen-source Tele-LLMs, the first series of language models ranging from 1B to\n8B parameters, specifically tailored for telecommunications. Our evaluations\ndemonstrate that these models outperform their general-purpose counterparts on\nTele-Eval while retaining their previously acquired capabilities, thus avoiding\nthe catastrophic forgetting phenomenon."
                },
                "authors": [
                    {
                        "name": "Ali Maatouk"
                    },
                    {
                        "name": "Kenny Chirino Ampudia"
                    },
                    {
                        "name": "Rex Ying"
                    },
                    {
                        "name": "Leandros Tassiulas"
                    }
                ],
                "author_detail": {
                    "name": "Leandros Tassiulas"
                },
                "author": "Leandros Tassiulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05303v1",
                "updated": "2024-09-09T03:17:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    3,
                    17,
                    28,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T03:17:28Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    3,
                    17,
                    28,
                    0,
                    253,
                    0
                ],
                "title": "Resource-Efficient Generative AI Model Deployment in Mobile Edge\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Efficient Generative AI Model Deployment in Mobile Edge\n  Networks"
                },
                "summary": "The surging development of Artificial Intelligence-Generated Content (AIGC)\nmarks a transformative era of the content creation and production. Edge servers\npromise attractive benefits, e.g., reduced service delay and backhaul traffic\nload, for hosting AIGC services compared to cloud-based solutions. However, the\nscarcity of available resources on the edge pose significant challenges in\ndeploying generative AI models. In this paper, by characterizing the resource\nand delay demands of typical generative AI models, we find that the consumption\nof storage and GPU memory, as well as the model switching delay represented by\nI/O delay during the preloading phase, are significant and vary across models.\nThese multidimensional coupling factors render it difficult to make efficient\nedge model deployment decisions. Hence, we present a collaborative edge-cloud\nframework aiming to properly manage generative AI model deployment on the edge.\nSpecifically, we formulate edge model deployment problem considering\nheterogeneous features of models as an optimization problem, and propose a\nmodel-level decision selection algorithm to solve it. It enables pooled\nresource sharing and optimizes the trade-off between resource consumption and\ndelay in edge generative AI model deployment. Simulation results validate the\nefficacy of the proposed algorithm compared with baselines, demonstrating its\npotential to reduce overall costs by providing feature-aware model deployment\ndecisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surging development of Artificial Intelligence-Generated Content (AIGC)\nmarks a transformative era of the content creation and production. Edge servers\npromise attractive benefits, e.g., reduced service delay and backhaul traffic\nload, for hosting AIGC services compared to cloud-based solutions. However, the\nscarcity of available resources on the edge pose significant challenges in\ndeploying generative AI models. In this paper, by characterizing the resource\nand delay demands of typical generative AI models, we find that the consumption\nof storage and GPU memory, as well as the model switching delay represented by\nI/O delay during the preloading phase, are significant and vary across models.\nThese multidimensional coupling factors render it difficult to make efficient\nedge model deployment decisions. Hence, we present a collaborative edge-cloud\nframework aiming to properly manage generative AI model deployment on the edge.\nSpecifically, we formulate edge model deployment problem considering\nheterogeneous features of models as an optimization problem, and propose a\nmodel-level decision selection algorithm to solve it. It enables pooled\nresource sharing and optimizes the trade-off between resource consumption and\ndelay in edge generative AI model deployment. Simulation results validate the\nefficacy of the proposed algorithm compared with baselines, demonstrating its\npotential to reduce overall costs by providing feature-aware model deployment\ndecisions."
                },
                "authors": [
                    {
                        "name": "Yuxin Liang"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Yuanyuan He"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04214v2",
                "updated": "2024-09-09T02:46:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    2,
                    46,
                    34,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-06T12:11:06Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    12,
                    11,
                    6,
                    4,
                    250,
                    0
                ],
                "title": "Diagram Formalization Enhanced Multi-Modal Geometry Problem Solver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagram Formalization Enhanced Multi-Modal Geometry Problem Solver"
                },
                "summary": "Mathematical reasoning remains an ongoing challenge for AI models, especially\nfor geometry problems that require both linguistic and visual signals. As the\nvision encoders of most MLLMs are trained on natural scenes, they often\nstruggle to understand geometric diagrams, performing no better in geometry\nproblem solving than LLMs that only process text. This limitation is amplified\nby the lack of effective methods for representing geometric relationships. To\naddress these issues, we introduce the Diagram Formalization Enhanced Geometry\nProblem Solver (DFE-GPS), a new framework that integrates visual features,\ngeometric formal language, and natural language representations. We propose a\nnovel synthetic data approach and create a large-scale geometric dataset,\nSynthGeo228K, annotated with both formal and natural language captions,\ndesigned to enhance the vision encoder for a better understanding of geometric\nstructures. Our framework improves MLLMs' ability to process geometric diagrams\nand extends their application to open-ended tasks on the formalgeo7k dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning remains an ongoing challenge for AI models, especially\nfor geometry problems that require both linguistic and visual signals. As the\nvision encoders of most MLLMs are trained on natural scenes, they often\nstruggle to understand geometric diagrams, performing no better in geometry\nproblem solving than LLMs that only process text. This limitation is amplified\nby the lack of effective methods for representing geometric relationships. To\naddress these issues, we introduce the Diagram Formalization Enhanced Geometry\nProblem Solver (DFE-GPS), a new framework that integrates visual features,\ngeometric formal language, and natural language representations. We propose a\nnovel synthetic data approach and create a large-scale geometric dataset,\nSynthGeo228K, annotated with both formal and natural language captions,\ndesigned to enhance the vision encoder for a better understanding of geometric\nstructures. Our framework improves MLLMs' ability to process geometric diagrams\nand extends their application to open-ended tasks on the formalgeo7k dataset."
                },
                "authors": [
                    {
                        "name": "Zeren Zhang"
                    },
                    {
                        "name": "Jo-Ku Cheng"
                    },
                    {
                        "name": "Jingyang Deng"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Jinwen Ma"
                    },
                    {
                        "name": "Ziran Qin"
                    },
                    {
                        "name": "Xiaokai Zhang"
                    },
                    {
                        "name": "Na Zhu"
                    },
                    {
                        "name": "Tuo Leng"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Leng"
                },
                "author": "Tuo Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05286v1",
                "updated": "2024-09-09T02:41:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    2,
                    41,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T02:41:00Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    2,
                    41,
                    0,
                    0,
                    253,
                    0
                ],
                "title": "Seek and Solve Reasoning for Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seek and Solve Reasoning for Table Question Answering"
                },
                "summary": "Table-based Question Answering (TQA) involves answering questions based on\ntabular data. The complexity of table structures and question logic makes this\ntask difficult even for Large Language Models (LLMs). This paper improves TQA\nperformance by leveraging LLMs' reasoning capabilities. Inspired by how humans\nsolve TQA tasks, we propose a Seek-and-Solve pipeline that instructs the LLM to\nfirst seek relevant information and then answer questions. The two stages are\nintegrated at the reasoning level, and their Chain of Thought (CoT) paths are\nintegrated into a coherent Seek-and-Solve CoT (SS-CoT). Furthermore, we present\na compact single-stage TQA-solving prompt distilled from the pipeline.\nExperiments demonstrate that under In-Context Learning settings, using samples\nwith SS-CoT paths as demonstrations, the TQA-solving prompt can effectively\nguide the LLM to solve complex TQA tasks, resulting in improved performance and\nreliability. Our results highlight the importance of properly eliciting LLMs'\nreasoning capabilities in solving complex TQA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-based Question Answering (TQA) involves answering questions based on\ntabular data. The complexity of table structures and question logic makes this\ntask difficult even for Large Language Models (LLMs). This paper improves TQA\nperformance by leveraging LLMs' reasoning capabilities. Inspired by how humans\nsolve TQA tasks, we propose a Seek-and-Solve pipeline that instructs the LLM to\nfirst seek relevant information and then answer questions. The two stages are\nintegrated at the reasoning level, and their Chain of Thought (CoT) paths are\nintegrated into a coherent Seek-and-Solve CoT (SS-CoT). Furthermore, we present\na compact single-stage TQA-solving prompt distilled from the pipeline.\nExperiments demonstrate that under In-Context Learning settings, using samples\nwith SS-CoT paths as demonstrations, the TQA-solving prompt can effectively\nguide the LLM to solve complex TQA tasks, resulting in improved performance and\nreliability. Our results highlight the importance of properly eliciting LLMs'\nreasoning capabilities in solving complex TQA tasks."
                },
                "authors": [
                    {
                        "name": "Ruya Jiang"
                    },
                    {
                        "name": "Chun Wang"
                    },
                    {
                        "name": "Weihong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Weihong Deng"
                },
                "author": "Weihong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03515v2",
                "updated": "2024-09-09T01:55:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    1,
                    55,
                    3,
                    0,
                    253,
                    0
                ],
                "published": "2024-08-07T02:48:22Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    2,
                    48,
                    22,
                    2,
                    220,
                    0
                ],
                "title": "A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic\n  Systems"
                },
                "summary": "The integration of Large Language Models (LLMs) like GPT-4o into robotic\nsystems represents a significant advancement in embodied artificial\nintelligence. These models can process multi-modal prompts, enabling them to\ngenerate more context-aware responses. However, this integration is not without\nchallenges. One of the primary concerns is the potential security risks\nassociated with using LLMs in robotic navigation tasks. These tasks require\nprecise and reliable responses to ensure safe and effective operation.\nMulti-modal prompts, while enhancing the robot's understanding, also introduce\ncomplexities that can be exploited maliciously. For instance, adversarial\ninputs designed to mislead the model can lead to incorrect or dangerous\nnavigational decisions. This study investigates the impact of prompt injections\non mobile robot performance in LLM-integrated systems and explores secure\nprompt strategies to mitigate these risks. Our findings demonstrate a\nsubstantial overall improvement of approximately 30.8% in both attack detection\nand system performance with the implementation of robust defence mechanisms,\nhighlighting their critical role in enhancing security and reliability in\nmission-oriented tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) like GPT-4o into robotic\nsystems represents a significant advancement in embodied artificial\nintelligence. These models can process multi-modal prompts, enabling them to\ngenerate more context-aware responses. However, this integration is not without\nchallenges. One of the primary concerns is the potential security risks\nassociated with using LLMs in robotic navigation tasks. These tasks require\nprecise and reliable responses to ensure safe and effective operation.\nMulti-modal prompts, while enhancing the robot's understanding, also introduce\ncomplexities that can be exploited maliciously. For instance, adversarial\ninputs designed to mislead the model can lead to incorrect or dangerous\nnavigational decisions. This study investigates the impact of prompt injections\non mobile robot performance in LLM-integrated systems and explores secure\nprompt strategies to mitigate these risks. Our findings demonstrate a\nsubstantial overall improvement of approximately 30.8% in both attack detection\nand system performance with the implementation of robust defence mechanisms,\nhighlighting their critical role in enhancing security and reliability in\nmission-oriented tasks."
                },
                "authors": [
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Xiangrui Kong"
                    },
                    {
                        "name": "Conan Dewitt"
                    },
                    {
                        "name": "Thomas Braunl"
                    },
                    {
                        "name": "Jin B. Hong"
                    }
                ],
                "author_detail": {
                    "name": "Jin B. Hong"
                },
                "author": "Jin B. Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05258v1",
                "updated": "2024-09-09T00:47:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    0,
                    47,
                    30,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T00:47:30Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    0,
                    47,
                    30,
                    0,
                    253,
                    0
                ],
                "title": "Towards Automated Machine Learning Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Machine Learning Research"
                },
                "summary": "This paper explores a top-down approach to automating incremental advances in\nmachine learning research through component-level innovation, facilitated by\nLarge Language Models (LLMs). Our framework systematically generates novel\ncomponents, validates their feasibility, and evaluates their performance\nagainst existing baselines. A key distinction of this approach lies in how\nthese novel components are generated. Unlike traditional AutoML and NAS\nmethods, which often rely on a bottom-up combinatorial search over predefined,\nhardcoded base components, our method leverages the cross-domain knowledge\nembedded in LLMs to propose new components that may not be confined to any\nhard-coded predefined set. By incorporating a reward model to prioritize\npromising hypotheses, we aim to improve the efficiency of the hypothesis\ngeneration and evaluation process. We hope this approach offers a new avenue\nfor exploration and contributes to the ongoing dialogue in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores a top-down approach to automating incremental advances in\nmachine learning research through component-level innovation, facilitated by\nLarge Language Models (LLMs). Our framework systematically generates novel\ncomponents, validates their feasibility, and evaluates their performance\nagainst existing baselines. A key distinction of this approach lies in how\nthese novel components are generated. Unlike traditional AutoML and NAS\nmethods, which often rely on a bottom-up combinatorial search over predefined,\nhardcoded base components, our method leverages the cross-domain knowledge\nembedded in LLMs to propose new components that may not be confined to any\nhard-coded predefined set. By incorporating a reward model to prioritize\npromising hypotheses, we aim to improve the efficiency of the hypothesis\ngeneration and evaluation process. We hope this approach offers a new avenue\nfor exploration and contributes to the ongoing dialogue in the field."
                },
                "authors": [
                    {
                        "name": "Shervin Ardeshir"
                    }
                ],
                "author_detail": {
                    "name": "Shervin Ardeshir"
                },
                "author": "Shervin Ardeshir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05247v1",
                "updated": "2024-09-08T23:51:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    23,
                    51,
                    4,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T23:51:04Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    23,
                    51,
                    4,
                    6,
                    252,
                    0
                ],
                "title": "Socially Responsible Data for Large Multilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socially Responsible Data for Large Multilingual Language Models"
                },
                "summary": "Large Language Models (LLMs) have rapidly increased in size and apparent\ncapabilities in the last three years, but their training data is largely\nEnglish text. There is growing interest in multilingual LLMs, and various\nefforts are striving for models to accommodate languages of communities outside\nof the Global North, which include many languages that have been historically\nunderrepresented in digital realms. These languages have been coined as \"low\nresource languages\" or \"long-tail languages\", and LLMs performance on these\nlanguages is generally poor. While expanding the use of LLMs to more languages\nmay bring many potential benefits, such as assisting cross-community\ncommunication and language preservation, great care must be taken to ensure\nthat data collection on these languages is not extractive and that it does not\nreproduce exploitative practices of the past. Collecting data from languages\nspoken by previously colonized people, indigenous people, and non-Western\nlanguages raises many complex sociopolitical and ethical questions, e.g.,\naround consent, cultural safety, and data sovereignty. Furthermore, linguistic\ncomplexity and cultural nuances are often lost in LLMs. This position paper\nbuilds on recent scholarship, and our own work, and outlines several relevant\nsocial, cultural, and ethical considerations and potential ways to mitigate\nthem through qualitative research, community partnerships, and participatory\ndesign approaches. We provide twelve recommendations for consideration when\ncollecting language data on underrepresented language communities outside of\nthe Global North.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have rapidly increased in size and apparent\ncapabilities in the last three years, but their training data is largely\nEnglish text. There is growing interest in multilingual LLMs, and various\nefforts are striving for models to accommodate languages of communities outside\nof the Global North, which include many languages that have been historically\nunderrepresented in digital realms. These languages have been coined as \"low\nresource languages\" or \"long-tail languages\", and LLMs performance on these\nlanguages is generally poor. While expanding the use of LLMs to more languages\nmay bring many potential benefits, such as assisting cross-community\ncommunication and language preservation, great care must be taken to ensure\nthat data collection on these languages is not extractive and that it does not\nreproduce exploitative practices of the past. Collecting data from languages\nspoken by previously colonized people, indigenous people, and non-Western\nlanguages raises many complex sociopolitical and ethical questions, e.g.,\naround consent, cultural safety, and data sovereignty. Furthermore, linguistic\ncomplexity and cultural nuances are often lost in LLMs. This position paper\nbuilds on recent scholarship, and our own work, and outlines several relevant\nsocial, cultural, and ethical considerations and potential ways to mitigate\nthem through qualitative research, community partnerships, and participatory\ndesign approaches. We provide twelve recommendations for consideration when\ncollecting language data on underrepresented language communities outside of\nthe Global North."
                },
                "authors": [
                    {
                        "name": "Andrew Smart"
                    },
                    {
                        "name": "Ben Hutchinson"
                    },
                    {
                        "name": "Lameck Mbangula Amugongo"
                    },
                    {
                        "name": "Suzanne Dikker"
                    },
                    {
                        "name": "Alex Zito"
                    },
                    {
                        "name": "Amber Ebinama"
                    },
                    {
                        "name": "Zara Wudiri"
                    },
                    {
                        "name": "Ding Wang"
                    },
                    {
                        "name": "Erin van Liemt"
                    },
                    {
                        "name": "João Sedoc"
                    },
                    {
                        "name": "Seyi Olojo"
                    },
                    {
                        "name": "Stanley Uwakwe"
                    },
                    {
                        "name": "Edem Wornyo"
                    },
                    {
                        "name": "Sonja Schmer-Galunder"
                    },
                    {
                        "name": "Jamila Smith-Loud"
                    }
                ],
                "author_detail": {
                    "name": "Jamila Smith-Loud"
                },
                "author": "Jamila Smith-Loud",
                "arxiv_journal_ref": "ACM conference on Equity and Access in Algorithms, Mechanisms, and\n  Optimization, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06682v2",
                "updated": "2024-09-08T20:33:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    33,
                    3,
                    6,
                    252,
                    0
                ],
                "published": "2024-05-05T18:56:46Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    18,
                    56,
                    46,
                    6,
                    126,
                    0
                ],
                "title": "Self-Reflection in LLM Agents: Effects on Problem-Solving Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Reflection in LLM Agents: Effects on Problem-Solving Performance"
                },
                "summary": "In this study, we investigated the effects of self-reflection in large\nlanguage models (LLMs) on problem-solving performance. We instructed nine\npopular LLMs to answer a series of multiple-choice questions to provide a\nperformance baseline. For each incorrectly answered question, we instructed\neight types of self-reflecting LLM agents to reflect on their mistakes and\nprovide themselves with guidance to improve problem-solving. Then, using this\nguidance, each self-reflecting agent attempted to re-answer the same questions.\nOur results indicate that LLM agents are able to significantly improve their\nproblem-solving performance through self-reflection ($p < 0.001$). In addition,\nwe compared the various types of self-reflection to determine their individual\ncontribution to performance. All code and data are available on GitHub at\nhttps://github.com/matthewrenze/self-reflection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigated the effects of self-reflection in large\nlanguage models (LLMs) on problem-solving performance. We instructed nine\npopular LLMs to answer a series of multiple-choice questions to provide a\nperformance baseline. For each incorrectly answered question, we instructed\neight types of self-reflecting LLM agents to reflect on their mistakes and\nprovide themselves with guidance to improve problem-solving. Then, using this\nguidance, each self-reflecting agent attempted to re-answer the same questions.\nOur results indicate that LLM agents are able to significantly improve their\nproblem-solving performance through self-reflection ($p < 0.001$). In addition,\nwe compared the various types of self-reflection to determine their individual\ncontribution to performance. All code and data are available on GitHub at\nhttps://github.com/matthewrenze/self-reflection"
                },
                "authors": [
                    {
                        "name": "Matthew Renze"
                    },
                    {
                        "name": "Erhan Guven"
                    }
                ],
                "author_detail": {
                    "name": "Erhan Guven"
                },
                "author": "Erhan Guven",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19612v2",
                "updated": "2024-09-08T20:32:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    32,
                    47,
                    6,
                    252,
                    0
                ],
                "published": "2024-05-30T02:00:03Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    2,
                    0,
                    3,
                    3,
                    151,
                    0
                ],
                "title": "Keyword-driven Retrieval-Augmented Large Language Models for Cold-start\n  User Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyword-driven Retrieval-Augmented Large Language Models for Cold-start\n  User Recommendations"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have shown significant\npotential in enhancing recommender systems. However, addressing the cold-start\nrecommendation problem, where users lack historical data, remains a\nconsiderable challenge. In this paper, we introduce KALM4Rec (Keyword-driven\nRetrieval-Augmented Large Language Models for Cold-start User Recommendations),\na novel framework specifically designed to tackle this problem by requiring\nonly a few input keywords from users in a practical scenario of cold-start user\nrestaurant recommendations. KALM4Rec operates in two main stages: candidates\nretrieval and LLM-based candidates re-ranking. In the first stage,\nkeyword-driven retrieval models are used to identify potential candidates,\naddressing LLMs' limitations in processing extensive tokens and reducing the\nrisk of generating misleading information. In the second stage, we employ LLMs\nwith various prompting strategies, including zero-shot and few-shot techniques,\nto re-rank these candidates by integrating multiple examples directly into the\nLLM prompts. Our evaluation, using a Yelp restaurant dataset with user reviews\nfrom three English-speaking cities, shows that our proposed framework\nsignificantly improves recommendation quality. Specifically, the integration of\nin-context instructions with LLMs for re-ranking markedly enhances the\nperformance of the cold-start user recommender system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have shown significant\npotential in enhancing recommender systems. However, addressing the cold-start\nrecommendation problem, where users lack historical data, remains a\nconsiderable challenge. In this paper, we introduce KALM4Rec (Keyword-driven\nRetrieval-Augmented Large Language Models for Cold-start User Recommendations),\na novel framework specifically designed to tackle this problem by requiring\nonly a few input keywords from users in a practical scenario of cold-start user\nrestaurant recommendations. KALM4Rec operates in two main stages: candidates\nretrieval and LLM-based candidates re-ranking. In the first stage,\nkeyword-driven retrieval models are used to identify potential candidates,\naddressing LLMs' limitations in processing extensive tokens and reducing the\nrisk of generating misleading information. In the second stage, we employ LLMs\nwith various prompting strategies, including zero-shot and few-shot techniques,\nto re-rank these candidates by integrating multiple examples directly into the\nLLM prompts. Our evaluation, using a Yelp restaurant dataset with user reviews\nfrom three English-speaking cities, shows that our proposed framework\nsignificantly improves recommendation quality. Specifically, the integration of\nin-context instructions with LLMs for re-ranking markedly enhances the\nperformance of the cold-start user recommender system."
                },
                "authors": [
                    {
                        "name": "Hai-Dang Kieu"
                    },
                    {
                        "name": "Minh Duc Nguyen"
                    },
                    {
                        "name": "Thanh-Son Nguyen"
                    },
                    {
                        "name": "Dung D. Le"
                    }
                ],
                "author_detail": {
                    "name": "Dung D. Le"
                },
                "author": "Dung D. Le",
                "arxiv_comment": "10 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00217v2",
                "updated": "2024-09-08T20:08:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    8,
                    16,
                    6,
                    252,
                    0
                ],
                "published": "2024-08-30T19:14:17Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    19,
                    14,
                    17,
                    4,
                    243,
                    0
                ],
                "title": "ProGRes: Prompted Generative Rescoring on ASR n-Best",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProGRes: Prompted Generative Rescoring on ASR n-Best"
                },
                "summary": "Large Language Models (LLMs) have shown their ability to improve the\nperformance of speech recognizers by effectively rescoring the n-best\nhypotheses generated during the beam search process. However, the best way to\nexploit recent generative instruction-tuned LLMs for hypothesis rescoring is\nstill unclear. This paper proposes a novel method that uses instruction-tuned\nLLMs to dynamically expand the n-best speech recognition hypotheses with new\nhypotheses generated through appropriately-prompted LLMs. Specifically, we\nintroduce a new zero-shot method for ASR n-best rescoring, which combines\nconfidence scores, LLM sequence scoring, and prompt-based hypothesis\ngeneration. We compare Llama-3-Instruct, GPT-3.5 Turbo, and GPT-4 Turbo as\nprompt-based generators with Llama-3 as sequence scorer LLM. We evaluated our\napproach using different speech recognizers and observed significant relative\nimprovement in the word error rate (WER) ranging from 5% to 25%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown their ability to improve the\nperformance of speech recognizers by effectively rescoring the n-best\nhypotheses generated during the beam search process. However, the best way to\nexploit recent generative instruction-tuned LLMs for hypothesis rescoring is\nstill unclear. This paper proposes a novel method that uses instruction-tuned\nLLMs to dynamically expand the n-best speech recognition hypotheses with new\nhypotheses generated through appropriately-prompted LLMs. Specifically, we\nintroduce a new zero-shot method for ASR n-best rescoring, which combines\nconfidence scores, LLM sequence scoring, and prompt-based hypothesis\ngeneration. We compare Llama-3-Instruct, GPT-3.5 Turbo, and GPT-4 Turbo as\nprompt-based generators with Llama-3 as sequence scorer LLM. We evaluated our\napproach using different speech recognizers and observed significant relative\nimprovement in the word error rate (WER) ranging from 5% to 25%."
                },
                "authors": [
                    {
                        "name": "Ada Defne Tur"
                    },
                    {
                        "name": "Adel Moumen"
                    },
                    {
                        "name": "Mirco Ravanelli"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Ravanelli"
                },
                "author": "Mirco Ravanelli",
                "arxiv_comment": "IEEE Spoken Language Technology Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01527v2",
                "updated": "2024-09-08T19:59:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    19,
                    59,
                    6,
                    6,
                    252,
                    0
                ],
                "published": "2024-08-02T18:40:10Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    40,
                    10,
                    4,
                    215,
                    0
                ],
                "title": "Using LLMs to Establish Implicit User Sentiment of Software Desirability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs to Establish Implicit User Sentiment of Software Desirability"
                },
                "summary": "This study explores the use of LLMs for providing quantitative zero-shot\nsentiment analysis of implicit software desirability, addressing a critical\nchallenge in product evaluation where traditional review scores, though\nconvenient, fail to capture the richness of qualitative user feedback.\nInnovations include establishing a method that 1) works with qualitative user\nexperience data without the need for explicit review scores, 2) focuses on\nimplicit user satisfaction, and 3) provides scaled numerical sentiment\nanalysis, offering a more nuanced understanding of user sentiment, instead of\nsimply classifying sentiment as positive, neutral, or negative.\n  Data is collected using the Microsoft Product Desirability Toolkit (PDT), a\nwell-known qualitative user experience analysis tool. For initial exploration,\nthe PDT metric was given to users of two software systems. PDT data was fed\nthrough several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and through a\nleading transfer learning technique, Twitter-Roberta-Base-Sentiment, and Vader,\na leading sentiment analysis tool. Each system was asked to evaluate the data\nin two ways, by looking at the sentiment expressed in the PDT word/explanation\npairs; and by looking at the sentiment expressed by the users in their grouped\nselection of five words and explanations, as a whole. Each LLM provided a\nsentiment score, its confidence (low, medium, high) in the score, and an\nexplanation of the score.\n  All LLMs tested were able to statistically detect user sentiment from the\nusers' grouped data, whereas TRBS and Vader were not. The confidence and\nexplanation of confidence provided by the LLMs assisted in understanding user\nsentiment. This study adds deeper understanding of evaluating user experiences,\ntoward the goal of creating a universal tool that quantifies implicit\nsentiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of LLMs for providing quantitative zero-shot\nsentiment analysis of implicit software desirability, addressing a critical\nchallenge in product evaluation where traditional review scores, though\nconvenient, fail to capture the richness of qualitative user feedback.\nInnovations include establishing a method that 1) works with qualitative user\nexperience data without the need for explicit review scores, 2) focuses on\nimplicit user satisfaction, and 3) provides scaled numerical sentiment\nanalysis, offering a more nuanced understanding of user sentiment, instead of\nsimply classifying sentiment as positive, neutral, or negative.\n  Data is collected using the Microsoft Product Desirability Toolkit (PDT), a\nwell-known qualitative user experience analysis tool. For initial exploration,\nthe PDT metric was given to users of two software systems. PDT data was fed\nthrough several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and through a\nleading transfer learning technique, Twitter-Roberta-Base-Sentiment, and Vader,\na leading sentiment analysis tool. Each system was asked to evaluate the data\nin two ways, by looking at the sentiment expressed in the PDT word/explanation\npairs; and by looking at the sentiment expressed by the users in their grouped\nselection of five words and explanations, as a whole. Each LLM provided a\nsentiment score, its confidence (low, medium, high) in the score, and an\nexplanation of the score.\n  All LLMs tested were able to statistically detect user sentiment from the\nusers' grouped data, whereas TRBS and Vader were not. The confidence and\nexplanation of confidence provided by the LLMs assisted in understanding user\nsentiment. This study adds deeper understanding of evaluating user experiences,\ntoward the goal of creating a universal tool that quantifies implicit\nsentiment."
                },
                "authors": [
                    {
                        "name": "Sherri Weitl-Harms"
                    },
                    {
                        "name": "John D. Hastings"
                    },
                    {
                        "name": "Jonah Lum"
                    }
                ],
                "author_detail": {
                    "name": "Jonah Lum"
                },
                "author": "Jonah Lum",
                "arxiv_comment": "6 pages, 2 figures, 2 tables, updated to incorporate feedback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.2.8; I.2.6; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05207v1",
                "updated": "2024-09-08T19:50:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    19,
                    50,
                    25,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T19:50:25Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    19,
                    50,
                    25,
                    6,
                    252,
                    0
                ],
                "title": "Low Latency Transformer Inference on FPGAs for Physics Applications with\n  hls4ml",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Latency Transformer Inference on FPGAs for Physics Applications with\n  hls4ml"
                },
                "summary": "This study presents an efficient implementation of transformer architectures\nin Field-Programmable Gate Arrays(FPGAs) using hls4ml. We demonstrate the\nstrategy for implementing the multi-head attention, softmax, and normalization\nlayer and evaluate three distinct models. Their deployment on VU13P FPGA chip\nachieved latency less than 2us, demonstrating the potential for real-time\napplications. HLS4ML compatibility with any TensorFlow-built transformer model\nfurther enhances the scalability and applicability of this work. Index Terms:\nFPGAs, machine learning, transformers, high energy physics, LIGO",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents an efficient implementation of transformer architectures\nin Field-Programmable Gate Arrays(FPGAs) using hls4ml. We demonstrate the\nstrategy for implementing the multi-head attention, softmax, and normalization\nlayer and evaluate three distinct models. Their deployment on VU13P FPGA chip\nachieved latency less than 2us, demonstrating the potential for real-time\napplications. HLS4ML compatibility with any TensorFlow-built transformer model\nfurther enhances the scalability and applicability of this work. Index Terms:\nFPGAs, machine learning, transformers, high energy physics, LIGO"
                },
                "authors": [
                    {
                        "name": "Zhixing Jiang"
                    },
                    {
                        "name": "Dennis Yin"
                    },
                    {
                        "name": "Yihui Chen"
                    },
                    {
                        "name": "Elham E Khoda"
                    },
                    {
                        "name": "Scott Hauck"
                    },
                    {
                        "name": "Shih-Chieh Hsu"
                    },
                    {
                        "name": "Ekaterina Govorkova"
                    },
                    {
                        "name": "Philip Harris"
                    },
                    {
                        "name": "Vladimir Loncar"
                    },
                    {
                        "name": "Eric A. Moreno"
                    }
                ],
                "author_detail": {
                    "name": "Eric A. Moreno"
                },
                "author": "Eric A. Moreno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05197v1",
                "updated": "2024-09-08T19:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    19,
                    22,
                    58,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T19:22:58Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    19,
                    22,
                    58,
                    6,
                    252,
                    0
                ],
                "title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?"
                },
                "summary": "State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge."
                },
                "authors": [
                    {
                        "name": "Neeladri Bhuiya"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "arxiv_comment": "16 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17124v2",
                "updated": "2024-09-08T19:17:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    19,
                    17,
                    32,
                    6,
                    252,
                    0
                ],
                "published": "2024-02-27T01:37:23Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    1,
                    37,
                    23,
                    1,
                    58,
                    0
                ],
                "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large\n  Language Models"
                },
                "summary": "For a LLM to be trustworthy, its confidence level should be well-calibrated\nwith its actual performance. While it is now common sense that LLM performances\nare greatly impacted by prompts, the confidence calibration in prompting LLMs\nhas yet to be thoroughly explored. In this paper, we explore how different\nprompting strategies influence LLM confidence calibration and how it could be\nimproved. We conduct extensive experiments on six prompting methods in the\nquestion-answering context and we observe that, while these methods help\nimprove the expected LLM calibration, they also trigger LLMs to be\nover-confident when responding to some instances. Inspired by human cognition,\nwe propose Fact-and-Reflection (FaR) prompting, which improves the LLM\ncalibration in two steps. First, FaR elicits the known \"facts\" that are\nrelevant to the input prompt from the LLM. And then it asks the model to\n\"reflect\" over them to generate the final answer. Experiments show that FaR\nprompting achieves significantly better calibration; it lowers the Expected\nCalibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR\nprompting even elicits the capability of verbally expressing concerns in less\nconfident scenarios, which helps trigger retrieval augmentation for solving\nthese harder instances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a LLM to be trustworthy, its confidence level should be well-calibrated\nwith its actual performance. While it is now common sense that LLM performances\nare greatly impacted by prompts, the confidence calibration in prompting LLMs\nhas yet to be thoroughly explored. In this paper, we explore how different\nprompting strategies influence LLM confidence calibration and how it could be\nimproved. We conduct extensive experiments on six prompting methods in the\nquestion-answering context and we observe that, while these methods help\nimprove the expected LLM calibration, they also trigger LLMs to be\nover-confident when responding to some instances. Inspired by human cognition,\nwe propose Fact-and-Reflection (FaR) prompting, which improves the LLM\ncalibration in two steps. First, FaR elicits the known \"facts\" that are\nrelevant to the input prompt from the LLM. And then it asks the model to\n\"reflect\" over them to generate the final answer. Experiments show that FaR\nprompting achieves significantly better calibration; it lowers the Expected\nCalibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR\nprompting even elicits the capability of verbally expressing concerns in less\nconfident scenarios, which helps trigger retrieval augmentation for solving\nthese harder instances."
                },
                "authors": [
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Xiaoman Pan"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Tongshuang Wu"
                    },
                    {
                        "name": "Jianshu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianshu Chen"
                },
                "author": "Jianshu Chen",
                "arxiv_comment": "17 pages, 10 figures",
                "arxiv_journal_ref": "Findings of the Association for Computational Linguistics ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15695v2",
                "updated": "2024-09-08T19:06:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    19,
                    6,
                    37,
                    6,
                    252,
                    0
                ],
                "published": "2024-06-22T00:14:48Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    0,
                    14,
                    48,
                    5,
                    174,
                    0
                ],
                "title": "SS-GEN: A Social Story Generation Framework with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SS-GEN: A Social Story Generation Framework with Large Language Models"
                },
                "summary": "Children with Autism Spectrum Disorder (ASD) often misunderstand social\nsituations and struggle to participate in daily routines. Social Stories are\ntraditionally crafted by psychology experts under strict constraints to address\nthese challenges but are costly and limited in diversity. As Large Language\nModels (LLMs) advance, there's an opportunity to develop more automated,\naffordable, and accessible methods to generate Social Stories in real-time with\nbroad coverage. However, adapting LLMs to meet the unique and strict\nconstraints of Social Stories is a challenging issue. To this end, we propose\n\\textbf{SS-GEN}, a \\textbf{S}ocial \\textbf{S}tory \\textbf{GEN}eration framework\nwith LLMs. Firstly, we develop a constraint-driven sophisticated strategy named\n\\textbf{\\textsc{StarSow}} to hierarchically prompt LLMs to generate Social\nStories at scale, followed by rigorous human filtering to build a high-quality\ndataset. Additionally, we introduce \\textbf{quality assessment criteria} to\nevaluate the effectiveness of these generated stories. Considering that\npowerful closed-source large models require very complex instructions and\nexpensive API fees, we finally fine-tune smaller language models with our\ncurated high-quality dataset, achieving comparable results at lower costs and\nwith simpler instruction and deployment. This work marks a significant step in\nleveraging AI to personalize Social Stories cost-effectively for autistic\nchildren at scale, which we hope can encourage future research. The prompt,\ncode and data will release in the \\texttt{Technical Appendix} and \\texttt{Code\n\\& Data Appendix} at \\url{https://github.com/MIMIFY/SS-GEN}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Children with Autism Spectrum Disorder (ASD) often misunderstand social\nsituations and struggle to participate in daily routines. Social Stories are\ntraditionally crafted by psychology experts under strict constraints to address\nthese challenges but are costly and limited in diversity. As Large Language\nModels (LLMs) advance, there's an opportunity to develop more automated,\naffordable, and accessible methods to generate Social Stories in real-time with\nbroad coverage. However, adapting LLMs to meet the unique and strict\nconstraints of Social Stories is a challenging issue. To this end, we propose\n\\textbf{SS-GEN}, a \\textbf{S}ocial \\textbf{S}tory \\textbf{GEN}eration framework\nwith LLMs. Firstly, we develop a constraint-driven sophisticated strategy named\n\\textbf{\\textsc{StarSow}} to hierarchically prompt LLMs to generate Social\nStories at scale, followed by rigorous human filtering to build a high-quality\ndataset. Additionally, we introduce \\textbf{quality assessment criteria} to\nevaluate the effectiveness of these generated stories. Considering that\npowerful closed-source large models require very complex instructions and\nexpensive API fees, we finally fine-tune smaller language models with our\ncurated high-quality dataset, achieving comparable results at lower costs and\nwith simpler instruction and deployment. This work marks a significant step in\nleveraging AI to personalize Social Stories cost-effectively for autistic\nchildren at scale, which we hope can encourage future research. The prompt,\ncode and data will release in the \\texttt{Technical Appendix} and \\texttt{Code\n\\& Data Appendix} at \\url{https://github.com/MIMIFY/SS-GEN}."
                },
                "authors": [
                    {
                        "name": "Yi Feng"
                    },
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Guanqun Bi"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Liping Jing"
                    },
                    {
                        "name": "Jian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jian Yu"
                },
                "author": "Jian Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05177v1",
                "updated": "2024-09-08T18:24:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    18,
                    24,
                    26,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T18:24:26Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    18,
                    24,
                    26,
                    6,
                    252,
                    0
                ],
                "title": "Insights from Benchmarking Frontier Language Models on Web App Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights from Benchmarking Frontier Language Models on Web App Code\n  Generation"
                },
                "summary": "This paper presents insights from evaluating 16 frontier large language\nmodels (LLMs) on the WebApp1K benchmark, a test suite designed to assess the\nability of LLMs to generate web application code. The results reveal that while\nall models possess similar underlying knowledge, their performance is\ndifferentiated by the frequency of mistakes they make. By analyzing lines of\ncode (LOC) and failure distributions, we find that writing correct code is more\ncomplex than generating incorrect code. Furthermore, prompt engineering shows\nlimited efficacy in reducing errors beyond specific cases. These findings\nsuggest that further advancements in coding LLM should emphasize on model\nreliability and mistake minimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents insights from evaluating 16 frontier large language\nmodels (LLMs) on the WebApp1K benchmark, a test suite designed to assess the\nability of LLMs to generate web application code. The results reveal that while\nall models possess similar underlying knowledge, their performance is\ndifferentiated by the frequency of mistakes they make. By analyzing lines of\ncode (LOC) and failure distributions, we find that writing correct code is more\ncomplex than generating incorrect code. Furthermore, prompt engineering shows\nlimited efficacy in reducing errors beyond specific cases. These findings\nsuggest that further advancements in coding LLM should emphasize on model\nreliability and mistake minimization."
                },
                "authors": [
                    {
                        "name": "Yi Cui"
                    }
                ],
                "author_detail": {
                    "name": "Yi Cui"
                },
                "author": "Yi Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13043v2",
                "updated": "2024-09-08T17:46:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    17,
                    46,
                    36,
                    6,
                    252,
                    0
                ],
                "published": "2024-04-19T17:57:29Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    17,
                    57,
                    29,
                    4,
                    110,
                    0
                ],
                "title": "Data Alignment for Zero-Shot Concept Generation in Dermatology AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Alignment for Zero-Shot Concept Generation in Dermatology AI"
                },
                "summary": "AI in dermatology is evolving at a rapid pace but the major limitation to\ntraining trustworthy classifiers is the scarcity of data with ground-truth\nconcept level labels, which are meta-labels semantically meaningful to humans.\nFoundation models like CLIP providing zero-shot capabilities can help alleviate\nthis challenge by leveraging vast amounts of image-caption pairs available on\nthe internet. CLIP can be fine-tuned using domain specific image-caption pairs\nto improve classification performance. However, CLIP's pre-training data is not\nwell-aligned with the medical jargon that clinicians use to perform diagnoses.\nThe development of large language models (LLMs) in recent years has led to the\npossibility of leveraging the expressive nature of these models to generate\nrich text. Our goal is to use these models to generate caption text that aligns\nwell with both the clinical lexicon and with the natural human language used in\nCLIP's pre-training data. Starting with captions used for images in PubMed\narticles, we extend them by passing the raw captions through an LLM fine-tuned\non the field's several textbooks. We find that using captions generated by an\nexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept\nclassification performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI in dermatology is evolving at a rapid pace but the major limitation to\ntraining trustworthy classifiers is the scarcity of data with ground-truth\nconcept level labels, which are meta-labels semantically meaningful to humans.\nFoundation models like CLIP providing zero-shot capabilities can help alleviate\nthis challenge by leveraging vast amounts of image-caption pairs available on\nthe internet. CLIP can be fine-tuned using domain specific image-caption pairs\nto improve classification performance. However, CLIP's pre-training data is not\nwell-aligned with the medical jargon that clinicians use to perform diagnoses.\nThe development of large language models (LLMs) in recent years has led to the\npossibility of leveraging the expressive nature of these models to generate\nrich text. Our goal is to use these models to generate caption text that aligns\nwell with both the clinical lexicon and with the natural human language used in\nCLIP's pre-training data. Starting with captions used for images in PubMed\narticles, we extend them by passing the raw captions through an LLM fine-tuned\non the field's several textbooks. We find that using captions generated by an\nexpressive fine-tuned LLM like GPT-3.5 improves downstream zero-shot concept\nclassification performance."
                },
                "authors": [
                    {
                        "name": "Soham Gadgil"
                    },
                    {
                        "name": "Mahtab Bigverdi"
                    }
                ],
                "author_detail": {
                    "name": "Mahtab Bigverdi"
                },
                "author": "Mahtab Bigverdi",
                "arxiv_comment": "Accepted as a workshop paper to ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.03312v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.03312v9",
                "updated": "2024-09-08T17:23:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    17,
                    23,
                    16,
                    6,
                    252,
                    0
                ],
                "published": "2023-08-07T05:40:58Z",
                "published_parsed": [
                    2023,
                    8,
                    7,
                    5,
                    40,
                    58,
                    0,
                    219,
                    0
                ],
                "title": "Exploiting Code Symmetries for Learning Program Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Code Symmetries for Learning Program Semantics"
                },
                "summary": "This paper tackles the challenge of teaching code semantics to Large Language\nModels (LLMs) for program analysis by incorporating code symmetries into the\nmodel architecture. We introduce a group-theoretic framework that defines code\nsymmetries as semantics-preserving transformations, where forming a code\nsymmetry group enables precise and efficient reasoning of code semantics. Our\nsolution, SymC, develops a novel variant of self-attention that is provably\nequivariant to code symmetries from the permutation group defined over the\nprogram dependence graph. SymC obtains superior performance on five program\nanalysis tasks, outperforming state-of-the-art code models without any\npre-training. Our results suggest that code LLMs that encode the code\nstructural prior via the code symmetry group generalize better and faster.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the challenge of teaching code semantics to Large Language\nModels (LLMs) for program analysis by incorporating code symmetries into the\nmodel architecture. We introduce a group-theoretic framework that defines code\nsymmetries as semantics-preserving transformations, where forming a code\nsymmetry group enables precise and efficient reasoning of code semantics. Our\nsolution, SymC, develops a novel variant of self-attention that is provably\nequivariant to code symmetries from the permutation group defined over the\nprogram dependence graph. SymC obtains superior performance on five program\nanalysis tasks, outperforming state-of-the-art code models without any\npre-training. Our results suggest that code LLMs that encode the code\nstructural prior via the code symmetry group generalize better and faster."
                },
                "authors": [
                    {
                        "name": "Kexin Pei"
                    },
                    {
                        "name": "Weichen Li"
                    },
                    {
                        "name": "Qirui Jin"
                    },
                    {
                        "name": "Shuyang Liu"
                    },
                    {
                        "name": "Scott Geng"
                    },
                    {
                        "name": "Lorenzo Cavallaro"
                    },
                    {
                        "name": "Junfeng Yang"
                    },
                    {
                        "name": "Suman Jana"
                    }
                ],
                "author_detail": {
                    "name": "Suman Jana"
                },
                "author": "Suman Jana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.03312v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.03312v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14105v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14105v3",
                "updated": "2024-09-08T17:15:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    17,
                    15,
                    17,
                    6,
                    252,
                    0
                ],
                "published": "2024-05-23T02:14:17Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    2,
                    14,
                    17,
                    3,
                    144,
                    0
                ],
                "title": "Distributed Speculative Inference of Large Language Models is Provably\n  Faster",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Speculative Inference of Large Language Models is Provably\n  Faster"
                },
                "summary": "Accelerating the inference of large language models (LLMs) is an important\nchallenge in artificial intelligence. This paper introduces Distributed\nSpeculative Inference (DSI), a novel distributed inference algorithm that is\nprovably faster than speculative inference (SI)\n[leviathan2023fast,chen2023accelerating,miao2023specinfer] and traditional\nautoregressive inference (non-SI). Like other SI algorithms, DSI works on\nfrozen LLMs, requiring no training or architectural modifications, and it\npreserves the target distribution. Prior studies on SI have demonstrated\nempirical speedups (compared to non-SI) but require fast and accurate drafters,\nwhich are often unavailable in practice. We identify a gap where SI can be\nslower than non-SI given slower or less accurate drafters. We close this gap by\nproving that DSI is faster than both SI and non-SI--given any drafters. DSI\nintroduces a novel type of task parallelism called Speculation Parallelism\n(SP), which orchestrates target and drafter instances to overlap in time,\ncreating a new foundational tradeoff between computational resources and\nlatency. DSI is not only faster than SI but also supports LLMs that cannot be\naccelerated with SI. Our simulations show speedups of off-the-shelf LLMs in\nrealistic single-node settings where DSI is 1.29-1.92x faster than SI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating the inference of large language models (LLMs) is an important\nchallenge in artificial intelligence. This paper introduces Distributed\nSpeculative Inference (DSI), a novel distributed inference algorithm that is\nprovably faster than speculative inference (SI)\n[leviathan2023fast,chen2023accelerating,miao2023specinfer] and traditional\nautoregressive inference (non-SI). Like other SI algorithms, DSI works on\nfrozen LLMs, requiring no training or architectural modifications, and it\npreserves the target distribution. Prior studies on SI have demonstrated\nempirical speedups (compared to non-SI) but require fast and accurate drafters,\nwhich are often unavailable in practice. We identify a gap where SI can be\nslower than non-SI given slower or less accurate drafters. We close this gap by\nproving that DSI is faster than both SI and non-SI--given any drafters. DSI\nintroduces a novel type of task parallelism called Speculation Parallelism\n(SP), which orchestrates target and drafter instances to overlap in time,\ncreating a new foundational tradeoff between computational resources and\nlatency. DSI is not only faster than SI but also supports LLMs that cannot be\naccelerated with SI. Our simulations show speedups of off-the-shelf LLMs in\nrealistic single-node settings where DSI is 1.29-1.92x faster than SI."
                },
                "authors": [
                    {
                        "name": "Nadav Timor"
                    },
                    {
                        "name": "Jonathan Mamou"
                    },
                    {
                        "name": "Daniel Korat"
                    },
                    {
                        "name": "Moshe Berchansky"
                    },
                    {
                        "name": "Oren Pereg"
                    },
                    {
                        "name": "Moshe Wasserblat"
                    },
                    {
                        "name": "Tomer Galanti"
                    },
                    {
                        "name": "Michal Gordon"
                    },
                    {
                        "name": "David Harel"
                    }
                ],
                "author_detail": {
                    "name": "David Harel"
                },
                "author": "David Harel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14105v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14105v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02839v2",
                "updated": "2024-09-08T17:09:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    17,
                    9,
                    16,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-04T16:09:28Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    9,
                    28,
                    2,
                    248,
                    0
                ],
                "title": "Jäger: Automated Telephone Call Traceback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jäger: Automated Telephone Call Traceback"
                },
                "summary": "Unsolicited telephone calls that facilitate fraud or unlawful telemarketing\ncontinue to overwhelm network users and the regulators who prosecute them. The\nfirst step in prosecuting phone abuse is traceback -- identifying the call\noriginator. This fundamental investigative task currently requires hours of\nmanual effort per call. In this paper, we introduce J\\\"ager, a distributed\nsecure call traceback system. J\\\"ager can trace a call in a few seconds, even\nwith partial deployment, while cryptographically preserving the privacy of call\nparties, carrier trade secrets like peers and call volume, and limiting the\nthreat of bulk analysis. We establish definitions and requirements of secure\ntraceback, then develop a suite of protocols that meet these requirements using\nwitness encryption, oblivious pseudorandom functions, and group signatures. We\nprove these protocols secure in the universal composibility framework. We then\ndemonstrate that J\\\"ager has low compute and bandwidth costs per call, and\nthese costs scale linearly with call volume. J\\\"ager provides an efficient,\nsecure, privacy-preserving system to revolutionize telephone abuse\ninvestigation with minimal costs to operators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsolicited telephone calls that facilitate fraud or unlawful telemarketing\ncontinue to overwhelm network users and the regulators who prosecute them. The\nfirst step in prosecuting phone abuse is traceback -- identifying the call\noriginator. This fundamental investigative task currently requires hours of\nmanual effort per call. In this paper, we introduce J\\\"ager, a distributed\nsecure call traceback system. J\\\"ager can trace a call in a few seconds, even\nwith partial deployment, while cryptographically preserving the privacy of call\nparties, carrier trade secrets like peers and call volume, and limiting the\nthreat of bulk analysis. We establish definitions and requirements of secure\ntraceback, then develop a suite of protocols that meet these requirements using\nwitness encryption, oblivious pseudorandom functions, and group signatures. We\nprove these protocols secure in the universal composibility framework. We then\ndemonstrate that J\\\"ager has low compute and bandwidth costs per call, and\nthese costs scale linearly with call volume. J\\\"ager provides an efficient,\nsecure, privacy-preserving system to revolutionize telephone abuse\ninvestigation with minimal costs to operators."
                },
                "authors": [
                    {
                        "name": "David Adei"
                    },
                    {
                        "name": "Varun Madathil"
                    },
                    {
                        "name": "Sathvik Prasad"
                    },
                    {
                        "name": "Bradley Reaves"
                    },
                    {
                        "name": "Alessandra Scafuro"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Scafuro"
                },
                "author": "Alessandra Scafuro",
                "arxiv_doi": "10.1145/3658644.3690290",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690290",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 2024 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS '24), October 14---18, 2024, Salt Lake City, UT,\n  USA. ACM, New York, NY, USA, 24 pages",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05152v1",
                "updated": "2024-09-08T16:35:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    16,
                    35,
                    19,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T16:35:19Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    16,
                    35,
                    19,
                    6,
                    252,
                    0
                ],
                "title": "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs"
                },
                "summary": "Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation."
                },
                "authors": [
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Cheng Peng"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "Work in progress; code is available at\n  https://github.com/zjunlp/OneGen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05965v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05965v3",
                "updated": "2024-09-08T16:19:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    16,
                    19,
                    53,
                    6,
                    252,
                    0
                ],
                "published": "2024-07-08T14:04:58Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    14,
                    4,
                    58,
                    0,
                    190,
                    0
                ],
                "title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models"
                },
                "summary": "The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset including real-world prompts, LLM-generated prompts and jailbreak\nattack-based prompts. Based on our evaluation results, we draw several\nimportant findings, including: 1) no single model excels in all aspects, with\ndifferent models showing various strengths; 2) the correlation between GPT-4\nassessments and manual reviews is generally high; 3) there is a trade-off\nbetween the usability and safety of text-to-video generative models. This\nindicates that as the field of video generation rapidly advances, safety risks\nare set to surge, highlighting the urgency of prioritizing video safety. We\nhope that T2VSafetyBench can provide insights for better understanding the\nsafety of video generation in the era of generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of Sora leads to a new era in text-to-video (T2V)\ngeneration. Along with this comes the rising concern about its security risks.\nThe generated videos may contain illegal or unethical content, and there is a\nlack of comprehensive quantitative understanding of their safety, posing a\nchallenge to their reliability and practical deployment. Previous evaluations\nprimarily focus on the quality of video generation. While some evaluations of\ntext-to-image models have considered safety, they cover fewer aspects and do\nnot address the unique temporal risk inherent in video generation. To bridge\nthis research gap, we introduce T2VSafetyBench, a new benchmark designed for\nconducting safety-critical assessments of text-to-video models. We define 12\ncritical aspects of video generation safety and construct a malicious prompt\ndataset including real-world prompts, LLM-generated prompts and jailbreak\nattack-based prompts. Based on our evaluation results, we draw several\nimportant findings, including: 1) no single model excels in all aspects, with\ndifferent models showing various strengths; 2) the correlation between GPT-4\nassessments and manual reviews is generally high; 3) there is a trade-off\nbetween the usability and safety of text-to-video generative models. This\nindicates that as the field of video generation rapidly advances, safety risks\nare set to surge, highlighting the urgency of prioritizing video safety. We\nhope that T2VSafetyBench can provide insights for better understanding the\nsafety of video generation in the era of generative AI."
                },
                "authors": [
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Yinpeng Dong"
                    },
                    {
                        "name": "Lijia Yu"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Xiao-Shan Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Shan Gao"
                },
                "author": "Xiao-Shan Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05965v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05965v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03627v5",
                "updated": "2024-09-08T15:01:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    15,
                    1,
                    56,
                    6,
                    252,
                    0
                ],
                "published": "2024-07-04T04:30:04Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    4,
                    30,
                    4,
                    3,
                    186,
                    0
                ],
                "title": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems."
                },
                "authors": [
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Sukmin Cho"
                    },
                    {
                        "name": "SeungYoon Han"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "20 pages",
                "arxiv_journal_ref": "KnowledgeNLP@ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00124v2",
                "updated": "2024-09-08T14:49:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    14,
                    49,
                    46,
                    6,
                    252,
                    0
                ],
                "published": "2024-08-28T17:19:20Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    19,
                    20,
                    2,
                    241,
                    0
                ],
                "title": "Leveraging Large Language Models for Wireless Symbol Detection via\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Wireless Symbol Detection via\n  In-Context Learning"
                },
                "summary": "Deep neural networks (DNNs) have made significant strides in tackling\nchallenging tasks in wireless systems, especially when an accurate wireless\nmodel is not available. However, when available data is limited, traditional\nDNNs often yield subpar results due to underfitting. At the same time, large\nlanguage models (LLMs) exemplified by GPT-3, have remarkably showcased their\ncapabilities across a broad range of natural language processing tasks. But\nwhether and how LLMs can benefit challenging non-language tasks in wireless\nsystems is unexplored. In this work, we propose to leverage the in-context\nlearning ability (a.k.a. prompting) of LLMs to solve wireless tasks in the low\ndata regime without any training or fine-tuning, unlike DNNs which require\ntraining. We further demonstrate that the performance of LLMs varies\nsignificantly when employed with different prompt templates. To solve this\nissue, we employ the latest LLM calibration methods. Our results reveal that\nusing LLMs via ICL methods generally outperforms traditional DNNs on the symbol\ndemodulation task and yields highly confident predictions when coupled with\ncalibration techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) have made significant strides in tackling\nchallenging tasks in wireless systems, especially when an accurate wireless\nmodel is not available. However, when available data is limited, traditional\nDNNs often yield subpar results due to underfitting. At the same time, large\nlanguage models (LLMs) exemplified by GPT-3, have remarkably showcased their\ncapabilities across a broad range of natural language processing tasks. But\nwhether and how LLMs can benefit challenging non-language tasks in wireless\nsystems is unexplored. In this work, we propose to leverage the in-context\nlearning ability (a.k.a. prompting) of LLMs to solve wireless tasks in the low\ndata regime without any training or fine-tuning, unlike DNNs which require\ntraining. We further demonstrate that the performance of LLMs varies\nsignificantly when employed with different prompt templates. To solve this\nissue, we employ the latest LLM calibration methods. Our results reveal that\nusing LLMs via ICL methods generally outperforms traditional DNNs on the symbol\ndemodulation task and yields highly confident predictions when coupled with\ncalibration techniques."
                },
                "authors": [
                    {
                        "name": "Momin Abbas"
                    },
                    {
                        "name": "Koushik Kar"
                    },
                    {
                        "name": "Tianyi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Chen"
                },
                "author": "Tianyi Chen",
                "arxiv_comment": "Accepted at IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05112v1",
                "updated": "2024-09-08T14:45:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    14,
                    45,
                    47,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T14:45:47Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    14,
                    45,
                    47,
                    6,
                    252,
                    0
                ],
                "title": "WaterSeeker: Efficient Detection of Watermarked Segments in Large\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaterSeeker: Efficient Detection of Watermarked Segments in Large\n  Documents"
                },
                "summary": "Watermarking algorithms for large language models (LLMs) have attained high\naccuracy in detecting LLM-generated text. However, existing methods primarily\nfocus on distinguishing fully watermarked text from non-watermarked text,\noverlooking real-world scenarios where LLMs generate only small sections within\nlarge documents. In this scenario, balancing time complexity and detection\nperformance poses significant challenges. This paper presents WaterSeeker, a\nnovel approach to efficiently detect and locate watermarked segments amid\nextensive natural text. It first applies an efficient anomaly extraction method\nto preliminarily locate suspicious watermarked regions. Following this, it\nconducts a local traversal and performs full-text detection for more precise\nverification. Theoretical analysis and experimental results demonstrate that\nWaterSeeker achieves a superior balance between detection accuracy and\ncomputational efficiency. Moreover, WaterSeeker's localization ability supports\nthe development of interpretable AI detection systems. This work pioneers a new\ndirection in watermarked segment detection, facilitating more reliable\nAI-generated content identification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking algorithms for large language models (LLMs) have attained high\naccuracy in detecting LLM-generated text. However, existing methods primarily\nfocus on distinguishing fully watermarked text from non-watermarked text,\noverlooking real-world scenarios where LLMs generate only small sections within\nlarge documents. In this scenario, balancing time complexity and detection\nperformance poses significant challenges. This paper presents WaterSeeker, a\nnovel approach to efficiently detect and locate watermarked segments amid\nextensive natural text. It first applies an efficient anomaly extraction method\nto preliminarily locate suspicious watermarked regions. Following this, it\nconducts a local traversal and performs full-text detection for more precise\nverification. Theoretical analysis and experimental results demonstrate that\nWaterSeeker achieves a superior balance between detection accuracy and\ncomputational efficiency. Moreover, WaterSeeker's localization ability supports\nthe development of interpretable AI detection systems. This work pioneers a new\ndirection in watermarked segment detection, facilitating more reliable\nAI-generated content identification."
                },
                "authors": [
                    {
                        "name": "Leyi Pan"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Yijian Lu"
                    },
                    {
                        "name": "Zitian Gao"
                    },
                    {
                        "name": "Yichen Di"
                    },
                    {
                        "name": "Lijie Wen"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "18 pages, 5 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15223v2",
                "updated": "2024-09-08T14:29:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    14,
                    29,
                    38,
                    6,
                    252,
                    0
                ],
                "published": "2023-12-23T11:09:40Z",
                "published_parsed": [
                    2023,
                    12,
                    23,
                    11,
                    9,
                    40,
                    5,
                    357,
                    0
                ],
                "title": "A Survey on Large Language Models for Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Models for Software Engineering"
                },
                "summary": "Software Engineering (SE) is the systematic design, development, maintenance,\nand management of software applications underpinning the digital infrastructure\nof our modern world. Very recently, the SE community has seen a rapidly\nincreasing number of techniques employing Large Language Models (LLMs) to\nautomate a broad range of SE tasks. Nevertheless, existing information of the\napplications, effects, and possible limitations of LLMs within SE is still not\nwell-studied.\n  In this paper, we provide a systematic survey to summarize the current\nstate-of-the-art research in the LLM-based SE community. We summarize 62\nrepresentative LLMs of Code across three model architectures, 15 pre-training\nobjectives across four categories, and 16 downstream tasks across five\ncategories. We then present a detailed summarization of the recent SE studies\nfor which LLMs are commonly utilized, including 947 studies for 112 specific\ncode-related tasks across five crucial phases within the SE workflow. We also\ndiscuss several critical aspects during the integration of LLMs into SE, such\nas empirical evaluation, benchmarking, security and reliability, domain tuning,\ncompressing and distillation. Finally, we highlight several challenges and\npotential opportunities on applying LLMs for future SE studies, such as\nexploring domain LLMs and constructing clean evaluation datasets. Overall, our\nwork can help researchers gain a comprehensive understanding about the\nachievements of the existing LLM-based SE studies and promote the practical\napplication of these techniques. Our artifacts are publicly available and will\nbe continuously updated at the living repository:\nhttps://github.com/iSEngLab/AwesomeLLM4SE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Engineering (SE) is the systematic design, development, maintenance,\nand management of software applications underpinning the digital infrastructure\nof our modern world. Very recently, the SE community has seen a rapidly\nincreasing number of techniques employing Large Language Models (LLMs) to\nautomate a broad range of SE tasks. Nevertheless, existing information of the\napplications, effects, and possible limitations of LLMs within SE is still not\nwell-studied.\n  In this paper, we provide a systematic survey to summarize the current\nstate-of-the-art research in the LLM-based SE community. We summarize 62\nrepresentative LLMs of Code across three model architectures, 15 pre-training\nobjectives across four categories, and 16 downstream tasks across five\ncategories. We then present a detailed summarization of the recent SE studies\nfor which LLMs are commonly utilized, including 947 studies for 112 specific\ncode-related tasks across five crucial phases within the SE workflow. We also\ndiscuss several critical aspects during the integration of LLMs into SE, such\nas empirical evaluation, benchmarking, security and reliability, domain tuning,\ncompressing and distillation. Finally, we highlight several challenges and\npotential opportunities on applying LLMs for future SE studies, such as\nexploring domain LLMs and constructing clean evaluation datasets. Overall, our\nwork can help researchers gain a comprehensive understanding about the\nachievements of the existing LLM-based SE studies and promote the practical\napplication of these techniques. Our artifacts are publicly available and will\nbe continuously updated at the living repository:\nhttps://github.com/iSEngLab/AwesomeLLM4SE."
                },
                "authors": [
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Yang Xie"
                    },
                    {
                        "name": "Yaxin Zhang"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Shengcheng Yu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "arxiv_comment": "updating 1009 papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09384v2",
                "updated": "2024-09-08T13:59:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    13,
                    59,
                    39,
                    6,
                    252,
                    0
                ],
                "published": "2024-04-14T23:45:23Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    23,
                    45,
                    23,
                    6,
                    105,
                    0
                ],
                "title": "Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software\n  Verification and Falsification Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tasks People Prompt: A Taxonomy of LLM Downstream Tasks in Software\n  Verification and Falsification Approaches"
                },
                "summary": "Prompting has become one of the main approaches to leverage emergent\ncapabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al.\nTMLR 2022, Wei et al. NeurIPS 2022]. Recently, researchers and practitioners\nhave been \"playing\" with prompts (e.g., In-Context Learning) to see how to make\nthe most of pre-trained Language Models. By homogeneously dissecting more than\na hundred articles, we investigate how software testing and verification\nresearch communities have leveraged LLMs capabilities. First, we validate that\ndownstream tasks are adequate to convey a nontrivial modular blueprint of\nprompt-based proposals in scope. Moreover, we name and classify the concrete\ndownstream tasks we recover in both validation research papers and solution\nproposals. In order to perform classification, mapping, and analysis, we also\ndevelop a novel downstream-task taxonomy. The main taxonomy requirement is to\nhighlight commonalities while exhibiting variation points of task types that\nenable pinpointing emerging patterns in a varied spectrum of Software\nEngineering problems that encompasses testing, fuzzing, fault localization,\nvulnerability detection, static analysis, and program verification approaches.\nAvenues for future research are also discussed based on conceptual clusters\ninduced by the taxonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting has become one of the main approaches to leverage emergent\ncapabilities of Large Language Models [Brown et al. NeurIPS 2020, Wei et al.\nTMLR 2022, Wei et al. NeurIPS 2022]. Recently, researchers and practitioners\nhave been \"playing\" with prompts (e.g., In-Context Learning) to see how to make\nthe most of pre-trained Language Models. By homogeneously dissecting more than\na hundred articles, we investigate how software testing and verification\nresearch communities have leveraged LLMs capabilities. First, we validate that\ndownstream tasks are adequate to convey a nontrivial modular blueprint of\nprompt-based proposals in scope. Moreover, we name and classify the concrete\ndownstream tasks we recover in both validation research papers and solution\nproposals. In order to perform classification, mapping, and analysis, we also\ndevelop a novel downstream-task taxonomy. The main taxonomy requirement is to\nhighlight commonalities while exhibiting variation points of task types that\nenable pinpointing emerging patterns in a varied spectrum of Software\nEngineering problems that encompasses testing, fuzzing, fault localization,\nvulnerability detection, static analysis, and program verification approaches.\nAvenues for future research are also discussed based on conceptual clusters\ninduced by the taxonomy."
                },
                "authors": [
                    {
                        "name": "Víctor A. Braberman"
                    },
                    {
                        "name": "Flavia Bonomo-Braberman"
                    },
                    {
                        "name": "Yiannis Charalambous"
                    },
                    {
                        "name": "Juan G. Colonna"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    },
                    {
                        "name": "Rosiane de Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Rosiane de Freitas"
                },
                "author": "Rosiane de Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.3.1; D.2.4; D.2.5; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05093v1",
                "updated": "2024-09-08T13:38:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    13,
                    38,
                    59,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T13:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    13,
                    38,
                    59,
                    6,
                    252,
                    0
                ],
                "title": "CloudNativeSim: a toolkit for modeling and simulation of cloud-native\n  applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudNativeSim: a toolkit for modeling and simulation of cloud-native\n  applications"
                },
                "summary": "Cloud-native applications are increasingly becoming popular in modern\nsoftware design. Employing a microservice-based architecture into these\napplications is a prevalent strategy that enhances system availability and\nflexibility. However, cloud-native applications also introduce new challenges,\nsuch as frequent inter-service communication and the complexity of managing\nheterogeneous codebases and hardware, resulting in unpredictable complexity and\ndynamism. Furthermore, as applications scale, only limited research teams or\nenterprises possess the resources for large-scale deployment and testing, which\nimpedes progress in the cloud-native domain. To address these challenges, we\npropose CloudNativeSim, a simulator for cloud-native applications with a\nmicroservice-based architecture. CloudNativeSim offers several key benefits:\n(i) comprehensive and dynamic modeling for cloud-native applications, (ii) an\nextended simulation framework with new policy interfaces for scheduling\ncloud-native applications, and (iii) support for customized application\nscenarios and user feedback based on Quality of Service (QoS) metrics.\nCloudNativeSim can be easily deployed on standard computers to manage a high\nvolume of requests and services. Its performance was validated through a case\nstudy, demonstrating higher than 94.5% accuracy in terms of response time. The\nstudy further highlights the feasibility of CloudNativeSim by illustrating the\neffects of various scaling policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-native applications are increasingly becoming popular in modern\nsoftware design. Employing a microservice-based architecture into these\napplications is a prevalent strategy that enhances system availability and\nflexibility. However, cloud-native applications also introduce new challenges,\nsuch as frequent inter-service communication and the complexity of managing\nheterogeneous codebases and hardware, resulting in unpredictable complexity and\ndynamism. Furthermore, as applications scale, only limited research teams or\nenterprises possess the resources for large-scale deployment and testing, which\nimpedes progress in the cloud-native domain. To address these challenges, we\npropose CloudNativeSim, a simulator for cloud-native applications with a\nmicroservice-based architecture. CloudNativeSim offers several key benefits:\n(i) comprehensive and dynamic modeling for cloud-native applications, (ii) an\nextended simulation framework with new policy interfaces for scheduling\ncloud-native applications, and (iii) support for customized application\nscenarios and user feedback based on Quality of Service (QoS) metrics.\nCloudNativeSim can be easily deployed on standard computers to manage a high\nvolume of requests and services. Its performance was validated through a case\nstudy, demonstrating higher than 94.5% accuracy in terms of response time. The\nstudy further highlights the feasibility of CloudNativeSim by illustrating the\neffects of various scaling policies."
                },
                "authors": [
                    {
                        "name": "Jingfeng Wu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Yiyuan He"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05092v1",
                "updated": "2024-09-08T13:33:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    13,
                    33,
                    40,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T13:33:40Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    13,
                    33,
                    40,
                    6,
                    252,
                    0
                ],
                "title": "Towards an AI/ML-driven SMO Framework in O-RAN: Scenarios, Solutions,\n  and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards an AI/ML-driven SMO Framework in O-RAN: Scenarios, Solutions,\n  and Challenges"
                },
                "summary": "The emergence of the open radio access network (O-RAN) architecture offers a\nparadigm shift in cellular network management and service orchestration,\nleveraging data-driven, intent-based, autonomous, and intelligent solutions.\nWithin O-RAN, the service management and orchestration (SMO) framework plays a\npivotal role in managing network functions (NFs), resource allocation, service\nprovisioning, and others. However, the increasing complexity and scale of\nO-RANs demand autonomous and intelligent models for optimizing SMO operations.\nTo achieve this goal, it is essential to integrate intelligence and automation\ninto the operations of SMO. In this manuscript, we propose three scenarios for\nintegrating machine learning (ML) algorithms into SMO. We then focus on\nexploring one of the scenarios in which the non-real-time RAN intelligence\ncontroller (Non-RT RIC) plays a major role in data collection, as well as model\ntraining, deployment, and refinement, by proposing a centralized ML\narchitecture. Finally, we identify potential challenges associated with\nimplementing a centralized ML solution within SMO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of the open radio access network (O-RAN) architecture offers a\nparadigm shift in cellular network management and service orchestration,\nleveraging data-driven, intent-based, autonomous, and intelligent solutions.\nWithin O-RAN, the service management and orchestration (SMO) framework plays a\npivotal role in managing network functions (NFs), resource allocation, service\nprovisioning, and others. However, the increasing complexity and scale of\nO-RANs demand autonomous and intelligent models for optimizing SMO operations.\nTo achieve this goal, it is essential to integrate intelligence and automation\ninto the operations of SMO. In this manuscript, we propose three scenarios for\nintegrating machine learning (ML) algorithms into SMO. We then focus on\nexploring one of the scenarios in which the non-real-time RAN intelligence\ncontroller (Non-RT RIC) plays a major role in data collection, as well as model\ntraining, deployment, and refinement, by proposing a centralized ML\narchitecture. Finally, we identify potential challenges associated with\nimplementing a centralized ML solution within SMO."
                },
                "authors": [
                    {
                        "name": "Mohammad Asif Habibi"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Merve Saimler"
                    },
                    {
                        "name": "Ignacio Labrador Pavon"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05071v1",
                "updated": "2024-09-08T12:14:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    12,
                    14,
                    38,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T12:14:38Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    12,
                    14,
                    38,
                    6,
                    252,
                    0
                ],
                "title": "High-Transmission Mid-Infrared Bandpass Filters Using Hybrid\n  Metal-Dielectric Metasurfaces for CO2 Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Transmission Mid-Infrared Bandpass Filters Using Hybrid\n  Metal-Dielectric Metasurfaces for CO2 Sensing"
                },
                "summary": "Mid-infrared (MIR) spectroscopy is a powerful technique employed for a\nvariety of applications, including gas sensing, industrial inspection,\nastronomy, surveillance, and imaging. Thin-film narrowband interference\nfilters, targeted to specific absorption bands of target molecules, are\ncommonly deployed for cost-effective MIR sensing systems. These devices require\ncomplex and time-consuming fabrication processes. Also, their customization on\nthe micro-scale for emerging miniaturized applications is challenging.\nPlasmonic nanostructure arrays operating in reflection and transmission modes\nhave been developed for MIR. However, they experience undesirable\ncharacteristics, such as broad spectra and low reflection/transmission\nefficiencies. All-dielectric metasurfaces have low intrinsic losses and have\nemerged as a substitute for plasmonic metasurfaces in MIR spectroscopy.\nNevertheless, they typically operate only in reflection mode. In this work, we\npresent a hybrid metal-dielectric metasurface for MIR spectroscopy operating in\ntransmission mode. The metasurface is composed of germanium (Ge) atop aluminum\n(Al) cylinders, and we show that the transmission response arises because of\nthe hybridization of modes arising from the Ge and the Al structures. The\npresented metasurface has a high transmission efficiency of 80 % at $\\lambda =\n2.6\\ \\mu\\text{m}$, and a narrow full-width-at-half-maximum of $0.4\\\n\\mu\\text{m}$. We show numerical simulations, successful fabrication using a\nstraightforward fabrication method, and deployment as the in-line optical\nfilter in a CO$_2$ gas detection with a limit of detection of ~0.04% (a few\nhundred ppm). Our work demonstrates the potential for hybrid metasurfaces as\nin-line gas sensing optical filters in MIR spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mid-infrared (MIR) spectroscopy is a powerful technique employed for a\nvariety of applications, including gas sensing, industrial inspection,\nastronomy, surveillance, and imaging. Thin-film narrowband interference\nfilters, targeted to specific absorption bands of target molecules, are\ncommonly deployed for cost-effective MIR sensing systems. These devices require\ncomplex and time-consuming fabrication processes. Also, their customization on\nthe micro-scale for emerging miniaturized applications is challenging.\nPlasmonic nanostructure arrays operating in reflection and transmission modes\nhave been developed for MIR. However, they experience undesirable\ncharacteristics, such as broad spectra and low reflection/transmission\nefficiencies. All-dielectric metasurfaces have low intrinsic losses and have\nemerged as a substitute for plasmonic metasurfaces in MIR spectroscopy.\nNevertheless, they typically operate only in reflection mode. In this work, we\npresent a hybrid metal-dielectric metasurface for MIR spectroscopy operating in\ntransmission mode. The metasurface is composed of germanium (Ge) atop aluminum\n(Al) cylinders, and we show that the transmission response arises because of\nthe hybridization of modes arising from the Ge and the Al structures. The\npresented metasurface has a high transmission efficiency of 80 % at $\\lambda =\n2.6\\ \\mu\\text{m}$, and a narrow full-width-at-half-maximum of $0.4\\\n\\mu\\text{m}$. We show numerical simulations, successful fabrication using a\nstraightforward fabrication method, and deployment as the in-line optical\nfilter in a CO$_2$ gas detection with a limit of detection of ~0.04% (a few\nhundred ppm). Our work demonstrates the potential for hybrid metasurfaces as\nin-line gas sensing optical filters in MIR spectroscopy."
                },
                "authors": [
                    {
                        "name": "Amr Soliman"
                    },
                    {
                        "name": "C Williams"
                    },
                    {
                        "name": "Richard Hopper"
                    },
                    {
                        "name": "Florin Udrea"
                    },
                    {
                        "name": "Haider Butt"
                    },
                    {
                        "name": "Timothy D. Wilkinson"
                    }
                ],
                "author_detail": {
                    "name": "Timothy D. Wilkinson"
                },
                "author": "Timothy D. Wilkinson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05068v1",
                "updated": "2024-09-08T12:10:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    12,
                    10,
                    13,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T12:10:13Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    12,
                    10,
                    13,
                    6,
                    252,
                    0
                ],
                "title": "Improving Early Detection of Gravitational Waves from Binary Neutron\n  Stars Using CNNs and FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Early Detection of Gravitational Waves from Binary Neutron\n  Stars Using CNNs and FPGAs"
                },
                "summary": "The detection of gravitational waves (GWs) from binary neutron stars (BNSs)\nwith possible telescope follow-ups opens a window to ground-breaking\ndiscoveries in the field of multi-messenger astronomy. With the improved\nsensitivity of current and future GW detectors, more BNS detections are\nexpected in the future. Therefore, enhancing low-latency GW search algorithms\nto achieve rapid speed, high accuracy, and low computational cost is essential.\nOne innovative solution to reduce latency is the use of machine learning (ML)\nmethods embedded in field-programmable gate arrays (FPGAs). In this work, we\npresent a novel \\texttt{WaveNet}-based method, leveraging the state-of-the-art\nML model, to produce early-warning alerts for BNS systems. Using simulated GW\nsignals embedded in Gaussian noise from the Advanced LIGO and Advanced Virgo\ndetectors' third observing run (O3) as a proof-of-concept dataset, we\ndemonstrate significant performance improvements. Compared to the current\nleading ML-based early-warning system, our approach enhances detection accuracy\nfrom 66.81\\% to 76.22\\% at a 1\\% false alarm probability. Furthermore, we\nevaluate the time, energy, and economical cost of our model across CPU, GPU,\nand FPGA platforms, showcasing its potential for deployment in real-time\ngravitational wave detection pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of gravitational waves (GWs) from binary neutron stars (BNSs)\nwith possible telescope follow-ups opens a window to ground-breaking\ndiscoveries in the field of multi-messenger astronomy. With the improved\nsensitivity of current and future GW detectors, more BNS detections are\nexpected in the future. Therefore, enhancing low-latency GW search algorithms\nto achieve rapid speed, high accuracy, and low computational cost is essential.\nOne innovative solution to reduce latency is the use of machine learning (ML)\nmethods embedded in field-programmable gate arrays (FPGAs). In this work, we\npresent a novel \\texttt{WaveNet}-based method, leveraging the state-of-the-art\nML model, to produce early-warning alerts for BNS systems. Using simulated GW\nsignals embedded in Gaussian noise from the Advanced LIGO and Advanced Virgo\ndetectors' third observing run (O3) as a proof-of-concept dataset, we\ndemonstrate significant performance improvements. Compared to the current\nleading ML-based early-warning system, our approach enhances detection accuracy\nfrom 66.81\\% to 76.22\\% at a 1\\% false alarm probability. Furthermore, we\nevaluate the time, energy, and economical cost of our model across CPU, GPU,\nand FPGA platforms, showcasing its potential for deployment in real-time\ngravitational wave detection pipelines."
                },
                "authors": [
                    {
                        "name": "Ana Martins"
                    },
                    {
                        "name": "Melissa Lopez"
                    },
                    {
                        "name": "Quirijn Meijer"
                    },
                    {
                        "name": "Gregory Baltus"
                    },
                    {
                        "name": "Marc van der Sluys"
                    },
                    {
                        "name": "Chris Van Den Broeck"
                    },
                    {
                        "name": "Sarah Caudill"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Caudill"
                },
                "author": "Sarah Caudill",
                "arxiv_comment": "10 pages, 7 figures, 3 tables, submitted to IEEE Conference in Big\n  Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05055v1",
                "updated": "2024-09-08T10:58:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    10,
                    58,
                    45,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T10:58:45Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    10,
                    58,
                    45,
                    6,
                    252,
                    0
                ],
                "title": "Investigating the Role of Cultural Values in Adopting Large Language\n  Models for Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Role of Cultural Values in Adopting Large Language\n  Models for Software Engineering"
                },
                "summary": "As a socio-technical activity, software development involves the close\ninterconnection of people and technology. The integration of Large Language\nModels (LLMs) into this process exemplifies the socio-technical nature of\nsoftware development. Although LLMs influence the development process, software\ndevelopment remains fundamentally human-centric, necessitating an investigation\nof the human factors in this adoption. Thus, with this study we explore the\nfactors influencing the adoption of LLMs in software development, focusing on\nthe role of professionals' cultural values. Guided by the Unified Theory of\nAcceptance and Use of Technology (UTAUT2) and Hofstede's cultural dimensions,\nwe hypothesized that cultural values moderate the relationships within the\nUTAUT2 framework. Using Partial Least Squares-Structural Equation Modelling and\ndata from 188 software engineers, we found that habit and performance\nexpectancy are the primary drivers of LLM adoption, while cultural values do\nnot significantly moderate this process. These findings suggest that, by\nhighlighting how LLMs can boost performance and efficiency, organizations can\nencourage their use, no matter the cultural differences. Practical steps\ninclude offering training programs to demonstrate LLM benefits, creating a\nsupportive environment for regular use, and continuously tracking and sharing\nperformance improvements from using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a socio-technical activity, software development involves the close\ninterconnection of people and technology. The integration of Large Language\nModels (LLMs) into this process exemplifies the socio-technical nature of\nsoftware development. Although LLMs influence the development process, software\ndevelopment remains fundamentally human-centric, necessitating an investigation\nof the human factors in this adoption. Thus, with this study we explore the\nfactors influencing the adoption of LLMs in software development, focusing on\nthe role of professionals' cultural values. Guided by the Unified Theory of\nAcceptance and Use of Technology (UTAUT2) and Hofstede's cultural dimensions,\nwe hypothesized that cultural values moderate the relationships within the\nUTAUT2 framework. Using Partial Least Squares-Structural Equation Modelling and\ndata from 188 software engineers, we found that habit and performance\nexpectancy are the primary drivers of LLM adoption, while cultural values do\nnot significantly moderate this process. These findings suggest that, by\nhighlighting how LLMs can boost performance and efficiency, organizations can\nencourage their use, no matter the cultural differences. Practical steps\ninclude offering training programs to demonstrate LLM benefits, creating a\nsupportive environment for regular use, and continuously tracking and sharing\nperformance improvements from using LLMs."
                },
                "authors": [
                    {
                        "name": "Stefano Lambiase"
                    },
                    {
                        "name": "Gemma Catolino"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Filomena Ferrucci"
                    },
                    {
                        "name": "Daniel Russo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Russo"
                },
                "author": "Daniel Russo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05045v1",
                "updated": "2024-09-08T10:06:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    10,
                    6,
                    54,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T10:06:54Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    10,
                    6,
                    54,
                    6,
                    252,
                    0
                ],
                "title": "Using Large Language Models for Template Detection from Security Event\n  Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Template Detection from Security Event\n  Logs"
                },
                "summary": "In modern IT systems and computer networks, real-time and offline event log\nanalysis is a crucial part of cyber security monitoring. In particular, event\nlog analysis techniques are essential for the timely detection of cyber attacks\nand for assisting security experts with the analysis of past security\nincidents. The detection of line patterns or templates from unstructured\ntextual event logs has been identified as an important task of event log\nanalysis since detected templates represent event types in the event log and\nprepare the logs for downstream online or offline security monitoring tasks.\nDuring the last two decades, a number of template mining algorithms have been\nproposed. However, many proposed algorithms rely on traditional data mining\ntechniques, and the usage of Large Language Models (LLMs) has received less\nattention so far. Also, most approaches that harness LLMs are supervised, and\nunsupervised LLM-based template mining remains an understudied area. The\ncurrent paper addresses this research gap and investigates the application of\nLLMs for unsupervised detection of templates from unstructured security event\nlogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern IT systems and computer networks, real-time and offline event log\nanalysis is a crucial part of cyber security monitoring. In particular, event\nlog analysis techniques are essential for the timely detection of cyber attacks\nand for assisting security experts with the analysis of past security\nincidents. The detection of line patterns or templates from unstructured\ntextual event logs has been identified as an important task of event log\nanalysis since detected templates represent event types in the event log and\nprepare the logs for downstream online or offline security monitoring tasks.\nDuring the last two decades, a number of template mining algorithms have been\nproposed. However, many proposed algorithms rely on traditional data mining\ntechniques, and the usage of Large Language Models (LLMs) has received less\nattention so far. Also, most approaches that harness LLMs are supervised, and\nunsupervised LLM-based template mining remains an understudied area. The\ncurrent paper addresses this research gap and investigates the application of\nLLMs for unsupervised detection of templates from unstructured security event\nlogs."
                },
                "authors": [
                    {
                        "name": "Risto Vaarandi"
                    },
                    {
                        "name": "Hayretdin Bahsi"
                    }
                ],
                "author_detail": {
                    "name": "Hayretdin Bahsi"
                },
                "author": "Hayretdin Bahsi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01836v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01836v3",
                "updated": "2024-09-08T09:17:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    9,
                    17,
                    37,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-03T12:34:33Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    12,
                    34,
                    33,
                    1,
                    247,
                    0
                ],
                "title": "Reuse and Blend: Energy-Efficient Optical Neural Network Enabled by\n  Weight Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse and Blend: Energy-Efficient Optical Neural Network Enabled by\n  Weight Sharing"
                },
                "summary": "Optical neural networks (ONN) based on micro-ring resonators (MRR) have\nemerged as a promising alternative to significantly accelerating the massive\nmatrix-vector multiplication (MVM) operations in artificial intelligence (AI)\napplications. However, the limited scale of MRR arrays presents a challenge for\nAI acceleration. The disparity between the small MRR arrays and the large\nweight matrices in AI necessitates extensive MRR writings, including\nreprogramming and calibration, resulting in considerable latency and energy\noverheads. To address this problem, we propose a novel design methodology to\nlessen the need for frequent weight reloading. Specifically, we propose a reuse\nand blend (R&B) architecture to support efficient layer-wise and block-wise\nweight sharing, which allows weights to be reused several times between\nlayers/blocks. Experimental results demonstrate the R&B system can maintain\ncomparable accuracy with 69% energy savings and 57% latency improvement. These\nresults highlight the promise of the R&B to enable the efficient deployment of\nadvanced deep learning models on photonic accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical neural networks (ONN) based on micro-ring resonators (MRR) have\nemerged as a promising alternative to significantly accelerating the massive\nmatrix-vector multiplication (MVM) operations in artificial intelligence (AI)\napplications. However, the limited scale of MRR arrays presents a challenge for\nAI acceleration. The disparity between the small MRR arrays and the large\nweight matrices in AI necessitates extensive MRR writings, including\nreprogramming and calibration, resulting in considerable latency and energy\noverheads. To address this problem, we propose a novel design methodology to\nlessen the need for frequent weight reloading. Specifically, we propose a reuse\nand blend (R&B) architecture to support efficient layer-wise and block-wise\nweight sharing, which allows weights to be reused several times between\nlayers/blocks. Experimental results demonstrate the R&B system can maintain\ncomparable accuracy with 69% energy savings and 57% latency improvement. These\nresults highlight the promise of the R&B to enable the efficient deployment of\nadvanced deep learning models on photonic accelerators."
                },
                "authors": [
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Yuetong Fang"
                    },
                    {
                        "name": "Shaoliang Yu"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01836v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01836v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15639v2",
                "updated": "2024-09-08T08:50:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    50,
                    38,
                    6,
                    252,
                    0
                ],
                "published": "2024-04-24T04:25:04Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    4,
                    25,
                    4,
                    2,
                    115,
                    0
                ],
                "title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models\n  of Code"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress in code\ngeneration. It now becomes crucial to identify whether the code is AI-generated\nand to determine the specific model used, particularly for purposes such as\nprotecting Intellectual Property (IP) in industry and preventing cheating in\nprogramming exercises. To this end, several attempts have been made to insert\nwatermarks into machine-generated code. However, existing approaches are\nlimited to inserting only a single bit of information or overly depending on\nparticular code patterns. In this paper, we introduce CodeIP, a novel multi-bit\nwatermarking technique that embeds additional information to preserve crucial\nprovenance details, such as the vendor ID of an LLM, thereby safeguarding the\nIPs of LLMs in code generation. Furthermore, to ensure the syntactical\ncorrectness of the generated code, we propose constraining the sampling process\nfor predicting the next token by training a type predictor. Experiments\nconducted on a real-world dataset across five programming languages demonstrate\nthe effectiveness of CodeIP in watermarking LLMs for code generation while\nmaintaining the syntactical correctness of code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress in code\ngeneration. It now becomes crucial to identify whether the code is AI-generated\nand to determine the specific model used, particularly for purposes such as\nprotecting Intellectual Property (IP) in industry and preventing cheating in\nprogramming exercises. To this end, several attempts have been made to insert\nwatermarks into machine-generated code. However, existing approaches are\nlimited to inserting only a single bit of information or overly depending on\nparticular code patterns. In this paper, we introduce CodeIP, a novel multi-bit\nwatermarking technique that embeds additional information to preserve crucial\nprovenance details, such as the vendor ID of an LLM, thereby safeguarding the\nIPs of LLMs in code generation. Furthermore, to ensure the syntactical\ncorrectness of the generated code, we propose constraining the sampling process\nfor predicting the next token by training a type predictor. Experiments\nconducted on a real-world dataset across five programming languages demonstrate\nthe effectiveness of CodeIP in watermarking LLMs for code generation while\nmaintaining the syntactical correctness of code."
                },
                "authors": [
                    {
                        "name": "Batu Guan"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Zhangqian Bi"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05028v1",
                "updated": "2024-09-08T08:46:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    46,
                    5,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:46:05Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    46,
                    5,
                    6,
                    252,
                    0
                ],
                "title": "LLM-based Abstraction and Concretization for GUI Test Migration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Abstraction and Concretization for GUI Test Migration"
                },
                "summary": "GUI test migration aims to produce test cases with events and assertions to\ntest specific functionalities of a target app. Existing migration approaches\ntypically focus on the widget-mapping paradigm that maps widgets from source\napps to target apps. However, since different apps may implement the same\nfunctionality in different ways, direct mapping may result in incomplete or\nbuggy test cases, thus significantly impacting the effectiveness of testing\ntarget functionality and the practical applicability.\n  In this paper, we propose a new migration paradigm (i.e.,\nabstraction-concretization paradigm) that first abstracts the test logic for\nthe target functionality and then utilizes this logic to generate the concrete\nGUI test case. Furthermore, we introduce MACdroid, the first approach that\nmigrates GUI test cases based on this paradigm. Specifically, we propose an\nabstraction technique that utilizes source test cases from source apps\ntargeting the same functionality to extract a general test logic for that\nfunctionality. Then, we propose a concretization technique that utilizes the\ngeneral test logic to guide an LLM in generating the corresponding GUI test\ncase (including events and assertions) for the target app. We evaluate MACdroid\non two widely-used datasets (including 31 apps, 34 functionalities, and 123\ntest cases). On the FrUITeR dataset, the test cases generated by MACdroid\nsuccessfully test 64% of the target functionalities, improving the baselines by\n191%. On the Lin dataset, MACdroid successfully tests 75% of the target\nfunctionalities, outperforming the baselines by 42%. These results underscore\nthe effectiveness of MACdroid in GUI test migration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI test migration aims to produce test cases with events and assertions to\ntest specific functionalities of a target app. Existing migration approaches\ntypically focus on the widget-mapping paradigm that maps widgets from source\napps to target apps. However, since different apps may implement the same\nfunctionality in different ways, direct mapping may result in incomplete or\nbuggy test cases, thus significantly impacting the effectiveness of testing\ntarget functionality and the practical applicability.\n  In this paper, we propose a new migration paradigm (i.e.,\nabstraction-concretization paradigm) that first abstracts the test logic for\nthe target functionality and then utilizes this logic to generate the concrete\nGUI test case. Furthermore, we introduce MACdroid, the first approach that\nmigrates GUI test cases based on this paradigm. Specifically, we propose an\nabstraction technique that utilizes source test cases from source apps\ntargeting the same functionality to extract a general test logic for that\nfunctionality. Then, we propose a concretization technique that utilizes the\ngeneral test logic to guide an LLM in generating the corresponding GUI test\ncase (including events and assertions) for the target app. We evaluate MACdroid\non two widely-used datasets (including 31 apps, 34 functionalities, and 123\ntest cases). On the FrUITeR dataset, the test cases generated by MACdroid\nsuccessfully test 64% of the target functionalities, improving the baselines by\n191%. On the Lin dataset, MACdroid successfully tests 75% of the target\nfunctionalities, outperforming the baselines by 42%. These results underscore\nthe effectiveness of MACdroid in GUI test migration."
                },
                "authors": [
                    {
                        "name": "Yakun Zhang"
                    },
                    {
                        "name": "Chen Liu"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Jin Song Dong"
                    },
                    {
                        "name": "Dan Hao"
                    },
                    {
                        "name": "Lu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Zhang"
                },
                "author": "Lu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05021v1",
                "updated": "2024-09-08T08:22:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    22,
                    17,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:22:17Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    22,
                    17,
                    6,
                    252,
                    0
                ],
                "title": "Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text\n  against Neural Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-fused Attack: Advancing Aggressive and Stealthy Adversarial Text\n  against Neural Machine Translation"
                },
                "summary": "While neural machine translation (NMT) models achieve success in our daily\nlives, they show vulnerability to adversarial attacks. Despite being harmful,\nthese attacks also offer benefits for interpreting and enhancing NMT models,\nthus drawing increased research attention. However, existing studies on\nadversarial attacks are insufficient in both attacking ability and human\nimperceptibility due to their sole focus on the scope of language. This paper\nproposes a novel vision-fused attack (VFA) framework to acquire powerful\nadversarial text, i.e., more aggressive and stealthy. Regarding the attacking\nability, we design the vision-merged solution space enhancement strategy to\nenlarge the limited semantic solution space, which enables us to search for\nadversarial candidates with higher attacking ability. For human\nimperceptibility, we propose the perception-retained adversarial text selection\nstrategy to align the human text-reading mechanism. Thus, the finally selected\nadversarial text could be more deceptive. Extensive experiments on various\nmodels, including large language models (LLMs) like LLaMA and GPT-3.5, strongly\nsupport that VFA outperforms the comparisons by large margins (up to 81%/14%\nimprovements on ASR/SSIM).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While neural machine translation (NMT) models achieve success in our daily\nlives, they show vulnerability to adversarial attacks. Despite being harmful,\nthese attacks also offer benefits for interpreting and enhancing NMT models,\nthus drawing increased research attention. However, existing studies on\nadversarial attacks are insufficient in both attacking ability and human\nimperceptibility due to their sole focus on the scope of language. This paper\nproposes a novel vision-fused attack (VFA) framework to acquire powerful\nadversarial text, i.e., more aggressive and stealthy. Regarding the attacking\nability, we design the vision-merged solution space enhancement strategy to\nenlarge the limited semantic solution space, which enables us to search for\nadversarial candidates with higher attacking ability. For human\nimperceptibility, we propose the perception-retained adversarial text selection\nstrategy to align the human text-reading mechanism. Thus, the finally selected\nadversarial text could be more deceptive. Extensive experiments on various\nmodels, including large language models (LLMs) like LLaMA and GPT-3.5, strongly\nsupport that VFA outperforms the comparisons by large margins (up to 81%/14%\nimprovements on ASR/SSIM)."
                },
                "authors": [
                    {
                        "name": "Yanni Xue"
                    },
                    {
                        "name": "Haojie Hao"
                    },
                    {
                        "name": "Jiakai Wang"
                    },
                    {
                        "name": "Qiang Sheng"
                    },
                    {
                        "name": "Renshuai Tao"
                    },
                    {
                        "name": "Yu Liang"
                    },
                    {
                        "name": "Pu Feng"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "arxiv_comment": "IJCAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05016v1",
                "updated": "2024-09-08T08:10:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    10,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:10:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    10,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Using vs. Purchasing Industrial Robots: Adding an Organizational\n  Perspective to Industrial HRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using vs. Purchasing Industrial Robots: Adding an Organizational\n  Perspective to Industrial HRI"
                },
                "summary": "Purpose: Industrial robots allow manufacturing companies to increase\nproductivity and remain competitive. For robots to be used, they must be\naccepted by operators on the one hand and bought by decision-makers on the\nother. The roles involved in such organizational processes have very different\nperspectives. It is therefore essential for suppliers and robot customers to\nunderstand these motives so that robots can successfully be integrated on\nmanufacturing shopfloors. Methodology: We present findings of a qualitative\nstudy with operators and decision-makers from two Swiss manufacturing SMEs.\nUsing laddering interviews and means-end analysis, we compare operators' and\ndeciders' relevant elements and how these elements are linked to each other on\ndifferent abstraction levels. These findings represent drivers and barriers to\nthe acquisition, integration and acceptance of robots in the industry.\nFindings: We present the differing foci of operators and deciders, and how they\ncan be used by demanders as well as suppliers of robots to achieve robot\nacceptance and deployment. First, we present a list of relevant attributes,\nconsequences and values that constitute robot acceptance and/or rejection.\nSecond, we provide quantified relevancies for these elements, and how they\ndiffer between operators and deciders. And third, we demonstrate how the\nelements are linked with each other on different abstraction levels, and how\nthese links differ between the two groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Industrial robots allow manufacturing companies to increase\nproductivity and remain competitive. For robots to be used, they must be\naccepted by operators on the one hand and bought by decision-makers on the\nother. The roles involved in such organizational processes have very different\nperspectives. It is therefore essential for suppliers and robot customers to\nunderstand these motives so that robots can successfully be integrated on\nmanufacturing shopfloors. Methodology: We present findings of a qualitative\nstudy with operators and decision-makers from two Swiss manufacturing SMEs.\nUsing laddering interviews and means-end analysis, we compare operators' and\ndeciders' relevant elements and how these elements are linked to each other on\ndifferent abstraction levels. These findings represent drivers and barriers to\nthe acquisition, integration and acceptance of robots in the industry.\nFindings: We present the differing foci of operators and deciders, and how they\ncan be used by demanders as well as suppliers of robots to achieve robot\nacceptance and deployment. First, we present a list of relevant attributes,\nconsequences and values that constitute robot acceptance and/or rejection.\nSecond, we provide quantified relevancies for these elements, and how they\ndiffer between operators and deciders. And third, we demonstrate how the\nelements are linked with each other on different abstraction levels, and how\nthese links differ between the two groups."
                },
                "authors": [
                    {
                        "name": "Damian Hostettler"
                    }
                ],
                "author_detail": {
                    "name": "Damian Hostettler"
                },
                "author": "Damian Hostettler",
                "arxiv_comment": "25 pages, 2 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13985v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13985v3",
                "updated": "2024-09-08T07:44:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    7,
                    44,
                    45,
                    6,
                    252,
                    0
                ],
                "published": "2024-08-26T02:35:37Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    2,
                    35,
                    37,
                    0,
                    239,
                    0
                ],
                "title": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TF-Attack: Transferable and Fast Adversarial Attacks on Large Language\n  Models"
                },
                "summary": "With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the great advancements in large language models (LLMs), adversarial\nattacks against LLMs have recently attracted increasing attention. We found\nthat pre-existing adversarial attack methodologies exhibit limited\ntransferability and are notably inefficient, particularly when applied to LLMs.\nIn this paper, we analyze the core mechanisms of previous predominant\nadversarial attack methods, revealing that 1) the distributions of importance\nscore differ markedly among victim models, restricting the transferability; 2)\nthe sequential attack processes induces substantial time overheads. Based on\nthe above two insights, we introduce a new scheme, named TF-Attack, for\nTransferable and Fast adversarial attacks on LLMs. TF-Attack employs an\nexternal LLM as a third-party overseer rather than the victim model to identify\ncritical units within sentences. Moreover, TF-Attack introduces the concept of\nImportance Level, which allows for parallel substitutions of attacks. We\nconduct extensive experiments on 6 widely adopted benchmarks, evaluating the\nproposed method through both automatic and human metrics. Results show that our\nmethod consistently surpasses previous methods in transferability and delivers\nsignificant speed improvements, up to 20 times faster than earlier attack\nstrategies."
                },
                "authors": [
                    {
                        "name": "Zelin Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Mingming Yang"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13985v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13985v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05001v1",
                "updated": "2024-09-08T07:22:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    7,
                    22,
                    19,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T07:22:19Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    7,
                    22,
                    19,
                    6,
                    252,
                    0
                ],
                "title": "A Pair Programming Framework for Code Generation via Multi-Plan\n  Exploration and Feedback-Driven Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pair Programming Framework for Code Generation via Multi-Plan\n  Exploration and Feedback-Driven Refinement"
                },
                "summary": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. Although prior studies enhanced LLMs with prompting techniques and\ncode refinement, they still struggle with complex programming problems due to\nrigid solution plans. In this paper, we draw on pair programming practices to\npropose PairCoder, a novel LLM-based framework for code generation. PairCoder\nincorporates two collaborative LLM agents, namely a Navigator agent for\nhigh-level planning and a Driver agent for specific implementation. The\nNavigator is responsible for proposing promising solution plans, selecting the\ncurrent optimal plan, and directing the next iteration round based on execution\nfeedback. The Driver follows the guidance of Navigator to undertake initial\ncode generation, code testing, and refinement. This interleaved and iterative\nworkflow involves multi-plan exploration and feedback-based refinement, which\nmimics the collaboration of pair programmers. We evaluate PairCoder with both\nopen-source and closed-source LLMs on various code generation benchmarks.\nExtensive experimental results demonstrate the superior accuracy of PairCoder,\nachieving relative pass@1 improvements of 12.00%-162.43% compared to prompting\nLLMs directly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. Although prior studies enhanced LLMs with prompting techniques and\ncode refinement, they still struggle with complex programming problems due to\nrigid solution plans. In this paper, we draw on pair programming practices to\npropose PairCoder, a novel LLM-based framework for code generation. PairCoder\nincorporates two collaborative LLM agents, namely a Navigator agent for\nhigh-level planning and a Driver agent for specific implementation. The\nNavigator is responsible for proposing promising solution plans, selecting the\ncurrent optimal plan, and directing the next iteration round based on execution\nfeedback. The Driver follows the guidance of Navigator to undertake initial\ncode generation, code testing, and refinement. This interleaved and iterative\nworkflow involves multi-plan exploration and feedback-based refinement, which\nmimics the collaboration of pair programmers. We evaluate PairCoder with both\nopen-source and closed-source LLMs on various code generation benchmarks.\nExtensive experimental results demonstrate the superior accuracy of PairCoder,\nachieving relative pass@1 improvements of 12.00%-162.43% compared to prompting\nLLMs directly."
                },
                "authors": [
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Wei Hu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Hu"
                },
                "author": "Wei Hu",
                "arxiv_comment": "Accepted in the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10106v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10106v4",
                "updated": "2024-09-08T07:09:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    7,
                    9,
                    33,
                    6,
                    252,
                    0
                ],
                "published": "2024-07-14T07:21:54Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    7,
                    21,
                    54,
                    6,
                    196,
                    0
                ],
                "title": "DistillSeq: A Framework for Safety Alignment Testing in Large Language\n  Models using Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DistillSeq: A Framework for Safety Alignment Testing in Large Language\n  Models using Knowledge Distillation"
                },
                "summary": "Large Language Models (LLMs) have showcased their remarkable capabilities in\ndiverse domains, encompassing natural language understanding, translation, and\neven code generation. The potential for LLMs to generate harmful content is a\nsignificant concern. This risk necessitates rigorous testing and comprehensive\nevaluation of LLMs to ensure safe and responsible use. However, extensive\ntesting of LLMs requires substantial computational resources, making it an\nexpensive endeavor. Therefore, exploring cost-saving strategies during the\ntesting phase is crucial to balance the need for thorough evaluation with the\nconstraints of resource availability. To address this, our approach begins by\ntransferring the moderation knowledge from an LLM to a small model.\nSubsequently, we deploy two distinct strategies for generating malicious\nqueries: one based on a syntax tree approach, and the other leveraging an\nLLM-based method. Finally, our approach incorporates a sequential filter-test\nprocess designed to identify test cases that are prone to eliciting toxic\nresponses. Our research evaluated the efficacy of DistillSeq across four LLMs:\nGPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the\nobserved attack success rates on these LLMs stood at 31.5% for GPT-3.5, 21.4%\nfor GPT-4.0, 28.3% for Vicuna-13B, and 30.9% for Llama-13B. However, upon the\napplication of DistillSeq, these success rates notably increased to 58.5%,\n50.7%, 52.5%, and 54.4%, respectively. This translated to an average escalation\nin attack success rate by a factor of 93.0% when compared to scenarios without\nthe use of DistillSeq. Such findings highlight the significant enhancement\nDistillSeq offers in terms of reducing the time and resource investment\nrequired for effectively testing LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased their remarkable capabilities in\ndiverse domains, encompassing natural language understanding, translation, and\neven code generation. The potential for LLMs to generate harmful content is a\nsignificant concern. This risk necessitates rigorous testing and comprehensive\nevaluation of LLMs to ensure safe and responsible use. However, extensive\ntesting of LLMs requires substantial computational resources, making it an\nexpensive endeavor. Therefore, exploring cost-saving strategies during the\ntesting phase is crucial to balance the need for thorough evaluation with the\nconstraints of resource availability. To address this, our approach begins by\ntransferring the moderation knowledge from an LLM to a small model.\nSubsequently, we deploy two distinct strategies for generating malicious\nqueries: one based on a syntax tree approach, and the other leveraging an\nLLM-based method. Finally, our approach incorporates a sequential filter-test\nprocess designed to identify test cases that are prone to eliciting toxic\nresponses. Our research evaluated the efficacy of DistillSeq across four LLMs:\nGPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the\nobserved attack success rates on these LLMs stood at 31.5% for GPT-3.5, 21.4%\nfor GPT-4.0, 28.3% for Vicuna-13B, and 30.9% for Llama-13B. However, upon the\napplication of DistillSeq, these success rates notably increased to 58.5%,\n50.7%, 52.5%, and 54.4%, respectively. This translated to an average escalation\nin attack success rate by a factor of 93.0% when compared to scenarios without\nthe use of DistillSeq. Such findings highlight the significant enhancement\nDistillSeq offers in terms of reducing the time and resource investment\nrequired for effectively testing LLMs."
                },
                "authors": [
                    {
                        "name": "Mingke Yang"
                    },
                    {
                        "name": "Yuqi Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Ling Shi"
                    }
                ],
                "author_detail": {
                    "name": "Ling Shi"
                },
                "author": "Ling Shi",
                "arxiv_doi": "10.1145/3650212.3680304",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3650212.3680304",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.10106v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10106v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04344v2",
                "updated": "2024-09-08T06:56:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    56,
                    47,
                    6,
                    252,
                    0
                ],
                "published": "2024-08-08T10:04:50Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    10,
                    4,
                    50,
                    3,
                    221,
                    0
                ],
                "title": "Semantic-Enhanced Indirect Call Analysis with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Enhanced Indirect Call Analysis with Large Language Models"
                },
                "summary": "In contemporary software development, the widespread use of indirect calls to\nachieve dynamic features poses challenges in constructing precise control flow\ngraphs (CFGs), which further impacts the performance of downstream static\nanalysis tasks. To tackle this issue, various types of indirect call analyzers\nhave been proposed. However, they do not fully leverage the semantic\ninformation of the program, limiting their effectiveness in real-world\nscenarios. To address these issues, this paper proposes Semantic-Enhanced\nAnalysis (SEA), a new approach to enhance the effectiveness of indirect call\nanalysis. Our fundamental insight is that for common programming practices,\nindirect calls often exhibit semantic similarity with their invoked targets.\nThis semantic alignment serves as a supportive mechanism for static analysis\ntechniques in filtering out false targets. Notably, contemporary large language\nmodels (LLMs) are trained on extensive code corpora, encompassing tasks such as\ncode summarization, making them well-suited for semantic analysis.\nSpecifically, SEA leverages LLMs to generate natural language summaries of both\nindirect calls and target functions from multiple perspectives. Through further\nanalysis of these summaries, SEA can determine their suitability as\ncaller-callee pairs. Experimental results demonstrate that SEA can\nsignificantly enhance existing static analysis methods by producing more\nprecise target sets for indirect calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary software development, the widespread use of indirect calls to\nachieve dynamic features poses challenges in constructing precise control flow\ngraphs (CFGs), which further impacts the performance of downstream static\nanalysis tasks. To tackle this issue, various types of indirect call analyzers\nhave been proposed. However, they do not fully leverage the semantic\ninformation of the program, limiting their effectiveness in real-world\nscenarios. To address these issues, this paper proposes Semantic-Enhanced\nAnalysis (SEA), a new approach to enhance the effectiveness of indirect call\nanalysis. Our fundamental insight is that for common programming practices,\nindirect calls often exhibit semantic similarity with their invoked targets.\nThis semantic alignment serves as a supportive mechanism for static analysis\ntechniques in filtering out false targets. Notably, contemporary large language\nmodels (LLMs) are trained on extensive code corpora, encompassing tasks such as\ncode summarization, making them well-suited for semantic analysis.\nSpecifically, SEA leverages LLMs to generate natural language summaries of both\nindirect calls and target functions from multiple perspectives. Through further\nanalysis of these summaries, SEA can determine their suitability as\ncaller-callee pairs. Experimental results demonstrate that SEA can\nsignificantly enhance existing static analysis methods by producing more\nprecise target sets for indirect calls."
                },
                "authors": [
                    {
                        "name": "Baijun Cheng"
                    },
                    {
                        "name": "Cen Zhang"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Yao Guo"
                    },
                    {
                        "name": "Xiangqun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiangqun Chen"
                },
                "author": "Xiangqun Chen",
                "arxiv_doi": "10.1145/3691620.3695016",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3691620.3695016",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.04344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ASE'24",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.11461v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.11461v7",
                "updated": "2024-09-08T06:41:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    41,
                    12,
                    6,
                    252,
                    0
                ],
                "published": "2023-05-19T06:30:17Z",
                "published_parsed": [
                    2023,
                    5,
                    19,
                    6,
                    30,
                    17,
                    4,
                    139,
                    0
                ],
                "title": "Hint of Thought prompting: an explainable and zero-shot approach to\n  reasoning tasks with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hint of Thought prompting: an explainable and zero-shot approach to\n  reasoning tasks with LLMs"
                },
                "summary": "Prompting becomes an increasingly important research topic for better\nutilization of LLMs. Although simple prompting performs well on single-step\nquestions, it cannot permanently activate the correct knowledge path for\nmulti-step reasoning tasks. The chain of thought (CoT), which often contains\nzero-shot CoT and few-shot CoT, is a recently developed prompting method that\ncan explain the reasoning process to the LLM and outperforms simple prompting\nin three challenging reasoning tasks, including arithmetic, symbolic, and\ncommonsense reasoning. Inspired by zero-shot CoT, and further extending the\nzero-shot ability, this paper proposes a novel hint of thought (HoT) prompting\nwith explain-ability and zero-shot generalization. It is decomposed into three\nsteps: explainable sub-questions, logical reasoning, and answering. Such three\nsteps are sequentially ordered in step-by-step hints, which can be easily\nadjusted and explained to different tasks. Finally, experimental results\ndemonstrate that our HoT prompting has a significant advantage on the zero-shot\nreasoning task compared to existing zero-shot CoT. We did zero-shot experiments\non math tasks like GSM8K, ADDSUB, AQUA, SVAMP, and commonsense tasks such as\nStrategyQA. In particular, the accuracy of the proposed HoT prompting is\nimproved with GSM8K from 40.50% to 70.65%, with AQUA from 31.9% to 46.4%, with\nSVAMP from 63.7% to 76.9%, and with ADDSUB from 74.7% to 87.34%, respectively,\nwhich even defeats the competitive PoT approach on GSM8k, AQUA, and SVAMP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting becomes an increasingly important research topic for better\nutilization of LLMs. Although simple prompting performs well on single-step\nquestions, it cannot permanently activate the correct knowledge path for\nmulti-step reasoning tasks. The chain of thought (CoT), which often contains\nzero-shot CoT and few-shot CoT, is a recently developed prompting method that\ncan explain the reasoning process to the LLM and outperforms simple prompting\nin three challenging reasoning tasks, including arithmetic, symbolic, and\ncommonsense reasoning. Inspired by zero-shot CoT, and further extending the\nzero-shot ability, this paper proposes a novel hint of thought (HoT) prompting\nwith explain-ability and zero-shot generalization. It is decomposed into three\nsteps: explainable sub-questions, logical reasoning, and answering. Such three\nsteps are sequentially ordered in step-by-step hints, which can be easily\nadjusted and explained to different tasks. Finally, experimental results\ndemonstrate that our HoT prompting has a significant advantage on the zero-shot\nreasoning task compared to existing zero-shot CoT. We did zero-shot experiments\non math tasks like GSM8K, ADDSUB, AQUA, SVAMP, and commonsense tasks such as\nStrategyQA. In particular, the accuracy of the proposed HoT prompting is\nimproved with GSM8K from 40.50% to 70.65%, with AQUA from 31.9% to 46.4%, with\nSVAMP from 63.7% to 76.9%, and with ADDSUB from 74.7% to 87.34%, respectively,\nwhich even defeats the competitive PoT approach on GSM8k, AQUA, and SVAMP."
                },
                "authors": [
                    {
                        "name": "Ioktong Lei"
                    },
                    {
                        "name": "Zhidong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhidong Deng"
                },
                "author": "Zhidong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.11461v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.11461v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]