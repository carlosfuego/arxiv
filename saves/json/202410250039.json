[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v5",
                "updated": "2024-10-21T22:56:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    22,
                    56,
                    6,
                    0,
                    295,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v1",
                "updated": "2024-10-21T16:44:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v1",
                "updated": "2024-10-16T08:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cdric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v1",
                "updated": "2024-10-12T10:38:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09397v1",
                "updated": "2024-10-12T07:01:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T07:01:30Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09237v1",
                "updated": "2024-10-11T20:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T20:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor"
                },
                "summary": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}."
                },
                "authors": [
                    {
                        "name": "Sahar Ahmadi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Morteza Saberi"
                    },
                    {
                        "name": "Md. Towsif Abir"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Farookh Hussain"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08895v1",
                "updated": "2024-10-11T15:12:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:12:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation"
                },
                "summary": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Kun Ding"
                    },
                    {
                        "name": "Qiang Yu"
                    },
                    {
                        "name": "Haojian Zhang"
                    },
                    {
                        "name": "Gaofeng Meng"
                    },
                    {
                        "name": "Shiming Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Xiang"
                },
                "author": "Shiming Xiang",
                "arxiv_comment": "submitted to IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v1",
                "updated": "2024-10-11T12:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08618v1",
                "updated": "2024-10-11T08:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:33:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination"
                },
                "summary": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Qiulin Tian"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Tong Xin"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v1",
                "updated": "2024-10-11T07:24:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03462v3",
                "updated": "2024-10-11T02:18:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    18,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-07T11:57:40Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    11,
                    57,
                    40,
                    6,
                    7,
                    0
                ],
                "title": "Long Context Compression with Activation Beacon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Compression with Activation Beacon"
                },
                "summary": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}."
                },
                "authors": [
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Qiwei Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Newer version of Activation Beacon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08391v1",
                "updated": "2024-10-10T21:55:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T21:55:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "KV Prediction for Improved Time to First Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Prediction for Improved Time to First Token"
                },
                "summary": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction ."
                },
                "authors": [
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Chenfan Sun"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v3",
                "updated": "2024-10-10T16:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations"
                },
                "summary": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12850v1",
                "updated": "2024-10-10T15:24:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    24,
                    12,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:24:12Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    24,
                    12,
                    3,
                    284,
                    0
                ],
                "title": "RecurFormer: Not All Transformer Heads Need Self-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecurFormer: Not All Transformer Heads Need Self-Attention"
                },
                "summary": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs."
                },
                "authors": [
                    {
                        "name": "Ruiqing Yan"
                    },
                    {
                        "name": "Linghan Zheng"
                    },
                    {
                        "name": "Xingbo Du"
                    },
                    {
                        "name": "Han Zou"
                    },
                    {
                        "name": "Yufeng Guo"
                    },
                    {
                        "name": "Jianfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Yang"
                },
                "author": "Jianfei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01195v2",
                "updated": "2024-10-10T11:01:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    1,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T10:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    10,
                    58,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping"
                },
                "summary": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qijie Ge"
                    },
                    {
                        "name": "Lulu Suo"
                    },
                    {
                        "name": "Weijie Tang"
                    },
                    {
                        "name": "Zhengyu Wei"
                    },
                    {
                        "name": "Longxiang Huang"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04793v2",
                "updated": "2024-10-10T05:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-07T03:08:14Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    3,
                    8,
                    14,
                    6,
                    98,
                    0
                ],
                "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget"
                },
                "summary": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07590v1",
                "updated": "2024-10-10T03:52:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:52:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text"
                },
                "summary": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
                },
                "authors": [
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Yutian Rong"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07579v1",
                "updated": "2024-10-10T03:28:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:28:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching"
                },
                "summary": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy."
                },
                "authors": [
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Jingwen Ye"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06627v1",
                "updated": "2024-10-09T07:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:22:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions"
                },
                "summary": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections."
                },
                "authors": [
                    {
                        "name": "Muhammad Morshed Alam"
                    },
                    {
                        "name": "Muhammad Yeasir Aarafat"
                    },
                    {
                        "name": "Tamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Hossain"
                },
                "author": "Tamim Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13941v2",
                "updated": "2024-10-09T04:11:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    11,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-20T02:20:21Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    20,
                    21,
                    3,
                    172,
                    0
                ],
                "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture"
                },
                "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Haobin Tan"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Pavan Balaji"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Balaji"
                },
                "author": "Pavan Balaji",
                "arxiv_doi": "10.1145/3649329.3658266",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3658266",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by DAC 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06497v1",
                "updated": "2024-10-09T02:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T02:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System"
                },
                "summary": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements."
                },
                "authors": [
                    {
                        "name": "Fang Zhou"
                    },
                    {
                        "name": "Yaning Huang"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Dai Li"
                    },
                    {
                        "name": "Zhongke Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xiao Xin"
                    },
                    {
                        "name": "Abdallah Aboelela"
                    },
                    {
                        "name": "Zheliang Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jeff Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "ChongLin Sun"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Mindi Yuan"
                    },
                    {
                        "name": "Emanuele Maccherani"
                    },
                    {
                        "name": "Taha Hayat"
                    },
                    {
                        "name": "John Guo"
                    },
                    {
                        "name": "Varna Puvvada"
                    },
                    {
                        "name": "Uladzimir Pashkevich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzimir Pashkevich"
                },
                "author": "Uladzimir Pashkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v4",
                "updated": "2024-10-09T01:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    12,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01527v2",
                "updated": "2024-10-08T19:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    34,
                    3,
                    1,
                    282,
                    0
                ],
                "published": "2024-07-01T17:59:47Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    47,
                    0,
                    183,
                    0
                ],
                "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches"
                },
                "summary": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench."
                },
                "authors": [
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Songchen Li"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05927v1",
                "updated": "2024-10-08T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications"
                },
                "summary": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kourtzanidis"
                    },
                    {
                        "name": "Panagiotis Dimitrakellis"
                    },
                    {
                        "name": "Dimitrios Rakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Rakopoulos"
                },
                "author": "Dimitrios Rakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Dielectrics and Electrical\n  Insulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05863v1",
                "updated": "2024-10-08T09:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "title": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework"
                },
                "summary": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates."
                },
                "authors": [
                    {
                        "name": "Yunfei Yang"
                    },
                    {
                        "name": "Zhenghao Qi"
                    },
                    {
                        "name": "Honghuan Wu"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Tieyao Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yimin Tu"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "arxiv_comment": "CIKM 2024 applied research track, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05854v1",
                "updated": "2024-10-08T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks"
                },
                "summary": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs."
                },
                "authors": [
                    {
                        "name": "Ruben Hias"
                    },
                    {
                        "name": "Weihong Wang"
                    },
                    {
                        "name": "Jan Vanhoof"
                    },
                    {
                        "name": "Tom Van Cutsem"
                    }
                ],
                "author_detail": {
                    "name": "Tom Van Cutsem"
                },
                "author": "Tom Van Cutsem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12018v2",
                "updated": "2024-10-08T04:25:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    4,
                    25,
                    41,
                    1,
                    282,
                    0
                ],
                "published": "2024-06-17T18:34:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    34,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling"
                },
                "summary": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xiyuan Zou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v3",
                "updated": "2024-10-07T01:27:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    27,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v1",
                "updated": "2024-10-06T19:36:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04252v1",
                "updated": "2024-10-05T18:20:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T18:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "title": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation"
                },
                "summary": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC."
                },
                "authors": [
                    {
                        "name": "Yusuke Teranishi"
                    },
                    {
                        "name": "Shoma Hiraoka"
                    },
                    {
                        "name": "Wataru Mizukami"
                    },
                    {
                        "name": "Masao Okita"
                    },
                    {
                        "name": "Fumihiko Ino"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiko Ino"
                },
                "author": "Fumihiko Ino",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v1",
                "updated": "2024-10-04T22:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v1",
                "updated": "2024-10-04T15:23:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMProxy: Reducing Cost to Access Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMProxy: Reducing Cost to Access Large Language Models"
                },
                "summary": "In this paper, we make a case for a proxy for large language models which has\nexplicit support for cost-saving optimizations. We design LLMProxy, which\nsupports three key optimizations: model selection, context management, and\ncaching. These optimizations present tradeoffs in terms of cost, inference\ntime, and response quality, which applications can navigate through our high\nlevel, bidirectional interface. As a case study, we implement a WhatsApp-based\nQ&A service that uses LLMProxy to provide a rich set of features to the users.\nThis service is deployed on a small scale (100+ users) leveraging the cloud; it\nhas been operational for 15+ weeks and users have asked 1400+ questions so far.\nWe report on the experiences of running this service as well as microbenchmark\nthe specific benefits of the various cost-optimizations we present in this\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we make a case for a proxy for large language models which has\nexplicit support for cost-saving optimizations. We design LLMProxy, which\nsupports three key optimizations: model selection, context management, and\ncaching. These optimizations present tradeoffs in terms of cost, inference\ntime, and response quality, which applications can navigate through our high\nlevel, bidirectional interface. As a case study, we implement a WhatsApp-based\nQ&A service that uses LLMProxy to provide a rich set of features to the users.\nThis service is deployed on a small scale (100+ users) leveraging the cloud; it\nhas been operational for 15+ weeks and users have asked 1400+ questions so far.\nWe report on the experiences of running this service as well as microbenchmark\nthe specific benefits of the various cost-optimizations we present in this\npaper."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v2",
                "updated": "2024-10-04T10:14:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    17,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v2",
                "updated": "2024-10-04T07:54:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    7,
                    54,
                    58,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12016v2",
                "updated": "2024-10-04T06:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    26,
                    20,
                    4,
                    278,
                    0
                ],
                "published": "2024-06-17T18:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    33,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization"
                },
                "summary": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method."
                },
                "authors": [
                    {
                        "name": "Seungwoo Son"
                    },
                    {
                        "name": "Wonpyo Park"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Kyuyeun Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2024 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03111v1",
                "updated": "2024-10-04T03:10:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T03:10:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy"
                },
                "summary": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance."
                },
                "authors": [
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Kuang Wang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.18074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18074v1",
                "updated": "2024-10-23T17:56:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    56,
                    33,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:56:33Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    56,
                    33,
                    2,
                    297,
                    0
                ],
                "title": "UnCLe: Unsupervised Continual Learning of Depth Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UnCLe: Unsupervised Continual Learning of Depth Completion"
                },
                "summary": "We propose UnCLe, a standardized benchmark for Unsupervised Continual\nLearning of a multimodal depth estimation task: Depth completion aims to infer\na dense depth map from a pair of synchronized RGB image and sparse depth map.\nWe benchmark depth completion models under the practical scenario of\nunsupervised learning over continuous streams of data. Existing methods are\ntypically trained on a static, or stationary, dataset. However, when adapting\nto novel non-stationary distributions, they \"catastrophically forget\"\npreviously learned information. UnCLe simulates these non-stationary\ndistributions by adapting depth completion models to sequences of datasets\ncontaining diverse scenes captured from distinct domains using different visual\nand range sensors. We adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual learning of depth\ncompletion. We benchmark these models for indoor and outdoor and investigate\nthe degree of catastrophic forgetting through standard quantitative metrics.\nFurthermore, we introduce model inversion quality as an additional measure of\nforgetting. We find that unsupervised continual learning of depth completion is\nan open problem, and we invite researchers to leverage UnCLe as a development\nplatform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose UnCLe, a standardized benchmark for Unsupervised Continual\nLearning of a multimodal depth estimation task: Depth completion aims to infer\na dense depth map from a pair of synchronized RGB image and sparse depth map.\nWe benchmark depth completion models under the practical scenario of\nunsupervised learning over continuous streams of data. Existing methods are\ntypically trained on a static, or stationary, dataset. However, when adapting\nto novel non-stationary distributions, they \"catastrophically forget\"\npreviously learned information. UnCLe simulates these non-stationary\ndistributions by adapting depth completion models to sequences of datasets\ncontaining diverse scenes captured from distinct domains using different visual\nand range sensors. We adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual learning of depth\ncompletion. We benchmark these models for indoor and outdoor and investigate\nthe degree of catastrophic forgetting through standard quantitative metrics.\nFurthermore, we introduce model inversion quality as an additional measure of\nforgetting. We find that unsupervised continual learning of depth completion is\nan open problem, and we invite researchers to leverage UnCLe as a development\nplatform."
                },
                "authors": [
                    {
                        "name": "Suchisrit Gangopadhyay"
                    },
                    {
                        "name": "Xien Chen"
                    },
                    {
                        "name": "Michael Chu"
                    },
                    {
                        "name": "Patrick Rim"
                    },
                    {
                        "name": "Hyoungseob Park"
                    },
                    {
                        "name": "Alex Wong"
                    }
                ],
                "author_detail": {
                    "name": "Alex Wong"
                },
                "author": "Alex Wong",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18071v1",
                "updated": "2024-10-23T17:54:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    54,
                    43,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:54:43Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    54,
                    43,
                    2,
                    297,
                    0
                ],
                "title": "TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing\n  Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing\n  Prompts"
                },
                "summary": "Recently, multimodal large language models (MLLMs) have received much\nattention for their impressive capabilities. The evaluation of MLLMs is\nbecoming critical to analyzing attributes of MLLMs and providing valuable\ninsights. However, current benchmarks overlook the problem of prompt\nsensitivity - minor prompt variations may lead to significant performance\nfluctuations. Thus, inappropriate prompts may obscure the models' capabilities,\nunderestimating the models' performance. Moreover, different models have\ndifferent preferences for different prompts, and thus, using the same prompt\nfor all models will cause evaluation bias. This paper analyzes this deficiency\nin existing benchmarks and further introduces a new evaluation framework named\nTP-Eval, which introduces a prompt customization method to reduce evaluation\nbiases and tap models' potential. TP-Eval will rewrite the original prompts to\ndifferent customized prompts for different models. In particular, we propose\nsome well-designed modules for prompt customization tailored to the scenario of\nMLLM evaluation. Extensive experiments demonstrate the effectiveness of our\napproach to uncovering models' capabilities, and TP-Eval should benefit the\ncommunity in developing more comprehensive and convincing MLLM evaluation\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, multimodal large language models (MLLMs) have received much\nattention for their impressive capabilities. The evaluation of MLLMs is\nbecoming critical to analyzing attributes of MLLMs and providing valuable\ninsights. However, current benchmarks overlook the problem of prompt\nsensitivity - minor prompt variations may lead to significant performance\nfluctuations. Thus, inappropriate prompts may obscure the models' capabilities,\nunderestimating the models' performance. Moreover, different models have\ndifferent preferences for different prompts, and thus, using the same prompt\nfor all models will cause evaluation bias. This paper analyzes this deficiency\nin existing benchmarks and further introduces a new evaluation framework named\nTP-Eval, which introduces a prompt customization method to reduce evaluation\nbiases and tap models' potential. TP-Eval will rewrite the original prompts to\ndifferent customized prompts for different models. In particular, we propose\nsome well-designed modules for prompt customization tailored to the scenario of\nMLLM evaluation. Extensive experiments demonstrate the effectiveness of our\napproach to uncovering models' capabilities, and TP-Eval should benefit the\ncommunity in developing more comprehensive and convincing MLLM evaluation\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Xie"
                    },
                    {
                        "name": "Tianhua Li"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03185v2",
                "updated": "2024-10-23T17:52:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    52,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-03-05T18:22:15Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    18,
                    22,
                    15,
                    1,
                    65,
                    0
                ],
                "title": "Correlated Proxies: A New Definition and Improved Mitigation for Reward\n  Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correlated Proxies: A New Definition and Improved Mitigation for Reward\n  Hacking"
                },
                "summary": "Because it is difficult to precisely specify complex objectives,\nreinforcement learning policies are often optimized using flawed proxy rewards\nthat seem to capture the true objective. However, optimizing proxy rewards\nfrequently leads to reward hacking: the optimized reward function ceases to be\na good proxy, and the resulting policy performs poorly with respect to the\nunspecified true reward. Principled solutions to reward hacking have been\nimpeded by the lack of a good definition for the problem. To address this, we\nintroduce a definition of reward hacking based on the correlation between proxy\nand true rewards for states and actions seen by a \"base policy\" that breaks\ndown under optimization. We show that this definition captures reward hacking\nbehavior across several realistic settings, including in reinforcement learning\nfrom human feedback (RLHF). We then show theoretically that regularization to\nthe base policy can effectively prevent reward hacking. While current RLHF\napproaches apply a KL penalty between the action distributions of policies, our\ntheory suggests that it is more effective to regularize using the $\\chi^2$\ndivergence between the policies' occupancy measures. We intuitively show why\nthis type of regularization is superior and demonstrate that it better\nmitigates reward hacking in practice across four realistic domains, including\nRLHF for LLMs. Our code is available at https://github.com/cassidylaidlaw/orpo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Because it is difficult to precisely specify complex objectives,\nreinforcement learning policies are often optimized using flawed proxy rewards\nthat seem to capture the true objective. However, optimizing proxy rewards\nfrequently leads to reward hacking: the optimized reward function ceases to be\na good proxy, and the resulting policy performs poorly with respect to the\nunspecified true reward. Principled solutions to reward hacking have been\nimpeded by the lack of a good definition for the problem. To address this, we\nintroduce a definition of reward hacking based on the correlation between proxy\nand true rewards for states and actions seen by a \"base policy\" that breaks\ndown under optimization. We show that this definition captures reward hacking\nbehavior across several realistic settings, including in reinforcement learning\nfrom human feedback (RLHF). We then show theoretically that regularization to\nthe base policy can effectively prevent reward hacking. While current RLHF\napproaches apply a KL penalty between the action distributions of policies, our\ntheory suggests that it is more effective to regularize using the $\\chi^2$\ndivergence between the policies' occupancy measures. We intuitively show why\nthis type of regularization is superior and demonstrate that it better\nmitigates reward hacking in practice across four realistic domains, including\nRLHF for LLMs. Our code is available at https://github.com/cassidylaidlaw/orpo."
                },
                "authors": [
                    {
                        "name": "Cassidy Laidlaw"
                    },
                    {
                        "name": "Shivam Singhal"
                    },
                    {
                        "name": "Anca Dragan"
                    }
                ],
                "author_detail": {
                    "name": "Anca Dragan"
                },
                "author": "Anca Dragan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19442v2",
                "updated": "2024-10-23T17:46:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    46,
                    13,
                    2,
                    297,
                    0
                ],
                "published": "2024-04-30T10:45:40Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    10,
                    45,
                    40,
                    1,
                    121,
                    0
                ],
                "title": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs"
                },
                "summary": "Nigeria is a multilingual country with 500+ languages. Naija is a\nNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed\nlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has\nmainly been a spoken language until recently, there are now various platforms\npublishing exclusively in Naija such as Naija Wikipedia. However, it is hard to\ndistinguish by non-native from a larger pidgin languages spoken across West\nAfrica known as West African Pidgin English (WAPE) -- which is more simplied\nand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news\nplatform publishes exclusively in WAPE to cater for several countries in West\nAfrica. In our paper, we show through statistical analyses and Machine\nTranslation experiments that these two creole varieties do not represent each\nother (i.e., there are linguistic differences in word order and vocabulary) and\nGenerative AI operates only based on WAPE. In other words, Naija is\nunder-represented in Generative AI, and it is hard to teach LLMs with few\nexamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nigeria is a multilingual country with 500+ languages. Naija is a\nNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed\nlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has\nmainly been a spoken language until recently, there are now various platforms\npublishing exclusively in Naija such as Naija Wikipedia. However, it is hard to\ndistinguish by non-native from a larger pidgin languages spoken across West\nAfrica known as West African Pidgin English (WAPE) -- which is more simplied\nand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news\nplatform publishes exclusively in WAPE to cater for several countries in West\nAfrica. In our paper, we show through statistical analyses and Machine\nTranslation experiments that these two creole varieties do not represent each\nother (i.e., there are linguistic differences in word order and vocabulary) and\nGenerative AI operates only based on WAPE. In other words, Naija is\nunder-represented in Generative AI, and it is hard to teach LLMs with few\nexamples."
                },
                "authors": [
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "A. Seza Doruz"
                    },
                    {
                        "name": "Iyanuoluwa Shode"
                    },
                    {
                        "name": "Anuoluwapo Aremu"
                    }
                ],
                "author_detail": {
                    "name": "Anuoluwapo Aremu"
                },
                "author": "Anuoluwapo Aremu",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15762v2",
                "updated": "2024-10-23T17:42:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    42,
                    39,
                    2,
                    297,
                    0
                ],
                "published": "2024-07-22T16:13:38Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    16,
                    13,
                    38,
                    0,
                    204,
                    0
                ],
                "title": "Conditional Language Policy: A General Framework for Steerable\n  Multi-Objective Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Language Policy: A General Framework for Steerable\n  Multi-Objective Finetuning"
                },
                "summary": "Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge is to develop\nsteerable language models that trade-off multiple (conflicting) objectives in a\nflexible and efficient manner. This paper presents Conditional Language Policy\n(CLP), a general framework for finetuning language models on multiple\nobjectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through extensive experiments and ablations on two\nsummarization datasets, we show that CLP learns steerable language models that\noutperform and Pareto-dominate the existing approaches for multi-objective\nfinetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge is to develop\nsteerable language models that trade-off multiple (conflicting) objectives in a\nflexible and efficient manner. This paper presents Conditional Language Policy\n(CLP), a general framework for finetuning language models on multiple\nobjectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through extensive experiments and ablations on two\nsummarization datasets, we show that CLP learns steerable language models that\noutperform and Pareto-dominate the existing approaches for multi-objective\nfinetuning."
                },
                "authors": [
                    {
                        "name": "Kaiwen Wang"
                    },
                    {
                        "name": "Rahul Kidambi"
                    },
                    {
                        "name": "Ryan Sullivan"
                    },
                    {
                        "name": "Alekh Agarwal"
                    },
                    {
                        "name": "Christoph Dann"
                    },
                    {
                        "name": "Andrea Michi"
                    },
                    {
                        "name": "Marco Gelmi"
                    },
                    {
                        "name": "Yunxuan Li"
                    },
                    {
                        "name": "Raghav Gupta"
                    },
                    {
                        "name": "Avinava Dubey"
                    },
                    {
                        "name": "Alexandre Ram"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Le Hou"
                    },
                    {
                        "name": "Hongkun Yu"
                    },
                    {
                        "name": "Amr Ahmed"
                    },
                    {
                        "name": "Aranyak Mehta"
                    },
                    {
                        "name": "Lonard Hussenot"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Edouard Leurent"
                    }
                ],
                "author_detail": {
                    "name": "Edouard Leurent"
                },
                "author": "Edouard Leurent",
                "arxiv_comment": "40 pages. Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04522v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04522v2",
                "updated": "2024-10-23T17:39:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    39,
                    3,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-07T17:56:04Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    56,
                    4,
                    1,
                    128,
                    0
                ],
                "title": "Astrometric Redshifts of Supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astrometric Redshifts of Supernovae"
                },
                "summary": "Differential Chromatic Refraction (DCR) is caused by the wavelength\ndependence of our atmosphere's refractive index, which shifts the apparent\npositions of stars and galaxies and distorts their shapes depending on their\nspectral energy distributions (SEDs). While this effect is typically mitigated\nand corrected for in imaging observations, we investigate how DCR can instead\nbe used to our advantage to infer the redshifts of supernovae from multi-band,\ntime-series imaging data. We simulate Type Ia supernovae (SNe Ia) in the\nproposed Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) Deep\nDrilling Field (DDF), and evaluate astrometric redshifts. We find that the\nredshift accuracy improves dramatically with the statistical quality of the\nastrometric measurements as well as with the accuracy of the astrometric\nsolution. For a conservative choice of a 5-mas systematic uncertainty floor, we\nfind that our redshift estimation is accurate at $z < 0.6$. We then combine our\nastrometric redshifts with both host galaxy photometric redshifts and\nsupernovae photometric (light-curve) redshifts and show that this considerably\nimproves the overall redshift estimates. These astrometric redshifts will be\nvaluable especially since Rubin will discover a vast number of supernovae for\nwhich we will not be able to obtain spectroscopic redshifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Chromatic Refraction (DCR) is caused by the wavelength\ndependence of our atmosphere's refractive index, which shifts the apparent\npositions of stars and galaxies and distorts their shapes depending on their\nspectral energy distributions (SEDs). While this effect is typically mitigated\nand corrected for in imaging observations, we investigate how DCR can instead\nbe used to our advantage to infer the redshifts of supernovae from multi-band,\ntime-series imaging data. We simulate Type Ia supernovae (SNe Ia) in the\nproposed Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) Deep\nDrilling Field (DDF), and evaluate astrometric redshifts. We find that the\nredshift accuracy improves dramatically with the statistical quality of the\nastrometric measurements as well as with the accuracy of the astrometric\nsolution. For a conservative choice of a 5-mas systematic uncertainty floor, we\nfind that our redshift estimation is accurate at $z < 0.6$. We then combine our\nastrometric redshifts with both host galaxy photometric redshifts and\nsupernovae photometric (light-curve) redshifts and show that this considerably\nimproves the overall redshift estimates. These astrometric redshifts will be\nvaluable especially since Rubin will discover a vast number of supernovae for\nwhich we will not be able to obtain spectroscopic redshifts."
                },
                "authors": [
                    {
                        "name": "Jaemyoung Jason Lee"
                    },
                    {
                        "name": "Masao Sako"
                    },
                    {
                        "name": "Richard Kessler"
                    },
                    {
                        "name": "Alex I. Malz"
                    },
                    {
                        "name": "The LSST Dark Energy Science Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "The LSST Dark Energy Science Collaboration"
                },
                "author": "The LSST Dark Energy Science Collaboration",
                "arxiv_comment": "27 pages, 24 figures, accepted by The Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04522v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04522v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02916v2",
                "updated": "2024-10-23T17:26:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    26,
                    6,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-03T19:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    19,
                    7,
                    53,
                    3,
                    277,
                    0
                ],
                "title": "Safeguard is a Double-edged Sword: Denial-of-service Attack on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguard is a Double-edged Sword: Denial-of-service Attack on Large\n  Language Models"
                },
                "summary": "Safety is a paramount concern of large language models (LLMs) in their open\ndeployment. To this end, safeguard methods aim to enforce the ethical and\nresponsible use of LLMs through safety alignment or guardrail mechanisms.\nHowever, we found that the malicious attackers could exploit false positives of\nsafeguards, i.e., fooling the safeguard model to block safe content mistakenly,\nleading to a new denial-of-service (DoS) attack on LLMs. Specifically, by\nsoftware or phishing attacks on user client software, attackers insert a short,\nseemingly innocuous adversarial prompt into to user prompt templates in\nconfiguration files; thus, this prompt appears in final user requests without\nvisibility in the user interface and is not trivial to identify. By designing\nan optimization process that utilizes gradient and attention information, our\nattack can automatically generate seemingly safe adversarial prompts,\napproximately only 30 characters long, that universally block over 97\\% of user\nrequests on Llama Guard 3. The attack presents a new dimension of evaluating\nLLM safeguards focusing on false positives, fundamentally different from the\nclassic jailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety is a paramount concern of large language models (LLMs) in their open\ndeployment. To this end, safeguard methods aim to enforce the ethical and\nresponsible use of LLMs through safety alignment or guardrail mechanisms.\nHowever, we found that the malicious attackers could exploit false positives of\nsafeguards, i.e., fooling the safeguard model to block safe content mistakenly,\nleading to a new denial-of-service (DoS) attack on LLMs. Specifically, by\nsoftware or phishing attacks on user client software, attackers insert a short,\nseemingly innocuous adversarial prompt into to user prompt templates in\nconfiguration files; thus, this prompt appears in final user requests without\nvisibility in the user interface and is not trivial to identify. By designing\nan optimization process that utilizes gradient and attention information, our\nattack can automatically generate seemingly safe adversarial prompts,\napproximately only 30 characters long, that universally block over 97\\% of user\nrequests on Llama Guard 3. The attack presents a new dimension of evaluating\nLLM safeguards focusing on false positives, fundamentally different from the\nclassic jailbreak."
                },
                "authors": [
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Ziyang Xiong"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18050v1",
                "updated": "2024-10-23T17:24:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    24,
                    58,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:24:58Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    24,
                    58,
                    2,
                    297,
                    0
                ],
                "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering"
                },
                "summary": "Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG."
                },
                "authors": [
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Yukuo Cen"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Shicheng Tan"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18040v1",
                "updated": "2024-10-23T17:07:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    7,
                    32,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:07:32Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    7,
                    32,
                    2,
                    297,
                    0
                ],
                "title": "Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for\n  Russian Scientific Keyphrases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for\n  Russian Scientific Keyphrases"
                },
                "summary": "Keyphrase selection is a challenging task in natural language processing that\nhas a wide range of applications. Adapting existing supervised and unsupervised\nsolutions for the Russian language faces several limitations due to the rich\nmorphology of Russian and the limited number of training datasets available.\nRecent studies conducted on English texts show that large language models\n(LLMs) successfully address the task of generating keyphrases. LLMs allow\nachieving impressive results without task-specific fine-tuning, using text\nprompts instead. In this work, we access the performance of prompt-based\nmethods for generating keyphrases for Russian scientific abstracts. First, we\ncompare the performance of zero-shot and few-shot prompt-based methods,\nfine-tuned models, and unsupervised methods. Then we assess strategies for\nselecting keyphrase examples in a few-shot setting. We present the outcomes of\nhuman evaluation of the generated keyphrases and analyze the strengths and\nweaknesses of the models through expert assessment. Our results suggest that\nprompt-based methods can outperform common baselines even using simple text\nprompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyphrase selection is a challenging task in natural language processing that\nhas a wide range of applications. Adapting existing supervised and unsupervised\nsolutions for the Russian language faces several limitations due to the rich\nmorphology of Russian and the limited number of training datasets available.\nRecent studies conducted on English texts show that large language models\n(LLMs) successfully address the task of generating keyphrases. LLMs allow\nachieving impressive results without task-specific fine-tuning, using text\nprompts instead. In this work, we access the performance of prompt-based\nmethods for generating keyphrases for Russian scientific abstracts. First, we\ncompare the performance of zero-shot and few-shot prompt-based methods,\nfine-tuned models, and unsupervised methods. Then we assess strategies for\nselecting keyphrase examples in a few-shot setting. We present the outcomes of\nhuman evaluation of the generated keyphrases and analyze the strengths and\nweaknesses of the models through expert assessment. Our results suggest that\nprompt-based methods can outperform common baselines even using simple text\nprompts."
                },
                "authors": [
                    {
                        "name": "Anna Glazkova"
                    },
                    {
                        "name": "Dmitry Morozov"
                    },
                    {
                        "name": "Timur Garipov"
                    }
                ],
                "author_detail": {
                    "name": "Timur Garipov"
                },
                "author": "Timur Garipov",
                "arxiv_comment": "The 12th International Conference on Analysis of Images, Social\n  Networks and Texts (AIST'2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.7.m; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18038v1",
                "updated": "2024-10-23T17:06:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    6,
                    56,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:06:56Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    6,
                    56,
                    2,
                    297,
                    0
                ],
                "title": "POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM\n  Inference"
                },
                "summary": "Each request in LLM inference goes through two phases: compute-bound prefill\nand memory-bandwidth-bound decode. To improve GPU utilization, recent systems\nuse hybrid batching that combines the prefill and decode phases of different\nrequests into the same batch. Hybrid batching works well for linear operations\nas it amortizes the cost of loading model weights from HBM. However, attention\ncomputation in hybrid batches remains inefficient because existing attention\nkernels are optimized for either prefill or decode.\n  In this paper, we present POD-Attention -- the first GPU kernel that\nefficiently computes attention for hybrid batches. POD-Attention aims to\nmaximize the utilization of both compute and memory bandwidth by carefully\nallocating the GPU's resources such that prefill and decode operations happen\nconcurrently on the same multiprocessor. We integrate POD-Attention in a\nstate-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up\nattention computation by up to 75% (mean 28%) and increases LLM serving\nthroughput by up to 22% in offline inference. In online inference,\nPOD-Attention enables lower time-to-first-token (TTFT), time-between-tokens\n(TBT), and request execution latency versus Sarathi-Serve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Each request in LLM inference goes through two phases: compute-bound prefill\nand memory-bandwidth-bound decode. To improve GPU utilization, recent systems\nuse hybrid batching that combines the prefill and decode phases of different\nrequests into the same batch. Hybrid batching works well for linear operations\nas it amortizes the cost of loading model weights from HBM. However, attention\ncomputation in hybrid batches remains inefficient because existing attention\nkernels are optimized for either prefill or decode.\n  In this paper, we present POD-Attention -- the first GPU kernel that\nefficiently computes attention for hybrid batches. POD-Attention aims to\nmaximize the utilization of both compute and memory bandwidth by carefully\nallocating the GPU's resources such that prefill and decode operations happen\nconcurrently on the same multiprocessor. We integrate POD-Attention in a\nstate-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up\nattention computation by up to 75% (mean 28%) and increases LLM serving\nthroughput by up to 22% in offline inference. In online inference,\nPOD-Attention enables lower time-to-first-token (TTFT), time-between-tokens\n(TBT), and request execution latency versus Sarathi-Serve."
                },
                "authors": [
                    {
                        "name": "Aditya K Kamath"
                    },
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Simon Peter"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18035v1",
                "updated": "2024-10-23T17:04:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    4,
                    40,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:04:40Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    4,
                    40,
                    2,
                    297,
                    0
                ],
                "title": "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language\n  Models Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language\n  Models Fine-tuning"
                },
                "summary": "Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are\nhighly effective parameter-efficient fine-tuning (PEFT) methods. However, they\nintroduce significant latency in multi-tenant settings due to the LoRA modules\nand MOE routers added to multiple linear modules in the Transformer layer. To\naddress this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel\nand efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods\nby considering each LoRA module as an expert and employing a prompt-aware\nrouting mechanism. This mechanism calculates expert routing results once before\ngenerating the first new token and reuses these results for subsequent tokens,\nreducing latency. Extensive experiments and analysis on commonsense reasoning\ntasks, math reasoning tasks, and widely used LLM evaluation benchmarks\ndemonstrate that MiLoRA consistently outperforms strong PEFT baselines with\ncomparable tunable parameter budgets. Additionally, MiLoRA significantly\nreduces latency in multi-tenant settings compared to previous LoRA-based\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are\nhighly effective parameter-efficient fine-tuning (PEFT) methods. However, they\nintroduce significant latency in multi-tenant settings due to the LoRA modules\nand MOE routers added to multiple linear modules in the Transformer layer. To\naddress this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel\nand efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods\nby considering each LoRA module as an expert and employing a prompt-aware\nrouting mechanism. This mechanism calculates expert routing results once before\ngenerating the first new token and reuses these results for subsequent tokens,\nreducing latency. Extensive experiments and analysis on commonsense reasoning\ntasks, math reasoning tasks, and widely used LLM evaluation benchmarks\ndemonstrate that MiLoRA consistently outperforms strong PEFT baselines with\ncomparable tunable parameter budgets. Additionally, MiLoRA significantly\nreduces latency in multi-tenant settings compared to previous LoRA-based\nmethods."
                },
                "authors": [
                    {
                        "name": "Jingfan Zhang"
                    },
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Dan Chen"
                    },
                    {
                        "name": "Xing Tian"
                    },
                    {
                        "name": "Huanran Zheng"
                    },
                    {
                        "name": "Wei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhu"
                },
                "author": "Wei Zhu",
                "arxiv_comment": "Accepted by EMNLP 2024 Findings. arXiv admin note: substantial text\n  overlap with arXiv:2405.18203",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18032v1",
                "updated": "2024-10-23T17:02:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    2,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:02:59Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    2,
                    59,
                    2,
                    297,
                    0
                ],
                "title": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration"
                },
                "summary": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Qizhi Chu"
                    },
                    {
                        "name": "Yubin Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yaoqi Liu"
                    },
                    {
                        "name": "Zekai Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Cheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yang"
                },
                "author": "Cheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12025v2",
                "updated": "2024-10-23T17:01:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    1,
                    5,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-21T22:35:19Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    35,
                    19,
                    2,
                    234,
                    0
                ],
                "title": "Exploring Large Language Models for Feature Selection: A Data-centric\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Models for Feature Selection: A Data-centric\n  Perspective"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\ninfluenced various domains, leveraging their exceptional few-shot and zero-shot\nlearning capabilities. In this work, we aim to explore and understand the\nLLMs-based feature selection methods from a data-centric perspective. We begin\nby categorizing existing feature selection methods with LLMs into two groups:\ndata-driven feature selection which requires numerical values of samples to do\nstatistical inference and text-based feature selection which utilizes prior\nknowledge of LLMs to do semantical associations using descriptive context. We\nconduct experiments in both classification and regression tasks with LLMs in\nvarious sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the\neffectiveness and robustness of text-based feature selection methods and\nshowcase their potentials using a real-world medical application. We also\ndiscuss the challenges and future opportunities in employing LLMs for feature\nselection, offering insights for further research and development in this\nemerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has significantly\ninfluenced various domains, leveraging their exceptional few-shot and zero-shot\nlearning capabilities. In this work, we aim to explore and understand the\nLLMs-based feature selection methods from a data-centric perspective. We begin\nby categorizing existing feature selection methods with LLMs into two groups:\ndata-driven feature selection which requires numerical values of samples to do\nstatistical inference and text-based feature selection which utilizes prior\nknowledge of LLMs to do semantical associations using descriptive context. We\nconduct experiments in both classification and regression tasks with LLMs in\nvarious sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the\neffectiveness and robustness of text-based feature selection methods and\nshowcase their potentials using a real-world medical application. We also\ndiscuss the challenges and future opportunities in employing LLMs for feature\nselection, offering insights for further research and development in this\nemerging field."
                },
                "authors": [
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "Accepted by SIGKDD Explorations (December 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18021v1",
                "updated": "2024-10-23T16:51:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    51,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:51:31Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    51,
                    31,
                    2,
                    297,
                    0
                ],
                "title": "Deep Nonparametric Inference for Conditional Hazard Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Nonparametric Inference for Conditional Hazard Function"
                },
                "summary": "We propose a novel deep learning approach to nonparametric statistical\ninference for the conditional hazard function of survival time with\nright-censored data. We use a deep neural network (DNN) to approximate the\nlogarithm of a conditional hazard function given covariates and obtain a DNN\nlikelihood-based estimator of the conditional hazard function. Such an\nestimation approach renders model flexibility and hence relaxes structural and\nfunctional assumptions on conditional hazard or survival functions. We\nestablish the nonasymptotic error bound and functional asymptotic normality of\nthe proposed estimator. Subsequently, we develop new one-sample tests for\ngoodness-of-fit evaluation and two-sample tests for treatment comparison. Both\nsimulation studies and real application analysis show superior performances of\nthe proposed estimators and tests in comparison with existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel deep learning approach to nonparametric statistical\ninference for the conditional hazard function of survival time with\nright-censored data. We use a deep neural network (DNN) to approximate the\nlogarithm of a conditional hazard function given covariates and obtain a DNN\nlikelihood-based estimator of the conditional hazard function. Such an\nestimation approach renders model flexibility and hence relaxes structural and\nfunctional assumptions on conditional hazard or survival functions. We\nestablish the nonasymptotic error bound and functional asymptotic normality of\nthe proposed estimator. Subsequently, we develop new one-sample tests for\ngoodness-of-fit evaluation and two-sample tests for treatment comparison. Both\nsimulation studies and real application analysis show superior performances of\nthe proposed estimators and tests in comparison with existing methods."
                },
                "authors": [
                    {
                        "name": "Wen Su"
                    },
                    {
                        "name": "Kin-Yat Liu"
                    },
                    {
                        "name": "Guosheng Yin"
                    },
                    {
                        "name": "Jian Huang"
                    },
                    {
                        "name": "Xingqiu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xingqiu Zhao"
                },
                "author": "Xingqiu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18012v1",
                "updated": "2024-10-23T16:40:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    40,
                    38,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:40:38Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    40,
                    38,
                    2,
                    297,
                    0
                ],
                "title": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC\n  Meeting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC\n  Meeting"
                },
                "summary": "The Federal Funds rate in the United States plays a significant role in both\ndomestic and international financial markets. However, research has\npredominantly focused on the effects of adjustments to the Federal Funds rate\nrather than on the decision-making process itself. Recent advancements in large\nlanguage models(LLMs) offer a potential method for reconstructing the original\nFOMC meetings, which are responsible for setting the Federal Funds rate. In\nthis paper, we propose a five-stage FOMC meeting simulation framework, MiniFed,\nwhich employs LLM agents to simulate real-world FOMC meeting members and\noptimize the FOMC structure. This framework effectively revitalizes the FOMC\nmeeting process and facilitates projections of the Federal Funds rate.\nExperimental results demonstrate that our proposed MiniFed framework achieves\nboth high accuracy in Federal Funds rate projections and behavioral alignment\nwith the agents' real-world counterparts. Given that few studies have focused\non employing LLM agents to simulate large-scale real-world conferences, our\nwork can serve as a benchmark for future developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Federal Funds rate in the United States plays a significant role in both\ndomestic and international financial markets. However, research has\npredominantly focused on the effects of adjustments to the Federal Funds rate\nrather than on the decision-making process itself. Recent advancements in large\nlanguage models(LLMs) offer a potential method for reconstructing the original\nFOMC meetings, which are responsible for setting the Federal Funds rate. In\nthis paper, we propose a five-stage FOMC meeting simulation framework, MiniFed,\nwhich employs LLM agents to simulate real-world FOMC meeting members and\noptimize the FOMC structure. This framework effectively revitalizes the FOMC\nmeeting process and facilitates projections of the Federal Funds rate.\nExperimental results demonstrate that our proposed MiniFed framework achieves\nboth high accuracy in Federal Funds rate projections and behavioral alignment\nwith the agents' real-world counterparts. Given that few studies have focused\non employing LLM agents to simulate large-scale real-world conferences, our\nwork can serve as a benchmark for future developments."
                },
                "authors": [
                    {
                        "name": "Sungil Seok"
                    },
                    {
                        "name": "Qiyuan Yang"
                    },
                    {
                        "name": "Juan Feng"
                    },
                    {
                        "name": "Shuide Wen"
                    },
                    {
                        "name": "Wenming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wenming Yang"
                },
                "author": "Wenming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16934v2",
                "updated": "2024-10-23T16:38:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    38,
                    6,
                    2,
                    297,
                    0
                ],
                "published": "2024-01-30T11:57:04Z",
                "published_parsed": [
                    2024,
                    1,
                    30,
                    11,
                    57,
                    4,
                    1,
                    30,
                    0
                ],
                "title": "Extreme emission line galaxies detected in JADES JWST/NIRSpec I:\n  inferred galaxy properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme emission line galaxies detected in JADES JWST/NIRSpec I:\n  inferred galaxy properties"
                },
                "summary": "Extreme emission line galaxies (EELGs) exhibit large equivalent widths (EW)\nin their rest-optical emission lines ([OIII]$\\lambda5007$ or H$\\alpha$\nrest-frame EW$ > 750\\r{A}$) which can be tied to a recent upturn in star\nformation rate, due to the sensitivity of the nebular line emission and the\nrest-optical continuum to young ($<10$Myr) and evolved stellar populations,\nrespectively. By studying a sample of 85 star forming galaxies (SFGs), spanning\nthe redshift and magnitude interval $3 <z<9.5$ and $-16>$ M$_{UV}>-21$, in the\nJWST Advanced Deep Extragalactic Survey (JADES) with NIRSpec/prism\nspectroscopy, we determine that SFGs initiate an EELG phase when entering a\nsignificant burst of star formation, with the highest EWs observed in EELGs\nwith the youngest luminosity-weighted ages ($<5$ Myr old) and the highest burst\nintensity (those with the greatest excess between their current and long-term\naverage SFR). We spectroscopically confirm that a greater proportion of SFGs\nare in an EELG phase at high redshift in our UV-selected sample ($61\\pm4\\%$ in\nour $z>5.7$ high-redshift bin, compared to $23^{+4}_{-1}\\%$ in our\nlowest-redshift bin $3<z<4.1$) due to the combined evolution of metallicity,\nionisation parameter and star formation histories with redshift. We report that\nthe EELGs within our sample exhibit a higher average ionisation efficiency\n($\\log_{10}(\\xi_{ion}^{HII}/$erg$^{-1}$Hz)$=25.5\\pm0.2$) than the non-EELGs.\nHigh-redshift EELGs therefore comprise a population of efficient ionising\nphoton producers. Additionally, we report that $53\\%$ (9/17) of EELGs at\n$z>5.7$ have observed Lyman-$\\alpha$ emission, potentially lying within large\nionised regions. The high detection rate of Lyman-$\\alpha$ emitters in our EELG\nselection suggests that the physical conditions associated with entering an\nEELG phase also promote the escape of Lyman-$\\alpha$ photons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme emission line galaxies (EELGs) exhibit large equivalent widths (EW)\nin their rest-optical emission lines ([OIII]$\\lambda5007$ or H$\\alpha$\nrest-frame EW$ > 750\\r{A}$) which can be tied to a recent upturn in star\nformation rate, due to the sensitivity of the nebular line emission and the\nrest-optical continuum to young ($<10$Myr) and evolved stellar populations,\nrespectively. By studying a sample of 85 star forming galaxies (SFGs), spanning\nthe redshift and magnitude interval $3 <z<9.5$ and $-16>$ M$_{UV}>-21$, in the\nJWST Advanced Deep Extragalactic Survey (JADES) with NIRSpec/prism\nspectroscopy, we determine that SFGs initiate an EELG phase when entering a\nsignificant burst of star formation, with the highest EWs observed in EELGs\nwith the youngest luminosity-weighted ages ($<5$ Myr old) and the highest burst\nintensity (those with the greatest excess between their current and long-term\naverage SFR). We spectroscopically confirm that a greater proportion of SFGs\nare in an EELG phase at high redshift in our UV-selected sample ($61\\pm4\\%$ in\nour $z>5.7$ high-redshift bin, compared to $23^{+4}_{-1}\\%$ in our\nlowest-redshift bin $3<z<4.1$) due to the combined evolution of metallicity,\nionisation parameter and star formation histories with redshift. We report that\nthe EELGs within our sample exhibit a higher average ionisation efficiency\n($\\log_{10}(\\xi_{ion}^{HII}/$erg$^{-1}$Hz)$=25.5\\pm0.2$) than the non-EELGs.\nHigh-redshift EELGs therefore comprise a population of efficient ionising\nphoton producers. Additionally, we report that $53\\%$ (9/17) of EELGs at\n$z>5.7$ have observed Lyman-$\\alpha$ emission, potentially lying within large\nionised regions. The high detection rate of Lyman-$\\alpha$ emitters in our EELG\nselection suggests that the physical conditions associated with entering an\nEELG phase also promote the escape of Lyman-$\\alpha$ photons."
                },
                "authors": [
                    {
                        "name": "Kit Boyett"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Emma Curtis-Lake"
                    },
                    {
                        "name": "Jacopo Chevallard"
                    },
                    {
                        "name": "Alex J. Cameron"
                    },
                    {
                        "name": "Gareth C. Jones"
                    },
                    {
                        "name": "Aayush Saxena"
                    },
                    {
                        "name": "Stphane Charlot"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Imaan E. B. Wallace"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Chris Willott"
                    },
                    {
                        "name": "Stacey Alberts"
                    },
                    {
                        "name": "Daniel J. Eisenstein"
                    },
                    {
                        "name": "Kevin Hainline"
                    },
                    {
                        "name": "Ryan Hausen"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Marcia Rieke"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Daniel P. Stark"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Zuyi Chen"
                    },
                    {
                        "name": "Eiichi Egami"
                    },
                    {
                        "name": "Ryan Endsley"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Isaac Laseter"
                    },
                    {
                        "name": "Tobias J. Looser"
                    },
                    {
                        "name": "Michael V. Maseda"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Irene Shivaei"
                    },
                    {
                        "name": "Charlotte Simmonds"
                    },
                    {
                        "name": "Renske Smit"
                    },
                    {
                        "name": "Hannah bler"
                    },
                    {
                        "name": "Joris Witstok"
                    }
                ],
                "author_detail": {
                    "name": "Joris Witstok"
                },
                "author": "Joris Witstok",
                "arxiv_comment": "34 pages, 25 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18006v1",
                "updated": "2024-10-23T16:28:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    28,
                    1,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:28:01Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    28,
                    1,
                    2,
                    297,
                    0
                ],
                "title": "Limit Laws for Gromov-Wasserstein Alignment with Applications to Testing\n  Graph Isomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limit Laws for Gromov-Wasserstein Alignment with Applications to Testing\n  Graph Isomorphisms"
                },
                "summary": "The Gromov-Wasserstein (GW) distance enables comparing metric measure spaces\nbased solely on their internal structure, making it invariant to isomorphic\ntransformations. This property is particularly useful for comparing datasets\nthat naturally admit isomorphic representations, such as unlabelled graphs or\nobjects embedded in space. However, apart from the recently derived empirical\nconvergence rates for the quadratic GW problem, a statistical theory for valid\nestimation and inference remains largely obscure. Pushing the frontier of\nstatistical GW further, this work derives the first limit laws for the\nempirical GW distance across several settings of interest: (i)~discrete,\n(ii)~semi-discrete, and (iii)~general distributions under moment constraints\nunder the entropically regularized GW distance. The derivations rely on a novel\nstability analysis of the GW functional in the marginal distributions. The\nlimit laws then follow by an adaptation of the functional delta method. As\nasymptotic normality fails to hold in most cases, we establish the consistency\nof an efficient estimation procedure for the limiting law in the discrete case,\nbypassing the need for computationally intensive resampling methods. We apply\nthese findings to testing whether collections of unlabelled graphs are\ngenerated from distributions that are isomorphic to each other.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gromov-Wasserstein (GW) distance enables comparing metric measure spaces\nbased solely on their internal structure, making it invariant to isomorphic\ntransformations. This property is particularly useful for comparing datasets\nthat naturally admit isomorphic representations, such as unlabelled graphs or\nobjects embedded in space. However, apart from the recently derived empirical\nconvergence rates for the quadratic GW problem, a statistical theory for valid\nestimation and inference remains largely obscure. Pushing the frontier of\nstatistical GW further, this work derives the first limit laws for the\nempirical GW distance across several settings of interest: (i)~discrete,\n(ii)~semi-discrete, and (iii)~general distributions under moment constraints\nunder the entropically regularized GW distance. The derivations rely on a novel\nstability analysis of the GW functional in the marginal distributions. The\nlimit laws then follow by an adaptation of the functional delta method. As\nasymptotic normality fails to hold in most cases, we establish the consistency\nof an efficient estimation procedure for the limiting law in the discrete case,\nbypassing the need for computationally intensive resampling methods. We apply\nthese findings to testing whether collections of unlabelled graphs are\ngenerated from distributions that are isomorphic to each other."
                },
                "authors": [
                    {
                        "name": "Gabriel Rioux"
                    },
                    {
                        "name": "Ziv Goldfeld"
                    },
                    {
                        "name": "Kengo Kato"
                    }
                ],
                "author_detail": {
                    "name": "Kengo Kato"
                },
                "author": "Kengo Kato",
                "arxiv_comment": "65 pages. 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16664v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16664v3",
                "updated": "2024-10-23T16:27:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    27,
                    48,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-26T15:35:24Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    15,
                    35,
                    24,
                    0,
                    57,
                    0
                ],
                "title": "LLM-Assisted Multi-Teacher Continual Learning for Visual Question\n  Answering in Robotic Surgery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Multi-Teacher Continual Learning for Visual Question\n  Answering in Robotic Surgery"
                },
                "summary": "Visual question answering (VQA) is crucial for promoting surgical education.\nIn practice, the needs of trainees are constantly evolving, such as learning\nmore surgical types, adapting to different robots, and learning new surgical\ninstruments and techniques for various surgeries. However, patient data privacy\noften restricts the availability of old data when updating the model,\nnecessitating an exemplar-free continual learning (CL) setup. Prior CL studies\noverlooked two vital problems in the surgical domain: 1) large domain shifts\nfrom diverse surgical operations collected from multiple sources, and 2) severe\ndata imbalance arising from the uneven presence of surgical instruments or\nactivities. This paper proposes addressing these problems with a multimodal\nlarge language model (LLM) and an adaptive weight assignment methodology. We\nfirst develop a new multi-teacher CL framework that leverages a multimodal LLM\nas the additional teacher. The strong generalization ability of the LLM can\nbridge the knowledge gap when domain shifts and data imbalances occur. We then\nput forth a novel data processing method that transforms complex LLM embeddings\ninto logits compatible with our CL framework. We further design an adaptive\nweight assignment approach that balances the generalization ability of the LLM\nand the domain expertise of the old CL model. Finally, to comprehensively test\nthe effectiveness of our proposed method, we have also constructed two new\nsurgical VQA datasets that are largely different from existing ones and could\nbe valuable resources for future research. Extensive experimental results on\nthe tested datasets demonstrate the superiority of our method to other advanced\nCL schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual question answering (VQA) is crucial for promoting surgical education.\nIn practice, the needs of trainees are constantly evolving, such as learning\nmore surgical types, adapting to different robots, and learning new surgical\ninstruments and techniques for various surgeries. However, patient data privacy\noften restricts the availability of old data when updating the model,\nnecessitating an exemplar-free continual learning (CL) setup. Prior CL studies\noverlooked two vital problems in the surgical domain: 1) large domain shifts\nfrom diverse surgical operations collected from multiple sources, and 2) severe\ndata imbalance arising from the uneven presence of surgical instruments or\nactivities. This paper proposes addressing these problems with a multimodal\nlarge language model (LLM) and an adaptive weight assignment methodology. We\nfirst develop a new multi-teacher CL framework that leverages a multimodal LLM\nas the additional teacher. The strong generalization ability of the LLM can\nbridge the knowledge gap when domain shifts and data imbalances occur. We then\nput forth a novel data processing method that transforms complex LLM embeddings\ninto logits compatible with our CL framework. We further design an adaptive\nweight assignment approach that balances the generalization ability of the LLM\nand the domain expertise of the old CL model. Finally, to comprehensively test\nthe effectiveness of our proposed method, we have also constructed two new\nsurgical VQA datasets that are largely different from existing ones and could\nbe valuable resources for future research. Extensive experimental results on\nthe tested datasets demonstrate the superiority of our method to other advanced\nCL schemes."
                },
                "authors": [
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Kexin Chen"
                    },
                    {
                        "name": "Yue Zhan"
                    },
                    {
                        "name": "Chang Han Low"
                    },
                    {
                        "name": "Tao You"
                    },
                    {
                        "name": "Mobarakol Islam"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Yueming Jin"
                    },
                    {
                        "name": "Guangyong Chen"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "arxiv_comment": "This paper has been accapted by 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16664v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16664v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17270v2",
                "updated": "2024-10-23T16:27:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    27,
                    20,
                    2,
                    297,
                    0
                ],
                "published": "2024-09-25T18:35:45Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    35,
                    45,
                    2,
                    269,
                    0
                ],
                "title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains."
                },
                "authors": [
                    {
                        "name": "Debargha Ganguly"
                    },
                    {
                        "name": "Srinivasan Iyengar"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Shivkumar Kalyanaraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivkumar Kalyanaraman"
                },
                "author": "Shivkumar Kalyanaraman",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) System 2 Reasoning At Scale Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18003v1",
                "updated": "2024-10-23T16:25:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    36,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:36Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    36,
                    2,
                    297,
                    0
                ],
                "title": "Inferring stability properties of chaotic systems on autoencoders'\n  latent spaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring stability properties of chaotic systems on autoencoders'\n  latent spaces"
                },
                "summary": "The data-driven learning of solutions of partial differential equations can\nbe based on a divide-and-conquer strategy. First, the high dimensional data is\ncompressed to a latent space with an autoencoder; and, second, the temporal\ndynamics are inferred on the latent space with a form of recurrent neural\nnetwork. In chaotic systems and turbulence, convolutional autoencoders and echo\nstate networks (CAE-ESN) successfully forecast the dynamics, but little is\nknown about whether the stability properties can also be inferred. We show that\nthe CAE-ESN model infers the invariant stability properties and the geometry of\nthe tangent space in the low-dimensional manifold (i.e. the latent space)\nthrough Lyapunov exponents and covariant Lyapunov vectors. This work opens up\nnew opportunities for inferring the stability of high-dimensional chaotic\nsystems in latent spaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The data-driven learning of solutions of partial differential equations can\nbe based on a divide-and-conquer strategy. First, the high dimensional data is\ncompressed to a latent space with an autoencoder; and, second, the temporal\ndynamics are inferred on the latent space with a form of recurrent neural\nnetwork. In chaotic systems and turbulence, convolutional autoencoders and echo\nstate networks (CAE-ESN) successfully forecast the dynamics, but little is\nknown about whether the stability properties can also be inferred. We show that\nthe CAE-ESN model infers the invariant stability properties and the geometry of\nthe tangent space in the low-dimensional manifold (i.e. the latent space)\nthrough Lyapunov exponents and covariant Lyapunov vectors. This work opens up\nnew opportunities for inferring the stability of high-dimensional chaotic\nsystems in latent spaces."
                },
                "authors": [
                    {
                        "name": "Elise zalp"
                    },
                    {
                        "name": "Luca Magri"
                    }
                ],
                "author_detail": {
                    "name": "Luca Magri"
                },
                "author": "Luca Magri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17984v1",
                "updated": "2024-10-23T15:51:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    51,
                    43,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:51:43Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    51,
                    43,
                    2,
                    297,
                    0
                ],
                "title": "Linking the primordial composition of planet building disks to the\n  present-day composition of rocky exoplanets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linking the primordial composition of planet building disks to the\n  present-day composition of rocky exoplanets"
                },
                "summary": "The composition of rocky planets is strongly driven by the primordial\nmaterials in the protoplanetary disk, which can be inferred from the abundances\nof the host star. Understanding this compositional link is crucial for\ncharacterizing exoplanets. We aim to investigate the relationship between the\ncompositions of low-mass planets and their host stars. We determined the\nprimordial compositions of host stars using high-precision present-day stellar\nabundances and stellar evolutionary models. These primordial abundances were\nthen input into a stoichiometric model to estimate the composition of\nplanet-building blocks. Additionally, we employed a three-component planetary\ninterior model (core, mantle, water in different phases) to estimate planetary\ncompositions based only on their radius and mass. We found that although\nstellar abundances vary over time, relevant abundance ratios like Fe/Mg remain\nrelatively constant during the main sequence evolution for low temperature\nstars. A strong correlation is found between the iron-to-silicate mass fraction\nof protoplanetary disks and planets, while no significant correlation was\nobserved for water mass fractions. The Fe/Mg ratio varies significantly between\nplanets and their stars, indicating substantial disk-driven compositional\ndiversity, and this ratio also correlates with planetary radius. While stellar\nabundances, as a proxy of the composition of protoplanetary disk, provide a\nbaseline for planetary composition, significant deviations arise due to complex\ndisk processes, challenging the assumption of a direct, one-to-one elemental\nrelationship between stars and their planets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The composition of rocky planets is strongly driven by the primordial\nmaterials in the protoplanetary disk, which can be inferred from the abundances\nof the host star. Understanding this compositional link is crucial for\ncharacterizing exoplanets. We aim to investigate the relationship between the\ncompositions of low-mass planets and their host stars. We determined the\nprimordial compositions of host stars using high-precision present-day stellar\nabundances and stellar evolutionary models. These primordial abundances were\nthen input into a stoichiometric model to estimate the composition of\nplanet-building blocks. Additionally, we employed a three-component planetary\ninterior model (core, mantle, water in different phases) to estimate planetary\ncompositions based only on their radius and mass. We found that although\nstellar abundances vary over time, relevant abundance ratios like Fe/Mg remain\nrelatively constant during the main sequence evolution for low temperature\nstars. A strong correlation is found between the iron-to-silicate mass fraction\nof protoplanetary disks and planets, while no significant correlation was\nobserved for water mass fractions. The Fe/Mg ratio varies significantly between\nplanets and their stars, indicating substantial disk-driven compositional\ndiversity, and this ratio also correlates with planetary radius. While stellar\nabundances, as a proxy of the composition of protoplanetary disk, provide a\nbaseline for planetary composition, significant deviations arise due to complex\ndisk processes, challenging the assumption of a direct, one-to-one elemental\nrelationship between stars and their planets."
                },
                "authors": [
                    {
                        "name": "V. Adibekyan"
                    },
                    {
                        "name": "M. Deal"
                    },
                    {
                        "name": "C. Dorn"
                    },
                    {
                        "name": "I. Dittrich"
                    },
                    {
                        "name": "B. M. T. B. Soares"
                    },
                    {
                        "name": "S. G. Sousa"
                    },
                    {
                        "name": "N. C. Santos"
                    },
                    {
                        "name": "B. Bitsch"
                    },
                    {
                        "name": "C. Mordasini"
                    },
                    {
                        "name": "S. C. C. Barros"
                    },
                    {
                        "name": "D. Bossini"
                    },
                    {
                        "name": "T. L. Campante"
                    },
                    {
                        "name": "E. Delgado Mena"
                    },
                    {
                        "name": "O. D. S. Demangeon"
                    },
                    {
                        "name": "P. Figueira"
                    },
                    {
                        "name": "N. Moedas"
                    },
                    {
                        "name": "Zh. Martirosyan"
                    },
                    {
                        "name": "G. Israelian"
                    },
                    {
                        "name": "A. A. Hakobyan"
                    }
                ],
                "author_detail": {
                    "name": "A. A. Hakobyan"
                },
                "author": "A. A. Hakobyan",
                "arxiv_comment": "Accepted by A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14979v2",
                "updated": "2024-10-23T15:43:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    43,
                    28,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-19T05:01:56Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    5,
                    1,
                    56,
                    5,
                    293,
                    0
                ],
                "title": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration"
                },
                "summary": "Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence."
                },
                "authors": [
                    {
                        "name": "Wei Xie"
                    },
                    {
                        "name": "Shuoyoucheng Ma"
                    },
                    {
                        "name": "Zhenhua Wang"
                    },
                    {
                        "name": "Enze Wang"
                    },
                    {
                        "name": "Baosheng Wang"
                    },
                    {
                        "name": "Jinshu Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinshu Su"
                },
                "author": "Jinshu Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17970v1",
                "updated": "2024-10-23T15:36:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    36,
                    8,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:36:08Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    36,
                    8,
                    2,
                    297,
                    0
                ],
                "title": "Optical Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical Generative Models"
                },
                "summary": "Generative models cover various application areas, including image, video and\nmusic synthesis, natural language processing, and molecular design, among many\nothers. As digital generative models become larger, scalable inference in a\nfast and energy-efficient manner becomes a challenge. Here, we present optical\ngenerative models inspired by diffusion models, where a shallow and fast\ndigital encoder first maps random noise into phase patterns that serve as\noptical generative seeds for a desired data distribution; a jointly-trained\nfree-space-based reconfigurable decoder all-optically processes these\ngenerative seeds to create novel images (never seen before) following the\ntarget data distribution. Except for the illumination power and the random seed\ngeneration through a shallow encoder, these optical generative models do not\nconsume computing power during the synthesis of novel images. We report the\noptical generation of monochrome and multi-color novel images of handwritten\ndigits, fashion products, butterflies, and human faces, following the data\ndistributions of MNIST, Fashion MNIST, Butterflies-100, and Celeb-A datasets,\nrespectively, achieving an overall performance comparable to digital neural\nnetwork-based generative models. To experimentally demonstrate optical\ngenerative models, we used visible light to generate, in a snapshot, novel\nimages of handwritten digits and fashion products. These optical generative\nmodels might pave the way for energy-efficient, scalable and rapid inference\ntasks, further exploiting the potentials of optics and photonics for artificial\nintelligence-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models cover various application areas, including image, video and\nmusic synthesis, natural language processing, and molecular design, among many\nothers. As digital generative models become larger, scalable inference in a\nfast and energy-efficient manner becomes a challenge. Here, we present optical\ngenerative models inspired by diffusion models, where a shallow and fast\ndigital encoder first maps random noise into phase patterns that serve as\noptical generative seeds for a desired data distribution; a jointly-trained\nfree-space-based reconfigurable decoder all-optically processes these\ngenerative seeds to create novel images (never seen before) following the\ntarget data distribution. Except for the illumination power and the random seed\ngeneration through a shallow encoder, these optical generative models do not\nconsume computing power during the synthesis of novel images. We report the\noptical generation of monochrome and multi-color novel images of handwritten\ndigits, fashion products, butterflies, and human faces, following the data\ndistributions of MNIST, Fashion MNIST, Butterflies-100, and Celeb-A datasets,\nrespectively, achieving an overall performance comparable to digital neural\nnetwork-based generative models. To experimentally demonstrate optical\ngenerative models, we used visible light to generate, in a snapshot, novel\nimages of handwritten digits and fashion products. These optical generative\nmodels might pave the way for energy-efficient, scalable and rapid inference\ntasks, further exploiting the potentials of optics and photonics for artificial\nintelligence-generated content."
                },
                "authors": [
                    {
                        "name": "Shiqi Chen"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Hanlong Chen"
                    },
                    {
                        "name": "Aydogan Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Aydogan Ozcan"
                },
                "author": "Aydogan Ozcan",
                "arxiv_comment": "24 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17966v1",
                "updated": "2024-10-23T15:34:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    34,
                    6,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:34:06Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    34,
                    6,
                    2,
                    297,
                    0
                ],
                "title": "A Wavelet Diffusion GAN for Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Wavelet Diffusion GAN for Image Super-Resolution"
                },
                "summary": "In recent years, diffusion models have emerged as a superior alternative to\ngenerative adversarial networks (GANs) for high-fidelity image generation, with\nwide applications in text-to-image generation, image-to-image translation, and\nsuper-resolution. However, their real-time feasibility is hindered by slow\ntraining and inference speeds. This study addresses this challenge by proposing\na wavelet-based conditional Diffusion GAN scheme for Single-Image\nSuper-Resolution (SISR). Our approach utilizes the diffusion GAN paradigm to\nreduce the timesteps required by the reverse diffusion process and the Discrete\nWavelet Transform (DWT) to achieve dimensionality reduction, decreasing\ntraining and inference times significantly. The results of an experimental\nvalidation on the CelebA-HQ dataset confirm the effectiveness of our proposed\nscheme. Our approach outperforms other state-of-the-art methodologies\nsuccessfully ensuring high-fidelity output while overcoming inherent drawbacks\nassociated with diffusion models in time-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, diffusion models have emerged as a superior alternative to\ngenerative adversarial networks (GANs) for high-fidelity image generation, with\nwide applications in text-to-image generation, image-to-image translation, and\nsuper-resolution. However, their real-time feasibility is hindered by slow\ntraining and inference speeds. This study addresses this challenge by proposing\na wavelet-based conditional Diffusion GAN scheme for Single-Image\nSuper-Resolution (SISR). Our approach utilizes the diffusion GAN paradigm to\nreduce the timesteps required by the reverse diffusion process and the Discrete\nWavelet Transform (DWT) to achieve dimensionality reduction, decreasing\ntraining and inference times significantly. The results of an experimental\nvalidation on the CelebA-HQ dataset confirm the effectiveness of our proposed\nscheme. Our approach outperforms other state-of-the-art methodologies\nsuccessfully ensuring high-fidelity output while overcoming inherent drawbacks\nassociated with diffusion models in time-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Lorenzo Aloisi"
                    },
                    {
                        "name": "Luigi Sigillo"
                    },
                    {
                        "name": "Aurelio Uncini"
                    },
                    {
                        "name": "Danilo Comminiello"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Comminiello"
                },
                "author": "Danilo Comminiello",
                "arxiv_comment": "The paper has been accepted at Italian Workshop on Neural Networks\n  (WIRN) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17957v1",
                "updated": "2024-10-23T15:27:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    27,
                    37,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:27:37Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    27,
                    37,
                    2,
                    297,
                    0
                ],
                "title": "MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCUBERT: Memory-Efficient BERT Inference on Commodity Microcontrollers"
                },
                "summary": "In this paper, we propose MCUBERT to enable language models like BERT on tiny\nmicrocontroller units (MCUs) through network and scheduling co-optimization. We\nobserve the embedding table contributes to the major storage bottleneck for\ntiny BERT models. Hence, at the network level, we propose an MCU-aware\ntwo-stage neural architecture search algorithm based on clustered low-rank\napproximation for embedding compression. To reduce the inference memory\nrequirements, we further propose a novel fine-grained MCU-friendly scheduling\nstrategy. Through careful computation tiling and re-ordering as well as kernel\ndesign, we drastically increase the input sequence lengths supported on MCUs\nwithout any latency or accuracy penalty. MCUBERT reduces the parameter size of\nBERT-tiny and BERT-mini by 5.7$\\times$ and 3.0$\\times$ and the execution memory\nby 3.5$\\times$ and 4.3$\\times$, respectively. MCUBERT also achieves 1.5$\\times$\nlatency reduction. For the first time, MCUBERT enables lightweight BERT models\non commodity MCUs and processing more than 512 tokens with less than 256KB of\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose MCUBERT to enable language models like BERT on tiny\nmicrocontroller units (MCUs) through network and scheduling co-optimization. We\nobserve the embedding table contributes to the major storage bottleneck for\ntiny BERT models. Hence, at the network level, we propose an MCU-aware\ntwo-stage neural architecture search algorithm based on clustered low-rank\napproximation for embedding compression. To reduce the inference memory\nrequirements, we further propose a novel fine-grained MCU-friendly scheduling\nstrategy. Through careful computation tiling and re-ordering as well as kernel\ndesign, we drastically increase the input sequence lengths supported on MCUs\nwithout any latency or accuracy penalty. MCUBERT reduces the parameter size of\nBERT-tiny and BERT-mini by 5.7$\\times$ and 3.0$\\times$ and the execution memory\nby 3.5$\\times$ and 4.3$\\times$, respectively. MCUBERT also achieves 1.5$\\times$\nlatency reduction. For the first time, MCUBERT enables lightweight BERT models\non commodity MCUs and processing more than 512 tokens with less than 256KB of\nmemory."
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Renze Chen"
                    },
                    {
                        "name": "Taiqiang Wu"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676747",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676747",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.17957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICCAD 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17952v1",
                "updated": "2024-10-23T15:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    16,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    16,
                    2,
                    297,
                    0
                ],
                "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large\n  Language Models to Specialized Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large\n  Language Models to Specialized Domains"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the question-answering (QA)\nabilities of large language models (LLMs) by integrating external knowledge.\nHowever, adapting general-purpose RAG systems to specialized fields such as\nscience and medicine poses unique challenges due to distribution shifts and\nlimited access to domain-specific data. To tackle this, we propose SimRAG, a\nself-training approach that equips the LLM with joint capabilities of question\nanswering and question generation for domain adaptation. Our method first\nfine-tunes the LLM on instruction-following, question-answering, and\nsearch-related data. Then, it prompts the same LLM to generate diverse\ndomain-relevant questions from unlabeled corpora, with an additional filtering\nstrategy to retain high-quality synthetic examples. By leveraging these\nsynthetic examples, the LLM can improve their performance on domain-specific\nRAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three\ndomains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the question-answering (QA)\nabilities of large language models (LLMs) by integrating external knowledge.\nHowever, adapting general-purpose RAG systems to specialized fields such as\nscience and medicine poses unique challenges due to distribution shifts and\nlimited access to domain-specific data. To tackle this, we propose SimRAG, a\nself-training approach that equips the LLM with joint capabilities of question\nanswering and question generation for domain adaptation. Our method first\nfine-tunes the LLM on instruction-following, question-answering, and\nsearch-related data. Then, it prompts the same LLM to generate diverse\ndomain-relevant questions from unlabeled corpora, with an additional filtering\nstrategy to retain high-quality synthetic examples. By leveraging these\nsynthetic examples, the LLM can improve their performance on domain-specific\nRAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three\ndomains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%."
                },
                "authors": [
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Sreyashi Nag"
                    },
                    {
                        "name": "Zhenwei Dai"
                    },
                    {
                        "name": "Yaochen Xie"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Chen Luo"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Joyce C. Ho"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Qi He"
                    }
                ],
                "author_detail": {
                    "name": "Qi He"
                },
                "author": "Qi He",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17950v1",
                "updated": "2024-10-23T15:23:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    23,
                    23,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:23:23Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    23,
                    23,
                    2,
                    297,
                    0
                ],
                "title": "Benchmarking Floworks against OpenAI & Anthropic: A Novel Framework for\n  Enhanced LLM Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Floworks against OpenAI & Anthropic: A Novel Framework for\n  Enhanced LLM Function Calling"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in various\ndomains, yet their economic impact has been limited by challenges in tool use\nand function calling. This paper introduces ThorV2, a novel architecture that\nsignificantly enhances LLMs' function calling abilities. We develop a\ncomprehensive benchmark focused on HubSpot CRM operations to evaluate ThorV2\nagainst leading models from OpenAI and Anthropic. Our results demonstrate that\nThorV2 outperforms existing models in accuracy, reliability, latency, and cost\nefficiency for both single and multi-API calling tasks. We also show that\nThorV2 is far more reliable and scales better to multistep tasks compared to\ntraditional models. Our work offers the tantalizing possibility of more\naccurate function-calling compared to today's best-performing models using\nsignificantly smaller LLMs. These advancements have significant implications\nfor the development of more capable AI assistants and the broader application\nof LLMs in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in various\ndomains, yet their economic impact has been limited by challenges in tool use\nand function calling. This paper introduces ThorV2, a novel architecture that\nsignificantly enhances LLMs' function calling abilities. We develop a\ncomprehensive benchmark focused on HubSpot CRM operations to evaluate ThorV2\nagainst leading models from OpenAI and Anthropic. Our results demonstrate that\nThorV2 outperforms existing models in accuracy, reliability, latency, and cost\nefficiency for both single and multi-API calling tasks. We also show that\nThorV2 is far more reliable and scales better to multistep tasks compared to\ntraditional models. Our work offers the tantalizing possibility of more\naccurate function-calling compared to today's best-performing models using\nsignificantly smaller LLMs. These advancements have significant implications\nfor the development of more capable AI assistants and the broader application\nof LLMs in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Nirav Bhan"
                    },
                    {
                        "name": "Shival Gupta"
                    },
                    {
                        "name": "Sai Manaswini"
                    },
                    {
                        "name": "Ritik Baba"
                    },
                    {
                        "name": "Narun Yadav"
                    },
                    {
                        "name": "Hillori Desai"
                    },
                    {
                        "name": "Yash Choudhary"
                    },
                    {
                        "name": "Aman Pawar"
                    },
                    {
                        "name": "Sarthak Shrivastava"
                    },
                    {
                        "name": "Sudipta Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Sudipta Biswas"
                },
                "author": "Sudipta Biswas",
                "arxiv_comment": "15 pages for main paper, 21 pages in total including references and\n  appendix, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12295v2",
                "updated": "2024-10-23T15:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    23,
                    0,
                    2,
                    297,
                    0
                ],
                "published": "2024-06-18T05:59:28Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    5,
                    59,
                    28,
                    1,
                    170,
                    0
                ],
                "title": "Fast and Slow Generating: An Empirical Study on Large and Small Language\n  Models Collaborative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Slow Generating: An Empirical Study on Large and Small Language\n  Models Collaborative Decoding"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive capabilities across various\napplications but encounter substantial challenges such as high inference\nlatency, considerable training costs, and the generation of hallucinations.\nCollaborative decoding between large and small language models (SLMs) presents\na promising strategy to mitigate these issues through methods including\nspeculative decoding, contrastive decoding, and emulator or proxy fine-tuning.\nHowever, the specifics of such collaborations, particularly from a unified\nperspective, remain largely unexplored. Inspired by dual-process cognitive\ntheory, we propose a unified framework in this paper, termed Fast and Slow\nGenerating (FS-GEN). Within this framework, LLMs (sometimes along with SLMs)\nare categorized as System 2 (slow and deliberate), while independent SLMs are\ndesignated as System 1 (fast and intuitive). We provide a comprehensive\nanalysis of these collaborative methodologies, elucidating their common\nproperties and shedding light on the differential knowledge capabilities of\nSystem 2 versus System 1 through the FS-GEN framework. Our findings indicate\nthat only a small proportion of collaborative interactions (approximately less\nthan 20\\% in most instances) are necessary across various methods. These\ninteractions between System 1 and System 2 conform to a scaling law related to\nthe parameter ratios, enabling predictable collaboration. Furthermore, we\nexplore the specific conditions under which collaboration proves most\neffective, particularly from an uncertainty perspective, offering novel\ninsights that may guide future optimization efforts. Our research underscores\nthat the fundamental distinction between System 1 and System 2 lies in the\nuncertainty of next token predictions, where interventions by System 2 are\ncrucial to support System 1. Code for Reproduction:\nhttps://github.com/TsinghuaC3I/FS-GEN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive capabilities across various\napplications but encounter substantial challenges such as high inference\nlatency, considerable training costs, and the generation of hallucinations.\nCollaborative decoding between large and small language models (SLMs) presents\na promising strategy to mitigate these issues through methods including\nspeculative decoding, contrastive decoding, and emulator or proxy fine-tuning.\nHowever, the specifics of such collaborations, particularly from a unified\nperspective, remain largely unexplored. Inspired by dual-process cognitive\ntheory, we propose a unified framework in this paper, termed Fast and Slow\nGenerating (FS-GEN). Within this framework, LLMs (sometimes along with SLMs)\nare categorized as System 2 (slow and deliberate), while independent SLMs are\ndesignated as System 1 (fast and intuitive). We provide a comprehensive\nanalysis of these collaborative methodologies, elucidating their common\nproperties and shedding light on the differential knowledge capabilities of\nSystem 2 versus System 1 through the FS-GEN framework. Our findings indicate\nthat only a small proportion of collaborative interactions (approximately less\nthan 20\\% in most instances) are necessary across various methods. These\ninteractions between System 1 and System 2 conform to a scaling law related to\nthe parameter ratios, enabling predictable collaboration. Furthermore, we\nexplore the specific conditions under which collaboration proves most\neffective, particularly from an uncertainty perspective, offering novel\ninsights that may guide future optimization efforts. Our research underscores\nthat the fundamental distinction between System 1 and System 2 lies in the\nuncertainty of next token predictions, where interventions by System 2 are\ncrucial to support System 1. Code for Reproduction:\nhttps://github.com/TsinghuaC3I/FS-GEN"
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "update figures and results on Pythia Series",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01416v2",
                "updated": "2024-10-23T15:10:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    10,
                    17,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-02T10:55:15Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    10,
                    55,
                    15,
                    2,
                    276,
                    0
                ],
                "title": "Two-level atom witness of thermalization of multimode optical fibers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-level atom witness of thermalization of multimode optical fibers"
                },
                "summary": "In the present project, we study the dynamics of the two-level system coupled\nwith the multimode optical system. In particular, we considered a square\nlattice of optical fibers. We aimed to answer whether we can infer information\nabout the thermalization of optical modes through the thermalization of\ntwo-level atoms. After averaging over the set of modes, the dynamic of the\ntwo-level system is free of quantum revivals, and that is the signature of\nthermalization. We showed that the temperature of the two-level system\nincreases with the temperature of optical modes and mean photon number. In the\nhigh-temperature limit of optical modes, the temperature of the level system\ntends to be infinity, and level populations are equal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the present project, we study the dynamics of the two-level system coupled\nwith the multimode optical system. In particular, we considered a square\nlattice of optical fibers. We aimed to answer whether we can infer information\nabout the thermalization of optical modes through the thermalization of\ntwo-level atoms. After averaging over the set of modes, the dynamic of the\ntwo-level system is free of quantum revivals, and that is the signature of\nthermalization. We showed that the temperature of the two-level system\nincreases with the temperature of optical modes and mean photon number. In the\nhigh-temperature limit of optical modes, the temperature of the level system\ntends to be infinity, and level populations are equal."
                },
                "authors": [
                    {
                        "name": "M. Wanic"
                    },
                    {
                        "name": "R. Khomeriki"
                    },
                    {
                        "name": "S. Stagraczynski"
                    },
                    {
                        "name": "M. I. Katsnelson"
                    },
                    {
                        "name": "Z. Toklikishvili"
                    },
                    {
                        "name": "L. Chotorlishvili"
                    }
                ],
                "author_detail": {
                    "name": "L. Chotorlishvili"
                },
                "author": "L. Chotorlishvili",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04957v3",
                "updated": "2024-10-23T15:08:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    8,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-07T15:40:22Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    15,
                    40,
                    22,
                    2,
                    38,
                    0
                ],
                "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfidencing LLMs from the Grouping Loss Perspective"
                },
                "summary": "Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to\ngenerating hallucinated answers in a confident tone. While efforts to elicit\nand calibrate confidence scores have proven useful, recent findings show that\ncontrolling uncertainty must go beyond calibration: predicted scores may\ndeviate significantly from the actual posterior probabilities due to the impact\nof grouping loss. In this work, we construct a new evaluation dataset derived\nfrom a knowledge base to assess confidence scores given to answers of Mistral\nand LLaMA. Experiments show that they tend to be overconfident. Further, we\nshow that they are more overconfident on some answers than others, \\emph{eg}\ndepending on the nationality of the person in the query. In\nuncertainty-quantification theory, this is grouping loss. To address this, we\npropose a solution to reconfidence LLMs, canceling not only calibration but\nalso grouping loss. The LLMs, after the reconfidencing process, indicate\nimproved confidence alignment with the accuracy of their responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to\ngenerating hallucinated answers in a confident tone. While efforts to elicit\nand calibrate confidence scores have proven useful, recent findings show that\ncontrolling uncertainty must go beyond calibration: predicted scores may\ndeviate significantly from the actual posterior probabilities due to the impact\nof grouping loss. In this work, we construct a new evaluation dataset derived\nfrom a knowledge base to assess confidence scores given to answers of Mistral\nand LLaMA. Experiments show that they tend to be overconfident. Further, we\nshow that they are more overconfident on some answers than others, \\emph{eg}\ndepending on the nationality of the person in the query. In\nuncertainty-quantification theory, this is grouping loss. To address this, we\npropose a solution to reconfidence LLMs, canceling not only calibration but\nalso grouping loss. The LLMs, after the reconfidencing process, indicate\nimproved confidence alignment with the accuracy of their responses."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Alexandre Perez-Lebel"
                    },
                    {
                        "name": "Fabian M. Suchanek"
                    },
                    {
                        "name": "Gal Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gal Varoquaux"
                },
                "author": "Gal Varoquaux",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01622v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01622v4",
                "updated": "2024-10-23T15:02:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    2,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-02T18:39:51Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    18,
                    39,
                    51,
                    4,
                    33,
                    0
                ],
                "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents"
                },
                "summary": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents."
                },
                "authors": [
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Tinghui Zhu"
                    },
                    {
                        "name": "Renze Lou"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "arxiv_comment": "ICML 2024 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01622v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01622v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17935v1",
                "updated": "2024-10-23T15:00:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    0,
                    30,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:00:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    0,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Semi-Implicit Functional Gradient Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Implicit Functional Gradient Flow"
                },
                "summary": "Particle-based variational inference methods (ParVIs) use non-parametric\nvariational families represented by particles to approximate the target\ndistribution according to the kernelized Wasserstein gradient flow for the\nKullback-Leibler (KL) divergence. Recent works introduce functional gradient\nflows to substitute the kernel for better flexibility. However, the\ndeterministic updating mechanism may suffer from limited exploration and\nrequire expensive repetitive runs for new samples. In this paper, we propose\nSemi-Implicit Functional Gradient flow (SIFG), a functional gradient ParVI\nmethod that uses perturbed particles as the approximation family. The\ncorresponding functional gradient flow, which can be estimated via denoising\nscore matching, exhibits strong theoretical convergence guarantee. We also\npresent an adaptive version of our method to automatically choose the suitable\nnoise magnitude. Extensive experiments demonstrate the effectiveness and\nefficiency of the proposed framework on both simulated and real data problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Particle-based variational inference methods (ParVIs) use non-parametric\nvariational families represented by particles to approximate the target\ndistribution according to the kernelized Wasserstein gradient flow for the\nKullback-Leibler (KL) divergence. Recent works introduce functional gradient\nflows to substitute the kernel for better flexibility. However, the\ndeterministic updating mechanism may suffer from limited exploration and\nrequire expensive repetitive runs for new samples. In this paper, we propose\nSemi-Implicit Functional Gradient flow (SIFG), a functional gradient ParVI\nmethod that uses perturbed particles as the approximation family. The\ncorresponding functional gradient flow, which can be estimated via denoising\nscore matching, exhibits strong theoretical convergence guarantee. We also\npresent an adaptive version of our method to automatically choose the suitable\nnoise magnitude. Extensive experiments demonstrate the effectiveness and\nefficiency of the proposed framework on both simulated and real data problems."
                },
                "authors": [
                    {
                        "name": "Shiyue Zhang"
                    },
                    {
                        "name": "Ziheng Cheng"
                    },
                    {
                        "name": "Cheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Zhang"
                },
                "author": "Cheng Zhang",
                "arxiv_comment": "31 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17931v1",
                "updated": "2024-10-23T14:54:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    54,
                    39,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:54:39Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    54,
                    39,
                    2,
                    297,
                    0
                ],
                "title": "ARAS: An Adaptive Low-Cost ReRAM-Based Accelerator for DNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARAS: An Adaptive Low-Cost ReRAM-Based Accelerator for DNNs"
                },
                "summary": "Processing Using Memory (PUM) accelerators have the potential to perform Deep\nNeural Network (DNN) inference by using arrays of memory cells as computation\nengines. Among various memory technologies, ReRAM crossbars show promising\nperformance in computing dot-product operations in the analog domain.\nNevertheless, the expensive writing procedure of ReRAM cells has led\nresearchers to design accelerators whose crossbars have enough capacity to\nstore the full DNN. Given the tremendous and continuous increase in DNN model\nsizes, this approach is unfeasible for some networks, or inefficient due to the\nhuge hardware requirements. Those accelerators lack the flexibility to adapt to\nany given DNN model, facing an challenge.\n  To address this issue we introduce ARAS, a cost-effective ReRAM-based\naccelerator that employs a smart scheduler to adapt different DNNs to the\nresource-limited hardware. ARAS also overlaps the computation of a layer with\nthe weight writing of several layers to mitigate the high writing latency of\nReRAM. Furthermore, ARAS introduces three optimizations aimed at reducing the\nenergy overheads of writing in ReRAM. Our key optimization capitalizes on the\nobservation that DNN weights can be re-encoded to augment their similarity\nbetween layers, increasing the amount of bitwise values that are equal or\nsimilar when overwriting ReRAM cells and, hence, reducing the amount of energy\nrequired to update the cells. Overall, ARAS greatly reduces the ReRAM writing\nactivity. We evaluate ARAS on a popular set of DNNs. ARAS provides up to 2.2x\nspeedup and 45% energy savings over a baseline PUM accelerator without any\noptimization. Compared to a TPU-like accelerator, ARAS provides up to 1.5x\nspeedup and 61% energy savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing Using Memory (PUM) accelerators have the potential to perform Deep\nNeural Network (DNN) inference by using arrays of memory cells as computation\nengines. Among various memory technologies, ReRAM crossbars show promising\nperformance in computing dot-product operations in the analog domain.\nNevertheless, the expensive writing procedure of ReRAM cells has led\nresearchers to design accelerators whose crossbars have enough capacity to\nstore the full DNN. Given the tremendous and continuous increase in DNN model\nsizes, this approach is unfeasible for some networks, or inefficient due to the\nhuge hardware requirements. Those accelerators lack the flexibility to adapt to\nany given DNN model, facing an challenge.\n  To address this issue we introduce ARAS, a cost-effective ReRAM-based\naccelerator that employs a smart scheduler to adapt different DNNs to the\nresource-limited hardware. ARAS also overlaps the computation of a layer with\nthe weight writing of several layers to mitigate the high writing latency of\nReRAM. Furthermore, ARAS introduces three optimizations aimed at reducing the\nenergy overheads of writing in ReRAM. Our key optimization capitalizes on the\nobservation that DNN weights can be re-encoded to augment their similarity\nbetween layers, increasing the amount of bitwise values that are equal or\nsimilar when overwriting ReRAM cells and, hence, reducing the amount of energy\nrequired to update the cells. Overall, ARAS greatly reduces the ReRAM writing\nactivity. We evaluate ARAS on a popular set of DNNs. ARAS provides up to 2.2x\nspeedup and 45% energy savings over a baseline PUM accelerator without any\noptimization. Compared to a TPU-like accelerator, ARAS provides up to 1.5x\nspeedup and 61% energy savings."
                },
                "authors": [
                    {
                        "name": "Mohammad Sabri"
                    },
                    {
                        "name": "Marc Riera"
                    },
                    {
                        "name": "Antonio Gonzlez"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Gonzlez"
                },
                "author": "Antonio Gonzlez",
                "arxiv_comment": "13 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04033v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04033v3",
                "updated": "2024-10-23T14:50:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    50,
                    51,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "On provable privacy vulnerabilities of graph representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On provable privacy vulnerabilities of graph representations"
                },
                "summary": "Graph representation learning (GRL) is critical for extracting insights from\ncomplex network structures, but it also raises security concerns due to\npotential privacy vulnerabilities in these representations. This paper\ninvestigates the structural vulnerabilities in graph neural models where\nsensitive topological information can be inferred through edge reconstruction\nattacks. Our research primarily addresses the theoretical underpinnings of\nsimilarity-based edge reconstruction attacks (SERA), furnishing a\nnon-asymptotic analysis of their reconstruction capacities. Moreover, we\npresent empirical corroboration indicating that such attacks can perfectly\nreconstruct sparse graphs as graph size increases. Conversely, we establish\nthat sparsity is a critical factor for SERA's effectiveness, as demonstrated\nthrough analysis and experiments on (dense) stochastic block models. Finally,\nwe explore the resilience of private graph representations produced via noisy\naggregation (NAG) mechanism against SERA. Through theoretical analysis and\nempirical assessments, we affirm the mitigation of SERA using NAG . In\nparallel, we also empirically delineate instances wherein SERA demonstrates\nboth efficacy and deficiency in its capacity to function as an instrument for\nelucidating the trade-off between privacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph representation learning (GRL) is critical for extracting insights from\ncomplex network structures, but it also raises security concerns due to\npotential privacy vulnerabilities in these representations. This paper\ninvestigates the structural vulnerabilities in graph neural models where\nsensitive topological information can be inferred through edge reconstruction\nattacks. Our research primarily addresses the theoretical underpinnings of\nsimilarity-based edge reconstruction attacks (SERA), furnishing a\nnon-asymptotic analysis of their reconstruction capacities. Moreover, we\npresent empirical corroboration indicating that such attacks can perfectly\nreconstruct sparse graphs as graph size increases. Conversely, we establish\nthat sparsity is a critical factor for SERA's effectiveness, as demonstrated\nthrough analysis and experiments on (dense) stochastic block models. Finally,\nwe explore the resilience of private graph representations produced via noisy\naggregation (NAG) mechanism against SERA. Through theoretical analysis and\nempirical assessments, we affirm the mitigation of SERA using NAG . In\nparallel, we also empirically delineate instances wherein SERA demonstrates\nboth efficacy and deficiency in its capacity to function as an instrument for\nelucidating the trade-off between privacy and utility."
                },
                "authors": [
                    {
                        "name": "Ruofan Wu"
                    },
                    {
                        "name": "Guanhua Fang"
                    },
                    {
                        "name": "Qiying Pan"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Tengfei Liu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqiang Wang"
                },
                "author": "Weiqiang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04033v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04033v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.05876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.05876v3",
                "updated": "2024-10-23T14:48:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    48,
                    20,
                    2,
                    297,
                    0
                ],
                "published": "2023-11-10T05:24:04Z",
                "published_parsed": [
                    2023,
                    11,
                    10,
                    5,
                    24,
                    4,
                    4,
                    314,
                    0
                ],
                "title": "Trends in Integration of Knowledge and Large Language Models: A Survey\n  and Taxonomy of Methods, Benchmarks, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trends in Integration of Knowledge and Large Language Models: A Survey\n  and Taxonomy of Methods, Benchmarks, and Applications"
                },
                "summary": "Large language models (LLMs) exhibit superior performance on various natural\nlanguage tasks, but they are susceptible to issues stemming from outdated data\nand domain-specific limitations. In order to address these challenges,\nresearchers have pursued two primary strategies, knowledge editing and\nretrieval augmentation, to enhance LLMs by incorporating external information\nfrom different aspects. Nevertheless, there is still a notable absence of a\ncomprehensive survey. In this paper, we propose a review to discuss the trends\nin integration of knowledge and large language models, including taxonomy of\nmethods, benchmarks, and applications. In addition, we conduct an in-depth\nanalysis of different methods and point out potential research directions in\nthe future. We hope this survey offers the community quick access and a\ncomprehensive overview of this research area, with the intention of inspiring\nfuture research endeavors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit superior performance on various natural\nlanguage tasks, but they are susceptible to issues stemming from outdated data\nand domain-specific limitations. In order to address these challenges,\nresearchers have pursued two primary strategies, knowledge editing and\nretrieval augmentation, to enhance LLMs by incorporating external information\nfrom different aspects. Nevertheless, there is still a notable absence of a\ncomprehensive survey. In this paper, we propose a review to discuss the trends\nin integration of knowledge and large language models, including taxonomy of\nmethods, benchmarks, and applications. In addition, we conduct an in-depth\nanalysis of different methods and point out potential research directions in\nthe future. We hope this survey offers the community quick access and a\ncomprehensive overview of this research area, with the intention of inspiring\nfuture research endeavors."
                },
                "authors": [
                    {
                        "name": "Zhangyin Feng"
                    },
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Weijiang Yu"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Qianglong Chen"
                    },
                    {
                        "name": "Weihua Peng"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting liu"
                },
                "author": "Ting liu",
                "arxiv_comment": "Work in progress; 22 pages. This work has been submitted to the IEEE\n  for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.05876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.05876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17922v1",
                "updated": "2024-10-23T14:40:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    40,
                    37,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:40:37Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    40,
                    37,
                    2,
                    297,
                    0
                ],
                "title": "Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced\n  Defense in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced\n  Defense in Large Language Models"
                },
                "summary": "With the extensive deployment of Large Language Models (LLMs), ensuring their\nsafety has become increasingly critical. However, existing defense methods\noften struggle with two key issues: (i) inadequate defense capabilities,\nparticularly in domain-specific scenarios like chemistry, where a lack of\nspecialized knowledge can lead to the generation of harmful responses to\nmalicious queries. (ii) over-defensiveness, which compromises the general\nutility and responsiveness of LLMs. To mitigate these issues, we introduce a\nmulti-agents-based defense framework, Guide for Defense (G4D), which leverages\naccurate external information to provide an unbiased summary of user intentions\nand analytically grounded safety response guidance. Extensive experiments on\npopular jailbreak attacks and benign datasets show that our G4D can enhance\nLLM's robustness against jailbreak attacks on general and domain-specific\nscenarios without compromising the model's general functionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the extensive deployment of Large Language Models (LLMs), ensuring their\nsafety has become increasingly critical. However, existing defense methods\noften struggle with two key issues: (i) inadequate defense capabilities,\nparticularly in domain-specific scenarios like chemistry, where a lack of\nspecialized knowledge can lead to the generation of harmful responses to\nmalicious queries. (ii) over-defensiveness, which compromises the general\nutility and responsiveness of LLMs. To mitigate these issues, we introduce a\nmulti-agents-based defense framework, Guide for Defense (G4D), which leverages\naccurate external information to provide an unbiased summary of user intentions\nand analytically grounded safety response guidance. Extensive experiments on\npopular jailbreak attacks and benign datasets show that our G4D can enhance\nLLM's robustness against jailbreak attacks on general and domain-specific\nscenarios without compromising the model's general functionality."
                },
                "authors": [
                    {
                        "name": "He Cao"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Zijing Liu"
                    },
                    {
                        "name": "Bing Feng"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Yu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu Li"
                },
                "author": "Yu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01119v2",
                "updated": "2024-10-23T14:37:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    37,
                    50,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-02T09:00:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    9,
                    0,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer"
                },
                "summary": "Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks."
                },
                "authors": [
                    {
                        "name": "Robert Belanec"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Maria Bielikova"
                    }
                ],
                "author_detail": {
                    "name": "Maria Bielikova"
                },
                "author": "Maria Bielikova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17916v1",
                "updated": "2024-10-23T14:34:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    34,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:34:15Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    34,
                    15,
                    2,
                    297,
                    0
                ],
                "title": "Reconciling PTA and JWST and preparing for LISA with \\texttt{POMPOCO}: a\n  Parametrisation Of the Massive black hole POpulation for Comparison to\n  Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconciling PTA and JWST and preparing for LISA with \\texttt{POMPOCO}: a\n  Parametrisation Of the Massive black hole POpulation for Comparison to\n  Observations"
                },
                "summary": "We develop a parametrised model to describe the formation and evolution of\nmassive black holes, designed for comparisons with both electromagnetic and\ngravitational wave observations. Using an extended Press-Schechter formalism,\nwe generate dark matter halo merger trees. We then seed and evolve massive\nblack holes through parameterised prescriptions. This approach, which avoids\nsolving differential equations, is computationally efficient, enabling us to\nanalyse observational data and infer the parameters of our model in a fully\nBayesian framework. We find that observations of the black hole luminosity\nfunction are compatible with the nHz gravitational wave signal (likely)\nmeasured by PTAs, provided we allow for an increased luminosity function at\nhigh redshift ($4-7$), as recently suggested by JWST observations. Our model\ncan simultaneously reproduce the bulk of the $M_*-M_{\\rm BH}$ relation at\n$z-0$, as well as its outliers, something cosmological simulations struggle to\ndo. The inferred model parameters are consistent with expectations from\nobservations and more complex simulations: They favour heavier black hole seeds\nand short delays between halo and black hole mergers, while requiring\nsupper-Edington accretion episodes lasting a few tens of million years, which\nin our model are linked to galaxy mergers. We find accretion to be suppressed\nin the most massive black holes below $z\\simeq 2.5$, consistently with the\nanti-hierarchical growth hypothesis. Finally, our predictions for LISA,\nalthough fairly broad, are in agreement with previous models. Our model offers\na new perspective on the apparent tensions between the black hole luminosity\nfunction and the latest JWST and PTA results. Its flexibility makes it ideal to\nfully exploit the potential of future gravitational wave observations of\nmassive black hole binaries with LISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a parametrised model to describe the formation and evolution of\nmassive black holes, designed for comparisons with both electromagnetic and\ngravitational wave observations. Using an extended Press-Schechter formalism,\nwe generate dark matter halo merger trees. We then seed and evolve massive\nblack holes through parameterised prescriptions. This approach, which avoids\nsolving differential equations, is computationally efficient, enabling us to\nanalyse observational data and infer the parameters of our model in a fully\nBayesian framework. We find that observations of the black hole luminosity\nfunction are compatible with the nHz gravitational wave signal (likely)\nmeasured by PTAs, provided we allow for an increased luminosity function at\nhigh redshift ($4-7$), as recently suggested by JWST observations. Our model\ncan simultaneously reproduce the bulk of the $M_*-M_{\\rm BH}$ relation at\n$z-0$, as well as its outliers, something cosmological simulations struggle to\ndo. The inferred model parameters are consistent with expectations from\nobservations and more complex simulations: They favour heavier black hole seeds\nand short delays between halo and black hole mergers, while requiring\nsupper-Edington accretion episodes lasting a few tens of million years, which\nin our model are linked to galaxy mergers. We find accretion to be suppressed\nin the most massive black holes below $z\\simeq 2.5$, consistently with the\nanti-hierarchical growth hypothesis. Finally, our predictions for LISA,\nalthough fairly broad, are in agreement with previous models. Our model offers\na new perspective on the apparent tensions between the black hole luminosity\nfunction and the latest JWST and PTA results. Its flexibility makes it ideal to\nfully exploit the potential of future gravitational wave observations of\nmassive black hole binaries with LISA."
                },
                "authors": [
                    {
                        "name": "A. Toubiana"
                    },
                    {
                        "name": "L. Sberna"
                    },
                    {
                        "name": "M. Volonteri"
                    },
                    {
                        "name": "E. Barausse"
                    },
                    {
                        "name": "S. Babak"
                    },
                    {
                        "name": "R. Enficiaud"
                    },
                    {
                        "name": "D. Izquierdo Villalba"
                    },
                    {
                        "name": "J. R. Gair"
                    },
                    {
                        "name": "J. E. Greene"
                    },
                    {
                        "name": "H. Quelquejay Leclere"
                    }
                ],
                "author_detail": {
                    "name": "H. Quelquejay Leclere"
                },
                "author": "H. Quelquejay Leclere",
                "arxiv_comment": "13 pages, 15 with appendix. 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.00279v3",
                "updated": "2024-10-23T14:33:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    33,
                    44,
                    2,
                    297,
                    0
                ],
                "published": "2023-07-01T09:18:24Z",
                "published_parsed": [
                    2023,
                    7,
                    1,
                    9,
                    18,
                    24,
                    5,
                    182,
                    0
                ],
                "title": "Let Me Teach You: Pedagogical Foundations of Feedback for Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Me Teach You: Pedagogical Foundations of Feedback for Language\n  Models"
                },
                "summary": "Natural Language Feedback (NLF) is an increasingly popular mechanism for\naligning Large Language Models (LLMs) to human preferences. Despite the\ndiversity of the information it can convey, NLF methods are often hand-designed\nand arbitrary, with little systematic grounding. At the same time, research in\nlearning sciences has long established several effective feedback models. In\nthis opinion piece, we compile ideas from pedagogy to introduce FELT, a\nfeedback framework for LLMs that outlines various characteristics of the\nfeedback space, and a feedback content taxonomy based on these variables,\nproviding a general mapping of the feedback space. In addition to streamlining\nNLF designs, FELT also brings out new, unexplored directions for research in\nNLF. We make our taxonomy available to the community, providing guides and\nexamples for mapping our categorizations to future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Feedback (NLF) is an increasingly popular mechanism for\naligning Large Language Models (LLMs) to human preferences. Despite the\ndiversity of the information it can convey, NLF methods are often hand-designed\nand arbitrary, with little systematic grounding. At the same time, research in\nlearning sciences has long established several effective feedback models. In\nthis opinion piece, we compile ideas from pedagogy to introduce FELT, a\nfeedback framework for LLMs that outlines various characteristics of the\nfeedback space, and a feedback content taxonomy based on these variables,\nproviding a general mapping of the feedback space. In addition to streamlining\nNLF designs, FELT also brings out new, unexplored directions for research in\nNLF. We make our taxonomy available to the community, providing guides and\nexamples for mapping our categorizations to future research."
                },
                "authors": [
                    {
                        "name": "Beatriz Borges"
                    },
                    {
                        "name": "Niket Tandon"
                    },
                    {
                        "name": "Tanja Kser"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "arxiv_comment": "EMNLP 2024; 9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08511v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08511v3",
                "updated": "2024-10-23T14:21:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    21,
                    40,
                    2,
                    297,
                    0
                ],
                "published": "2024-03-13T13:16:26Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    13,
                    16,
                    26,
                    2,
                    73,
                    0
                ],
                "title": "A Multimodal Fusion Network For Student Emotion Recognition Based on\n  Transformer and Tensor Product",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multimodal Fusion Network For Student Emotion Recognition Based on\n  Transformer and Tensor Product"
                },
                "summary": "This paper introduces a new multi-modal model based on the Transformer\narchitecture and tensor product fusion strategy, combining BERT's text vectors\nand ViT's image vectors to classify students' psychological conditions, with an\naccuracy of 93.65%. The purpose of the study is to accurately analyze the\nmental health status of students from various data sources. This paper\ndiscusses modal fusion methods, including early, late and intermediate fusion,\nto overcome the challenges of integrating multi-modal information. Ablation\nstudies compare the performance of different models and fusion techniques,\nshowing that the proposed model outperforms existing methods such as CLIP and\nViLBERT in terms of accuracy and inference speed. Conclusions indicate that\nwhile this model has significant advantages in emotion recognition, its\npotential to incorporate other data modalities provides areas for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a new multi-modal model based on the Transformer\narchitecture and tensor product fusion strategy, combining BERT's text vectors\nand ViT's image vectors to classify students' psychological conditions, with an\naccuracy of 93.65%. The purpose of the study is to accurately analyze the\nmental health status of students from various data sources. This paper\ndiscusses modal fusion methods, including early, late and intermediate fusion,\nto overcome the challenges of integrating multi-modal information. Ablation\nstudies compare the performance of different models and fusion techniques,\nshowing that the proposed model outperforms existing methods such as CLIP and\nViLBERT in terms of accuracy and inference speed. Conclusions indicate that\nwhile this model has significant advantages in emotion recognition, its\npotential to incorporate other data modalities provides areas for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Ao Xiang"
                    },
                    {
                        "name": "Zongqing Qi"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Qin Yang"
                    },
                    {
                        "name": "Danqing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Danqing Ma"
                },
                "author": "Danqing Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08511v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08511v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06927v2",
                "updated": "2024-10-23T14:01:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    1,
                    27,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-13T14:29:00Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    29,
                    0,
                    1,
                    226,
                    0
                ],
                "title": "Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class\n  Feature Compensator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class\n  Feature Compensator"
                },
                "summary": "Dataset distillation has emerged as a technique aiming to condense\ninformative features from large, natural datasets into a compact and synthetic\nform. While recent advancements have refined this technique, its performance is\nbottlenecked by the prevailing class-specific synthesis paradigm. Under this\nparadigm, synthetic data is optimized exclusively for a pre-assigned one-hot\nlabel, creating an implicit class barrier in feature condensation. This leads\nto inefficient utilization of the distillation budget and oversight of\ninter-class feature distributions, which ultimately limits the effectiveness\nand efficiency, as demonstrated in our analysis. To overcome these constraints,\nthis paper presents the Inter-class Feature Compensator (INFER), an innovative\ndistillation approach that transcends the class-specific data-label framework\nwidely utilized in current dataset distillation methods. Specifically, INFER\nleverages a Universal Feature Compensator (UFC) to enhance feature integration\nacross classes, enabling the generation of multiple additional synthetic\ninstances from a single UFC input. This significantly improves the efficiency\nof the distillation budget. Moreover, INFER enriches inter-class interactions\nduring the distillation, thereby enhancing the effectiveness and\ngeneralizability of the distilled data. By allowing for the linear\ninterpolation of labels similar to those in the original dataset, INFER\nmeticulously optimizes the synthetic data and dramatically reduces the size of\nsoft labels in the synthetic dataset to almost zero, establishing a new\nbenchmark for efficiency and effectiveness in dataset distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation has emerged as a technique aiming to condense\ninformative features from large, natural datasets into a compact and synthetic\nform. While recent advancements have refined this technique, its performance is\nbottlenecked by the prevailing class-specific synthesis paradigm. Under this\nparadigm, synthetic data is optimized exclusively for a pre-assigned one-hot\nlabel, creating an implicit class barrier in feature condensation. This leads\nto inefficient utilization of the distillation budget and oversight of\ninter-class feature distributions, which ultimately limits the effectiveness\nand efficiency, as demonstrated in our analysis. To overcome these constraints,\nthis paper presents the Inter-class Feature Compensator (INFER), an innovative\ndistillation approach that transcends the class-specific data-label framework\nwidely utilized in current dataset distillation methods. Specifically, INFER\nleverages a Universal Feature Compensator (UFC) to enhance feature integration\nacross classes, enabling the generation of multiple additional synthetic\ninstances from a single UFC input. This significantly improves the efficiency\nof the distillation budget. Moreover, INFER enriches inter-class interactions\nduring the distillation, thereby enhancing the effectiveness and\ngeneralizability of the distilled data. By allowing for the linear\ninterpolation of labels similar to those in the original dataset, INFER\nmeticulously optimizes the synthetic data and dramatically reduces the size of\nsoft labels in the synthetic dataset to almost zero, establishing a new\nbenchmark for efficiency and effectiveness in dataset distillation."
                },
                "authors": [
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Jiawei Du"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Joey Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Joey Tianyi Zhou"
                },
                "author": "Joey Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14703v2",
                "updated": "2024-10-23T14:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    1,
                    14,
                    2,
                    297,
                    0
                ],
                "published": "2024-06-20T19:50:56Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    19,
                    50,
                    56,
                    3,
                    172,
                    0
                ],
                "title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction."
                },
                "authors": [
                    {
                        "name": "Seungbeen Lee"
                    },
                    {
                        "name": "Seungwon Lim"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Giyeong Oh"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Beong-woo Kwak"
                    },
                    {
                        "name": "Yeonsoo Lee"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "Preprint; Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07799v2",
                "updated": "2024-10-23T14:00:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    0,
                    40,
                    2,
                    297,
                    0
                ],
                "published": "2024-07-10T16:16:02Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    16,
                    16,
                    2,
                    2,
                    192,
                    0
                ],
                "title": "Attribute or Abstain: Large Language Models as Long Document Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute or Abstain: Large Language Models as Long Document Assistants"
                },
                "summary": "LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiments with different approaches to attribution on 5 LLMs\nof different sizes.\n  We find that citation, i.e. response generation and evidence extraction in\none step, performs best for large and fine-tuned models, while additional\nretrieval can help for small, prompted models. We investigate whether the \"Lost\nin the Middle'' phenomenon exists for attribution, but do not find this. We\nalso find that evidence quality can predict response quality on datasets with\nsimple responses, but not so for complex responses, as models struggle with\nproviding evidence for complex claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiments with different approaches to attribution on 5 LLMs\nof different sizes.\n  We find that citation, i.e. response generation and evidence extraction in\none step, performs best for large and fine-tuned models, while additional\nretrieval can help for small, prompted models. We investigate whether the \"Lost\nin the Middle'' phenomenon exists for attribution, but do not find this. We\nalso find that evidence quality can predict response quality on datasets with\nsimple responses, but not so for complex responses, as models struggle with\nproviding evidence for complex claims."
                },
                "authors": [
                    {
                        "name": "Jan Buchmann"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Accepted at EMNLP 2024. Code and data:\n  https://github.com/UKPLab/arxiv2024-attribute-or-abstain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17885v1",
                "updated": "2024-10-23T13:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    58,
                    39,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:58:39Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    58,
                    39,
                    2,
                    297,
                    0
                ],
                "title": "R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric\n  Reasoning in Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric\n  Reasoning in Large Multimodal Models"
                },
                "summary": "Existing Large Multimodal Models (LMMs) struggle with mathematical geometric\nreasoning due to a lack of high-quality image-text paired data. Current\ngeometric data generation approaches, which apply preset templates to generate\ngeometric data or use Large Language Models (LLMs) to rephrase questions and\nanswers (Q&A), unavoidably limit data accuracy and diversity. To synthesize\nhigher-quality data, we propose a two-stage Reverse Chain-of-Thought (R-CoT)\ngeometry problem generation pipeline. First, we introduce GeoChain to produce\nhigh-fidelity geometric images and corresponding descriptions highlighting\nrelations among geometric elements. We then design a Reverse A&Q method that\nreasons step-by-step based on the descriptions and generates questions in\nreverse from the reasoning results. Experiments demonstrate that the proposed\nmethod brings significant and consistent improvements on multiple LMM\nbaselines, achieving new performance records in the 2B, 7B, and 8B settings.\nNotably, R-CoT-8B significantly outperforms previous state-of-the-art\nopen-source mathematical models by 16.6% on MathVista and 9.2% on GeoQA, while\nalso surpassing the closed-source model GPT-4o by an average of 13% across both\ndatasets. The code is available at https://github.com/dle666/R-CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Large Multimodal Models (LMMs) struggle with mathematical geometric\nreasoning due to a lack of high-quality image-text paired data. Current\ngeometric data generation approaches, which apply preset templates to generate\ngeometric data or use Large Language Models (LLMs) to rephrase questions and\nanswers (Q&A), unavoidably limit data accuracy and diversity. To synthesize\nhigher-quality data, we propose a two-stage Reverse Chain-of-Thought (R-CoT)\ngeometry problem generation pipeline. First, we introduce GeoChain to produce\nhigh-fidelity geometric images and corresponding descriptions highlighting\nrelations among geometric elements. We then design a Reverse A&Q method that\nreasons step-by-step based on the descriptions and generates questions in\nreverse from the reasoning results. Experiments demonstrate that the proposed\nmethod brings significant and consistent improvements on multiple LMM\nbaselines, achieving new performance records in the 2B, 7B, and 8B settings.\nNotably, R-CoT-8B significantly outperforms previous state-of-the-art\nopen-source mathematical models by 16.6% on MathVista and 9.2% on GeoQA, while\nalso surpassing the closed-source model GPT-4o by an average of 13% across both\ndatasets. The code is available at https://github.com/dle666/R-CoT."
                },
                "authors": [
                    {
                        "name": "Linger Deng"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Dongliang Luo"
                    },
                    {
                        "name": "Liang Wu"
                    },
                    {
                        "name": "Chengquan Zhang"
                    },
                    {
                        "name": "Pengyuan Lyu"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Gang Zhang"
                    },
                    {
                        "name": "Errui Ding"
                    },
                    {
                        "name": "Yingying Zhu"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17881v1",
                "updated": "2024-10-23T13:53:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    53,
                    26,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:53:26Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    53,
                    26,
                    2,
                    297,
                    0
                ],
                "title": "AdaRankGrad: Adaptive Gradient-Rank and Moments for Memory-Efficient\n  LLMs Training and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaRankGrad: Adaptive Gradient-Rank and Moments for Memory-Efficient\n  LLMs Training and Fine-Tuning"
                },
                "summary": "Training and fine-tuning large language models (LLMs) come with challenges\nrelated to memory and computational requirements due to the increasing size of\nthe model weights and the optimizer states. Various techniques have been\ndeveloped to tackle these challenges, such as low-rank adaptation (LoRA), which\ninvolves introducing a parallel trainable low-rank matrix to the fixed\npre-trained weights at each layer. However, these methods often fall short\ncompared to the full-rank weight training approach, as they restrict the\nparameter search to a low-rank subspace. This limitation can disrupt training\ndynamics and require a full-rank warm start to mitigate the impact. In this\npaper, we introduce a new method inspired by a phenomenon we formally prove: as\ntraining progresses, the rank of the estimated layer gradients gradually\ndecreases, and asymptotically approaches rank one. Leveraging this, our\napproach involves adaptively reducing the rank of the gradients during Adam\noptimization steps, using an efficient online-updating low-rank projections\nrule. We further present a randomized SVD scheme for efficiently finding the\nprojection matrix. Our technique enables full-parameter fine-tuning with\nadaptive low-rank gradient updates, significantly reducing overall memory\nrequirements during training compared to state-of-the-art methods while\nimproving model performance in both pretraining and fine-tuning. Finally, we\nprovide a convergence analysis of our method and demonstrate its merits for\ntraining and fine-tuning language and biological foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and fine-tuning large language models (LLMs) come with challenges\nrelated to memory and computational requirements due to the increasing size of\nthe model weights and the optimizer states. Various techniques have been\ndeveloped to tackle these challenges, such as low-rank adaptation (LoRA), which\ninvolves introducing a parallel trainable low-rank matrix to the fixed\npre-trained weights at each layer. However, these methods often fall short\ncompared to the full-rank weight training approach, as they restrict the\nparameter search to a low-rank subspace. This limitation can disrupt training\ndynamics and require a full-rank warm start to mitigate the impact. In this\npaper, we introduce a new method inspired by a phenomenon we formally prove: as\ntraining progresses, the rank of the estimated layer gradients gradually\ndecreases, and asymptotically approaches rank one. Leveraging this, our\napproach involves adaptively reducing the rank of the gradients during Adam\noptimization steps, using an efficient online-updating low-rank projections\nrule. We further present a randomized SVD scheme for efficiently finding the\nprojection matrix. Our technique enables full-parameter fine-tuning with\nadaptive low-rank gradient updates, significantly reducing overall memory\nrequirements during training compared to state-of-the-art methods while\nimproving model performance in both pretraining and fine-tuning. Finally, we\nprovide a convergence analysis of our method and demonstrate its merits for\ntraining and fine-tuning language and biological foundation models."
                },
                "authors": [
                    {
                        "name": "Yehonathan Refael"
                    },
                    {
                        "name": "Jonathan Svirsky"
                    },
                    {
                        "name": "Boris Shustin"
                    },
                    {
                        "name": "Wasim Huleihel"
                    },
                    {
                        "name": "Ofir Lindenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Lindenbaum"
                },
                "author": "Ofir Lindenbaum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17031v2",
                "updated": "2024-10-23T13:52:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    52,
                    51,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-22T13:57:55Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    13,
                    57,
                    55,
                    1,
                    296,
                    0
                ],
                "title": "GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks"
                },
                "summary": "The increasing demand for spatiotemporal data and modeling tasks in\ngeosciences has made geospatial code generation technology a critical factor in\nenhancing productivity. Although large language models (LLMs) have demonstrated\npotential in code generation tasks, they often encounter issues such as refusal\nto code or hallucination in geospatial code generation due to a lack of\ndomain-specific knowledge and code corpora. To address these challenges, this\npaper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along\nwith the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and\nLoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first\nLLM focused on geospatial code generation, fine-tuned from Code Llama-7B.\nFurthermore, we establish a comprehensive geospatial code evaluation framework,\nincorporating option matching, expert validation, and prompt engineering\nscoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the\nGeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms\nother models in multiple-choice accuracy by 9.1% to 32.1%, in code\nsummarization ability by 1.7% to 25.4%, and in code generation capability by\n1.2% to 25.1%. This paper provides a solution and empirical validation for\nenhancing LLMs' performance in geospatial code generation, extends the\nboundaries of domain-specific model applications, and offers valuable insights\ninto unlocking their potential in geospatial code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for spatiotemporal data and modeling tasks in\ngeosciences has made geospatial code generation technology a critical factor in\nenhancing productivity. Although large language models (LLMs) have demonstrated\npotential in code generation tasks, they often encounter issues such as refusal\nto code or hallucination in geospatial code generation due to a lack of\ndomain-specific knowledge and code corpora. To address these challenges, this\npaper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along\nwith the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and\nLoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first\nLLM focused on geospatial code generation, fine-tuned from Code Llama-7B.\nFurthermore, we establish a comprehensive geospatial code evaluation framework,\nincorporating option matching, expert validation, and prompt engineering\nscoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the\nGeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms\nother models in multiple-choice accuracy by 9.1% to 32.1%, in code\nsummarization ability by 1.7% to 25.4%, and in code generation capability by\n1.2% to 25.1%. This paper provides a solution and empirical validation for\nenhancing LLMs' performance in geospatial code generation, extends the\nboundaries of domain-specific model applications, and offers valuable insights\ninto unlocking their potential in geospatial code generation."
                },
                "authors": [
                    {
                        "name": "Shuyang Hou"
                    },
                    {
                        "name": "Zhangxiao Shen"
                    },
                    {
                        "name": "Anqi Zhao"
                    },
                    {
                        "name": "Jianyuan Liang"
                    },
                    {
                        "name": "Zhipeng Gui"
                    },
                    {
                        "name": "Xuefeng Guan"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Huayi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Huayi Wu"
                },
                "author": "Huayi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17878v1",
                "updated": "2024-10-23T13:50:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    50,
                    27,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:50:27Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    50,
                    27,
                    2,
                    297,
                    0
                ],
                "title": "Relaxed Equivariance via Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed Equivariance via Multitask Learning"
                },
                "summary": "Incorporating equivariance as an inductive bias into deep learning\narchitectures to take advantage of the data symmetry has been successful in\nmultiple applications, such as chemistry and dynamical systems. In particular,\nroto-translations are crucial for effectively modeling geometric graphs and\nmolecules, where understanding the 3D structures enhances generalization.\nHowever, equivariant models often pose challenges due to their high\ncomputational complexity. In this paper, we introduce REMUL, a training\nprocedure for approximating equivariance with multitask learning. We show that\nunconstrained models (which do not build equivariance into the architecture)\ncan learn approximate symmetries by minimizing an additional simple\nequivariance loss. By formulating equivariance as a new learning objective, we\ncan control the level of approximate equivariance in the model. Our method\nachieves competitive performance compared to equivariant baselines while being\n$10 \\times$ faster at inference and $2.5 \\times$ at training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating equivariance as an inductive bias into deep learning\narchitectures to take advantage of the data symmetry has been successful in\nmultiple applications, such as chemistry and dynamical systems. In particular,\nroto-translations are crucial for effectively modeling geometric graphs and\nmolecules, where understanding the 3D structures enhances generalization.\nHowever, equivariant models often pose challenges due to their high\ncomputational complexity. In this paper, we introduce REMUL, a training\nprocedure for approximating equivariance with multitask learning. We show that\nunconstrained models (which do not build equivariance into the architecture)\ncan learn approximate symmetries by minimizing an additional simple\nequivariance loss. By formulating equivariance as a new learning objective, we\ncan control the level of approximate equivariance in the model. Our method\nachieves competitive performance compared to equivariant baselines while being\n$10 \\times$ faster at inference and $2.5 \\times$ at training."
                },
                "authors": [
                    {
                        "name": "Ahmed A. Elhag"
                    },
                    {
                        "name": "T. Konstantin Rusch"
                    },
                    {
                        "name": "Francesco Di Giovanni"
                    },
                    {
                        "name": "Michael Bronstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bronstein"
                },
                "author": "Michael Bronstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17875v1",
                "updated": "2024-10-23T13:47:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    47,
                    5,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:47:05Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    47,
                    5,
                    2,
                    297,
                    0
                ],
                "title": "Understanding Layer Significance in LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Layer Significance in LLM Alignment"
                },
                "summary": "Aligning large language models (LLMs) through fine-tuning is essential for\ntailoring them to specific applications. Therefore, understanding what LLMs\nlearn during the alignment process is crucial. Recent studies suggest that\nalignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To delve deeper into LLM alignment, we propose to\nidentify which layers within LLMs are most critical to the alignment process,\nthereby uncovering how alignment influences model behavior at a granular level.\nWe propose a novel approach to identify the important layers for LLM alignment\n(ILA). It involves learning a binary mask for each incremental weight matrix in\nthe LoRA algorithm, indicating the significance of each layer. ILA consistently\nidentifies important layers across various alignment datasets, with nearly 90%\noverlap even with substantial dataset differences, highlighting fundamental\npatterns in LLM alignment. Experimental results indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) through fine-tuning is essential for\ntailoring them to specific applications. Therefore, understanding what LLMs\nlearn during the alignment process is crucial. Recent studies suggest that\nalignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To delve deeper into LLM alignment, we propose to\nidentify which layers within LLMs are most critical to the alignment process,\nthereby uncovering how alignment influences model behavior at a granular level.\nWe propose a novel approach to identify the important layers for LLM alignment\n(ILA). It involves learning a binary mask for each incremental weight matrix in\nthe LoRA algorithm, indicating the significance of each layer. ILA consistently\nidentifies important layers across various alignment datasets, with nearly 90%\noverlap even with substantial dataset differences, highlighting fundamental\npatterns in LLM alignment. Experimental results indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss."
                },
                "authors": [
                    {
                        "name": "Guangyuan Shi"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Xiaoyu Dong"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Xuanyu Zhang"
                    },
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16577v2",
                "updated": "2024-10-23T13:36:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    36,
                    39,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-21T23:30:36Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    23,
                    30,
                    36,
                    0,
                    295,
                    0
                ],
                "title": "Bayesian High-dimensional Linear Regression with Sparse\n  Projection-posterior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian High-dimensional Linear Regression with Sparse\n  Projection-posterior"
                },
                "summary": "We consider a novel Bayesian approach to estimation, uncertainty\nquantification, and variable selection for a high-dimensional linear regression\nmodel under sparsity. The number of predictors can be nearly exponentially\nlarge relative to the sample size. We put a conjugate normal prior initially\ndisregarding sparsity, but for making an inference, instead of the original\nmultivariate normal posterior, we use the posterior distribution induced by a\nmap transforming the vector of regression coefficients to a sparse vector\nobtained by minimizing the sum of squares of deviations plus a suitably scaled\n$\\ell_1$-penalty on the vector. We show that the resulting sparse\nprojection-posterior distribution contracts around the true value of the\nparameter at the optimal rate adapted to the sparsity of the vector. We show\nthat the true sparsity structure gets a large sparse projection-posterior\nprobability. We further show that an appropriately recentred credible ball has\nthe correct asymptotic frequentist coverage. Finally, we describe how the\ncomputational burden can be distributed to many machines, each dealing with\nonly a small fraction of the whole dataset. We conduct a comprehensive\nsimulation study under a variety of settings and found that the proposed method\nperforms well for finite sample sizes. We also apply the method to several real\ndatasets, including the ADNI data, and compare its performance with the\nstate-of-the-art methods. We implemented the method in the \\texttt{R} package\ncalled \\texttt{sparseProj}, and all computations have been carried out using\nthis package.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a novel Bayesian approach to estimation, uncertainty\nquantification, and variable selection for a high-dimensional linear regression\nmodel under sparsity. The number of predictors can be nearly exponentially\nlarge relative to the sample size. We put a conjugate normal prior initially\ndisregarding sparsity, but for making an inference, instead of the original\nmultivariate normal posterior, we use the posterior distribution induced by a\nmap transforming the vector of regression coefficients to a sparse vector\nobtained by minimizing the sum of squares of deviations plus a suitably scaled\n$\\ell_1$-penalty on the vector. We show that the resulting sparse\nprojection-posterior distribution contracts around the true value of the\nparameter at the optimal rate adapted to the sparsity of the vector. We show\nthat the true sparsity structure gets a large sparse projection-posterior\nprobability. We further show that an appropriately recentred credible ball has\nthe correct asymptotic frequentist coverage. Finally, we describe how the\ncomputational burden can be distributed to many machines, each dealing with\nonly a small fraction of the whole dataset. We conduct a comprehensive\nsimulation study under a variety of settings and found that the proposed method\nperforms well for finite sample sizes. We also apply the method to several real\ndatasets, including the ADNI data, and compare its performance with the\nstate-of-the-art methods. We implemented the method in the \\texttt{R} package\ncalled \\texttt{sparseProj}, and all computations have been carried out using\nthis package."
                },
                "authors": [
                    {
                        "name": "Samhita Pal"
                    },
                    {
                        "name": "Subhashis Ghoshal"
                    }
                ],
                "author_detail": {
                    "name": "Subhashis Ghoshal"
                },
                "author": "Subhashis Ghoshal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17864v1",
                "updated": "2024-10-23T13:35:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    35,
                    55,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:35:55Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    35,
                    55,
                    2,
                    297,
                    0
                ],
                "title": "Longitudinal Causal Inference with Selective Eligibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Longitudinal Causal Inference with Selective Eligibility"
                },
                "summary": "Dropout often threatens the validity of causal inference in longitudinal\nstudies. While existing studies have focused on the problem of missing outcomes\ncaused by treatment, we study an important but overlooked source of dropout,\nselective eligibility. For example, patients may become ineligible for\nsubsequent treatments due to severe side effects or complete recovery.\nSelective eligibility differs from the problem of ``truncation by death''\nbecause dropout occurs after observing the outcome but before receiving the\nsubsequent treatment. This difference makes the standard approach to dropout\ninapplicable. We propose a general methodological framework for longitudinal\ncausal inference with selective eligibility. By focusing on subgroups of units\nwho would become eligible for treatment given a specific treatment history, we\ndefine the time-specific eligible treatment effect (ETE) and expected number of\noutcome events (EOE) under a treatment sequence of interest. Assuming a\ngeneralized version of sequential ignorability, we derive two nonparametric\nidentification formulae, each leveraging different parts of the observed data\ndistribution. We then derive the efficient influence function of each causal\nestimand, yielding the corresponding doubly robust estimator. Finally, we apply\nthe proposed methodology to an impact evaluation of a pre-trial risk assessment\ninstrument in the criminal justice system, in which selective eligibility\narises due to recidivism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dropout often threatens the validity of causal inference in longitudinal\nstudies. While existing studies have focused on the problem of missing outcomes\ncaused by treatment, we study an important but overlooked source of dropout,\nselective eligibility. For example, patients may become ineligible for\nsubsequent treatments due to severe side effects or complete recovery.\nSelective eligibility differs from the problem of ``truncation by death''\nbecause dropout occurs after observing the outcome but before receiving the\nsubsequent treatment. This difference makes the standard approach to dropout\ninapplicable. We propose a general methodological framework for longitudinal\ncausal inference with selective eligibility. By focusing on subgroups of units\nwho would become eligible for treatment given a specific treatment history, we\ndefine the time-specific eligible treatment effect (ETE) and expected number of\noutcome events (EOE) under a treatment sequence of interest. Assuming a\ngeneralized version of sequential ignorability, we derive two nonparametric\nidentification formulae, each leveraging different parts of the observed data\ndistribution. We then derive the efficient influence function of each causal\nestimand, yielding the corresponding doubly robust estimator. Finally, we apply\nthe proposed methodology to an impact evaluation of a pre-trial risk assessment\ninstrument in the criminal justice system, in which selective eligibility\narises due to recidivism."
                },
                "authors": [
                    {
                        "name": "Zhichao Jiang"
                    },
                    {
                        "name": "Eli Ben-Michael"
                    },
                    {
                        "name": "D. James Greiner"
                    },
                    {
                        "name": "Ryan Halen"
                    },
                    {
                        "name": "Kosuke Imai"
                    }
                ],
                "author_detail": {
                    "name": "Kosuke Imai"
                },
                "author": "Kosuke Imai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17851v1",
                "updated": "2024-10-23T13:20:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    20,
                    42,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:20:42Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    20,
                    42,
                    2,
                    297,
                    0
                ],
                "title": "The Probabilistic Tsetlin Machine: A Novel Approach to Uncertainty\n  Quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Probabilistic Tsetlin Machine: A Novel Approach to Uncertainty\n  Quantification"
                },
                "summary": "Tsetlin Machines (TMs) have emerged as a compelling alternative to\nconventional deep learning methods, offering notable advantages such as smaller\nmemory footprint, faster inference, fault-tolerant properties, and\ninterpretability. Although various adaptations of TMs have expanded their\napplicability across diverse domains, a fundamental gap remains in\nunderstanding how TMs quantify uncertainty in their predictions. In response,\nthis paper introduces the Probabilistic Tsetlin Machine (PTM) framework, aimed\nat providing a robust, reliable, and interpretable approach for uncertainty\nquantification. Unlike the original TM, the PTM learns the probability of\nstaying on each state of each Tsetlin Automaton (TA) across all clauses. These\nprobabilities are updated using the feedback tables that are part of the TM\nframework: Type I and Type II feedback. During inference, TAs decide their\nactions by sampling states based on learned probability distributions, akin to\nBayesian neural networks when generating weight values. In our experimental\nanalysis, we first illustrate the spread of the probabilities across TA states\nfor the noisy-XOR dataset. Then we evaluate the PTM alongside benchmark models\nusing both simulated and real-world datasets. The experiments on the simulated\ndataset reveal the PTM's effectiveness in uncertainty quantification,\nparticularly in delineating decision boundaries and identifying regions of high\nuncertainty. Moreover, when applied to multiclass classification tasks using\nthe Iris dataset, the PTM demonstrates competitive performance in terms of\npredictive entropy and expected calibration error, showcasing its potential as\na reliable tool for uncertainty estimation. Our findings underscore the\nimportance of selecting appropriate models for accurate uncertainty\nquantification in predictive tasks, with the PTM offering a particularly\ninterpretable and effective solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tsetlin Machines (TMs) have emerged as a compelling alternative to\nconventional deep learning methods, offering notable advantages such as smaller\nmemory footprint, faster inference, fault-tolerant properties, and\ninterpretability. Although various adaptations of TMs have expanded their\napplicability across diverse domains, a fundamental gap remains in\nunderstanding how TMs quantify uncertainty in their predictions. In response,\nthis paper introduces the Probabilistic Tsetlin Machine (PTM) framework, aimed\nat providing a robust, reliable, and interpretable approach for uncertainty\nquantification. Unlike the original TM, the PTM learns the probability of\nstaying on each state of each Tsetlin Automaton (TA) across all clauses. These\nprobabilities are updated using the feedback tables that are part of the TM\nframework: Type I and Type II feedback. During inference, TAs decide their\nactions by sampling states based on learned probability distributions, akin to\nBayesian neural networks when generating weight values. In our experimental\nanalysis, we first illustrate the spread of the probabilities across TA states\nfor the noisy-XOR dataset. Then we evaluate the PTM alongside benchmark models\nusing both simulated and real-world datasets. The experiments on the simulated\ndataset reveal the PTM's effectiveness in uncertainty quantification,\nparticularly in delineating decision boundaries and identifying regions of high\nuncertainty. Moreover, when applied to multiclass classification tasks using\nthe Iris dataset, the PTM demonstrates competitive performance in terms of\npredictive entropy and expected calibration error, showcasing its potential as\na reliable tool for uncertainty estimation. Our findings underscore the\nimportance of selecting appropriate models for accurate uncertainty\nquantification in predictive tasks, with the PTM offering a particularly\ninterpretable and effective solution."
                },
                "authors": [
                    {
                        "name": "K. Darshana Abeyrathna"
                    },
                    {
                        "name": "Sara El Mekkaoui"
                    },
                    {
                        "name": "Andreas Hafver"
                    },
                    {
                        "name": "Christian Agrell"
                    }
                ],
                "author_detail": {
                    "name": "Christian Agrell"
                },
                "author": "Christian Agrell",
                "arxiv_comment": "12 pages, 5 figures, 6 tables, accepted and presented at ICAAI 2024,\n  London",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15055v2",
                "updated": "2024-10-23T13:20:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    20,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-23T02:15:47Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    2,
                    15,
                    47,
                    4,
                    54,
                    0
                ],
                "title": "Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions"
                },
                "summary": "Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding."
                },
                "authors": [
                    {
                        "name": "Clement Neo"
                    },
                    {
                        "name": "Shay B. Cohen"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17840v1",
                "updated": "2024-10-23T13:05:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    5,
                    46,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:05:46Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    5,
                    46,
                    2,
                    297,
                    0
                ],
                "title": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for\n  LLMs"
                },
                "summary": "Serving systems for Large Language Models (LLMs) improve throughput by\nprocessing several requests concurrently. However, multiplexing hardware\nresources between concurrent requests involves non-trivial scheduling\ndecisions. Practical serving systems typically implement these decisions at two\nlevels: First, a load balancer routes requests to different servers which each\nhold a replica of the LLM. Then, on each server, an engine-level scheduler\ndecides when to run a request, or when to queue or preempt it. Improved\nscheduling policies may benefit a wide range of LLM deployments and can often\nbe implemented as \"drop-in replacements\" to a system's current policy. In this\nwork, we survey scheduling techniques from the literature and from practical\nserving systems. We find that schedulers from the literature often achieve good\nperformance but introduce significant complexity. In contrast, schedulers in\npractical deployments often leave easy performance gains on the table but are\neasy to implement, deploy and configure. This finding motivates us to introduce\ntwo new scheduling techniques, which are both easy to implement, and outperform\ncurrent techniques on production workload traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) improve throughput by\nprocessing several requests concurrently. However, multiplexing hardware\nresources between concurrent requests involves non-trivial scheduling\ndecisions. Practical serving systems typically implement these decisions at two\nlevels: First, a load balancer routes requests to different servers which each\nhold a replica of the LLM. Then, on each server, an engine-level scheduler\ndecides when to run a request, or when to queue or preempt it. Improved\nscheduling policies may benefit a wide range of LLM deployments and can often\nbe implemented as \"drop-in replacements\" to a system's current policy. In this\nwork, we survey scheduling techniques from the literature and from practical\nserving systems. We find that schedulers from the literature often achieve good\nperformance but introduce significant complexity. In contrast, schedulers in\npractical deployments often leave easy performance gains on the table but are\neasy to implement, deploy and configure. This finding motivates us to introduce\ntwo new scheduling techniques, which are both easy to implement, and outperform\ncurrent techniques on production workload traces."
                },
                "authors": [
                    {
                        "name": "Ferdi Kossmann"
                    },
                    {
                        "name": "Bruce Fontaine"
                    },
                    {
                        "name": "Daya Khudia"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "Samuel Madden"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Madden"
                },
                "author": "Samuel Madden",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15956v2",
                "updated": "2024-10-23T13:00:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    0,
                    27,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-21T12:34:17Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    12,
                    34,
                    17,
                    0,
                    295,
                    0
                ],
                "title": "Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs"
                },
                "summary": "Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Yanzhu Guo"
                    },
                    {
                        "name": "Simone Conia"
                    },
                    {
                        "name": "Zelin Zhou"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Saloni Potdar"
                    },
                    {
                        "name": "Henry Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Henry Xiao"
                },
                "author": "Henry Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12376v2",
                "updated": "2024-10-23T12:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    58,
                    14,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-16T08:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    48,
                    27,
                    2,
                    290,
                    0
                ],
                "title": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated\n  Shapefile Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated\n  Shapefile Processing"
                },
                "summary": "Vector data is one of the two core data structures in geographic information\nscience (GIS), essential for accurately storing and representing geospatial\ninformation. Shapefile, the most widely used vector data format, has become the\nindustry standard supported by all major geographic information systems.\nHowever, processing this data typically requires specialized GIS knowledge and\nskills, creating a barrier for researchers from other fields and impeding\ninterdisciplinary research in spatial data analysis. Moreover, while large\nlanguage models (LLMs) have made significant advancements in natural language\nprocessing and task automation, they still face challenges in handling the\ncomplex spatial and topological relationships inherent in GIS vector data. To\naddress these challenges, we propose ShapefileGPT, an innovative framework\npowered by LLMs, specifically designed to automate Shapefile tasks.\nShapefileGPT utilizes a multi-agent architecture, in which the planner agent is\nresponsible for task decomposition and supervision, while the worker agent\nexecutes the tasks. We developed a specialized function library for handling\nShapefiles and provided comprehensive API documentation, enabling the worker\nagent to operate Shapefiles efficiently through function calling. For\nevaluation, we developed a benchmark dataset based on authoritative textbooks,\nencompassing tasks in categories such as geometric operations and spatial\nqueries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the\nGPT series models. In comparison to traditional LLMs, ShapefileGPT effectively\nhandles complex vector data analysis tasks, overcoming the limitations of\ntraditional LLMs in spatial analysis. This breakthrough opens new pathways for\nadvancing automation and intelligence in the GIS field, with significant\npotential in interdisciplinary data analysis and application contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector data is one of the two core data structures in geographic information\nscience (GIS), essential for accurately storing and representing geospatial\ninformation. Shapefile, the most widely used vector data format, has become the\nindustry standard supported by all major geographic information systems.\nHowever, processing this data typically requires specialized GIS knowledge and\nskills, creating a barrier for researchers from other fields and impeding\ninterdisciplinary research in spatial data analysis. Moreover, while large\nlanguage models (LLMs) have made significant advancements in natural language\nprocessing and task automation, they still face challenges in handling the\ncomplex spatial and topological relationships inherent in GIS vector data. To\naddress these challenges, we propose ShapefileGPT, an innovative framework\npowered by LLMs, specifically designed to automate Shapefile tasks.\nShapefileGPT utilizes a multi-agent architecture, in which the planner agent is\nresponsible for task decomposition and supervision, while the worker agent\nexecutes the tasks. We developed a specialized function library for handling\nShapefiles and provided comprehensive API documentation, enabling the worker\nagent to operate Shapefiles efficiently through function calling. For\nevaluation, we developed a benchmark dataset based on authoritative textbooks,\nencompassing tasks in categories such as geometric operations and spatial\nqueries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the\nGPT series models. In comparison to traditional LLMs, ShapefileGPT effectively\nhandles complex vector data analysis tasks, overcoming the limitations of\ntraditional LLMs in spatial analysis. This breakthrough opens new pathways for\nadvancing automation and intelligence in the GIS field, with significant\npotential in interdisciplinary data analysis and application contexts."
                },
                "authors": [
                    {
                        "name": "Qingming Lin"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Huaxia Li"
                    },
                    {
                        "name": "Sensen Wu"
                    },
                    {
                        "name": "Yadong Li"
                    },
                    {
                        "name": "Kai Fang"
                    },
                    {
                        "name": "Hailin Feng"
                    },
                    {
                        "name": "Zhenhong Du"
                    },
                    {
                        "name": "Liuchang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liuchang Xu"
                },
                "author": "Liuchang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17055v2",
                "updated": "2024-10-23T12:55:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    55,
                    39,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-22T14:36:44Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    14,
                    36,
                    44,
                    1,
                    296,
                    0
                ],
                "title": "Optimal Design for Reward Modeling in RLHF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Design for Reward Modeling in RLHF"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) has become a popular\napproach to align language models (LMs) with human preferences. This method\ninvolves collecting a large dataset of human pairwise preferences across\nvarious text generations and using it to infer (implicitly or explicitly) a\nreward model. Numerous methods have been proposed to learn the reward model and\nalign a LM with it. However, the costly process of collecting human preferences\nhas received little attention and could benefit from theoretical insights. This\npaper addresses this issue and aims to formalize the reward training model in\nRLHF. We frame the selection of an effective dataset as a simple regret\nminimization task, using a linear contextual dueling bandit method. Given the\npotentially large number of arms, this approach is more coherent than the\nbest-arm identification setting. We then propose an offline framework for\nsolving this problem. Under appropriate assumptions - linearity of the reward\nmodel in the embedding space, and boundedness of the reward parameter - we\nderive bounds on the simple regret. Finally, we provide a lower bound that\nmatches our upper bound up to constant and logarithmic terms. To our knowledge,\nthis is the first theoretical contribution in this area to provide an offline\napproach as well as worst-case guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) has become a popular\napproach to align language models (LMs) with human preferences. This method\ninvolves collecting a large dataset of human pairwise preferences across\nvarious text generations and using it to infer (implicitly or explicitly) a\nreward model. Numerous methods have been proposed to learn the reward model and\nalign a LM with it. However, the costly process of collecting human preferences\nhas received little attention and could benefit from theoretical insights. This\npaper addresses this issue and aims to formalize the reward training model in\nRLHF. We frame the selection of an effective dataset as a simple regret\nminimization task, using a linear contextual dueling bandit method. Given the\npotentially large number of arms, this approach is more coherent than the\nbest-arm identification setting. We then propose an offline framework for\nsolving this problem. Under appropriate assumptions - linearity of the reward\nmodel in the embedding space, and boundedness of the reward parameter - we\nderive bounds on the simple regret. Finally, we provide a lower bound that\nmatches our upper bound up to constant and logarithmic terms. To our knowledge,\nthis is the first theoretical contribution in this area to provide an offline\napproach as well as worst-case guarantees."
                },
                "authors": [
                    {
                        "name": "Antoine Scheid"
                    },
                    {
                        "name": "Etienne Boursier"
                    },
                    {
                        "name": "Alain Durmus"
                    },
                    {
                        "name": "Michael I. Jordan"
                    },
                    {
                        "name": "Pierre Mnard"
                    },
                    {
                        "name": "Eric Moulines"
                    },
                    {
                        "name": "Michal Valko"
                    }
                ],
                "author_detail": {
                    "name": "Michal Valko"
                },
                "author": "Michal Valko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06022v2",
                "updated": "2024-10-23T12:49:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    49,
                    32,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-08T13:23:58Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    13,
                    23,
                    58,
                    1,
                    282,
                    0
                ],
                "title": "Can Language Models Induce Grammatical Knowledge from Indirect Evidence?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Induce Grammatical Knowledge from Indirect Evidence?"
                },
                "summary": "What kinds of and how much data is necessary for language models to induce\ngrammatical knowledge to judge sentence acceptability? Recent language models\nstill have much room for improvement in their data efficiency compared to\nhumans. This paper investigates whether language models efficiently use\nindirect data (indirect evidence), from which they infer sentence\nacceptability. In contrast, humans use indirect evidence efficiently, which is\nconsidered one of the inductive biases contributing to efficient language\nacquisition. To explore this question, we introduce the Wug InDirect Evidence\nTest (WIDET), a dataset consisting of training instances inserted into the\npre-training data and evaluation instances. We inject synthetic instances with\nnewly coined wug words into pretraining data and explore the model's behavior\non evaluation data that assesses grammatical acceptability regarding those\nwords. We prepare the injected instances by varying their levels of\nindirectness and quantity. Our experiments surprisingly show that language\nmodels do not induce grammatical knowledge even after repeated exposure to\ninstances with the same structure but differing only in lexical items from\nevaluation instances in certain language phenomena. Our findings suggest a\npotential direction for future research: developing models that use latent\nindirect evidence to induce grammatical knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What kinds of and how much data is necessary for language models to induce\ngrammatical knowledge to judge sentence acceptability? Recent language models\nstill have much room for improvement in their data efficiency compared to\nhumans. This paper investigates whether language models efficiently use\nindirect data (indirect evidence), from which they infer sentence\nacceptability. In contrast, humans use indirect evidence efficiently, which is\nconsidered one of the inductive biases contributing to efficient language\nacquisition. To explore this question, we introduce the Wug InDirect Evidence\nTest (WIDET), a dataset consisting of training instances inserted into the\npre-training data and evaluation instances. We inject synthetic instances with\nnewly coined wug words into pretraining data and explore the model's behavior\non evaluation data that assesses grammatical acceptability regarding those\nwords. We prepare the injected instances by varying their levels of\nindirectness and quantity. Our experiments surprisingly show that language\nmodels do not induce grammatical knowledge even after repeated exposure to\ninstances with the same structure but differing only in lexical items from\nevaluation instances in certain language phenomena. Our findings suggest a\npotential direction for future research: developing models that use latent\nindirect evidence to induce grammatical knowledge."
                },
                "authors": [
                    {
                        "name": "Miyu Oba"
                    },
                    {
                        "name": "Yohei Oseki"
                    },
                    {
                        "name": "Akiyo Fukatsu"
                    },
                    {
                        "name": "Akari Haga"
                    },
                    {
                        "name": "Hiroki Ouchi"
                    },
                    {
                        "name": "Taro Watanabe"
                    },
                    {
                        "name": "Saku Sugawara"
                    }
                ],
                "author_detail": {
                    "name": "Saku Sugawara"
                },
                "author": "Saku Sugawara",
                "arxiv_comment": "This paper is accepted at EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10568v2",
                "updated": "2024-10-23T12:37:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    37,
                    10,
                    2,
                    297,
                    0
                ],
                "published": "2024-09-14T04:17:24Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    4,
                    17,
                    24,
                    5,
                    258,
                    0
                ],
                "title": "On the limits of agency in agent-based models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the limits of agency in agent-based models"
                },
                "summary": "Agent-based modeling (ABM) seeks to understand the behavior of complex\nsystems by simulating a collection of agents that act and interact within an\nenvironment. Their practical utility requires capturing realistic environment\ndynamics and adaptive agent behavior while efficiently simulating million-size\npopulations. Recent advancements in large language models (LLMs) present an\nopportunity to enhance ABMs by using LLMs as agents with further potential to\ncapture adaptive behavior. However, the computational infeasibility of using\nLLMs for large populations has hindered their widespread adoption. In this\npaper, we introduce AgentTorch -- a framework that scales ABMs to millions of\nagents while capturing high-resolution agent behavior using LLMs. We benchmark\nthe utility of LLMs as ABM agents, exploring the trade-off between simulation\nscale and individual agency. Using the COVID-19 pandemic as a case study, we\ndemonstrate how AgentTorch can simulate 8.4 million agents representing New\nYork City, capturing the impact of isolation and employment behavior on health\nand economic outcomes. We compare the performance of different agent\narchitectures based on heuristic and LLM agents in predicting disease waves and\nunemployment rates. Furthermore, we showcase AgentTorch's capabilities for\nretrospective, counterfactual, and prospective analyses, highlighting how\nadaptive agent behavior can help overcome the limitations of historical data in\npolicy design. AgentTorch is an open-source project actively being used for\npolicy-making and scientific discovery around the world. The framework is\navailable here: github.com/AgentTorch/AgentTorch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-based modeling (ABM) seeks to understand the behavior of complex\nsystems by simulating a collection of agents that act and interact within an\nenvironment. Their practical utility requires capturing realistic environment\ndynamics and adaptive agent behavior while efficiently simulating million-size\npopulations. Recent advancements in large language models (LLMs) present an\nopportunity to enhance ABMs by using LLMs as agents with further potential to\ncapture adaptive behavior. However, the computational infeasibility of using\nLLMs for large populations has hindered their widespread adoption. In this\npaper, we introduce AgentTorch -- a framework that scales ABMs to millions of\nagents while capturing high-resolution agent behavior using LLMs. We benchmark\nthe utility of LLMs as ABM agents, exploring the trade-off between simulation\nscale and individual agency. Using the COVID-19 pandemic as a case study, we\ndemonstrate how AgentTorch can simulate 8.4 million agents representing New\nYork City, capturing the impact of isolation and employment behavior on health\nand economic outcomes. We compare the performance of different agent\narchitectures based on heuristic and LLM agents in predicting disease waves and\nunemployment rates. Furthermore, we showcase AgentTorch's capabilities for\nretrospective, counterfactual, and prospective analyses, highlighting how\nadaptive agent behavior can help overcome the limitations of historical data in\npolicy design. AgentTorch is an open-source project actively being used for\npolicy-making and scientific discovery around the world. The framework is\navailable here: github.com/AgentTorch/AgentTorch."
                },
                "authors": [
                    {
                        "name": "Ayush Chopra"
                    },
                    {
                        "name": "Shashank Kumar"
                    },
                    {
                        "name": "Nurullah Giray-Kuru"
                    },
                    {
                        "name": "Ramesh Raskar"
                    },
                    {
                        "name": "Arnau Quera-Bofarull"
                    }
                ],
                "author_detail": {
                    "name": "Arnau Quera-Bofarull"
                },
                "author": "Arnau Quera-Bofarull",
                "arxiv_comment": "19 pages, 5 appendices, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17822v1",
                "updated": "2024-10-23T12:32:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    32,
                    20,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T12:32:20Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    32,
                    20,
                    2,
                    297,
                    0
                ],
                "title": "DREB-Net: Dual-stream Restoration Embedding Blur-feature Fusion Network\n  for High-mobility UAV Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DREB-Net: Dual-stream Restoration Embedding Blur-feature Fusion Network\n  for High-mobility UAV Object Detection"
                },
                "summary": "Object detection algorithms are pivotal components of unmanned aerial vehicle\n(UAV) imaging systems, extensively employed in complex fields. However, images\ncaptured by high-mobility UAVs often suffer from motion blur cases, which\nsignificantly impedes the performance of advanced object detection algorithms.\nTo address these challenges, we propose an innovative object detection\nalgorithm specifically designed for blurry images, named DREB-Net (Dual-stream\nRestoration Embedding Blur-feature Fusion Network). First, DREB-Net addresses\nthe particularities of blurry image object detection problem by incorporating a\nBlurry image Restoration Auxiliary Branch (BRAB) during the training phase.\nSecond, it fuses the extracted shallow features via Multi-level\nAttention-Guided Feature Fusion (MAGFF) module, to extract richer features.\nHere, the MAGFF module comprises local attention modules and global attention\nmodules, which assign different weights to the branches. Then, during the\ninference phase, the deep feature extraction of the BRAB can be removed to\nreduce computational complexity and improve detection speed. In loss function,\na combined loss of MSE and SSIM is added to the BRAB to restore blurry images.\nFinally, DREB-Net introduces Fast Fourier Transform in the early stages of\nfeature extraction, via a Learnable Frequency domain Amplitude Modulation\nModule (LFAMM), to adjust feature amplitude and enhance feature processing\ncapability. Experimental results indicate that DREB-Net can still effectively\nperform object detection tasks under motion blur in captured images, showcasing\nexcellent performance and broad application prospects. Our source code will be\navailable at https://github.com/EEIC-Lab/DREB-Net.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detection algorithms are pivotal components of unmanned aerial vehicle\n(UAV) imaging systems, extensively employed in complex fields. However, images\ncaptured by high-mobility UAVs often suffer from motion blur cases, which\nsignificantly impedes the performance of advanced object detection algorithms.\nTo address these challenges, we propose an innovative object detection\nalgorithm specifically designed for blurry images, named DREB-Net (Dual-stream\nRestoration Embedding Blur-feature Fusion Network). First, DREB-Net addresses\nthe particularities of blurry image object detection problem by incorporating a\nBlurry image Restoration Auxiliary Branch (BRAB) during the training phase.\nSecond, it fuses the extracted shallow features via Multi-level\nAttention-Guided Feature Fusion (MAGFF) module, to extract richer features.\nHere, the MAGFF module comprises local attention modules and global attention\nmodules, which assign different weights to the branches. Then, during the\ninference phase, the deep feature extraction of the BRAB can be removed to\nreduce computational complexity and improve detection speed. In loss function,\na combined loss of MSE and SSIM is added to the BRAB to restore blurry images.\nFinally, DREB-Net introduces Fast Fourier Transform in the early stages of\nfeature extraction, via a Learnable Frequency domain Amplitude Modulation\nModule (LFAMM), to adjust feature amplitude and enhance feature processing\ncapability. Experimental results indicate that DREB-Net can still effectively\nperform object detection tasks under motion blur in captured images, showcasing\nexcellent performance and broad application prospects. Our source code will be\navailable at https://github.com/EEIC-Lab/DREB-Net.git."
                },
                "authors": [
                    {
                        "name": "Qingpeng Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Leyuan Fang"
                    },
                    {
                        "name": "Yuhan Kang"
                    },
                    {
                        "name": "Shutao Li"
                    },
                    {
                        "name": "Xiao Xiang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Xiang Zhu"
                },
                "author": "Xiao Xiang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17820v1",
                "updated": "2024-10-23T12:26:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    26,
                    10,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T12:26:10Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    26,
                    10,
                    2,
                    297,
                    0
                ],
                "title": "Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination"
                },
                "summary": "Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. While using even a smaller model as the discriminator, scaling\nthe generator leads to notable improvements in ToT performance, whereas scaling\nthe discriminator with a fixed generator yields only marginal gains. Our\nresults show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. While using even a smaller model as the discriminator, scaling\nthe generator leads to notable improvements in ToT performance, whereas scaling\nthe discriminator with a fixed generator yields only marginal gains. Our\nresults show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT."
                },
                "authors": [
                    {
                        "name": "Qiqi Chen"
                    },
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "Code: github.com/mainlp/tot-eval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04372v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04372v3",
                "updated": "2024-10-23T12:19:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    19,
                    16,
                    2,
                    297,
                    0
                ],
                "published": "2024-01-09T06:15:45Z",
                "published_parsed": [
                    2024,
                    1,
                    9,
                    6,
                    15,
                    45,
                    1,
                    9,
                    0
                ],
                "title": "Stable generative modeling using Schrdinger bridges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable generative modeling using Schrdinger bridges"
                },
                "summary": "We consider the problem of sampling from an unknown distribution for which\nonly a sufficiently large number of training samples are available. Such\nsettings have recently drawn considerable interest in the context of generative\nmodelling and Bayesian inference. In this paper, we propose a generative model\ncombining Schr\\\"odinger bridges and Langevin dynamics. Schr\\\"odinger bridges\nover an appropriate reversible reference process are used to approximate the\nconditional transition probability from the available training samples, which\nis then implemented in a discrete-time reversible Langevin sampler to generate\nnew samples. By setting the kernel bandwidth in the reference process to match\nthe time step size used in the unadjusted Langevin algorithm, our method\neffectively circumvents any stability issues typically associated with the\ntime-stepping of stiff stochastic differential equations. Moreover, we\nintroduce a novel split-step scheme, ensuring that the generated samples remain\nwithin the convex hull of the training samples. Our framework can be naturally\nextended to generate conditional samples and to Bayesian inference problems. We\ndemonstrate the performance of our proposed scheme through experiments on\nsynthetic datasets with increasing dimensions and on a stochastic subgrid-scale\nparametrization conditional sampling problem as well as generating sample\ntrajectories of a dynamical system using conditional sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of sampling from an unknown distribution for which\nonly a sufficiently large number of training samples are available. Such\nsettings have recently drawn considerable interest in the context of generative\nmodelling and Bayesian inference. In this paper, we propose a generative model\ncombining Schr\\\"odinger bridges and Langevin dynamics. Schr\\\"odinger bridges\nover an appropriate reversible reference process are used to approximate the\nconditional transition probability from the available training samples, which\nis then implemented in a discrete-time reversible Langevin sampler to generate\nnew samples. By setting the kernel bandwidth in the reference process to match\nthe time step size used in the unadjusted Langevin algorithm, our method\neffectively circumvents any stability issues typically associated with the\ntime-stepping of stiff stochastic differential equations. Moreover, we\nintroduce a novel split-step scheme, ensuring that the generated samples remain\nwithin the convex hull of the training samples. Our framework can be naturally\nextended to generate conditional samples and to Bayesian inference problems. We\ndemonstrate the performance of our proposed scheme through experiments on\nsynthetic datasets with increasing dimensions and on a stochastic subgrid-scale\nparametrization conditional sampling problem as well as generating sample\ntrajectories of a dynamical system using conditional sampling."
                },
                "authors": [
                    {
                        "name": "Georg A. Gottwald"
                    },
                    {
                        "name": "Fengyi Li"
                    },
                    {
                        "name": "Youssef Marzouk"
                    },
                    {
                        "name": "Sebastian Reich"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Reich"
                },
                "author": "Sebastian Reich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.04372v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04372v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60H10, 62F15, 62F30, 65C05, 65C40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17810v1",
                "updated": "2024-10-23T12:12:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    12,
                    56,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T12:12:56Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    12,
                    56,
                    2,
                    297,
                    0
                ],
                "title": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive\n  Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive\n  Contrastive Learning"
                },
                "summary": "Recent advancements in image-text matching have been notable, yet prevailing\nmodels predominantly cater to broad queries and struggle with accommodating\nfine-grained query intention. In this paper, we work towards the\n\\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM),\na task that the text and image involve specific entity-related information. The\nchallenge of this task mainly lies in the larger semantic gap in entity\nassociation modeling, comparing with the general image-text matching problem.To\nnarrow the huge semantic gap between the entity-centric text and the images, we\ntake the fundamental CLIP as the backbone and devise a multimodal attentive\ncontrastive learning framework to tam CLIP to adapt EITM problem, developing a\nmodel named EntityCLIP. The key of our multimodal attentive contrastive\nlearning is to generate interpretive explanation text using Large Language\nModels (LLMs) as the bridge clues. In specific, we proceed by extracting\nexplanatory text from off-the-shelf LLMs. This explanation text, coupled with\nthe image and text, is then input into our specially crafted Multimodal\nAttentive Experts (MMAE) module, which effectively integrates explanation texts\nto narrow the gap of the entity-related text and image in a shared semantic\nspace. Building on the enriched features derived from MMAE, we further design\nan effective Gated Integrative Image-text Matching (GI-ITM) strategy. The\nGI-ITM employs an adaptive gating mechanism to aggregate MMAE's features,\nsubsequently applying image-text matching constraints to steer the alignment\nbetween the text and the image. Extensive experiments are conducted on three\nsocial media news benchmarks including N24News, VisualNews, and GoodNews, the\nresults shows that our method surpasses the competition methods with a clear\nmargin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in image-text matching have been notable, yet prevailing\nmodels predominantly cater to broad queries and struggle with accommodating\nfine-grained query intention. In this paper, we work towards the\n\\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM),\na task that the text and image involve specific entity-related information. The\nchallenge of this task mainly lies in the larger semantic gap in entity\nassociation modeling, comparing with the general image-text matching problem.To\nnarrow the huge semantic gap between the entity-centric text and the images, we\ntake the fundamental CLIP as the backbone and devise a multimodal attentive\ncontrastive learning framework to tam CLIP to adapt EITM problem, developing a\nmodel named EntityCLIP. The key of our multimodal attentive contrastive\nlearning is to generate interpretive explanation text using Large Language\nModels (LLMs) as the bridge clues. In specific, we proceed by extracting\nexplanatory text from off-the-shelf LLMs. This explanation text, coupled with\nthe image and text, is then input into our specially crafted Multimodal\nAttentive Experts (MMAE) module, which effectively integrates explanation texts\nto narrow the gap of the entity-related text and image in a shared semantic\nspace. Building on the enriched features derived from MMAE, we further design\nan effective Gated Integrative Image-text Matching (GI-ITM) strategy. The\nGI-ITM employs an adaptive gating mechanism to aggregate MMAE's features,\nsubsequently applying image-text matching constraints to steer the alignment\nbetween the text and the image. Extensive experiments are conducted on three\nsocial media news benchmarks including N24News, VisualNews, and GoodNews, the\nresults shows that our method surpasses the competition methods with a clear\nmargin."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wang"
                    },
                    {
                        "name": "Yaxiong Wang"
                    },
                    {
                        "name": "Lianwei Wu"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Zhun Zhong"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17809v1",
                "updated": "2024-10-23T12:11:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    11,
                    26,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T12:11:26Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    11,
                    26,
                    2,
                    297,
                    0
                ],
                "title": "An Intelligent Agentic System for Complex Image Restoration Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Intelligent Agentic System for Complex Image Restoration Problems"
                },
                "summary": "Real-world image restoration (IR) is inherently complex and often requires\ncombining multiple specialized models to address diverse degradations. Inspired\nby human problem-solving, we propose AgenticIR, an agentic system that mimics\nthe human approach to image processing by following five key stages:\nPerception, Scheduling, Execution, Reflection, and Rescheduling. AgenticIR\nleverages large language models (LLMs) and vision-language models (VLMs) that\ninteract via text generation to dynamically operate a toolbox of IR models. We\nfine-tune VLMs for image quality analysis and employ LLMs for reasoning,\nguiding the system step by step. To compensate for LLMs' lack of specific IR\nknowledge and experience, we introduce a self-exploration method, allowing the\nLLM to observe and summarize restoration results into referenceable documents.\nExperiments demonstrate AgenticIR's potential in handling complex IR tasks,\nrepresenting a promising path toward achieving general intelligence in visual\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image restoration (IR) is inherently complex and often requires\ncombining multiple specialized models to address diverse degradations. Inspired\nby human problem-solving, we propose AgenticIR, an agentic system that mimics\nthe human approach to image processing by following five key stages:\nPerception, Scheduling, Execution, Reflection, and Rescheduling. AgenticIR\nleverages large language models (LLMs) and vision-language models (VLMs) that\ninteract via text generation to dynamically operate a toolbox of IR models. We\nfine-tune VLMs for image quality analysis and employ LLMs for reasoning,\nguiding the system step by step. To compensate for LLMs' lack of specific IR\nknowledge and experience, we introduce a self-exploration method, allowing the\nLLM to observe and summarize restoration results into referenceable documents.\nExperiments demonstrate AgenticIR's potential in handling complex IR tasks,\nrepresenting a promising path toward achieving general intelligence in visual\nprocessing."
                },
                "authors": [
                    {
                        "name": "Kaiwen Zhu"
                    },
                    {
                        "name": "Jinjin Gu"
                    },
                    {
                        "name": "Zhiyuan You"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Chao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Chao Dong"
                },
                "author": "Chao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17806v1",
                "updated": "2024-10-23T12:04:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    4,
                    52,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T12:04:52Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    4,
                    52,
                    2,
                    297,
                    0
                ],
                "title": "A multi-frequency, multi-epoch radio continuum study of the Arches\n  cluster with the VLA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A multi-frequency, multi-epoch radio continuum study of the Arches\n  cluster with the VLA"
                },
                "summary": "The Arches cluster, one of the most massive clusters in the Milky Way, is\nlocated about 30 pc in projection from the central massive black hole. With its\nhigh mass, young age, and location in the Galaxy's most extreme star forming\nenvironment, the Arches is an extraordinary laboratory to study massive stars\nand clusters. Our objective is to improve our knowledge of the properties of\nmassive stars and the Arches cluster through high angular resolution radio\ncontinuum studies. We observed the Arches cluster with the Karl G. Jansky Very\nLarge Array in the C- and X-bands throughout 2016, 2018, and 2022. We used the\nA-configuration to achieve the highest possible angular resolution and\ncross-matched the detected point sources with stars detected in the infrared,\nusing proper motion catalogues to ensure cluster membership. We report the most\nextensive radio point source catalogue of the cluster to date, with a total of\n25 radio detections. We also created the deepest radio images of the cluster so\nfar. Most of our stellar radio sources (12/18) show a positive spectral index,\nindicating that the dominant emission process is free-free thermal radiation,\nwhich probably originates from stellar winds. We found that radio variability\nis more frequent than what was inferred from previous observations, affecting\nup to 60% of the sources associated with bright stellar counterparts. We\npropose four of our detections (F6, F18, F19 and F26) as primary candidates for\ncolliding-wind binaries based on their consistent flat-to-negative spectral\nindex. We classify F7, F9, F12, F14, and F55 as secondary colliding wind binary\ncandidates based on their high flux and/or spectral index variability, and\nX-ray counterparts. Thus, we infer a 61% multiplicity fraction for the Arches\ncluster radio-stars when combining our findings with recent infrared radial\nvelocity studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Arches cluster, one of the most massive clusters in the Milky Way, is\nlocated about 30 pc in projection from the central massive black hole. With its\nhigh mass, young age, and location in the Galaxy's most extreme star forming\nenvironment, the Arches is an extraordinary laboratory to study massive stars\nand clusters. Our objective is to improve our knowledge of the properties of\nmassive stars and the Arches cluster through high angular resolution radio\ncontinuum studies. We observed the Arches cluster with the Karl G. Jansky Very\nLarge Array in the C- and X-bands throughout 2016, 2018, and 2022. We used the\nA-configuration to achieve the highest possible angular resolution and\ncross-matched the detected point sources with stars detected in the infrared,\nusing proper motion catalogues to ensure cluster membership. We report the most\nextensive radio point source catalogue of the cluster to date, with a total of\n25 radio detections. We also created the deepest radio images of the cluster so\nfar. Most of our stellar radio sources (12/18) show a positive spectral index,\nindicating that the dominant emission process is free-free thermal radiation,\nwhich probably originates from stellar winds. We found that radio variability\nis more frequent than what was inferred from previous observations, affecting\nup to 60% of the sources associated with bright stellar counterparts. We\npropose four of our detections (F6, F18, F19 and F26) as primary candidates for\ncolliding-wind binaries based on their consistent flat-to-negative spectral\nindex. We classify F7, F9, F12, F14, and F55 as secondary colliding wind binary\ncandidates based on their high flux and/or spectral index variability, and\nX-ray counterparts. Thus, we infer a 61% multiplicity fraction for the Arches\ncluster radio-stars when combining our findings with recent infrared radial\nvelocity studies."
                },
                "authors": [
                    {
                        "name": "M. Cano-Gonzlez"
                    },
                    {
                        "name": "R. Schdel"
                    },
                    {
                        "name": "A. Alberdi"
                    },
                    {
                        "name": "J. Moldn"
                    },
                    {
                        "name": "M. A. Prez-Torres"
                    },
                    {
                        "name": "F. Najarro"
                    },
                    {
                        "name": "A. T. Gallego-Calvente"
                    }
                ],
                "author_detail": {
                    "name": "A. T. Gallego-Calvente"
                },
                "author": "A. T. Gallego-Calvente",
                "arxiv_comment": "Accepted for publication in A&A (21st November 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17799v1",
                "updated": "2024-10-23T11:58:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    58,
                    58,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:58:58Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    58,
                    58,
                    2,
                    297,
                    0
                ],
                "title": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation"
                },
                "summary": "Full-duplex spoken dialogue systems significantly advance over traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex communication\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text-based large language model (LLM) backbone into a speech-text\ndialogue LLM, capable of generating text and speech in real time, without\nmodifying the architecture of the backbone LLM. The training process comprises\nthree stages: modality alignment, half-duplex dialogue learning, and\nfull-duplex dialogue learning. Throughout all training stages, we standardize\nthe data using a flattening operation, which allows us to unify the training\nmethods and the model architecture across different modalities and tasks. Our\napproach offers a straightforward modeling technique and a promising research\ndirection for developing efficient and natural end-to-end full-duplex spoken\ndialogue systems. Audio samples of dialogues generated by OmniFlatten can be\nfound at this web site (https://omniflatten.github.io/).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-duplex spoken dialogue systems significantly advance over traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex communication\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text-based large language model (LLM) backbone into a speech-text\ndialogue LLM, capable of generating text and speech in real time, without\nmodifying the architecture of the backbone LLM. The training process comprises\nthree stages: modality alignment, half-duplex dialogue learning, and\nfull-duplex dialogue learning. Throughout all training stages, we standardize\nthe data using a flattening operation, which allows us to unify the training\nmethods and the model architecture across different modalities and tasks. Our\napproach offers a straightforward modeling technique and a promising research\ndirection for developing efficient and natural end-to-end full-duplex spoken\ndialogue systems. Audio samples of dialogues generated by OmniFlatten can be\nfound at this web site (https://omniflatten.github.io/)."
                },
                "authors": [
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Luyao Cheng"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Siqi Zheng"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Chaohong Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chaohong Tan"
                },
                "author": "Chaohong Tan",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17153v2",
                "updated": "2024-10-23T11:56:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    56,
                    26,
                    2,
                    297,
                    0
                ],
                "published": "2024-04-26T04:55:35Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    4,
                    55,
                    35,
                    4,
                    117,
                    0
                ],
                "title": "A Unified Debugging Approach via LLM-Based Multi-Agent Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Debugging Approach via LLM-Based Multi-Agent Synergy"
                },
                "summary": "Software debugging is a time-consuming endeavor involving a series of steps,\nsuch as fault localization and patch generation, each requiring thorough\nanalysis and a deep understanding of the underlying logic. While large language\nmodels (LLMs) demonstrate promising potential in coding tasks, their\nperformance in debugging remains limited. Current LLM-based methods often focus\non isolated steps and struggle with complex bugs. In this paper, we propose the\nfirst end-to-end framework, FixAgent, for unified debugging through multi-agent\nsynergy. It mimics the entire cognitive processes of developers, with each\nagent specialized as a particular component of this process rather than\nmirroring the actions of an independent expert as in previous multi-agent\nsystems. Agents are coordinated through a three-level design, following a\ncognitive model of debugging, allowing adaptive handling of bugs with varying\ncomplexities. Experiments on extensive benchmarks demonstrate that FixAgent\nsignificantly outperforms state-of-the-art repair methods, fixing 1.25$\\times$\nto 2.56$\\times$ bugs on the repo-level benchmark, Defects4J. This performance\nis achieved without requiring ground-truth root-cause code statements, unlike\nthe baselines. Our source code is available on\nhttps://github.com/AcceptePapier/UniDebugger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software debugging is a time-consuming endeavor involving a series of steps,\nsuch as fault localization and patch generation, each requiring thorough\nanalysis and a deep understanding of the underlying logic. While large language\nmodels (LLMs) demonstrate promising potential in coding tasks, their\nperformance in debugging remains limited. Current LLM-based methods often focus\non isolated steps and struggle with complex bugs. In this paper, we propose the\nfirst end-to-end framework, FixAgent, for unified debugging through multi-agent\nsynergy. It mimics the entire cognitive processes of developers, with each\nagent specialized as a particular component of this process rather than\nmirroring the actions of an independent expert as in previous multi-agent\nsystems. Agents are coordinated through a three-level design, following a\ncognitive model of debugging, allowing adaptive handling of bugs with varying\ncomplexities. Experiments on extensive benchmarks demonstrate that FixAgent\nsignificantly outperforms state-of-the-art repair methods, fixing 1.25$\\times$\nto 2.56$\\times$ bugs on the repo-level benchmark, Defects4J. This performance\nis achieved without requiring ground-truth root-cause code statements, unlike\nthe baselines. Our source code is available on\nhttps://github.com/AcceptePapier/UniDebugger."
                },
                "authors": [
                    {
                        "name": "Cheryl Lee"
                    },
                    {
                        "name": "Chunqiu Steven Xia"
                    },
                    {
                        "name": "Longji Yang"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Zhouruixin Zhu"
                    },
                    {
                        "name": "Lingming Zhang"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14806v2",
                "updated": "2024-10-23T11:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    49,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-18T18:22:23Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    18,
                    22,
                    23,
                    4,
                    292,
                    0
                ],
                "title": "Characterizing seismic isolation using convolutional neural networks and\n  Wiener filters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing seismic isolation using convolutional neural networks and\n  Wiener filters"
                },
                "summary": "We investigate seismic motion propagation through a passively isolated\nmechanical system, using Wiener filters and convolutional neural networks with\ntime-dilation layers. The goal of this study was to explore the capabilities of\nneural networks and Wiener filters in characterizing a mechanical system from\nthe measurements. The mechanical system used is a testbed facility for\ntechnology development for current and future gravitational wave detectors,\n\"VATIGrav\", currently being commissioned at University of Hamburg. It consists\nof a large vacuum chamber mounted on four active vibration isolators with an\noptical table inside, mounted on four passive vibration isolators. In this\npaper we have used seismic data recorded on the ground and on the optical table\ninside the chamber. The data were divided in 6 hours for training and another 6\nhours for validation, focusing on inferring 150-second stretches of time series\nof table motion from the ground motion in the frequency range from\n$0.1~\\mathrm{Hz}$ to about $50~\\mathrm{Hz}$. We compare the performance of a\nneural network with FTT-based loss function and with Huber loss function to\nsingle-input, single-output (SISO) and multiple-input, single-output (MISO)\nWiener filters. To be able to compute very large MISO Wiener filters (with\n15,000 taps) we have optimized the calculations exploiting block-Toeplitz\nstructure of the matrix in Wiener-Hopf equations. We find that for the given\ntask SISO Wiener filters outperform MISO Wiener filters, mostly due to low\ncoherence between different motion axes. Neural network trained with Huber loss\nperforms slightly worse than Wiener filters. Neural network with FFT-based loss\noutperforms Wiener filters in some frequency regions, particularly with low\namplitudes and reduced coherence, while it tends to slightly underestimate the\npeaks, where Wiener filters perform better.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate seismic motion propagation through a passively isolated\nmechanical system, using Wiener filters and convolutional neural networks with\ntime-dilation layers. The goal of this study was to explore the capabilities of\nneural networks and Wiener filters in characterizing a mechanical system from\nthe measurements. The mechanical system used is a testbed facility for\ntechnology development for current and future gravitational wave detectors,\n\"VATIGrav\", currently being commissioned at University of Hamburg. It consists\nof a large vacuum chamber mounted on four active vibration isolators with an\noptical table inside, mounted on four passive vibration isolators. In this\npaper we have used seismic data recorded on the ground and on the optical table\ninside the chamber. The data were divided in 6 hours for training and another 6\nhours for validation, focusing on inferring 150-second stretches of time series\nof table motion from the ground motion in the frequency range from\n$0.1~\\mathrm{Hz}$ to about $50~\\mathrm{Hz}$. We compare the performance of a\nneural network with FTT-based loss function and with Huber loss function to\nsingle-input, single-output (SISO) and multiple-input, single-output (MISO)\nWiener filters. To be able to compute very large MISO Wiener filters (with\n15,000 taps) we have optimized the calculations exploiting block-Toeplitz\nstructure of the matrix in Wiener-Hopf equations. We find that for the given\ntask SISO Wiener filters outperform MISO Wiener filters, mostly due to low\ncoherence between different motion axes. Neural network trained with Huber loss\nperforms slightly worse than Wiener filters. Neural network with FFT-based loss\noutperforms Wiener filters in some frequency regions, particularly with low\namplitudes and reduced coherence, while it tends to slightly underestimate the\npeaks, where Wiener filters perform better."
                },
                "authors": [
                    {
                        "name": "Artem Basalaev"
                    },
                    {
                        "name": "Jan-Niklas Feldhusen"
                    },
                    {
                        "name": "Oliver Gerberding"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Gerberding"
                },
                "author": "Oliver Gerberding",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17787v1",
                "updated": "2024-10-23T11:37:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    37,
                    20,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:37:20Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    37,
                    20,
                    2,
                    297,
                    0
                ],
                "title": "Large Language Models Engineer Too Many Simple Features For Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Engineer Too Many Simple Features For Tabular Data"
                },
                "summary": "Tabular machine learning problems often require time-consuming and\nlabor-intensive feature engineering. Recent efforts have focused on using large\nlanguage models (LLMs) to capitalize on their potential domain knowledge. At\nthe same time, researchers have observed ethically concerning negative biases\nin other LLM-related use cases, such as text generation. These developments\nmotivated us to investigate whether LLMs exhibit a bias that negatively impacts\nthe performance of feature engineering. While not ethically concerning, such a\nbias could hinder practitioners from fully utilizing LLMs for automated data\nscience. Therefore, we propose a method to detect potential biases by detecting\nanomalies in the frequency of operators (e.g., adding two features) suggested\nby LLMs when engineering new features. Our experiments evaluate the bias of\nfour LLMs, two big frontier and two small open-source models, across 27 tabular\ndatasets. Our results indicate that LLMs are biased toward simple operators,\nsuch as addition, and can fail to utilize more complex operators, such as\ngrouping followed by aggregations. Furthermore, the bias can negatively impact\nthe predictive performance when using LLM-generated features. Our results call\nfor mitigating bias when using LLMs for feature engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular machine learning problems often require time-consuming and\nlabor-intensive feature engineering. Recent efforts have focused on using large\nlanguage models (LLMs) to capitalize on their potential domain knowledge. At\nthe same time, researchers have observed ethically concerning negative biases\nin other LLM-related use cases, such as text generation. These developments\nmotivated us to investigate whether LLMs exhibit a bias that negatively impacts\nthe performance of feature engineering. While not ethically concerning, such a\nbias could hinder practitioners from fully utilizing LLMs for automated data\nscience. Therefore, we propose a method to detect potential biases by detecting\nanomalies in the frequency of operators (e.g., adding two features) suggested\nby LLMs when engineering new features. Our experiments evaluate the bias of\nfour LLMs, two big frontier and two small open-source models, across 27 tabular\ndatasets. Our results indicate that LLMs are biased toward simple operators,\nsuch as addition, and can fail to utilize more complex operators, such as\ngrouping followed by aggregations. Furthermore, the bias can negatively impact\nthe predictive performance when using LLM-generated features. Our results call\nfor mitigating bias when using LLMs for feature engineering."
                },
                "authors": [
                    {
                        "name": "Jaris Kken"
                    },
                    {
                        "name": "Lennart Purucker"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05804v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05804v5",
                "updated": "2024-10-24T07:07:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    7,
                    43,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-09T14:42:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    14,
                    42,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning"
                },
                "summary": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey."
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05804v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05804v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17785v1",
                "updated": "2024-10-23T11:35:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    35,
                    44,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:35:44Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    35,
                    44,
                    2,
                    297,
                    0
                ],
                "title": "TranSPORTmer: A Holistic Approach to Trajectory Understanding in\n  Multi-Agent Sports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TranSPORTmer: A Holistic Approach to Trajectory Understanding in\n  Multi-Agent Sports"
                },
                "summary": "Understanding trajectories in multi-agent scenarios requires addressing\nvarious tasks, including predicting future movements, imputing missing\nobservations, inferring the status of unseen agents, and classifying different\nglobal states. Traditional data-driven approaches often handle these tasks\nseparately with specialized models. We introduce TranSPORTmer, a unified\ntransformer-based framework capable of addressing all these tasks, showcasing\nits application to the intricate dynamics of multi-agent sports scenarios like\nsoccer and basketball. Using Set Attention Blocks, TranSPORTmer effectively\ncaptures temporal dynamics and social interactions in an equivariant manner.\nThe model's tasks are guided by an input mask that conceals missing or\nyet-to-be-predicted observations. Additionally, we introduce a CLS extra agent\nto classify states along soccer trajectories, including passes, possessions,\nuncontrolled states, and out-of-play intervals, contributing to an enhancement\nin modeling trajectories. Evaluations on soccer and basketball datasets show\nthat TranSPORTmer outperforms state-of-the-art task-specific models in player\nforecasting, player forecasting-imputation, ball inference, and ball\nimputation. https://youtu.be/8VtSRm8oGoE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding trajectories in multi-agent scenarios requires addressing\nvarious tasks, including predicting future movements, imputing missing\nobservations, inferring the status of unseen agents, and classifying different\nglobal states. Traditional data-driven approaches often handle these tasks\nseparately with specialized models. We introduce TranSPORTmer, a unified\ntransformer-based framework capable of addressing all these tasks, showcasing\nits application to the intricate dynamics of multi-agent sports scenarios like\nsoccer and basketball. Using Set Attention Blocks, TranSPORTmer effectively\ncaptures temporal dynamics and social interactions in an equivariant manner.\nThe model's tasks are guided by an input mask that conceals missing or\nyet-to-be-predicted observations. Additionally, we introduce a CLS extra agent\nto classify states along soccer trajectories, including passes, possessions,\nuncontrolled states, and out-of-play intervals, contributing to an enhancement\nin modeling trajectories. Evaluations on soccer and basketball datasets show\nthat TranSPORTmer outperforms state-of-the-art task-specific models in player\nforecasting, player forecasting-imputation, ball inference, and ball\nimputation. https://youtu.be/8VtSRm8oGoE"
                },
                "authors": [
                    {
                        "name": "Guillem Capellera"
                    },
                    {
                        "name": "Luis Ferraz"
                    },
                    {
                        "name": "Antonio Rubio"
                    },
                    {
                        "name": "Antonio Agudo"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_comment": "Accepted to ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17782v1",
                "updated": "2024-10-23T11:32:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    32,
                    38,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:32:38Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    32,
                    38,
                    2,
                    297,
                    0
                ],
                "title": "Pointer: An Energy-Efficient ReRAM-based Point Cloud Recognition\n  Accelerator with Inter-layer and Intra-layer Optimizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pointer: An Energy-Efficient ReRAM-based Point Cloud Recognition\n  Accelerator with Inter-layer and Intra-layer Optimizations"
                },
                "summary": "Point cloud is an important data structure for a wide range of applications,\nincluding robotics, AR/VR, and autonomous driving. To process the point cloud,\nmany deep-learning-based point cloud recognition algorithms have been proposed.\nHowever, to meet the requirement of applications like autonomous driving, the\nalgorithm must be fast enough, rendering accelerators necessary at the\ninference stage. But existing point cloud accelerators are still inefficient\ndue to two challenges. First, the multi-layer perceptron (MLP) during feature\ncomputation is the performance bottleneck. Second, the feature vector fetching\noperation incurs heavy DRAM access.\n  In this paper, we propose Pointer, an efficient Resistive Random Access\nMemory (ReRAM)-based point cloud recognition accelerator with inter- and\nintra-layer optimizations. It proposes three techniques for point cloud\nacceleration. First, Pointer adopts ReRAM-based architecture to significantly\naccelerate the MLP in feature computation. Second, to reduce DRAM access,\nPointer proposes inter-layer coordination. It schedules the next layer to fetch\nthe results of the previous layer as soon as they are available, which allows\non-chip fetching thus reduces DRAM access. Third, Pointer proposes\ntopology-aware intra-layer reordering, which improves the execution order for\nbetter data locality. Pointer proves to achieve 40x to 393x speedup and 22x to\n163x energy efficiency over prior accelerators without any accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud is an important data structure for a wide range of applications,\nincluding robotics, AR/VR, and autonomous driving. To process the point cloud,\nmany deep-learning-based point cloud recognition algorithms have been proposed.\nHowever, to meet the requirement of applications like autonomous driving, the\nalgorithm must be fast enough, rendering accelerators necessary at the\ninference stage. But existing point cloud accelerators are still inefficient\ndue to two challenges. First, the multi-layer perceptron (MLP) during feature\ncomputation is the performance bottleneck. Second, the feature vector fetching\noperation incurs heavy DRAM access.\n  In this paper, we propose Pointer, an efficient Resistive Random Access\nMemory (ReRAM)-based point cloud recognition accelerator with inter- and\nintra-layer optimizations. It proposes three techniques for point cloud\nacceleration. First, Pointer adopts ReRAM-based architecture to significantly\naccelerate the MLP in feature computation. Second, to reduce DRAM access,\nPointer proposes inter-layer coordination. It schedules the next layer to fetch\nthe results of the previous layer as soon as they are available, which allows\non-chip fetching thus reduces DRAM access. Third, Pointer proposes\ntopology-aware intra-layer reordering, which improves the execution order for\nbetter data locality. Pointer proves to achieve 40x to 393x speedup and 22x to\n163x energy efficiency over prior accelerators without any accuracy loss."
                },
                "authors": [
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "Published in ASPDAC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17781v1",
                "updated": "2024-10-23T11:31:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    31,
                    52,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:31:52Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    31,
                    52,
                    2,
                    297,
                    0
                ],
                "title": "Evaluating Explanations Through LLMs: Beyond Traditional User Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Explanations Through LLMs: Beyond Traditional User Studies"
                },
                "summary": "As AI becomes fundamental in sectors like healthcare, explainable AI (XAI)\ntools are essential for trust and transparency. However, traditional user\nstudies used to evaluate these tools are often costly, time consuming, and\ndifficult to scale. In this paper, we explore the use of Large Language Models\n(LLMs) to replicate human participants to help streamline XAI evaluation. We\nreproduce a user study comparing counterfactual and causal explanations,\nreplicating human participants with seven LLMs under various settings. Our\nresults show that (i) LLMs can replicate most conclusions from the original\nstudy, (ii) different LLMs yield varying levels of alignment in the results,\nand (iii) experimental factors such as LLM memory and output variability affect\nalignment with human responses. These initial findings suggest that LLMs could\nprovide a scalable and cost-effective way to simplify qualitative XAI\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI becomes fundamental in sectors like healthcare, explainable AI (XAI)\ntools are essential for trust and transparency. However, traditional user\nstudies used to evaluate these tools are often costly, time consuming, and\ndifficult to scale. In this paper, we explore the use of Large Language Models\n(LLMs) to replicate human participants to help streamline XAI evaluation. We\nreproduce a user study comparing counterfactual and causal explanations,\nreplicating human participants with seven LLMs under various settings. Our\nresults show that (i) LLMs can replicate most conclusions from the original\nstudy, (ii) different LLMs yield varying levels of alignment in the results,\nand (iii) experimental factors such as LLM memory and output variability affect\nalignment with human responses. These initial findings suggest that LLMs could\nprovide a scalable and cost-effective way to simplify qualitative XAI\nevaluation."
                },
                "authors": [
                    {
                        "name": "Francesco Bombassei De Bona"
                    },
                    {
                        "name": "Gabriele Dominici"
                    },
                    {
                        "name": "Tim Miller"
                    },
                    {
                        "name": "Marc Langheinrich"
                    },
                    {
                        "name": "Martin Gjoreski"
                    }
                ],
                "author_detail": {
                    "name": "Martin Gjoreski"
                },
                "author": "Martin Gjoreski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17779v1",
                "updated": "2024-10-23T11:31:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    31,
                    6,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:31:06Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    31,
                    6,
                    2,
                    297,
                    0
                ],
                "title": "ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language\n  Tuning"
                },
                "summary": "Recent advancements in multimodal fusion have witnessed the remarkable\nsuccess of vision-language (VL) models, which excel in various multimodal\napplications such as image captioning and visual question answering. However,\nbuilding VL models requires substantial hardware resources, where efficiency is\nrestricted by two key factors: the extended input sequence of the language\nmodel with vision features demands more computational operations, and a large\nnumber of additional learnable parameters increase memory complexity. These\nchallenges significantly restrict the broader applicability of such models. To\nbridge this gap, we propose ADEM-VL, an efficient vision-language method that\ntunes VL models based on pretrained large language models (LLMs) by adopting a\nparameter-free cross-attention mechanism for similarity measurements in\nmultimodal fusion. This approach only requires embedding vision features into\nthe language space, significantly reducing the number of trainable parameters\nand accelerating both training and inference speeds. To enhance representation\nlearning in fusion module, we introduce an efficient multiscale feature\ngeneration scheme that requires only a single forward pass through the vision\nencoder. Moreover, we propose an adaptive fusion scheme that dynamically\ndiscards less relevant visual information for each text token based on its\nattention score. This ensures that the fusion process prioritizes the most\npertinent visual features. With experiments on various tasks including visual\nquestion answering, image captioning, and instruction-following, we demonstrate\nthat our framework outperforms existing approaches. Specifically, our method\nsurpasses existing methods by an average accuracy of 0.77% on ScienceQA\ndataset, with reduced training and inference latency, demonstrating the\nsuperiority of our framework. The code is available at\nhttps://github.com/Hao840/ADEM-VL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal fusion have witnessed the remarkable\nsuccess of vision-language (VL) models, which excel in various multimodal\napplications such as image captioning and visual question answering. However,\nbuilding VL models requires substantial hardware resources, where efficiency is\nrestricted by two key factors: the extended input sequence of the language\nmodel with vision features demands more computational operations, and a large\nnumber of additional learnable parameters increase memory complexity. These\nchallenges significantly restrict the broader applicability of such models. To\nbridge this gap, we propose ADEM-VL, an efficient vision-language method that\ntunes VL models based on pretrained large language models (LLMs) by adopting a\nparameter-free cross-attention mechanism for similarity measurements in\nmultimodal fusion. This approach only requires embedding vision features into\nthe language space, significantly reducing the number of trainable parameters\nand accelerating both training and inference speeds. To enhance representation\nlearning in fusion module, we introduce an efficient multiscale feature\ngeneration scheme that requires only a single forward pass through the vision\nencoder. Moreover, we propose an adaptive fusion scheme that dynamically\ndiscards less relevant visual information for each text token based on its\nattention score. This ensures that the fusion process prioritizes the most\npertinent visual features. With experiments on various tasks including visual\nquestion answering, image captioning, and instruction-following, we demonstrate\nthat our framework outperforms existing approaches. Specifically, our method\nsurpasses existing methods by an average accuracy of 0.77% on ScienceQA\ndataset, with reduced training and inference latency, demonstrating the\nsuperiority of our framework. The code is available at\nhttps://github.com/Hao840/ADEM-VL."
                },
                "authors": [
                    {
                        "name": "Zhiwei Hao"
                    },
                    {
                        "name": "Jianyuan Guo"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17770v1",
                "updated": "2024-10-23T11:19:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    19,
                    8,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:19:08Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    19,
                    8,
                    2,
                    297,
                    0
                ],
                "title": "Locating Information in Large Language Models via Random Matrix Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locating Information in Large Language Models via Random Matrix Theory"
                },
                "summary": "As large language models (LLMs) become central to AI applications, gaining a\ndeeper understanding of their inner workings is increasingly important. In this\nwork, we analyze the weight matrices of pretrained transformer models --\nspecifically BERT and Llama -- using random matrix theory (RMT) as a\nzero-information hypothesis. While randomly initialized weights perfectly agree\nwith RMT predictions, deviations emerge after training, allowing us to locate\nlearned structures within the models. We identify layer-type specific behaviors\nthat are consistent across all blocks and architectures considered. By\npinpointing regions that deviate from RMT predictions, we highlight areas of\nfeature learning and confirm this through comparisons with the activation\ncovariance matrices of the corresponding layers. Our method provides a\ndiagnostic tool for identifying relevant regions in transformer weights using\nonly the trained matrices. Additionally, we address the ongoing debate\nregarding the significance of small singular values in the context of\nfine-tuning and alignment in LLMs. Our findings reveal that, after fine-tuning,\nsmall singular values play a crucial role in the models' capabilities,\nsuggesting that removing them in an already aligned transformer can be\ndetrimental, as it may compromise model alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become central to AI applications, gaining a\ndeeper understanding of their inner workings is increasingly important. In this\nwork, we analyze the weight matrices of pretrained transformer models --\nspecifically BERT and Llama -- using random matrix theory (RMT) as a\nzero-information hypothesis. While randomly initialized weights perfectly agree\nwith RMT predictions, deviations emerge after training, allowing us to locate\nlearned structures within the models. We identify layer-type specific behaviors\nthat are consistent across all blocks and architectures considered. By\npinpointing regions that deviate from RMT predictions, we highlight areas of\nfeature learning and confirm this through comparisons with the activation\ncovariance matrices of the corresponding layers. Our method provides a\ndiagnostic tool for identifying relevant regions in transformer weights using\nonly the trained matrices. Additionally, we address the ongoing debate\nregarding the significance of small singular values in the context of\nfine-tuning and alignment in LLMs. Our findings reveal that, after fine-tuning,\nsmall singular values play a crucial role in the models' capabilities,\nsuggesting that removing them in an already aligned transformer can be\ndetrimental, as it may compromise model alignment."
                },
                "authors": [
                    {
                        "name": "Max Staats"
                    },
                    {
                        "name": "Matthias Thamm"
                    },
                    {
                        "name": "Bernd Rosenow"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Rosenow"
                },
                "author": "Bernd Rosenow",
                "arxiv_comment": "17 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16144v2",
                "updated": "2024-10-23T11:17:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    17,
                    42,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-21T16:14:57Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    14,
                    57,
                    0,
                    295,
                    0
                ],
                "title": "1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on\n  CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on\n  CPUs"
                },
                "summary": "Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and\nBitNet b1.58, present a promising approach to enhancing the efficiency of LLMs\nin terms of speed and energy consumption. These developments also enable local\nLLM deployment across a broad range of devices. In this work, we introduce\nbitnet.cpp, a tailored software stack designed to unlock the full potential of\n1-bit LLMs. Specifically, we develop a set of kernels to support fast and\nlossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments\ndemonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x\nto 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model\nsizes. The code is available at https://github.com/microsoft/BitNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and\nBitNet b1.58, present a promising approach to enhancing the efficiency of LLMs\nin terms of speed and energy consumption. These developments also enable local\nLLM deployment across a broad range of devices. In this work, we introduce\nbitnet.cpp, a tailored software stack designed to unlock the full potential of\n1-bit LLMs. Specifically, we develop a set of kernels to support fast and\nlossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments\ndemonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x\nto 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model\nsizes. The code is available at https://github.com/microsoft/BitNet."
                },
                "authors": [
                    {
                        "name": "Jinheng Wang"
                    },
                    {
                        "name": "Hansong Zhou"
                    },
                    {
                        "name": "Ting Song"
                    },
                    {
                        "name": "Shaoguang Mao"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13247v2",
                "updated": "2024-10-23T11:09:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    9,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T06:14:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    6,
                    14,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Collaborative AI in Sentiment Analysis: System Architecture, Data\n  Prediction and Deployment Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative AI in Sentiment Analysis: System Architecture, Data\n  Prediction and Deployment Strategies"
                },
                "summary": "The advancement of large language model (LLM) based artificial intelligence\ntechnologies has been a game-changer, particularly in sentiment analysis. This\nprogress has enabled a shift from highly specialized research environments to\npractical, widespread applications within the industry. However, integrating\ndiverse AI models for processing complex multimodal data and the associated\nhigh costs of feature extraction presents significant challenges. Motivated by\nthe marketing oriented software development +needs, our study introduces a\ncollaborative AI framework designed to efficiently distribute and resolve tasks\nacross various AI systems to address these issues. Initially, we elucidate the\nkey solutions derived from our development process, highlighting the role of\ngenerative AI models like \\emph{chatgpt}, \\emph{google gemini} in simplifying\nintricate sentiment analysis tasks into manageable, phased objectives.\nFurthermore, we present a detailed case study utilizing our collaborative AI\nsystem in edge and cloud, showcasing its effectiveness in analyzing sentiments\nacross diverse online media channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language model (LLM) based artificial intelligence\ntechnologies has been a game-changer, particularly in sentiment analysis. This\nprogress has enabled a shift from highly specialized research environments to\npractical, widespread applications within the industry. However, integrating\ndiverse AI models for processing complex multimodal data and the associated\nhigh costs of feature extraction presents significant challenges. Motivated by\nthe marketing oriented software development +needs, our study introduces a\ncollaborative AI framework designed to efficiently distribute and resolve tasks\nacross various AI systems to address these issues. Initially, we elucidate the\nkey solutions derived from our development process, highlighting the role of\ngenerative AI models like \\emph{chatgpt}, \\emph{google gemini} in simplifying\nintricate sentiment analysis tasks into manageable, phased objectives.\nFurthermore, we present a detailed case study utilizing our collaborative AI\nsystem in edge and cloud, showcasing its effectiveness in analyzing sentiments\nacross diverse online media channels."
                },
                "authors": [
                    {
                        "name": "Chaofeng Zhang"
                    },
                    {
                        "name": "Jia Hou"
                    },
                    {
                        "name": "Xueting Tan"
                    },
                    {
                        "name": "Gaolei Li"
                    },
                    {
                        "name": "Caijuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Caijuan Chen"
                },
                "author": "Caijuan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17765v1",
                "updated": "2024-10-23T11:06:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    6,
                    36,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:06:36Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    6,
                    36,
                    2,
                    297,
                    0
                ],
                "title": "Faster Language Models with Better Multi-Token Prediction Using Tensor\n  Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Language Models with Better Multi-Token Prediction Using Tensor\n  Decomposition"
                },
                "summary": "We propose a new model for multi-token prediction in transformers, aiming to\nenhance sampling efficiency without compromising accuracy. Motivated by recent\nwork that predicts the probabilities of subsequent tokens using multiple heads,\nwe connect this approach to rank-$1$ canonical tensor decomposition. By\ngeneralizing it to a rank-$r$ canonical probability decomposition, we develop\nan improved model that predicts multiple tokens simultaneously. This model can\nalso be interpreted as a mixture of experts, allowing us to leverage successful\ntechniques from that domain for efficient and robust training. Importantly, the\noverall overhead for training and sampling remains low. Our method demonstrates\nsignificant improvements in inference speed for both text and code generation\ntasks, proving particularly beneficial within the self-speculative decoding\nparadigm. It maintains its effectiveness across various model sizes and\ntraining epochs, highlighting its robustness and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new model for multi-token prediction in transformers, aiming to\nenhance sampling efficiency without compromising accuracy. Motivated by recent\nwork that predicts the probabilities of subsequent tokens using multiple heads,\nwe connect this approach to rank-$1$ canonical tensor decomposition. By\ngeneralizing it to a rank-$r$ canonical probability decomposition, we develop\nan improved model that predicts multiple tokens simultaneously. This model can\nalso be interpreted as a mixture of experts, allowing us to leverage successful\ntechniques from that domain for efficient and robust training. Importantly, the\noverall overhead for training and sampling remains low. Our method demonstrates\nsignificant improvements in inference speed for both text and code generation\ntasks, proving particularly beneficial within the self-speculative decoding\nparadigm. It maintains its effectiveness across various model sizes and\ntraining epochs, highlighting its robustness and scalability."
                },
                "authors": [
                    {
                        "name": "Artem Basharin"
                    },
                    {
                        "name": "Andrei Chertkov"
                    },
                    {
                        "name": "Ivan Oseledets"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Oseledets"
                },
                "author": "Ivan Oseledets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17752v1",
                "updated": "2024-10-23T10:29:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    29,
                    18,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T10:29:18Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    29,
                    18,
                    2,
                    297,
                    0
                ],
                "title": "AdaDiffSR: Adaptive Region-aware Dynamic Acceleration Diffusion Model\n  for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaDiffSR: Adaptive Region-aware Dynamic Acceleration Diffusion Model\n  for Real-World Image Super-Resolution"
                },
                "summary": "Diffusion models (DMs) have shown promising results on single-image\nsuper-resolution and other image-to-image translation tasks. Benefiting from\nmore computational resources and longer inference times, they are able to yield\nmore realistic images. Existing DMs-based super-resolution methods try to\nachieve an overall average recovery over all regions via iterative refinement,\nignoring the consideration that different input image regions require different\ntimesteps to reconstruct. In this work, we notice that previous DMs-based\nsuper-resolution methods suffer from wasting computational resources to\nreconstruct invisible details. To further improve the utilization of\ncomputational resources, we propose AdaDiffSR, a DMs-based SR pipeline with\ndynamic timesteps sampling strategy (DTSS). Specifically, by introducing the\nmulti-metrics latent entropy module (MMLE), we can achieve dynamic perception\nof the latent spatial information gain during the denoising process, thereby\nguiding the dynamic selection of the timesteps. In addition, we adopt a\nprogressive feature injection module (PFJ), which dynamically injects the\noriginal image features into the denoising process based on the current\ninformation gain, so as to generate images with both fidelity and realism.\nExperiments show that our AdaDiffSR achieves comparable performance over\ncurrent state-of-the-art DMs-based SR methods while consuming less\ncomputational resources and inference time on both synthetic and real-world\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have shown promising results on single-image\nsuper-resolution and other image-to-image translation tasks. Benefiting from\nmore computational resources and longer inference times, they are able to yield\nmore realistic images. Existing DMs-based super-resolution methods try to\nachieve an overall average recovery over all regions via iterative refinement,\nignoring the consideration that different input image regions require different\ntimesteps to reconstruct. In this work, we notice that previous DMs-based\nsuper-resolution methods suffer from wasting computational resources to\nreconstruct invisible details. To further improve the utilization of\ncomputational resources, we propose AdaDiffSR, a DMs-based SR pipeline with\ndynamic timesteps sampling strategy (DTSS). Specifically, by introducing the\nmulti-metrics latent entropy module (MMLE), we can achieve dynamic perception\nof the latent spatial information gain during the denoising process, thereby\nguiding the dynamic selection of the timesteps. In addition, we adopt a\nprogressive feature injection module (PFJ), which dynamically injects the\noriginal image features into the denoising process based on the current\ninformation gain, so as to generate images with both fidelity and realism.\nExperiments show that our AdaDiffSR achieves comparable performance over\ncurrent state-of-the-art DMs-based SR methods while consuming less\ncomputational resources and inference time on both synthetic and real-world\ndatasets."
                },
                "authors": [
                    {
                        "name": "Yuanting Fan"
                    },
                    {
                        "name": "Chengxu Liu"
                    },
                    {
                        "name": "Nengzhong Yin"
                    },
                    {
                        "name": "Changlong Gao"
                    },
                    {
                        "name": "Xueming Qian"
                    }
                ],
                "author_detail": {
                    "name": "Xueming Qian"
                },
                "author": "Xueming Qian",
                "arxiv_comment": "18 pages, 6 figures, ECCV2024 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17744v1",
                "updated": "2024-10-23T10:17:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    17,
                    13,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T10:17:13Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    17,
                    13,
                    2,
                    297,
                    0
                ],
                "title": "Learning Versatile Skills with Curriculum Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Versatile Skills with Curriculum Masking"
                },
                "summary": "Masked prediction has emerged as a promising pretraining paradigm in offline\nreinforcement learning (RL) due to its versatile masking schemes, enabling\nflexible inference across various downstream tasks with a unified model.\nDespite the versatility of masked prediction, it remains unclear how to balance\nthe learning of skills at different levels of complexity. To address this, we\npropose CurrMask, a curriculum masking pretraining paradigm for sequential\ndecision making. Motivated by how humans learn by organizing knowledge in a\ncurriculum, CurrMask adjusts its masking scheme during pretraining for learning\nversatile skills. Through extensive experiments, we show that CurrMask exhibits\nsuperior zero-shot performance on skill prompting tasks, goal-conditioned\nplanning tasks, and competitive finetuning performance on offline RL tasks.\nAdditionally, our analysis of training dynamics reveals that CurrMask gradually\nacquires skills of varying complexity by dynamically adjusting its masking\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked prediction has emerged as a promising pretraining paradigm in offline\nreinforcement learning (RL) due to its versatile masking schemes, enabling\nflexible inference across various downstream tasks with a unified model.\nDespite the versatility of masked prediction, it remains unclear how to balance\nthe learning of skills at different levels of complexity. To address this, we\npropose CurrMask, a curriculum masking pretraining paradigm for sequential\ndecision making. Motivated by how humans learn by organizing knowledge in a\ncurriculum, CurrMask adjusts its masking scheme during pretraining for learning\nversatile skills. Through extensive experiments, we show that CurrMask exhibits\nsuperior zero-shot performance on skill prompting tasks, goal-conditioned\nplanning tasks, and competitive finetuning performance on offline RL tasks.\nAdditionally, our analysis of training dynamics reveals that CurrMask gradually\nacquires skills of varying complexity by dynamically adjusting its masking\nscheme."
                },
                "authors": [
                    {
                        "name": "Yao Tang"
                    },
                    {
                        "name": "Zhihui Xie"
                    },
                    {
                        "name": "Zichuan Lin"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Shuai Li"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Li"
                },
                "author": "Shuai Li",
                "arxiv_comment": "NeurIPS 2024 poster, 21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17736v1",
                "updated": "2024-10-23T10:11:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    11,
                    40,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T10:11:40Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    11,
                    40,
                    2,
                    297,
                    0
                ],
                "title": "MojoBench: Language Modeling and Benchmarks for Mojo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MojoBench: Language Modeling and Benchmarks for Mojo"
                },
                "summary": "The recently introduced Mojo programming language (PL) by Modular, has\nreceived significant attention in the scientific community due to its claimed\nsignificant speed boost over Python. Despite advancements in code Large\nLanguage Models (LLMs) across various PLs, Mojo remains unexplored in this\ncontext. To address this gap, we introduce MojoBench, the first framework for\nMojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset\ndesigned for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM\npretrained and finetuned for Mojo code generation, which supports instructions\nin 5 natural languages (NLs). Our results show that Mojo-Coder achieves a\n30-35% performance improvement over leading models like GPT-4o and\nClaude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with\nunderrepresented and unseen PLs, offering potential strategies for enhancing\nmodel adaptability. MojoBench contributes to our understanding of LLM\ncapabilities and limitations in emerging programming paradigms fostering more\nrobust code generation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently introduced Mojo programming language (PL) by Modular, has\nreceived significant attention in the scientific community due to its claimed\nsignificant speed boost over Python. Despite advancements in code Large\nLanguage Models (LLMs) across various PLs, Mojo remains unexplored in this\ncontext. To address this gap, we introduce MojoBench, the first framework for\nMojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset\ndesigned for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM\npretrained and finetuned for Mojo code generation, which supports instructions\nin 5 natural languages (NLs). Our results show that Mojo-Coder achieves a\n30-35% performance improvement over leading models like GPT-4o and\nClaude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with\nunderrepresented and unseen PLs, offering potential strategies for enhancing\nmodel adaptability. MojoBench contributes to our understanding of LLM\ncapabilities and limitations in emerging programming paradigms fostering more\nrobust code generation systems."
                },
                "authors": [
                    {
                        "name": "Nishat Raihan"
                    },
                    {
                        "name": "Joanna C. S. Santos"
                    },
                    {
                        "name": "Marcos Zampieri"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Zampieri"
                },
                "author": "Marcos Zampieri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17734v1",
                "updated": "2024-10-23T10:07:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    7,
                    13,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T10:07:13Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    7,
                    13,
                    2,
                    297,
                    0
                ],
                "title": "YOLO-Vehicle-Pro: A Cloud-Edge Collaborative Framework for Object\n  Detection in Autonomous Driving under Adverse Weather Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YOLO-Vehicle-Pro: A Cloud-Edge Collaborative Framework for Object\n  Detection in Autonomous Driving under Adverse Weather Conditions"
                },
                "summary": "With the rapid advancement of autonomous driving technology, efficient and\naccurate object detection capabilities have become crucial factors in ensuring\nthe safety and reliability of autonomous driving systems. However, in\nlow-visibility environments such as hazy conditions, the performance of\ntraditional object detection algorithms often degrades significantly, failing\nto meet the demands of autonomous driving. To address this challenge, this\npaper proposes two innovative deep learning models: YOLO-Vehicle and\nYOLO-Vehicle-Pro. YOLO-Vehicle is an object detection model tailored\nspecifically for autonomous driving scenarios, employing multimodal fusion\ntechniques to combine image and textual information for object detection.\nYOLO-Vehicle-Pro builds upon this foundation by introducing an improved image\ndehazing algorithm, enhancing detection performance in low-visibility\nenvironments. In addition to model innovation, this paper also designs and\nimplements a cloud-edge collaborative object detection system, deploying models\non edge devices and offloading partial computational tasks to the cloud in\ncomplex situations. Experimental results demonstrate that on the KITTI dataset,\nthe YOLO-Vehicle-v1s model achieved 92.1% accuracy while maintaining a\ndetection speed of 226 FPS and an inference time of 12ms, meeting the real-time\nrequirements of autonomous driving. When processing hazy images, the\nYOLO-Vehicle-Pro model achieved a high accuracy of 82.3% mAP@50 on the Foggy\nCityscapes dataset while maintaining a detection speed of 43 FPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of autonomous driving technology, efficient and\naccurate object detection capabilities have become crucial factors in ensuring\nthe safety and reliability of autonomous driving systems. However, in\nlow-visibility environments such as hazy conditions, the performance of\ntraditional object detection algorithms often degrades significantly, failing\nto meet the demands of autonomous driving. To address this challenge, this\npaper proposes two innovative deep learning models: YOLO-Vehicle and\nYOLO-Vehicle-Pro. YOLO-Vehicle is an object detection model tailored\nspecifically for autonomous driving scenarios, employing multimodal fusion\ntechniques to combine image and textual information for object detection.\nYOLO-Vehicle-Pro builds upon this foundation by introducing an improved image\ndehazing algorithm, enhancing detection performance in low-visibility\nenvironments. In addition to model innovation, this paper also designs and\nimplements a cloud-edge collaborative object detection system, deploying models\non edge devices and offloading partial computational tasks to the cloud in\ncomplex situations. Experimental results demonstrate that on the KITTI dataset,\nthe YOLO-Vehicle-v1s model achieved 92.1% accuracy while maintaining a\ndetection speed of 226 FPS and an inference time of 12ms, meeting the real-time\nrequirements of autonomous driving. When processing hazy images, the\nYOLO-Vehicle-Pro model achieved a high accuracy of 82.3% mAP@50 on the Foggy\nCityscapes dataset while maintaining a detection speed of 43 FPS."
                },
                "authors": [
                    {
                        "name": "Xiguang Li"
                    },
                    {
                        "name": "Jiafu Chen"
                    },
                    {
                        "name": "Yunhe Sun"
                    },
                    {
                        "name": "Na Lin"
                    },
                    {
                        "name": "Ammar Hawbani"
                    },
                    {
                        "name": "Liang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhao"
                },
                "author": "Liang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14622v2",
                "updated": "2024-10-23T10:06:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    6,
                    10,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-22T15:10:45Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    15,
                    10,
                    45,
                    3,
                    53,
                    0
                ],
                "title": "From Keywords to Structured Summaries: Streamlining Scholarly\n  Information Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Keywords to Structured Summaries: Streamlining Scholarly\n  Information Access"
                },
                "summary": "This paper highlights the growing importance of information retrieval (IR)\nengines in the scientific community, addressing the inefficiency of traditional\nkeyword-based search engines due to the rising volume of publications. The\nproposed solution involves structured records, underpinning advanced\ninformation technology (IT) tools, including visualization dashboards, to\nrevolutionize how researchers access and filter articles, replacing the\ntraditional text-heavy approach. This vision is exemplified through a proof of\nconcept centered on the \"reproductive number estimate of infectious diseases\"\nresearch theme, using a fine-tuned large language model (LLM) to automate the\ncreation of structured records to populate a backend database that now goes\nbeyond keywords. The result is a next-generation information access system as\nan IR method accessible at https://orkg.org/usecases/r0-estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper highlights the growing importance of information retrieval (IR)\nengines in the scientific community, addressing the inefficiency of traditional\nkeyword-based search engines due to the rising volume of publications. The\nproposed solution involves structured records, underpinning advanced\ninformation technology (IT) tools, including visualization dashboards, to\nrevolutionize how researchers access and filter articles, replacing the\ntraditional text-heavy approach. This vision is exemplified through a proof of\nconcept centered on the \"reproductive number estimate of infectious diseases\"\nresearch theme, using a fine-tuned large language model (LLM) to automate the\ncreation of structured records to populate a backend database that now goes\nbeyond keywords. The result is a next-generation information access system as\nan IR method accessible at https://orkg.org/usecases/r0-estimates."
                },
                "authors": [
                    {
                        "name": "Mahsa Shamsabadi"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer D'Souza"
                },
                "author": "Jennifer D'Souza",
                "arxiv_comment": "8 pages, 3 figures | Accepted for publication as a poster paper at\n  the International Semantic Web Conference (ISWC 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04249v2",
                "updated": "2024-10-23T09:46:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    46,
                    43,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-05T18:11:14Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    11,
                    14,
                    5,
                    279,
                    0
                ],
                "title": "DiffSpec: Differential Testing with LLMs using Natural Language\n  Specifications and Code Artifacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffSpec: Differential Testing with LLMs using Natural Language\n  Specifications and Code Artifacts"
                },
                "summary": "Differential testing can be an effective way to find bugs in software systems\nwith multiple implementations that conform to the same specification, like\ncompilers, network protocol parsers, and language runtimes. Specifications for\nsuch systems are often standardized in natural language documents, like\nInstruction Set Architecture (ISA) specifications, Wasm specifications or IETF\nRFC's. Large Language Models (LLMs) have demonstrated potential in both\ngenerating tests and handling large volumes of natural language text, making\nthem well-suited for utilizing artifacts like specification documents, bug\nreports, and code implementations. In this work, we leverage natural language\nand code artifacts to guide LLMs to generate targeted, meaningful tests that\nhighlight meaningful behavioral differences between implementations, including\nthose corresponding to bugs. We introduce DiffSpec, a framework for generating\ndifferential tests with LLMs using prompt chaining. We demonstrate the efficacy\nof DiffSpec on two different systems, namely, eBPF runtimes and Wasm\nvalidators. Using DiffSpec, we generated 359 differentiating tests, uncovering\nat least four distinct and confirmed bugs in eBPF, including a kernel memory\nleak, inconsistent behavior in jump instructions, and undefined behavior when\nusing the stack pointer. We also found 279 differentiating tests in Wasm\nvalidators, that point to at least 2 confirmed and fixed bugs in Wizard Engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential testing can be an effective way to find bugs in software systems\nwith multiple implementations that conform to the same specification, like\ncompilers, network protocol parsers, and language runtimes. Specifications for\nsuch systems are often standardized in natural language documents, like\nInstruction Set Architecture (ISA) specifications, Wasm specifications or IETF\nRFC's. Large Language Models (LLMs) have demonstrated potential in both\ngenerating tests and handling large volumes of natural language text, making\nthem well-suited for utilizing artifacts like specification documents, bug\nreports, and code implementations. In this work, we leverage natural language\nand code artifacts to guide LLMs to generate targeted, meaningful tests that\nhighlight meaningful behavioral differences between implementations, including\nthose corresponding to bugs. We introduce DiffSpec, a framework for generating\ndifferential tests with LLMs using prompt chaining. We demonstrate the efficacy\nof DiffSpec on two different systems, namely, eBPF runtimes and Wasm\nvalidators. Using DiffSpec, we generated 359 differentiating tests, uncovering\nat least four distinct and confirmed bugs in eBPF, including a kernel memory\nleak, inconsistent behavior in jump instructions, and undefined behavior when\nusing the stack pointer. We also found 279 differentiating tests in Wasm\nvalidators, that point to at least 2 confirmed and fixed bugs in Wizard Engine."
                },
                "authors": [
                    {
                        "name": "Nikitha Rao"
                    },
                    {
                        "name": "Elizabeth Gilbert"
                    },
                    {
                        "name": "Tahina Ramananandro"
                    },
                    {
                        "name": "Nikhil Swamy"
                    },
                    {
                        "name": "Claire Le Goues"
                    },
                    {
                        "name": "Sarah Fakhoury"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Fakhoury"
                },
                "author": "Sarah Fakhoury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14282v3",
                "updated": "2024-10-23T09:42:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    42,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-06-20T13:07:38Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    7,
                    38,
                    3,
                    172,
                    0
                ],
                "title": "Learning to Plan for Retrieval-Augmented Large Language Models from\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Plan for Retrieval-Augmented Large Language Models from\n  Knowledge Graphs"
                },
                "summary": "Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data."
                },
                "authors": [
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Mingyang Chen"
                    },
                    {
                        "name": "Binbin Hu"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "EMNLP2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17714v1",
                "updated": "2024-10-23T09:40:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    40,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T09:40:15Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    40,
                    15,
                    2,
                    297,
                    0
                ],
                "title": "CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient\n  Semantic Steering in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient\n  Semantic Steering in Large Language Models"
                },
                "summary": "Despite their impressive capabilities, large language models (LLMs) often\nlack interpretability and can generate toxic content. While using LLMs as\nfoundation models and applying semantic steering methods are widely practiced,\nwe believe that efficient methods should be based on a thorough understanding\nof LLM behavior. To this end, we propose using eye movement measures to\ninterpret LLM behavior across layers. We find that LLMs exhibit patterns\nsimilar to human gaze across layers and different layers function differently.\nInspired by these findings, we introduce a heuristic steering layer selection\nand apply it to layer intervention methods via fine-tuning and inference. Using\nlanguage toxification and detoxification as test beds, we demonstrate that our\nproposed CogSteer methods achieve better results in terms of toxicity scores\nwhile efficiently saving 97% of the computational resources and 60% of the\ntraining time. Our model-agnostic approach can be adopted into various LLMs,\ncontributing to their interpretability and promoting trustworthiness for safe\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, large language models (LLMs) often\nlack interpretability and can generate toxic content. While using LLMs as\nfoundation models and applying semantic steering methods are widely practiced,\nwe believe that efficient methods should be based on a thorough understanding\nof LLM behavior. To this end, we propose using eye movement measures to\ninterpret LLM behavior across layers. We find that LLMs exhibit patterns\nsimilar to human gaze across layers and different layers function differently.\nInspired by these findings, we introduce a heuristic steering layer selection\nand apply it to layer intervention methods via fine-tuning and inference. Using\nlanguage toxification and detoxification as test beds, we demonstrate that our\nproposed CogSteer methods achieve better results in terms of toxicity scores\nwhile efficiently saving 97% of the computational resources and 60% of the\ntraining time. Our model-agnostic approach can be adopted into various LLMs,\ncontributing to their interpretability and promoting trustworthiness for safe\ndeployment."
                },
                "authors": [
                    {
                        "name": "Xintong Wang"
                    },
                    {
                        "name": "Jingheng Pan"
                    },
                    {
                        "name": "Longqin Jiang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Xingshan Li"
                    },
                    {
                        "name": "Chris Biemann"
                    }
                ],
                "author_detail": {
                    "name": "Chris Biemann"
                },
                "author": "Chris Biemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17711v1",
                "updated": "2024-10-23T09:36:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    36,
                    21,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T09:36:21Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    36,
                    21,
                    2,
                    297,
                    0
                ],
                "title": "Beware of Calibration Data for Pruning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beware of Calibration Data for Pruning Large Language Models"
                },
                "summary": "As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters.\nPrevious research has primarily focused on designing advanced pruning methods,\nwhile different calibration data's impact on pruning performance still lacks\nsystematical exploration. We fill this blank and surprisingly observe that the\neffects of calibration data even value more than designing advanced pruning\nstrategies, especially for high sparsity. Our preliminary exploration also\ndiscloses that using calibration data similar to the training data can yield\nbetter performance. As pre-training data is usually inaccessible for advanced\nLLMs, we further provide a self-generating calibration data synthesis strategy\nto construct feasible calibration data. We conduct experiments on the recent\nstrong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that\nthe proposed method outperforms commonly used calibration data and can\neffectively enhance strong pruning methods (e.g., Wanda, OWL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters.\nPrevious research has primarily focused on designing advanced pruning methods,\nwhile different calibration data's impact on pruning performance still lacks\nsystematical exploration. We fill this blank and surprisingly observe that the\neffects of calibration data even value more than designing advanced pruning\nstrategies, especially for high sparsity. Our preliminary exploration also\ndiscloses that using calibration data similar to the training data can yield\nbetter performance. As pre-training data is usually inaccessible for advanced\nLLMs, we further provide a self-generating calibration data synthesis strategy\nto construct feasible calibration data. We conduct experiments on the recent\nstrong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that\nthe proposed method outperforms commonly used calibration data and can\neffectively enhance strong pruning methods (e.g., Wanda, OWL)."
                },
                "authors": [
                    {
                        "name": "Yixin Ji"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17709v1",
                "updated": "2024-10-23T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    35,
                    35,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T09:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    35,
                    35,
                    2,
                    297,
                    0
                ],
                "title": "Deoxys: A Causal Inference Engine for Unhealthy Node Mitigation in\n  Large-scale Cloud Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deoxys: A Causal Inference Engine for Unhealthy Node Mitigation in\n  Large-scale Cloud Infrastructure"
                },
                "summary": "The presence of unhealthy nodes in cloud infrastructure signals the potential\nfailure of machines, which can significantly impact the availability and\nreliability of cloud services, resulting in negative customer experiences.\nEffectively addressing unhealthy node mitigation is therefore vital for\nsustaining cloud system performance. This paper introduces Deoxys, a causal\ninference engine tailored to recommending mitigation actions for unhealthy node\nin cloud systems to minimize virtual machine downtime and interruptions during\nunhealthy events. It employs double machine learning combined with causal\nforest to produce precise and reliable mitigation recommendations based solely\non limited observational data collected from the historical unhealthy events.\nTo enhance the causal inference model, Deoxys further incorporates a policy\nfallback mechanism based on model uncertainty and action overriding mechanisms\nto (i) improve the reliability of the system, and (ii) strike a good tradeoff\nbetween downtime reduction and resource utilization, thereby enhancing the\noverall system performance.\n  After deploying Deoxys in a large-scale cloud infrastructure at Microsoft,\nour observations demonstrate that Deoxys significantly reduces average VM\ndowntime by 53% compared to a legacy policy, while leading to 49.5% lower VM\ninterruption rate. This substantial improvement enhances the reliability and\nstability of cloud platforms, resulting in a seamless customer experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The presence of unhealthy nodes in cloud infrastructure signals the potential\nfailure of machines, which can significantly impact the availability and\nreliability of cloud services, resulting in negative customer experiences.\nEffectively addressing unhealthy node mitigation is therefore vital for\nsustaining cloud system performance. This paper introduces Deoxys, a causal\ninference engine tailored to recommending mitigation actions for unhealthy node\nin cloud systems to minimize virtual machine downtime and interruptions during\nunhealthy events. It employs double machine learning combined with causal\nforest to produce precise and reliable mitigation recommendations based solely\non limited observational data collected from the historical unhealthy events.\nTo enhance the causal inference model, Deoxys further incorporates a policy\nfallback mechanism based on model uncertainty and action overriding mechanisms\nto (i) improve the reliability of the system, and (ii) strike a good tradeoff\nbetween downtime reduction and resource utilization, thereby enhancing the\noverall system performance.\n  After deploying Deoxys in a large-scale cloud infrastructure at Microsoft,\nour observations demonstrate that Deoxys significantly reduces average VM\ndowntime by 53% compared to a legacy policy, while leading to 49.5% lower VM\ninterruption rate. This substantial improvement enhances the reliability and\nstability of cloud platforms, resulting in a seamless customer experience."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Randolph Yao"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Ze Li"
                    },
                    {
                        "name": "Shekhar Agrawal"
                    },
                    {
                        "name": "Binit R. Mishra"
                    },
                    {
                        "name": "Tri Tran"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Murali Chintalapati"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17700v1",
                "updated": "2024-10-23T09:22:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    22,
                    43,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T09:22:43Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    22,
                    43,
                    2,
                    297,
                    0
                ],
                "title": "Scalable Random Feature Latent Variable Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Random Feature Latent Variable Models"
                },
                "summary": "Random feature latent variable models (RFLVMs) represent the state-of-the-art\nin latent variable models, capable of handling non-Gaussian likelihoods and\neffectively uncovering patterns in high-dimensional data. However, their heavy\nreliance on Monte Carlo sampling results in scalability issues which makes it\ndifficult to use these models for datasets with a massive number of\nobservations. To scale up RFLVMs, we turn to the optimization-based variational\nBayesian inference (VBI) algorithm which is known for its scalability compared\nto sampling-based methods. However, implementing VBI for RFLVMs poses\nchallenges, such as the lack of explicit probability distribution functions\n(PDFs) for the Dirichlet process (DP) in the kernel learning component, and the\nincompatibility of existing VBI algorithms with RFLVMs. To address these\nissues, we introduce a stick-breaking construction for DP to obtain an explicit\nPDF and a novel VBI algorithm called ``block coordinate descent variational\ninference\" (BCD-VI). This enables the development of a scalable version of\nRFLVMs, or in short, SRFLVM. Our proposed method shows scalability,\ncomputational efficiency, superior performance in generating informative latent\nrepresentations and the ability of imputing missing data across various\nreal-world datasets, outperforming state-of-the-art competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random feature latent variable models (RFLVMs) represent the state-of-the-art\nin latent variable models, capable of handling non-Gaussian likelihoods and\neffectively uncovering patterns in high-dimensional data. However, their heavy\nreliance on Monte Carlo sampling results in scalability issues which makes it\ndifficult to use these models for datasets with a massive number of\nobservations. To scale up RFLVMs, we turn to the optimization-based variational\nBayesian inference (VBI) algorithm which is known for its scalability compared\nto sampling-based methods. However, implementing VBI for RFLVMs poses\nchallenges, such as the lack of explicit probability distribution functions\n(PDFs) for the Dirichlet process (DP) in the kernel learning component, and the\nincompatibility of existing VBI algorithms with RFLVMs. To address these\nissues, we introduce a stick-breaking construction for DP to obtain an explicit\nPDF and a novel VBI algorithm called ``block coordinate descent variational\ninference\" (BCD-VI). This enables the development of a scalable version of\nRFLVMs, or in short, SRFLVM. Our proposed method shows scalability,\ncomputational efficiency, superior performance in generating informative latent\nrepresentations and the ability of imputing missing data across various\nreal-world datasets, outperforming state-of-the-art competitors."
                },
                "authors": [
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Zhidi Lin"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Michael Minyi Zhang"
                    },
                    {
                        "name": "Pablo M. Olmos"
                    },
                    {
                        "name": "Petar M. Djuri"
                    }
                ],
                "author_detail": {
                    "name": "Petar M. Djuri"
                },
                "author": "Petar M. Djuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17698v2",
                "updated": "2024-10-24T02:42:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    2,
                    42,
                    39,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-23T09:20:51Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    20,
                    51,
                    2,
                    297,
                    0
                ],
                "title": "ProGeny II: the impact of libraries and model configurations on inferred\n  galaxy properties in SED fitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProGeny II: the impact of libraries and model configurations on inferred\n  galaxy properties in SED fitting"
                },
                "summary": "We use a volume-complete sample of ~8,000 galaxies from the GAMA survey to\ncharacterise the impact of stellar population libraries (SPLs) and model\nconfigurations on the resulting inferred galaxy properties from Spectral Energy\nDistribution (SED) fitting. We compare a fiducial SPL from ProGeny (a new tool\nthat can generate SPLs quickly and flexibly) against five other commonly used\nSPLs using the SED-fitting code ProSpect. The impact of selecting each SPL is\ncompared to the consequence of changing the model implementation in the SED\nfitting process, including the implementation of metallicity evolution versus a\nfixed or constant metallicity, and a functional parametric star formation\nhistory (SFH) versus a stepwise parametric (or \"non-parametric\") SFH.\nFurthermore, we use ProGeny to assess the impact of sub-SPL choices, including\nisochrone selection, atmosphere selection, and IMF selection. Through a\ncomparison of derived stellar masses, star formation rates, metallicities,\nages, and the inferred cosmic star formation history (CSFH), we rank the impact\nof varying choices. Overall the assumption of a solar metallicity creates the\ngreatest biases, with a substantial impact also caused by the choice of a\nspecific SPL. To recover a CSFH most consistent with observations, we advocate\nfor the use of the fiducial implementation with a skewed Normal functional form\nfor the SFH, and an evolving metallicity. While all current SPLs currently\nunderestimate the peak in the CSFH, ProGeny and FSPS are the closest to\nrecovering the observed CSFH.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use a volume-complete sample of ~8,000 galaxies from the GAMA survey to\ncharacterise the impact of stellar population libraries (SPLs) and model\nconfigurations on the resulting inferred galaxy properties from Spectral Energy\nDistribution (SED) fitting. We compare a fiducial SPL from ProGeny (a new tool\nthat can generate SPLs quickly and flexibly) against five other commonly used\nSPLs using the SED-fitting code ProSpect. The impact of selecting each SPL is\ncompared to the consequence of changing the model implementation in the SED\nfitting process, including the implementation of metallicity evolution versus a\nfixed or constant metallicity, and a functional parametric star formation\nhistory (SFH) versus a stepwise parametric (or \"non-parametric\") SFH.\nFurthermore, we use ProGeny to assess the impact of sub-SPL choices, including\nisochrone selection, atmosphere selection, and IMF selection. Through a\ncomparison of derived stellar masses, star formation rates, metallicities,\nages, and the inferred cosmic star formation history (CSFH), we rank the impact\nof varying choices. Overall the assumption of a solar metallicity creates the\ngreatest biases, with a substantial impact also caused by the choice of a\nspecific SPL. To recover a CSFH most consistent with observations, we advocate\nfor the use of the fiducial implementation with a skewed Normal functional form\nfor the SFH, and an evolving metallicity. While all current SPLs currently\nunderestimate the peak in the CSFH, ProGeny and FSPS are the closest to\nrecovering the observed CSFH."
                },
                "authors": [
                    {
                        "name": "Sabine Bellstedt"
                    },
                    {
                        "name": "Aaron S. G. Robotham"
                    }
                ],
                "author_detail": {
                    "name": "Aaron S. G. Robotham"
                },
                "author": "Aaron S. G. Robotham",
                "arxiv_comment": "22 pages, 24 figures, submitted for publication to MNRAS, companion\n  paper to Robotham & Bellstedt (ProGeny I). Abstract formatting fixed in this\n  update",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17692v1",
                "updated": "2024-10-23T09:14:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    14,
                    50,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T09:14:50Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    14,
                    50,
                    2,
                    297,
                    0
                ],
                "title": "Asymptotics for parametric martingale posteriors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotics for parametric martingale posteriors"
                },
                "summary": "The martingale posterior framework is a generalization of Bayesian inference\nwhere one elicits a sequence of one-step ahead predictive densities instead of\nthe likelihood and prior. Posterior sampling then involves the imputation of\nunseen observables, and can then be carried out in an expedient and\nparallelizable manner using predictive resampling without requiring Markov\nchain Monte Carlo. Recent work has investigated the use of plug-in parametric\npredictive densities, combined with stochastic gradient descent, to specify a\nparametric martingale posterior. This paper investigates the asymptotic\nproperties of this class of parametric martingale posteriors. In particular,\ntwo central limit theorems based on martingale limit theory are introduced and\napplied. The first is a predictive central limit theorem, which enables a\nsignificant acceleration of the predictive resampling scheme through a hybrid\nsampling algorithm based on a normal approximation. The second is a\nBernstein-von Mises result, which is novel for martingale posteriors, and\nprovides methodological guidance on attaining desirable frequentist properties.\nWe demonstrate the utility of the theoretical results in simulations and a real\ndata example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The martingale posterior framework is a generalization of Bayesian inference\nwhere one elicits a sequence of one-step ahead predictive densities instead of\nthe likelihood and prior. Posterior sampling then involves the imputation of\nunseen observables, and can then be carried out in an expedient and\nparallelizable manner using predictive resampling without requiring Markov\nchain Monte Carlo. Recent work has investigated the use of plug-in parametric\npredictive densities, combined with stochastic gradient descent, to specify a\nparametric martingale posterior. This paper investigates the asymptotic\nproperties of this class of parametric martingale posteriors. In particular,\ntwo central limit theorems based on martingale limit theory are introduced and\napplied. The first is a predictive central limit theorem, which enables a\nsignificant acceleration of the predictive resampling scheme through a hybrid\nsampling algorithm based on a normal approximation. The second is a\nBernstein-von Mises result, which is novel for martingale posteriors, and\nprovides methodological guidance on attaining desirable frequentist properties.\nWe demonstrate the utility of the theoretical results in simulations and a real\ndata example."
                },
                "authors": [
                    {
                        "name": "Edwin Fong"
                    },
                    {
                        "name": "Andrew Yiu"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Yiu"
                },
                "author": "Andrew Yiu",
                "arxiv_comment": "18 pages (main), 50 pages (total), 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04892v2",
                "updated": "2024-10-23T09:04:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    4,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-07T14:24:04Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    14,
                    24,
                    4,
                    2,
                    38,
                    0
                ],
                "title": "Probabilistic ML Verification via Weighted Model Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic ML Verification via Weighted Model Integration"
                },
                "summary": "In machine learning (ML) verification, the majority of procedures are\nnon-quantitative and therefore cannot be used for verifying probabilistic\nmodels, or be applied in domains where hard guarantees are practically\nunachievable. The probabilistic formal verification (PFV) of ML models is in\nits infancy, with the existing approaches limited to specific ML models,\nproperties, or both. This contrasts with standard formal methods techniques,\nwhose successful adoption in real-world scenarios is also due to their support\nfor a wide range of properties and diverse systems. We propose a unifying\nframework for the PFV of ML systems based on Weighted Model Integration (WMI),\na relatively recent formalism for probabilistic inference with algebraic and\nlogical constraints. Crucially, reducing the PFV of ML models to WMI enables\nthe verification of many properties of interest over a wide range of systems,\naddressing multiple limitations of deterministic verification and ad-hoc\nalgorithms. We substantiate the generality of the approach on prototypical\ntasks involving the verification of group fairness, monotonicity, robustness to\nnoise, probabilistic local robustness and equivalence among predictors. We\ncharacterize the challenges related to the scalability of the approach and,\nthrough our WMI-based perspective, we show how successful scaling techniques in\nthe ML verification literature can be generalized beyond their original scope.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In machine learning (ML) verification, the majority of procedures are\nnon-quantitative and therefore cannot be used for verifying probabilistic\nmodels, or be applied in domains where hard guarantees are practically\nunachievable. The probabilistic formal verification (PFV) of ML models is in\nits infancy, with the existing approaches limited to specific ML models,\nproperties, or both. This contrasts with standard formal methods techniques,\nwhose successful adoption in real-world scenarios is also due to their support\nfor a wide range of properties and diverse systems. We propose a unifying\nframework for the PFV of ML systems based on Weighted Model Integration (WMI),\na relatively recent formalism for probabilistic inference with algebraic and\nlogical constraints. Crucially, reducing the PFV of ML models to WMI enables\nthe verification of many properties of interest over a wide range of systems,\naddressing multiple limitations of deterministic verification and ad-hoc\nalgorithms. We substantiate the generality of the approach on prototypical\ntasks involving the verification of group fairness, monotonicity, robustness to\nnoise, probabilistic local robustness and equivalence among predictors. We\ncharacterize the challenges related to the scalability of the approach and,\nthrough our WMI-based perspective, we show how successful scaling techniques in\nthe ML verification literature can be generalized beyond their original scope."
                },
                "authors": [
                    {
                        "name": "Paolo Morettin"
                    },
                    {
                        "name": "Andrea Passerini"
                    },
                    {
                        "name": "Roberto Sebastiani"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Sebastiani"
                },
                "author": "Roberto Sebastiani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11534v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11534v2",
                "updated": "2024-10-23T09:03:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    3,
                    43,
                    2,
                    297,
                    0
                ],
                "published": "2024-01-21T16:07:20Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    16,
                    7,
                    20,
                    6,
                    21,
                    0
                ],
                "title": "Comparing dark matter and MOND hyphotheses from the distribution\n  function of A, F, early-G stars in the solar neighbourhood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing dark matter and MOND hyphotheses from the distribution\n  function of A, F, early-G stars in the solar neighbourhood"
                },
                "summary": "Dark matter is hypothetical matter believed to address the missing mass\nproblem in galaxies. However, alternative theories, such as Modified Newtonian\nDynamics (MOND), have been notably successful in explaining the missing mass\nproblem in various astrophysical systems. The vertical distribution function of\nstars in the solar neighbourhood serves as a proxy to constrain galactic\ndynamics in accordance to its contents. We employ both the vertical positional\nand velocity distribution of stars in cylindrical coordinates with a radius of\n150 pc and a half-height of 200 pc from the galactic plane. Our tracers consist\nof main-sequence A, F, and early-G stars from the GAIA, RAVE, APOGEE, GALAH,\nand LAMOST catalogues. We attempt to solve the missing mass in the solar\nneighbourhood, interpreting it as either dark matter or MOND. Subsequently, we\ncompare both hypotheses newtonian gravity with dark matter and MOND, using the\nBayes factor (BF) to determine which one is more favoured by the data. We found\nthat the inferred dark matter in the solar neighbourhood is in range of $\\sim\n(0.01$-$0.07)$ M$_{\\odot}$ pc$^{-3}$. We also determine that the MOND\nhypothesis's acceleration parameter $a_0$ is $(1.26 \\pm 0.13) \\times 10^{-10}$\nm s$^{-2}$ for simple interpolating function. The average of bayes factor for\nall tracers between the two hypotheses is $\\log \\textrm{BF}\\sim 0.1$, meaning\nno strong evidence in favour of either the dark matter or MOND hypotheses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark matter is hypothetical matter believed to address the missing mass\nproblem in galaxies. However, alternative theories, such as Modified Newtonian\nDynamics (MOND), have been notably successful in explaining the missing mass\nproblem in various astrophysical systems. The vertical distribution function of\nstars in the solar neighbourhood serves as a proxy to constrain galactic\ndynamics in accordance to its contents. We employ both the vertical positional\nand velocity distribution of stars in cylindrical coordinates with a radius of\n150 pc and a half-height of 200 pc from the galactic plane. Our tracers consist\nof main-sequence A, F, and early-G stars from the GAIA, RAVE, APOGEE, GALAH,\nand LAMOST catalogues. We attempt to solve the missing mass in the solar\nneighbourhood, interpreting it as either dark matter or MOND. Subsequently, we\ncompare both hypotheses newtonian gravity with dark matter and MOND, using the\nBayes factor (BF) to determine which one is more favoured by the data. We found\nthat the inferred dark matter in the solar neighbourhood is in range of $\\sim\n(0.01$-$0.07)$ M$_{\\odot}$ pc$^{-3}$. We also determine that the MOND\nhypothesis's acceleration parameter $a_0$ is $(1.26 \\pm 0.13) \\times 10^{-10}$\nm s$^{-2}$ for simple interpolating function. The average of bayes factor for\nall tracers between the two hypotheses is $\\log \\textrm{BF}\\sim 0.1$, meaning\nno strong evidence in favour of either the dark matter or MOND hypotheses."
                },
                "authors": [
                    {
                        "name": "M. A. Syaifudin"
                    },
                    {
                        "name": "M. I. Arifyanto"
                    },
                    {
                        "name": "H. R. T. Wulandari"
                    },
                    {
                        "name": "F. A. M. Mulki"
                    }
                ],
                "author_detail": {
                    "name": "F. A. M. Mulki"
                },
                "arxiv_affiliation": "Bosscha Observatory, Bandung Institute of Technology, Indonesia",
                "author": "F. A. M. Mulki",
                "arxiv_doi": "10.1093/mnras/stae2316",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2316",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.11534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11534v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 7 figures, 5 tables. Accepted for publication in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17672v1",
                "updated": "2024-10-23T08:46:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    46,
                    46,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T08:46:46Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    46,
                    46,
                    2,
                    297,
                    0
                ],
                "title": "Non-Hermitian Hamiltonian Approach for Two-Dimensional Spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Hermitian Hamiltonian Approach for Two-Dimensional Spectroscopy"
                },
                "summary": "Two-dimensional spectroscopy (2DS) offers significant advantages in terms of\nhigh temporal and frequency resolutions and signal-to-noise ratio. Until now,\nthe response-function (RF) formalism has been the prevalent theoretical\ndescription. In this study, we compare the non-Hermitian Hamiltonian (NHH)\nmethod with the RF formalism in a three-level system with a constant control\nfield. We obtain the signals from both approaches and compare their population\ndynamics and 2DS. We propose the quasi-Green function for the NHH method, which\nallows all possible Liouville paths to be inferred. Although the NHH method\noverestimates relaxations, it also provides a more comprehensive description.\nOur results demonstrate that the NHH method is more suitable than the RF\nformalism for investigating the systems that are either dissipative or complex\nvia the 2DS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-dimensional spectroscopy (2DS) offers significant advantages in terms of\nhigh temporal and frequency resolutions and signal-to-noise ratio. Until now,\nthe response-function (RF) formalism has been the prevalent theoretical\ndescription. In this study, we compare the non-Hermitian Hamiltonian (NHH)\nmethod with the RF formalism in a three-level system with a constant control\nfield. We obtain the signals from both approaches and compare their population\ndynamics and 2DS. We propose the quasi-Green function for the NHH method, which\nallows all possible Liouville paths to be inferred. Although the NHH method\noverestimates relaxations, it also provides a more comprehensive description.\nOur results demonstrate that the NHH method is more suitable than the RF\nformalism for investigating the systems that are either dissipative or complex\nvia the 2DS."
                },
                "authors": [
                    {
                        "name": "Hao-Yue Zhang"
                    },
                    {
                        "name": "Bin-Yao Huang"
                    },
                    {
                        "name": "Jing-Yi-Ran Jin"
                    },
                    {
                        "name": "Yi-Xuan Yao"
                    },
                    {
                        "name": "Qing Ai"
                    }
                ],
                "author_detail": {
                    "name": "Qing Ai"
                },
                "author": "Qing Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15993v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15993v4",
                "updated": "2024-10-23T08:33:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    33,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-04-24T17:10:35Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    17,
                    10,
                    35,
                    2,
                    115,
                    0
                ],
                "title": "Uncertainty Estimation and Quantification for LLMs: A Simple Supervised\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Estimation and Quantification for LLMs: A Simple Supervised\n  Approach"
                },
                "summary": "In this paper, we study the problem of uncertainty estimation and calibration\nfor LLMs. We begin by formulating the uncertainty estimation problem, a\nrelevant yet underexplored area in existing literature. We then propose a\nsupervised approach that leverages labeled datasets to estimate the uncertainty\nin LLMs' responses. Based on the formulation, we illustrate the difference\nbetween the uncertainty estimation for LLMs and that for standard ML models and\nexplain why the hidden neurons of the LLMs may contain uncertainty information.\nOur designed approach demonstrates the benefits of utilizing hidden activations\nto enhance uncertainty estimation across various tasks and shows robust\ntransferability in out-of-distribution settings. We distinguish the uncertainty\nestimation task from the uncertainty calibration task and show that better\nuncertainty estimation leads to better calibration performance. Furthermore,\nour method is easy to implement and adaptable to different levels of model\naccessibility including black box, grey box, and white box.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the problem of uncertainty estimation and calibration\nfor LLMs. We begin by formulating the uncertainty estimation problem, a\nrelevant yet underexplored area in existing literature. We then propose a\nsupervised approach that leverages labeled datasets to estimate the uncertainty\nin LLMs' responses. Based on the formulation, we illustrate the difference\nbetween the uncertainty estimation for LLMs and that for standard ML models and\nexplain why the hidden neurons of the LLMs may contain uncertainty information.\nOur designed approach demonstrates the benefits of utilizing hidden activations\nto enhance uncertainty estimation across various tasks and shows robust\ntransferability in out-of-distribution settings. We distinguish the uncertainty\nestimation task from the uncertainty calibration task and show that better\nuncertainty estimation leads to better calibration performance. Furthermore,\nour method is easy to implement and adaptable to different levels of model\naccessibility including black box, grey box, and white box."
                },
                "authors": [
                    {
                        "name": "Linyu Liu"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Xiaocheng Li"
                    },
                    {
                        "name": "Guanting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanting Chen"
                },
                "author": "Guanting Chen",
                "arxiv_comment": "29 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15993v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15993v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16237v2",
                "updated": "2024-10-23T08:31:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    31,
                    20,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-21T17:41:42Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    41,
                    42,
                    0,
                    295,
                    0
                ],
                "title": "IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in\n  Communicative Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in\n  Communicative Multi-Agent Systems"
                },
                "summary": "As large language model (LLM) agents increasingly integrate into our\ninfrastructure, their robust coordination and message synchronization become\nvital. The Byzantine Generals Problem (BGP) is a critical model for\nconstructing resilient multi-agent systems (MAS) under adversarial attacks. It\ndescribes a scenario where malicious agents with unknown identities exist in\nthe system-situations that, in our context, could result from LLM agents'\nhallucinations or external attacks. In BGP, the objective of the entire system\nis to reach a consensus on the action to be taken. Traditional BGP requires\nglobal consensus among all agents; however, in practical scenarios, global\nconsensus is not always necessary and can even be inefficient. Therefore, there\nis a pressing need to explore a refined version of BGP that aligns with the\nlocal coordination patterns observed in MAS. We refer to this refined version\nas Imperfect BGP (IBGP) in our research, aiming to address this discrepancy. To\ntackle this issue, we propose a framework that leverages consensus protocols\nwithin general MAS settings, providing provable resilience against\ncommunication attacks and adaptability to changing environments, as validated\nby empirical results. Additionally, we present a case study in a sensor network\nenvironment to illustrate the practical application of our protocol.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language model (LLM) agents increasingly integrate into our\ninfrastructure, their robust coordination and message synchronization become\nvital. The Byzantine Generals Problem (BGP) is a critical model for\nconstructing resilient multi-agent systems (MAS) under adversarial attacks. It\ndescribes a scenario where malicious agents with unknown identities exist in\nthe system-situations that, in our context, could result from LLM agents'\nhallucinations or external attacks. In BGP, the objective of the entire system\nis to reach a consensus on the action to be taken. Traditional BGP requires\nglobal consensus among all agents; however, in practical scenarios, global\nconsensus is not always necessary and can even be inefficient. Therefore, there\nis a pressing need to explore a refined version of BGP that aligns with the\nlocal coordination patterns observed in MAS. We refer to this refined version\nas Imperfect BGP (IBGP) in our research, aiming to address this discrepancy. To\ntackle this issue, we propose a framework that leverages consensus protocols\nwithin general MAS settings, providing provable resilience against\ncommunication attacks and adaptability to changing environments, as validated\nby empirical results. Additionally, we present a case study in a sensor network\nenvironment to illustrate the practical application of our protocol."
                },
                "authors": [
                    {
                        "name": "Yihuan Mao"
                    },
                    {
                        "name": "Yipeng Kang"
                    },
                    {
                        "name": "Peilun Li"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Chongjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chongjie Zhang"
                },
                "author": "Chongjie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10216v2",
                "updated": "2024-10-23T08:22:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    22,
                    44,
                    2,
                    297,
                    0
                ],
                "published": "2024-06-14T17:49:59Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    17,
                    49,
                    59,
                    4,
                    166,
                    0
                ],
                "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularizing Hidden States Enables Learning Generalizable Reward Model\n  for LLMs"
                },
                "summary": "Reward models trained on human preference data have been proven to\neffectively align Large Language Models (LLMs) with human intent within the\nframework of reinforcement learning from human feedback (RLHF). However,\ncurrent reward models have limited generalization capabilities to unseen\nprompts and responses, which can lead to an unexpected phenomenon known as\nreward over-optimization, resulting in a decline in actual performance due to\nexcessive optimization of rewards. While previous research has advocated for\nconstraining policy optimization, our study introduces a novel approach to\nenhance the reward model's generalization ability against distribution shifts\nby regularizing the hidden states. Specifically, we retain the base model's\nlanguage model head and incorporate a suite of text-generation losses to\npreserve the hidden states' text-generation capabilities, while concurrently\nlearning a reward head behind the same hidden states. Our experimental results\ndemonstrate that the introduced regularization technique markedly improves the\naccuracy of learned reward models across a variety of out-of-distribution (OOD)\ntasks and effectively alleviates the over-optimization issue in RLHF, offering\na more reliable and robust preference learning paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models trained on human preference data have been proven to\neffectively align Large Language Models (LLMs) with human intent within the\nframework of reinforcement learning from human feedback (RLHF). However,\ncurrent reward models have limited generalization capabilities to unseen\nprompts and responses, which can lead to an unexpected phenomenon known as\nreward over-optimization, resulting in a decline in actual performance due to\nexcessive optimization of rewards. While previous research has advocated for\nconstraining policy optimization, our study introduces a novel approach to\nenhance the reward model's generalization ability against distribution shifts\nby regularizing the hidden states. Specifically, we retain the base model's\nlanguage model head and incorporate a suite of text-generation losses to\npreserve the hidden states' text-generation capabilities, while concurrently\nlearning a reward head behind the same hidden states. Our experimental results\ndemonstrate that the introduced regularization technique markedly improves the\naccuracy of learned reward models across a variety of out-of-distribution (OOD)\ntasks and effectively alleviates the over-optimization issue in RLHF, offering\na more reliable and robust preference learning paradigm."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruomeng Ding"
                    },
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17657v1",
                "updated": "2024-10-23T08:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    19,
                    18,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T08:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    19,
                    18,
                    2,
                    297,
                    0
                ],
                "title": "ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents"
                },
                "summary": "Large Language Models (LLMs) have shown promising potential in the medical\ndomain, assisting with tasks like clinical note generation and patient\ncommunication. However, current LLMs are limited to text-based communication,\nhindering their ability to interact with diverse forms of information in\nclinical environments. Despite clinical agents succeeding in diverse signal\ninteraction, they are oriented to a single clinical scenario and hence fail for\nbroader applications. To evaluate clinical agents holistically, we propose\nClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting\nof 18 tasks across five key realistic clinical dimensions. Building on this, we\nintroduce ReflecTool, a novel framework that excels at utilizing\ndomain-specific tools within two stages. The first optimization stage\nprogressively enlarges a long-term memory by saving successful solving\nprocesses and tool-wise experience of agents in a tiny pre-defined training\nset. In the following inference stage, ReflecTool can search for supportive\nsuccessful demonstrations from already built long-term memory to guide the tool\nselection strategy, and a verifier improves the tool usage according to the\ntool-wise experience with two verification methods--iterative refinement and\ncandidate selection. Extensive experiments on ClinicalAgent Benchmark\ndemonstrate that ReflecTool surpasses the pure LLMs with more than 10 points\nand the well-established agent-based methods with 3 points, highlighting its\nadaptability and effectiveness in solving complex clinical tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising potential in the medical\ndomain, assisting with tasks like clinical note generation and patient\ncommunication. However, current LLMs are limited to text-based communication,\nhindering their ability to interact with diverse forms of information in\nclinical environments. Despite clinical agents succeeding in diverse signal\ninteraction, they are oriented to a single clinical scenario and hence fail for\nbroader applications. To evaluate clinical agents holistically, we propose\nClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting\nof 18 tasks across five key realistic clinical dimensions. Building on this, we\nintroduce ReflecTool, a novel framework that excels at utilizing\ndomain-specific tools within two stages. The first optimization stage\nprogressively enlarges a long-term memory by saving successful solving\nprocesses and tool-wise experience of agents in a tiny pre-defined training\nset. In the following inference stage, ReflecTool can search for supportive\nsuccessful demonstrations from already built long-term memory to guide the tool\nselection strategy, and a verifier improves the tool usage according to the\ntool-wise experience with two verification methods--iterative refinement and\ncandidate selection. Extensive experiments on ClinicalAgent Benchmark\ndemonstrate that ReflecTool surpasses the pure LLMs with more than 10 points\nand the well-established agent-based methods with 3 points, highlighting its\nadaptability and effectiveness in solving complex clinical tasks."
                },
                "authors": [
                    {
                        "name": "Yusheng Liao"
                    },
                    {
                        "name": "Shuyang Jiang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17656v1",
                "updated": "2024-10-23T08:18:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    18,
                    38,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T08:18:38Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    18,
                    38,
                    2,
                    297,
                    0
                ],
                "title": "AutoRNet: Automatically Optimizing Heuristics for Robust Network Design\n  via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRNet: Automatically Optimizing Heuristics for Robust Network Design\n  via Large Language Models"
                },
                "summary": "Achieving robust networks is a challenging problem due to its NP-hard nature\nand complex solution space. Current methods, from handcrafted feature\nextraction to deep learning, have made progress but remain rigid, requiring\nmanual design and large labeled datasets. To address these issues, we propose\nAutoRNet, a framework that integrates large language models (LLMs) with\nevolutionary algorithms to generate heuristics for robust network design. We\ndesign network optimization strategies to provide domain-specific prompts for\nLLMs, utilizing domain knowledge to generate advanced heuristics. Additionally,\nwe introduce an adaptive fitness function to balance convergence and diversity\nwhile maintaining degree distributions. AutoRNet is evaluated on sparse and\ndense scale-free networks, outperforming current methods by reducing the need\nfor manual design and large datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving robust networks is a challenging problem due to its NP-hard nature\nand complex solution space. Current methods, from handcrafted feature\nextraction to deep learning, have made progress but remain rigid, requiring\nmanual design and large labeled datasets. To address these issues, we propose\nAutoRNet, a framework that integrates large language models (LLMs) with\nevolutionary algorithms to generate heuristics for robust network design. We\ndesign network optimization strategies to provide domain-specific prompts for\nLLMs, utilizing domain knowledge to generate advanced heuristics. Additionally,\nwe introduce an adaptive fitness function to balance convergence and diversity\nwhile maintaining degree distributions. AutoRNet is evaluated on sparse and\ndense scale-free networks, outperforming current methods by reducing the need\nfor manual design and large datasets."
                },
                "authors": [
                    {
                        "name": "He Yu"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17648v1",
                "updated": "2024-10-23T08:07:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    7,
                    0,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T08:07:00Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    7,
                    0,
                    2,
                    297,
                    0
                ],
                "title": "Towards Active Participant-Centric Vertical Federated Learning: Some\n  Representations May Be All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Active Participant-Centric Vertical Federated Learning: Some\n  Representations May Be All You Need"
                },
                "summary": "Vertical Federated Learning (VFL) enables collaborative model training across\ndifferent participants with distinct features and common samples, while\npreserving data privacy. Existing VFL methodologies often struggle with\nrealistic data partitions, typically incurring high communication costs and\nsignificant operational complexity. In this work, we introduce a novel\nsimplified approach to VFL, Active Participant-Centric VFL (APC-VFL), that, to\nthe best of our knowledge, is the first to require only a single communication\nround between participants, and allows the active participant to do inference\nin a non collaborative fashion. This method integrates unsupervised\nrepresentation learning with knowledge distillation to achieve comparable\naccuracy to traditional VFL methods based on vertical split learning in\nclassical settings, reducing required communication rounds by up to\n$4200\\times$, while being more flexible. Our approach also shows improvements\ncompared to non-federated local models, as well as a comparable VFL proposal,\nVFedTrans, offering an efficient and flexible solution for collaborative\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) enables collaborative model training across\ndifferent participants with distinct features and common samples, while\npreserving data privacy. Existing VFL methodologies often struggle with\nrealistic data partitions, typically incurring high communication costs and\nsignificant operational complexity. In this work, we introduce a novel\nsimplified approach to VFL, Active Participant-Centric VFL (APC-VFL), that, to\nthe best of our knowledge, is the first to require only a single communication\nround between participants, and allows the active participant to do inference\nin a non collaborative fashion. This method integrates unsupervised\nrepresentation learning with knowledge distillation to achieve comparable\naccuracy to traditional VFL methods based on vertical split learning in\nclassical settings, reducing required communication rounds by up to\n$4200\\times$, while being more flexible. Our approach also shows improvements\ncompared to non-federated local models, as well as a comparable VFL proposal,\nVFedTrans, offering an efficient and flexible solution for collaborative\nlearning."
                },
                "authors": [
                    {
                        "name": "Jon Irureta"
                    },
                    {
                        "name": "Jon Imaz"
                    },
                    {
                        "name": "Aizea Lojo"
                    },
                    {
                        "name": "Marco Gonzlez"
                    },
                    {
                        "name": "Iigo Perona"
                    }
                ],
                "author_detail": {
                    "name": "Iigo Perona"
                },
                "author": "Iigo Perona",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.18075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18075v1",
                "updated": "2024-10-23T17:57:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    57,
                    14,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:57:14Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    57,
                    14,
                    2,
                    297,
                    0
                ],
                "title": "ProFL: Performative Robust Optimal Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProFL: Performative Robust Optimal Federated Learning"
                },
                "summary": "Performative prediction (PP) is a framework that captures distribution shifts\nthat occur during the training of machine learning models due to their\ndeployment. As the trained model is used, its generated data could cause the\nmodel to evolve, leading to deviations from the original data distribution. The\nimpact of such model-induced distribution shifts in the federated learning (FL)\nsetup remains unexplored despite being increasingly likely to transpire in\nreal-life use cases. Although Jin et al. (2024) recently extended PP to FL in a\nstraightforward manner, the resulting model only converges to a performative\nstable point, which may be far from optimal. The methods in Izzo et al. (2021);\nMiller et al. (2021) can find a performative optimal point in centralized\nsettings, but they require the performative risk to be convex and the training\ndata to be noiseless, assumptions often violated in realistic FL systems. This\npaper overcomes all of these shortcomings and proposes Performative robust\noptimal Federated Learning (ProFL), an algorithm that finds performative\noptimal points in FL from noisy and contaminated data. We present the\nconvergence analysis under the Polyak-Lojasiewicz condition, which applies to\nnon-convex objectives. Extensive experiments on multiple datasets validate our\nproposed algorithms' efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performative prediction (PP) is a framework that captures distribution shifts\nthat occur during the training of machine learning models due to their\ndeployment. As the trained model is used, its generated data could cause the\nmodel to evolve, leading to deviations from the original data distribution. The\nimpact of such model-induced distribution shifts in the federated learning (FL)\nsetup remains unexplored despite being increasingly likely to transpire in\nreal-life use cases. Although Jin et al. (2024) recently extended PP to FL in a\nstraightforward manner, the resulting model only converges to a performative\nstable point, which may be far from optimal. The methods in Izzo et al. (2021);\nMiller et al. (2021) can find a performative optimal point in centralized\nsettings, but they require the performative risk to be convex and the training\ndata to be noiseless, assumptions often violated in realistic FL systems. This\npaper overcomes all of these shortcomings and proposes Performative robust\noptimal Federated Learning (ProFL), an algorithm that finds performative\noptimal points in FL from noisy and contaminated data. We present the\nconvergence analysis under the Polyak-Lojasiewicz condition, which applies to\nnon-convex objectives. Extensive experiments on multiple datasets validate our\nproposed algorithms' efficiency."
                },
                "authors": [
                    {
                        "name": "Xue Zheng"
                    },
                    {
                        "name": "Tian Xie"
                    },
                    {
                        "name": "Xuwei Tan"
                    },
                    {
                        "name": "Aylin Yener"
                    },
                    {
                        "name": "Xueru Zhang"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Myungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Myungjin Lee"
                },
                "author": "Myungjin Lee",
                "arxiv_comment": "27 pages with Appendix, 18 figures. The paper has been submitted and\n  is currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18071v1",
                "updated": "2024-10-23T17:54:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    54,
                    43,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:54:43Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    54,
                    43,
                    2,
                    297,
                    0
                ],
                "title": "TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing\n  Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing\n  Prompts"
                },
                "summary": "Recently, multimodal large language models (MLLMs) have received much\nattention for their impressive capabilities. The evaluation of MLLMs is\nbecoming critical to analyzing attributes of MLLMs and providing valuable\ninsights. However, current benchmarks overlook the problem of prompt\nsensitivity - minor prompt variations may lead to significant performance\nfluctuations. Thus, inappropriate prompts may obscure the models' capabilities,\nunderestimating the models' performance. Moreover, different models have\ndifferent preferences for different prompts, and thus, using the same prompt\nfor all models will cause evaluation bias. This paper analyzes this deficiency\nin existing benchmarks and further introduces a new evaluation framework named\nTP-Eval, which introduces a prompt customization method to reduce evaluation\nbiases and tap models' potential. TP-Eval will rewrite the original prompts to\ndifferent customized prompts for different models. In particular, we propose\nsome well-designed modules for prompt customization tailored to the scenario of\nMLLM evaluation. Extensive experiments demonstrate the effectiveness of our\napproach to uncovering models' capabilities, and TP-Eval should benefit the\ncommunity in developing more comprehensive and convincing MLLM evaluation\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, multimodal large language models (MLLMs) have received much\nattention for their impressive capabilities. The evaluation of MLLMs is\nbecoming critical to analyzing attributes of MLLMs and providing valuable\ninsights. However, current benchmarks overlook the problem of prompt\nsensitivity - minor prompt variations may lead to significant performance\nfluctuations. Thus, inappropriate prompts may obscure the models' capabilities,\nunderestimating the models' performance. Moreover, different models have\ndifferent preferences for different prompts, and thus, using the same prompt\nfor all models will cause evaluation bias. This paper analyzes this deficiency\nin existing benchmarks and further introduces a new evaluation framework named\nTP-Eval, which introduces a prompt customization method to reduce evaluation\nbiases and tap models' potential. TP-Eval will rewrite the original prompts to\ndifferent customized prompts for different models. In particular, we propose\nsome well-designed modules for prompt customization tailored to the scenario of\nMLLM evaluation. Extensive experiments demonstrate the effectiveness of our\napproach to uncovering models' capabilities, and TP-Eval should benefit the\ncommunity in developing more comprehensive and convincing MLLM evaluation\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Xie"
                    },
                    {
                        "name": "Tianhua Li"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03185v2",
                "updated": "2024-10-23T17:52:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    52,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-03-05T18:22:15Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    18,
                    22,
                    15,
                    1,
                    65,
                    0
                ],
                "title": "Correlated Proxies: A New Definition and Improved Mitigation for Reward\n  Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correlated Proxies: A New Definition and Improved Mitigation for Reward\n  Hacking"
                },
                "summary": "Because it is difficult to precisely specify complex objectives,\nreinforcement learning policies are often optimized using flawed proxy rewards\nthat seem to capture the true objective. However, optimizing proxy rewards\nfrequently leads to reward hacking: the optimized reward function ceases to be\na good proxy, and the resulting policy performs poorly with respect to the\nunspecified true reward. Principled solutions to reward hacking have been\nimpeded by the lack of a good definition for the problem. To address this, we\nintroduce a definition of reward hacking based on the correlation between proxy\nand true rewards for states and actions seen by a \"base policy\" that breaks\ndown under optimization. We show that this definition captures reward hacking\nbehavior across several realistic settings, including in reinforcement learning\nfrom human feedback (RLHF). We then show theoretically that regularization to\nthe base policy can effectively prevent reward hacking. While current RLHF\napproaches apply a KL penalty between the action distributions of policies, our\ntheory suggests that it is more effective to regularize using the $\\chi^2$\ndivergence between the policies' occupancy measures. We intuitively show why\nthis type of regularization is superior and demonstrate that it better\nmitigates reward hacking in practice across four realistic domains, including\nRLHF for LLMs. Our code is available at https://github.com/cassidylaidlaw/orpo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Because it is difficult to precisely specify complex objectives,\nreinforcement learning policies are often optimized using flawed proxy rewards\nthat seem to capture the true objective. However, optimizing proxy rewards\nfrequently leads to reward hacking: the optimized reward function ceases to be\na good proxy, and the resulting policy performs poorly with respect to the\nunspecified true reward. Principled solutions to reward hacking have been\nimpeded by the lack of a good definition for the problem. To address this, we\nintroduce a definition of reward hacking based on the correlation between proxy\nand true rewards for states and actions seen by a \"base policy\" that breaks\ndown under optimization. We show that this definition captures reward hacking\nbehavior across several realistic settings, including in reinforcement learning\nfrom human feedback (RLHF). We then show theoretically that regularization to\nthe base policy can effectively prevent reward hacking. While current RLHF\napproaches apply a KL penalty between the action distributions of policies, our\ntheory suggests that it is more effective to regularize using the $\\chi^2$\ndivergence between the policies' occupancy measures. We intuitively show why\nthis type of regularization is superior and demonstrate that it better\nmitigates reward hacking in practice across four realistic domains, including\nRLHF for LLMs. Our code is available at https://github.com/cassidylaidlaw/orpo."
                },
                "authors": [
                    {
                        "name": "Cassidy Laidlaw"
                    },
                    {
                        "name": "Shivam Singhal"
                    },
                    {
                        "name": "Anca Dragan"
                    }
                ],
                "author_detail": {
                    "name": "Anca Dragan"
                },
                "author": "Anca Dragan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19442v2",
                "updated": "2024-10-23T17:46:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    46,
                    13,
                    2,
                    297,
                    0
                ],
                "published": "2024-04-30T10:45:40Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    10,
                    45,
                    40,
                    1,
                    121,
                    0
                ],
                "title": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Generative AI speak Nigerian-Pidgin?: Issues about\n  Representativeness and Bias for Multilingualism in LLMs"
                },
                "summary": "Nigeria is a multilingual country with 500+ languages. Naija is a\nNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed\nlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has\nmainly been a spoken language until recently, there are now various platforms\npublishing exclusively in Naija such as Naija Wikipedia. However, it is hard to\ndistinguish by non-native from a larger pidgin languages spoken across West\nAfrica known as West African Pidgin English (WAPE) -- which is more simplied\nand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news\nplatform publishes exclusively in WAPE to cater for several countries in West\nAfrica. In our paper, we show through statistical analyses and Machine\nTranslation experiments that these two creole varieties do not represent each\nother (i.e., there are linguistic differences in word order and vocabulary) and\nGenerative AI operates only based on WAPE. In other words, Naija is\nunder-represented in Generative AI, and it is hard to teach LLMs with few\nexamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nigeria is a multilingual country with 500+ languages. Naija is a\nNigerian-Pidgin spoken by approx. 120M speakers in Nigeria and it is a mixed\nlanguage (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has\nmainly been a spoken language until recently, there are now various platforms\npublishing exclusively in Naija such as Naija Wikipedia. However, it is hard to\ndistinguish by non-native from a larger pidgin languages spoken across West\nAfrica known as West African Pidgin English (WAPE) -- which is more simplied\nand understandable by wider audience in Ghana, Nigeria, and Cameroon. BBC news\nplatform publishes exclusively in WAPE to cater for several countries in West\nAfrica. In our paper, we show through statistical analyses and Machine\nTranslation experiments that these two creole varieties do not represent each\nother (i.e., there are linguistic differences in word order and vocabulary) and\nGenerative AI operates only based on WAPE. In other words, Naija is\nunder-represented in Generative AI, and it is hard to teach LLMs with few\nexamples."
                },
                "authors": [
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "A. Seza Doruz"
                    },
                    {
                        "name": "Iyanuoluwa Shode"
                    },
                    {
                        "name": "Anuoluwapo Aremu"
                    }
                ],
                "author_detail": {
                    "name": "Anuoluwapo Aremu"
                },
                "author": "Anuoluwapo Aremu",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18053v1",
                "updated": "2024-10-23T17:26:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    26,
                    52,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:26:52Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    26,
                    52,
                    2,
                    297,
                    0
                ],
                "title": "B-Side: Binary-Level Static System Call Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B-Side: Binary-Level Static System Call Identification"
                },
                "summary": "System call filtering is widely used to secure programs in multi-tenant\nenvironments, and to sandbox applications in modern desktop software deployment\nand package management systems. Filtering rules are hard to write and maintain\nmanually, hence generating them automatically is essential. To that aim,\nanalysis tools able to identify every system call that can legitimately be\ninvoked by a program are needed. Existing static analysis works lack precision\nbecause of a high number of false positives, and/or assume the availability of\nprogram/libraries source code -- something unrealistic in many scenarios such\nas cloud production environments.\n  We present B-Side, a static binary analysis tool able to identify a superset\nof the system calls that an x86-64 static/dynamic executable may invoke at\nruntime. B-Side assumes no access to program/libraries sources, and shows a\ngood degree of precision by leveraging symbolic execution, combined with a\nheuristic to detect system call wrappers, which represent an important source\nof precision loss in existing works. B-Side also allows to statically detect\nphases of execution in a program in which different filtering rules can be\napplied. We validate B-Side and demonstrate its higher precision compared to\nstate-of-the-art works: over a set of popular applications, B-Side's average\n$F_1$ score is 0.81, vs. 0.31 and 0.53 for competitors. Over 557 static and\ndynamically-compiled binaries taken from the Debian repositories, B-Side\nidentifies an average of 43 system calls, vs. 271 and 95 for two state-of-the\nart competitors. We further evaluate the strictness of the phase-based\nfiltering policies that can be obtained with B-Side.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System call filtering is widely used to secure programs in multi-tenant\nenvironments, and to sandbox applications in modern desktop software deployment\nand package management systems. Filtering rules are hard to write and maintain\nmanually, hence generating them automatically is essential. To that aim,\nanalysis tools able to identify every system call that can legitimately be\ninvoked by a program are needed. Existing static analysis works lack precision\nbecause of a high number of false positives, and/or assume the availability of\nprogram/libraries source code -- something unrealistic in many scenarios such\nas cloud production environments.\n  We present B-Side, a static binary analysis tool able to identify a superset\nof the system calls that an x86-64 static/dynamic executable may invoke at\nruntime. B-Side assumes no access to program/libraries sources, and shows a\ngood degree of precision by leveraging symbolic execution, combined with a\nheuristic to detect system call wrappers, which represent an important source\nof precision loss in existing works. B-Side also allows to statically detect\nphases of execution in a program in which different filtering rules can be\napplied. We validate B-Side and demonstrate its higher precision compared to\nstate-of-the-art works: over a set of popular applications, B-Side's average\n$F_1$ score is 0.81, vs. 0.31 and 0.53 for competitors. Over 557 static and\ndynamically-compiled binaries taken from the Debian repositories, B-Side\nidentifies an average of 43 system calls, vs. 271 and 95 for two state-of-the\nart competitors. We further evaluate the strictness of the phase-based\nfiltering policies that can be obtained with B-Side."
                },
                "authors": [
                    {
                        "name": "Gaspard Thvenon"
                    },
                    {
                        "name": "Kevin Nguetchouang"
                    },
                    {
                        "name": "Kahina Lazri"
                    },
                    {
                        "name": "Alain Tchana"
                    },
                    {
                        "name": "Pierre Olivier"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Olivier"
                },
                "author": "Pierre Olivier",
                "arxiv_doi": "10.1145/3652892.3700761",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3652892.3700761",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.18053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to appear in the 25th ACM/IFIP International Middleware\n  Conference (Middleware'24)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02916v2",
                "updated": "2024-10-23T17:26:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    26,
                    6,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-03T19:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    19,
                    7,
                    53,
                    3,
                    277,
                    0
                ],
                "title": "Safeguard is a Double-edged Sword: Denial-of-service Attack on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguard is a Double-edged Sword: Denial-of-service Attack on Large\n  Language Models"
                },
                "summary": "Safety is a paramount concern of large language models (LLMs) in their open\ndeployment. To this end, safeguard methods aim to enforce the ethical and\nresponsible use of LLMs through safety alignment or guardrail mechanisms.\nHowever, we found that the malicious attackers could exploit false positives of\nsafeguards, i.e., fooling the safeguard model to block safe content mistakenly,\nleading to a new denial-of-service (DoS) attack on LLMs. Specifically, by\nsoftware or phishing attacks on user client software, attackers insert a short,\nseemingly innocuous adversarial prompt into to user prompt templates in\nconfiguration files; thus, this prompt appears in final user requests without\nvisibility in the user interface and is not trivial to identify. By designing\nan optimization process that utilizes gradient and attention information, our\nattack can automatically generate seemingly safe adversarial prompts,\napproximately only 30 characters long, that universally block over 97\\% of user\nrequests on Llama Guard 3. The attack presents a new dimension of evaluating\nLLM safeguards focusing on false positives, fundamentally different from the\nclassic jailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety is a paramount concern of large language models (LLMs) in their open\ndeployment. To this end, safeguard methods aim to enforce the ethical and\nresponsible use of LLMs through safety alignment or guardrail mechanisms.\nHowever, we found that the malicious attackers could exploit false positives of\nsafeguards, i.e., fooling the safeguard model to block safe content mistakenly,\nleading to a new denial-of-service (DoS) attack on LLMs. Specifically, by\nsoftware or phishing attacks on user client software, attackers insert a short,\nseemingly innocuous adversarial prompt into to user prompt templates in\nconfiguration files; thus, this prompt appears in final user requests without\nvisibility in the user interface and is not trivial to identify. By designing\nan optimization process that utilizes gradient and attention information, our\nattack can automatically generate seemingly safe adversarial prompts,\napproximately only 30 characters long, that universally block over 97\\% of user\nrequests on Llama Guard 3. The attack presents a new dimension of evaluating\nLLM safeguards focusing on false positives, fundamentally different from the\nclassic jailbreak."
                },
                "authors": [
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Ziyang Xiong"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18050v1",
                "updated": "2024-10-23T17:24:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    24,
                    58,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:24:58Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    24,
                    58,
                    2,
                    297,
                    0
                ],
                "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for\n  Long-Context Question Answering"
                },
                "summary": "Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Question Answering (LCQA), a challenging task, aims to reason\nover long-context documents to yield accurate answers to questions. Existing\nlong-context Large Language Models (LLMs) for LCQA often struggle with the\n\"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this\nissue by providing external factual evidence. However, its chunking strategy\ndisrupts the global long-context information, and its low-quality retrieval in\nlong contexts hinders LLMs from identifying effective factual details due to\nsubstantial noise. To this end, we propose LongRAG, a general,\ndual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance\nRAG's understanding of complex long-context knowledge (i.e., global information\nand factual details). We design LongRAG as a plug-and-play paradigm,\nfacilitating adaptation to various domains and LLMs. Extensive experiments on\nthree multi-hop datasets demonstrate that LongRAG significantly outperforms\nlong-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG\n(up by 17.25%). Furthermore, we conduct quantitative ablation studies and\nmulti-dimensional analyses, highlighting the effectiveness of the system's\ncomponents and fine-tuning strategies. Data and code are available at\nhttps://github.com/QingFei1/LongRAG."
                },
                "authors": [
                    {
                        "name": "Qingfei Zhao"
                    },
                    {
                        "name": "Ruobing Wang"
                    },
                    {
                        "name": "Yukuo Cen"
                    },
                    {
                        "name": "Daren Zha"
                    },
                    {
                        "name": "Shicheng Tan"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18041v1",
                "updated": "2024-10-23T17:08:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    8,
                    1,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:08:01Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    8,
                    1,
                    2,
                    297,
                    0
                ],
                "title": "Evaluating the performance of machine-learning-based phase pickers when\n  applied to ocean bottom seismic data: Blanco oceanic transform fault as a\n  case study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the performance of machine-learning-based phase pickers when\n  applied to ocean bottom seismic data: Blanco oceanic transform fault as a\n  case study"
                },
                "summary": "Machine-learning-based phase pickers have been successfully leveraged to\nbuild high-resolution earthquake catalogs using seismic data on land. However,\ntheir performance when applied to ocean bottom seismic (OBS) data remains to be\nevaluated. In this study, we first adopt three machine-learning-based phase\npickers - EQTransformer, Pickblue, and OBSTansformer - to build three\nearthquake catalogs for the 350-km-long Blanco oceanic transform fault (BTF)\nbased on a year-long OBS deployment. We then systematically compare these\ncatalogs with an existing catalog which utilized a traditional workflow.\nResults indicate that the Pickblue-based catalog documents more events and/or\nprovides better-constrained locations than the other catalogs. The different\nperformances of the three phase pickers suggest that detailed assessment of\ncatalogs built using automatic workflows is necessary to prevent\nmisinterpretations, especially when applied to regions without training\nsamples. The Pickblue-based catalog reveals seismicity gaps in three\nextensional segments of BTF which likely represent aseismic slip zones affected\nby seawater infiltration. Furthermore, most earthquakes are shallower than the\n600-degree isotherm predicted by a half-space conductive cooling model, except\nfor the Blanco Ridge segment which has hosted 80% of the Mw > 6.0 earthquakes\nalong BTF since 1976. These Blanco Ridge deep earthquake clusters can be\nexplained by hydrothermal cooling or the serpentinization of mantle peridotite\ndue to seawater infiltration along conduits created by the deeper ruptures of\nlarge earthquakes. Our analyses also demonstrate the importance of careful\nexamination of automatically produced earthquake catalogs since mislocated\nevents can lead to very different interpretations of fault slip modes from\nseismicity distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-learning-based phase pickers have been successfully leveraged to\nbuild high-resolution earthquake catalogs using seismic data on land. However,\ntheir performance when applied to ocean bottom seismic (OBS) data remains to be\nevaluated. In this study, we first adopt three machine-learning-based phase\npickers - EQTransformer, Pickblue, and OBSTansformer - to build three\nearthquake catalogs for the 350-km-long Blanco oceanic transform fault (BTF)\nbased on a year-long OBS deployment. We then systematically compare these\ncatalogs with an existing catalog which utilized a traditional workflow.\nResults indicate that the Pickblue-based catalog documents more events and/or\nprovides better-constrained locations than the other catalogs. The different\nperformances of the three phase pickers suggest that detailed assessment of\ncatalogs built using automatic workflows is necessary to prevent\nmisinterpretations, especially when applied to regions without training\nsamples. The Pickblue-based catalog reveals seismicity gaps in three\nextensional segments of BTF which likely represent aseismic slip zones affected\nby seawater infiltration. Furthermore, most earthquakes are shallower than the\n600-degree isotherm predicted by a half-space conductive cooling model, except\nfor the Blanco Ridge segment which has hosted 80% of the Mw > 6.0 earthquakes\nalong BTF since 1976. These Blanco Ridge deep earthquake clusters can be\nexplained by hydrothermal cooling or the serpentinization of mantle peridotite\ndue to seawater infiltration along conduits created by the deeper ruptures of\nlarge earthquakes. Our analyses also demonstrate the importance of careful\nexamination of automatically produced earthquake catalogs since mislocated\nevents can lead to very different interpretations of fault slip modes from\nseismicity distribution."
                },
                "authors": [
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Yen Joe Tan"
                    }
                ],
                "author_detail": {
                    "name": "Yen Joe Tan"
                },
                "author": "Yen Joe Tan",
                "arxiv_comment": "38 pages and 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18040v1",
                "updated": "2024-10-23T17:07:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    7,
                    32,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:07:32Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    7,
                    32,
                    2,
                    297,
                    0
                ],
                "title": "Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for\n  Russian Scientific Keyphrases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for\n  Russian Scientific Keyphrases"
                },
                "summary": "Keyphrase selection is a challenging task in natural language processing that\nhas a wide range of applications. Adapting existing supervised and unsupervised\nsolutions for the Russian language faces several limitations due to the rich\nmorphology of Russian and the limited number of training datasets available.\nRecent studies conducted on English texts show that large language models\n(LLMs) successfully address the task of generating keyphrases. LLMs allow\nachieving impressive results without task-specific fine-tuning, using text\nprompts instead. In this work, we access the performance of prompt-based\nmethods for generating keyphrases for Russian scientific abstracts. First, we\ncompare the performance of zero-shot and few-shot prompt-based methods,\nfine-tuned models, and unsupervised methods. Then we assess strategies for\nselecting keyphrase examples in a few-shot setting. We present the outcomes of\nhuman evaluation of the generated keyphrases and analyze the strengths and\nweaknesses of the models through expert assessment. Our results suggest that\nprompt-based methods can outperform common baselines even using simple text\nprompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyphrase selection is a challenging task in natural language processing that\nhas a wide range of applications. Adapting existing supervised and unsupervised\nsolutions for the Russian language faces several limitations due to the rich\nmorphology of Russian and the limited number of training datasets available.\nRecent studies conducted on English texts show that large language models\n(LLMs) successfully address the task of generating keyphrases. LLMs allow\nachieving impressive results without task-specific fine-tuning, using text\nprompts instead. In this work, we access the performance of prompt-based\nmethods for generating keyphrases for Russian scientific abstracts. First, we\ncompare the performance of zero-shot and few-shot prompt-based methods,\nfine-tuned models, and unsupervised methods. Then we assess strategies for\nselecting keyphrase examples in a few-shot setting. We present the outcomes of\nhuman evaluation of the generated keyphrases and analyze the strengths and\nweaknesses of the models through expert assessment. Our results suggest that\nprompt-based methods can outperform common baselines even using simple text\nprompts."
                },
                "authors": [
                    {
                        "name": "Anna Glazkova"
                    },
                    {
                        "name": "Dmitry Morozov"
                    },
                    {
                        "name": "Timur Garipov"
                    }
                ],
                "author_detail": {
                    "name": "Timur Garipov"
                },
                "author": "Timur Garipov",
                "arxiv_comment": "The 12th International Conference on Analysis of Images, Social\n  Networks and Texts (AIST'2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.7.m; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18038v1",
                "updated": "2024-10-23T17:06:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    6,
                    56,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:06:56Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    6,
                    56,
                    2,
                    297,
                    0
                ],
                "title": "POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM\n  Inference"
                },
                "summary": "Each request in LLM inference goes through two phases: compute-bound prefill\nand memory-bandwidth-bound decode. To improve GPU utilization, recent systems\nuse hybrid batching that combines the prefill and decode phases of different\nrequests into the same batch. Hybrid batching works well for linear operations\nas it amortizes the cost of loading model weights from HBM. However, attention\ncomputation in hybrid batches remains inefficient because existing attention\nkernels are optimized for either prefill or decode.\n  In this paper, we present POD-Attention -- the first GPU kernel that\nefficiently computes attention for hybrid batches. POD-Attention aims to\nmaximize the utilization of both compute and memory bandwidth by carefully\nallocating the GPU's resources such that prefill and decode operations happen\nconcurrently on the same multiprocessor. We integrate POD-Attention in a\nstate-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up\nattention computation by up to 75% (mean 28%) and increases LLM serving\nthroughput by up to 22% in offline inference. In online inference,\nPOD-Attention enables lower time-to-first-token (TTFT), time-between-tokens\n(TBT), and request execution latency versus Sarathi-Serve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Each request in LLM inference goes through two phases: compute-bound prefill\nand memory-bandwidth-bound decode. To improve GPU utilization, recent systems\nuse hybrid batching that combines the prefill and decode phases of different\nrequests into the same batch. Hybrid batching works well for linear operations\nas it amortizes the cost of loading model weights from HBM. However, attention\ncomputation in hybrid batches remains inefficient because existing attention\nkernels are optimized for either prefill or decode.\n  In this paper, we present POD-Attention -- the first GPU kernel that\nefficiently computes attention for hybrid batches. POD-Attention aims to\nmaximize the utilization of both compute and memory bandwidth by carefully\nallocating the GPU's resources such that prefill and decode operations happen\nconcurrently on the same multiprocessor. We integrate POD-Attention in a\nstate-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up\nattention computation by up to 75% (mean 28%) and increases LLM serving\nthroughput by up to 22% in offline inference. In online inference,\nPOD-Attention enables lower time-to-first-token (TTFT), time-between-tokens\n(TBT), and request execution latency versus Sarathi-Serve."
                },
                "authors": [
                    {
                        "name": "Aditya K Kamath"
                    },
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Simon Peter"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18035v1",
                "updated": "2024-10-23T17:04:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    4,
                    40,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:04:40Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    4,
                    40,
                    2,
                    297,
                    0
                ],
                "title": "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language\n  Models Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiLoRA: Efficient Mixture of Low-Rank Adaptation for Large Language\n  Models Fine-tuning"
                },
                "summary": "Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are\nhighly effective parameter-efficient fine-tuning (PEFT) methods. However, they\nintroduce significant latency in multi-tenant settings due to the LoRA modules\nand MOE routers added to multiple linear modules in the Transformer layer. To\naddress this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel\nand efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods\nby considering each LoRA module as an expert and employing a prompt-aware\nrouting mechanism. This mechanism calculates expert routing results once before\ngenerating the first new token and reuses these results for subsequent tokens,\nreducing latency. Extensive experiments and analysis on commonsense reasoning\ntasks, math reasoning tasks, and widely used LLM evaluation benchmarks\ndemonstrate that MiLoRA consistently outperforms strong PEFT baselines with\ncomparable tunable parameter budgets. Additionally, MiLoRA significantly\nreduces latency in multi-tenant settings compared to previous LoRA-based\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptation (LoRA) and its mixture-of-experts (MOE) variants are\nhighly effective parameter-efficient fine-tuning (PEFT) methods. However, they\nintroduce significant latency in multi-tenant settings due to the LoRA modules\nand MOE routers added to multiple linear modules in the Transformer layer. To\naddress this issue, we propose Mixture of Low-Rank Adaptation (MiLoRA), a novel\nand efficient LoRA variant. MiLoRA differs from previous MOE-style LoRA methods\nby considering each LoRA module as an expert and employing a prompt-aware\nrouting mechanism. This mechanism calculates expert routing results once before\ngenerating the first new token and reuses these results for subsequent tokens,\nreducing latency. Extensive experiments and analysis on commonsense reasoning\ntasks, math reasoning tasks, and widely used LLM evaluation benchmarks\ndemonstrate that MiLoRA consistently outperforms strong PEFT baselines with\ncomparable tunable parameter budgets. Additionally, MiLoRA significantly\nreduces latency in multi-tenant settings compared to previous LoRA-based\nmethods."
                },
                "authors": [
                    {
                        "name": "Jingfan Zhang"
                    },
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Dan Chen"
                    },
                    {
                        "name": "Xing Tian"
                    },
                    {
                        "name": "Huanran Zheng"
                    },
                    {
                        "name": "Wei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhu"
                },
                "author": "Wei Zhu",
                "arxiv_comment": "Accepted by EMNLP 2024 Findings. arXiv admin note: substantial text\n  overlap with arXiv:2405.18203",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18032v1",
                "updated": "2024-10-23T17:02:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    2,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T17:02:59Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    2,
                    59,
                    2,
                    297,
                    0
                ],
                "title": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration"
                },
                "summary": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Qizhi Chu"
                    },
                    {
                        "name": "Yubin Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yaoqi Liu"
                    },
                    {
                        "name": "Zekai Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Cheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yang"
                },
                "author": "Cheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12025v2",
                "updated": "2024-10-23T17:01:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    1,
                    5,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-21T22:35:19Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    22,
                    35,
                    19,
                    2,
                    234,
                    0
                ],
                "title": "Exploring Large Language Models for Feature Selection: A Data-centric\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Models for Feature Selection: A Data-centric\n  Perspective"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\ninfluenced various domains, leveraging their exceptional few-shot and zero-shot\nlearning capabilities. In this work, we aim to explore and understand the\nLLMs-based feature selection methods from a data-centric perspective. We begin\nby categorizing existing feature selection methods with LLMs into two groups:\ndata-driven feature selection which requires numerical values of samples to do\nstatistical inference and text-based feature selection which utilizes prior\nknowledge of LLMs to do semantical associations using descriptive context. We\nconduct experiments in both classification and regression tasks with LLMs in\nvarious sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the\neffectiveness and robustness of text-based feature selection methods and\nshowcase their potentials using a real-world medical application. We also\ndiscuss the challenges and future opportunities in employing LLMs for feature\nselection, offering insights for further research and development in this\nemerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has significantly\ninfluenced various domains, leveraging their exceptional few-shot and zero-shot\nlearning capabilities. In this work, we aim to explore and understand the\nLLMs-based feature selection methods from a data-centric perspective. We begin\nby categorizing existing feature selection methods with LLMs into two groups:\ndata-driven feature selection which requires numerical values of samples to do\nstatistical inference and text-based feature selection which utilizes prior\nknowledge of LLMs to do semantical associations using descriptive context. We\nconduct experiments in both classification and regression tasks with LLMs in\nvarious sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the\neffectiveness and robustness of text-based feature selection methods and\nshowcase their potentials using a real-world medical application. We also\ndiscuss the challenges and future opportunities in employing LLMs for feature\nselection, offering insights for further research and development in this\nemerging field."
                },
                "authors": [
                    {
                        "name": "Dawei Li"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "Accepted by SIGKDD Explorations (December 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18012v1",
                "updated": "2024-10-23T16:40:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    40,
                    38,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:40:38Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    40,
                    38,
                    2,
                    297,
                    0
                ],
                "title": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC\n  Meeting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniFed : Integrating LLM-based Agentic-Workflow for Simulating FOMC\n  Meeting"
                },
                "summary": "The Federal Funds rate in the United States plays a significant role in both\ndomestic and international financial markets. However, research has\npredominantly focused on the effects of adjustments to the Federal Funds rate\nrather than on the decision-making process itself. Recent advancements in large\nlanguage models(LLMs) offer a potential method for reconstructing the original\nFOMC meetings, which are responsible for setting the Federal Funds rate. In\nthis paper, we propose a five-stage FOMC meeting simulation framework, MiniFed,\nwhich employs LLM agents to simulate real-world FOMC meeting members and\noptimize the FOMC structure. This framework effectively revitalizes the FOMC\nmeeting process and facilitates projections of the Federal Funds rate.\nExperimental results demonstrate that our proposed MiniFed framework achieves\nboth high accuracy in Federal Funds rate projections and behavioral alignment\nwith the agents' real-world counterparts. Given that few studies have focused\non employing LLM agents to simulate large-scale real-world conferences, our\nwork can serve as a benchmark for future developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Federal Funds rate in the United States plays a significant role in both\ndomestic and international financial markets. However, research has\npredominantly focused on the effects of adjustments to the Federal Funds rate\nrather than on the decision-making process itself. Recent advancements in large\nlanguage models(LLMs) offer a potential method for reconstructing the original\nFOMC meetings, which are responsible for setting the Federal Funds rate. In\nthis paper, we propose a five-stage FOMC meeting simulation framework, MiniFed,\nwhich employs LLM agents to simulate real-world FOMC meeting members and\noptimize the FOMC structure. This framework effectively revitalizes the FOMC\nmeeting process and facilitates projections of the Federal Funds rate.\nExperimental results demonstrate that our proposed MiniFed framework achieves\nboth high accuracy in Federal Funds rate projections and behavioral alignment\nwith the agents' real-world counterparts. Given that few studies have focused\non employing LLM agents to simulate large-scale real-world conferences, our\nwork can serve as a benchmark for future developments."
                },
                "authors": [
                    {
                        "name": "Sungil Seok"
                    },
                    {
                        "name": "Qiyuan Yang"
                    },
                    {
                        "name": "Juan Feng"
                    },
                    {
                        "name": "Shuide Wen"
                    },
                    {
                        "name": "Wenming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wenming Yang"
                },
                "author": "Wenming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16664v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16664v3",
                "updated": "2024-10-23T16:27:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    27,
                    48,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-26T15:35:24Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    15,
                    35,
                    24,
                    0,
                    57,
                    0
                ],
                "title": "LLM-Assisted Multi-Teacher Continual Learning for Visual Question\n  Answering in Robotic Surgery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Multi-Teacher Continual Learning for Visual Question\n  Answering in Robotic Surgery"
                },
                "summary": "Visual question answering (VQA) is crucial for promoting surgical education.\nIn practice, the needs of trainees are constantly evolving, such as learning\nmore surgical types, adapting to different robots, and learning new surgical\ninstruments and techniques for various surgeries. However, patient data privacy\noften restricts the availability of old data when updating the model,\nnecessitating an exemplar-free continual learning (CL) setup. Prior CL studies\noverlooked two vital problems in the surgical domain: 1) large domain shifts\nfrom diverse surgical operations collected from multiple sources, and 2) severe\ndata imbalance arising from the uneven presence of surgical instruments or\nactivities. This paper proposes addressing these problems with a multimodal\nlarge language model (LLM) and an adaptive weight assignment methodology. We\nfirst develop a new multi-teacher CL framework that leverages a multimodal LLM\nas the additional teacher. The strong generalization ability of the LLM can\nbridge the knowledge gap when domain shifts and data imbalances occur. We then\nput forth a novel data processing method that transforms complex LLM embeddings\ninto logits compatible with our CL framework. We further design an adaptive\nweight assignment approach that balances the generalization ability of the LLM\nand the domain expertise of the old CL model. Finally, to comprehensively test\nthe effectiveness of our proposed method, we have also constructed two new\nsurgical VQA datasets that are largely different from existing ones and could\nbe valuable resources for future research. Extensive experimental results on\nthe tested datasets demonstrate the superiority of our method to other advanced\nCL schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual question answering (VQA) is crucial for promoting surgical education.\nIn practice, the needs of trainees are constantly evolving, such as learning\nmore surgical types, adapting to different robots, and learning new surgical\ninstruments and techniques for various surgeries. However, patient data privacy\noften restricts the availability of old data when updating the model,\nnecessitating an exemplar-free continual learning (CL) setup. Prior CL studies\noverlooked two vital problems in the surgical domain: 1) large domain shifts\nfrom diverse surgical operations collected from multiple sources, and 2) severe\ndata imbalance arising from the uneven presence of surgical instruments or\nactivities. This paper proposes addressing these problems with a multimodal\nlarge language model (LLM) and an adaptive weight assignment methodology. We\nfirst develop a new multi-teacher CL framework that leverages a multimodal LLM\nas the additional teacher. The strong generalization ability of the LLM can\nbridge the knowledge gap when domain shifts and data imbalances occur. We then\nput forth a novel data processing method that transforms complex LLM embeddings\ninto logits compatible with our CL framework. We further design an adaptive\nweight assignment approach that balances the generalization ability of the LLM\nand the domain expertise of the old CL model. Finally, to comprehensively test\nthe effectiveness of our proposed method, we have also constructed two new\nsurgical VQA datasets that are largely different from existing ones and could\nbe valuable resources for future research. Extensive experimental results on\nthe tested datasets demonstrate the superiority of our method to other advanced\nCL schemes."
                },
                "authors": [
                    {
                        "name": "Yuyang Du"
                    },
                    {
                        "name": "Kexin Chen"
                    },
                    {
                        "name": "Yue Zhan"
                    },
                    {
                        "name": "Chang Han Low"
                    },
                    {
                        "name": "Tao You"
                    },
                    {
                        "name": "Mobarakol Islam"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Yueming Jin"
                    },
                    {
                        "name": "Guangyong Chen"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "arxiv_comment": "This paper has been accapted by 2024 IEEE International Conference on\n  Robotics and Automation (ICRA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16664v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16664v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17270v2",
                "updated": "2024-10-23T16:27:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    27,
                    20,
                    2,
                    297,
                    0
                ],
                "published": "2024-09-25T18:35:45Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    35,
                    45,
                    2,
                    269,
                    0
                ],
                "title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains."
                },
                "authors": [
                    {
                        "name": "Debargha Ganguly"
                    },
                    {
                        "name": "Srinivasan Iyengar"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Shivkumar Kalyanaraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivkumar Kalyanaraman"
                },
                "author": "Shivkumar Kalyanaraman",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024) System 2 Reasoning At Scale Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14979v2",
                "updated": "2024-10-23T15:43:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    43,
                    28,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-19T05:01:56Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    5,
                    1,
                    56,
                    5,
                    293,
                    0
                ],
                "title": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration"
                },
                "summary": "Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence."
                },
                "authors": [
                    {
                        "name": "Wei Xie"
                    },
                    {
                        "name": "Shuoyoucheng Ma"
                    },
                    {
                        "name": "Zhenhua Wang"
                    },
                    {
                        "name": "Enze Wang"
                    },
                    {
                        "name": "Baosheng Wang"
                    },
                    {
                        "name": "Jinshu Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinshu Su"
                },
                "author": "Jinshu Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17952v1",
                "updated": "2024-10-23T15:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    16,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    16,
                    2,
                    297,
                    0
                ],
                "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large\n  Language Models to Specialized Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large\n  Language Models to Specialized Domains"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the question-answering (QA)\nabilities of large language models (LLMs) by integrating external knowledge.\nHowever, adapting general-purpose RAG systems to specialized fields such as\nscience and medicine poses unique challenges due to distribution shifts and\nlimited access to domain-specific data. To tackle this, we propose SimRAG, a\nself-training approach that equips the LLM with joint capabilities of question\nanswering and question generation for domain adaptation. Our method first\nfine-tunes the LLM on instruction-following, question-answering, and\nsearch-related data. Then, it prompts the same LLM to generate diverse\ndomain-relevant questions from unlabeled corpora, with an additional filtering\nstrategy to retain high-quality synthetic examples. By leveraging these\nsynthetic examples, the LLM can improve their performance on domain-specific\nRAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three\ndomains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the question-answering (QA)\nabilities of large language models (LLMs) by integrating external knowledge.\nHowever, adapting general-purpose RAG systems to specialized fields such as\nscience and medicine poses unique challenges due to distribution shifts and\nlimited access to domain-specific data. To tackle this, we propose SimRAG, a\nself-training approach that equips the LLM with joint capabilities of question\nanswering and question generation for domain adaptation. Our method first\nfine-tunes the LLM on instruction-following, question-answering, and\nsearch-related data. Then, it prompts the same LLM to generate diverse\ndomain-relevant questions from unlabeled corpora, with an additional filtering\nstrategy to retain high-quality synthetic examples. By leveraging these\nsynthetic examples, the LLM can improve their performance on domain-specific\nRAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three\ndomains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%."
                },
                "authors": [
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Sreyashi Nag"
                    },
                    {
                        "name": "Zhenwei Dai"
                    },
                    {
                        "name": "Yaochen Xie"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Chen Luo"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Joyce C. Ho"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Qi He"
                    }
                ],
                "author_detail": {
                    "name": "Qi He"
                },
                "author": "Qi He",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17950v1",
                "updated": "2024-10-23T15:23:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    23,
                    23,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:23:23Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    23,
                    23,
                    2,
                    297,
                    0
                ],
                "title": "Benchmarking Floworks against OpenAI & Anthropic: A Novel Framework for\n  Enhanced LLM Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Floworks against OpenAI & Anthropic: A Novel Framework for\n  Enhanced LLM Function Calling"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in various\ndomains, yet their economic impact has been limited by challenges in tool use\nand function calling. This paper introduces ThorV2, a novel architecture that\nsignificantly enhances LLMs' function calling abilities. We develop a\ncomprehensive benchmark focused on HubSpot CRM operations to evaluate ThorV2\nagainst leading models from OpenAI and Anthropic. Our results demonstrate that\nThorV2 outperforms existing models in accuracy, reliability, latency, and cost\nefficiency for both single and multi-API calling tasks. We also show that\nThorV2 is far more reliable and scales better to multistep tasks compared to\ntraditional models. Our work offers the tantalizing possibility of more\naccurate function-calling compared to today's best-performing models using\nsignificantly smaller LLMs. These advancements have significant implications\nfor the development of more capable AI assistants and the broader application\nof LLMs in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in various\ndomains, yet their economic impact has been limited by challenges in tool use\nand function calling. This paper introduces ThorV2, a novel architecture that\nsignificantly enhances LLMs' function calling abilities. We develop a\ncomprehensive benchmark focused on HubSpot CRM operations to evaluate ThorV2\nagainst leading models from OpenAI and Anthropic. Our results demonstrate that\nThorV2 outperforms existing models in accuracy, reliability, latency, and cost\nefficiency for both single and multi-API calling tasks. We also show that\nThorV2 is far more reliable and scales better to multistep tasks compared to\ntraditional models. Our work offers the tantalizing possibility of more\naccurate function-calling compared to today's best-performing models using\nsignificantly smaller LLMs. These advancements have significant implications\nfor the development of more capable AI assistants and the broader application\nof LLMs in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Nirav Bhan"
                    },
                    {
                        "name": "Shival Gupta"
                    },
                    {
                        "name": "Sai Manaswini"
                    },
                    {
                        "name": "Ritik Baba"
                    },
                    {
                        "name": "Narun Yadav"
                    },
                    {
                        "name": "Hillori Desai"
                    },
                    {
                        "name": "Yash Choudhary"
                    },
                    {
                        "name": "Aman Pawar"
                    },
                    {
                        "name": "Sarthak Shrivastava"
                    },
                    {
                        "name": "Sudipta Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Sudipta Biswas"
                },
                "author": "Sudipta Biswas",
                "arxiv_comment": "15 pages for main paper, 21 pages in total including references and\n  appendix, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12295v2",
                "updated": "2024-10-23T15:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    23,
                    0,
                    2,
                    297,
                    0
                ],
                "published": "2024-06-18T05:59:28Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    5,
                    59,
                    28,
                    1,
                    170,
                    0
                ],
                "title": "Fast and Slow Generating: An Empirical Study on Large and Small Language\n  Models Collaborative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Slow Generating: An Empirical Study on Large and Small Language\n  Models Collaborative Decoding"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive capabilities across various\napplications but encounter substantial challenges such as high inference\nlatency, considerable training costs, and the generation of hallucinations.\nCollaborative decoding between large and small language models (SLMs) presents\na promising strategy to mitigate these issues through methods including\nspeculative decoding, contrastive decoding, and emulator or proxy fine-tuning.\nHowever, the specifics of such collaborations, particularly from a unified\nperspective, remain largely unexplored. Inspired by dual-process cognitive\ntheory, we propose a unified framework in this paper, termed Fast and Slow\nGenerating (FS-GEN). Within this framework, LLMs (sometimes along with SLMs)\nare categorized as System 2 (slow and deliberate), while independent SLMs are\ndesignated as System 1 (fast and intuitive). We provide a comprehensive\nanalysis of these collaborative methodologies, elucidating their common\nproperties and shedding light on the differential knowledge capabilities of\nSystem 2 versus System 1 through the FS-GEN framework. Our findings indicate\nthat only a small proportion of collaborative interactions (approximately less\nthan 20\\% in most instances) are necessary across various methods. These\ninteractions between System 1 and System 2 conform to a scaling law related to\nthe parameter ratios, enabling predictable collaboration. Furthermore, we\nexplore the specific conditions under which collaboration proves most\neffective, particularly from an uncertainty perspective, offering novel\ninsights that may guide future optimization efforts. Our research underscores\nthat the fundamental distinction between System 1 and System 2 lies in the\nuncertainty of next token predictions, where interventions by System 2 are\ncrucial to support System 1. Code for Reproduction:\nhttps://github.com/TsinghuaC3I/FS-GEN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive capabilities across various\napplications but encounter substantial challenges such as high inference\nlatency, considerable training costs, and the generation of hallucinations.\nCollaborative decoding between large and small language models (SLMs) presents\na promising strategy to mitigate these issues through methods including\nspeculative decoding, contrastive decoding, and emulator or proxy fine-tuning.\nHowever, the specifics of such collaborations, particularly from a unified\nperspective, remain largely unexplored. Inspired by dual-process cognitive\ntheory, we propose a unified framework in this paper, termed Fast and Slow\nGenerating (FS-GEN). Within this framework, LLMs (sometimes along with SLMs)\nare categorized as System 2 (slow and deliberate), while independent SLMs are\ndesignated as System 1 (fast and intuitive). We provide a comprehensive\nanalysis of these collaborative methodologies, elucidating their common\nproperties and shedding light on the differential knowledge capabilities of\nSystem 2 versus System 1 through the FS-GEN framework. Our findings indicate\nthat only a small proportion of collaborative interactions (approximately less\nthan 20\\% in most instances) are necessary across various methods. These\ninteractions between System 1 and System 2 conform to a scaling law related to\nthe parameter ratios, enabling predictable collaboration. Furthermore, we\nexplore the specific conditions under which collaboration proves most\neffective, particularly from an uncertainty perspective, offering novel\ninsights that may guide future optimization efforts. Our research underscores\nthat the fundamental distinction between System 1 and System 2 lies in the\nuncertainty of next token predictions, where interventions by System 2 are\ncrucial to support System 1. Code for Reproduction:\nhttps://github.com/TsinghuaC3I/FS-GEN"
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "update figures and results on Pythia Series",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04957v3",
                "updated": "2024-10-23T15:08:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    8,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-07T15:40:22Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    15,
                    40,
                    22,
                    2,
                    38,
                    0
                ],
                "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfidencing LLMs from the Grouping Loss Perspective"
                },
                "summary": "Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to\ngenerating hallucinated answers in a confident tone. While efforts to elicit\nand calibrate confidence scores have proven useful, recent findings show that\ncontrolling uncertainty must go beyond calibration: predicted scores may\ndeviate significantly from the actual posterior probabilities due to the impact\nof grouping loss. In this work, we construct a new evaluation dataset derived\nfrom a knowledge base to assess confidence scores given to answers of Mistral\nand LLaMA. Experiments show that they tend to be overconfident. Further, we\nshow that they are more overconfident on some answers than others, \\emph{eg}\ndepending on the nationality of the person in the query. In\nuncertainty-quantification theory, this is grouping loss. To address this, we\npropose a solution to reconfidence LLMs, canceling not only calibration but\nalso grouping loss. The LLMs, after the reconfidencing process, indicate\nimproved confidence alignment with the accuracy of their responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to\ngenerating hallucinated answers in a confident tone. While efforts to elicit\nand calibrate confidence scores have proven useful, recent findings show that\ncontrolling uncertainty must go beyond calibration: predicted scores may\ndeviate significantly from the actual posterior probabilities due to the impact\nof grouping loss. In this work, we construct a new evaluation dataset derived\nfrom a knowledge base to assess confidence scores given to answers of Mistral\nand LLaMA. Experiments show that they tend to be overconfident. Further, we\nshow that they are more overconfident on some answers than others, \\emph{eg}\ndepending on the nationality of the person in the query. In\nuncertainty-quantification theory, this is grouping loss. To address this, we\npropose a solution to reconfidence LLMs, canceling not only calibration but\nalso grouping loss. The LLMs, after the reconfidencing process, indicate\nimproved confidence alignment with the accuracy of their responses."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Alexandre Perez-Lebel"
                    },
                    {
                        "name": "Fabian M. Suchanek"
                    },
                    {
                        "name": "Gal Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gal Varoquaux"
                },
                "author": "Gal Varoquaux",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01622v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01622v4",
                "updated": "2024-10-23T15:02:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    2,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-02T18:39:51Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    18,
                    39,
                    51,
                    4,
                    33,
                    0
                ],
                "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents"
                },
                "summary": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents."
                },
                "authors": [
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Tinghui Zhu"
                    },
                    {
                        "name": "Renze Lou"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "arxiv_comment": "ICML 2024 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01622v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01622v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.05876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.05876v3",
                "updated": "2024-10-23T14:48:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    48,
                    20,
                    2,
                    297,
                    0
                ],
                "published": "2023-11-10T05:24:04Z",
                "published_parsed": [
                    2023,
                    11,
                    10,
                    5,
                    24,
                    4,
                    4,
                    314,
                    0
                ],
                "title": "Trends in Integration of Knowledge and Large Language Models: A Survey\n  and Taxonomy of Methods, Benchmarks, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trends in Integration of Knowledge and Large Language Models: A Survey\n  and Taxonomy of Methods, Benchmarks, and Applications"
                },
                "summary": "Large language models (LLMs) exhibit superior performance on various natural\nlanguage tasks, but they are susceptible to issues stemming from outdated data\nand domain-specific limitations. In order to address these challenges,\nresearchers have pursued two primary strategies, knowledge editing and\nretrieval augmentation, to enhance LLMs by incorporating external information\nfrom different aspects. Nevertheless, there is still a notable absence of a\ncomprehensive survey. In this paper, we propose a review to discuss the trends\nin integration of knowledge and large language models, including taxonomy of\nmethods, benchmarks, and applications. In addition, we conduct an in-depth\nanalysis of different methods and point out potential research directions in\nthe future. We hope this survey offers the community quick access and a\ncomprehensive overview of this research area, with the intention of inspiring\nfuture research endeavors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit superior performance on various natural\nlanguage tasks, but they are susceptible to issues stemming from outdated data\nand domain-specific limitations. In order to address these challenges,\nresearchers have pursued two primary strategies, knowledge editing and\nretrieval augmentation, to enhance LLMs by incorporating external information\nfrom different aspects. Nevertheless, there is still a notable absence of a\ncomprehensive survey. In this paper, we propose a review to discuss the trends\nin integration of knowledge and large language models, including taxonomy of\nmethods, benchmarks, and applications. In addition, we conduct an in-depth\nanalysis of different methods and point out potential research directions in\nthe future. We hope this survey offers the community quick access and a\ncomprehensive overview of this research area, with the intention of inspiring\nfuture research endeavors."
                },
                "authors": [
                    {
                        "name": "Zhangyin Feng"
                    },
                    {
                        "name": "Weitao Ma"
                    },
                    {
                        "name": "Weijiang Yu"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Qianglong Chen"
                    },
                    {
                        "name": "Weihua Peng"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting liu"
                },
                "author": "Ting liu",
                "arxiv_comment": "Work in progress; 22 pages. This work has been submitted to the IEEE\n  for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.05876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.05876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17927v1",
                "updated": "2024-10-23T14:47:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    47,
                    0,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:47:00Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    47,
                    0,
                    2,
                    297,
                    0
                ],
                "title": "Dynamic Modeling and Vibration Analysis of Large Deployable Mesh\n  Reflectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Modeling and Vibration Analysis of Large Deployable Mesh\n  Reflectors"
                },
                "summary": "Large deployable mesh reflectors are essential for space applications,\nproviding precise reflecting surfaces for high-gain antennas used in satellite\ncommunications, Earth observation, and deep-space missions. During on-orbit\nmissions, active shape adjustment and attitude control are crucial for\nmaintaining surface accuracy and proper orientation for these reflectors,\nensuring optimal performance. Preventing resonance through thorough dynamic\nmodeling and vibration analysis is vital to avoid structural damage and ensure\nstability and reliability. Existing dynamic modeling approaches, such as wave\nand finite element methods, often fail to accurately predict dynamic responses\ndue to the limited capability of handling three-dimensional reflectors or the\noversimplification of cable members of a reflector. This paper proposes the\nCartesian spatial discretization method for dynamic modeling and vibration\nanalysis of cable-network structures in large deployable mesh reflectors. This\nmethod defines cable member positions as a summation of internal and\nboundary-induced terms within a global Cartesian coordinate system. Numerical\nsimulation on a two-dimensional cable-network structure and a center-feed mesh\nreflector demonstrates the superiority of the proposed method over traditional\napproaches, highlighting its accuracy and versatility, and establishing it as a\nrobust tool for analyzing three-dimensional complex reflector configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large deployable mesh reflectors are essential for space applications,\nproviding precise reflecting surfaces for high-gain antennas used in satellite\ncommunications, Earth observation, and deep-space missions. During on-orbit\nmissions, active shape adjustment and attitude control are crucial for\nmaintaining surface accuracy and proper orientation for these reflectors,\nensuring optimal performance. Preventing resonance through thorough dynamic\nmodeling and vibration analysis is vital to avoid structural damage and ensure\nstability and reliability. Existing dynamic modeling approaches, such as wave\nand finite element methods, often fail to accurately predict dynamic responses\ndue to the limited capability of handling three-dimensional reflectors or the\noversimplification of cable members of a reflector. This paper proposes the\nCartesian spatial discretization method for dynamic modeling and vibration\nanalysis of cable-network structures in large deployable mesh reflectors. This\nmethod defines cable member positions as a summation of internal and\nboundary-induced terms within a global Cartesian coordinate system. Numerical\nsimulation on a two-dimensional cable-network structure and a center-feed mesh\nreflector demonstrates the superiority of the proposed method over traditional\napproaches, highlighting its accuracy and versatility, and establishing it as a\nrobust tool for analyzing three-dimensional complex reflector configurations."
                },
                "authors": [
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Christian Kazoleas"
                    },
                    {
                        "name": "Weidong Zhu"
                    },
                    {
                        "name": "Kai Zhou"
                    },
                    {
                        "name": "Sichen Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Sichen Yuan"
                },
                "author": "Sichen Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17922v1",
                "updated": "2024-10-23T14:40:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    40,
                    37,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:40:37Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    40,
                    37,
                    2,
                    297,
                    0
                ],
                "title": "Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced\n  Defense in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guide for Defense (G4D): Dynamic Guidance for Robust and Balanced\n  Defense in Large Language Models"
                },
                "summary": "With the extensive deployment of Large Language Models (LLMs), ensuring their\nsafety has become increasingly critical. However, existing defense methods\noften struggle with two key issues: (i) inadequate defense capabilities,\nparticularly in domain-specific scenarios like chemistry, where a lack of\nspecialized knowledge can lead to the generation of harmful responses to\nmalicious queries. (ii) over-defensiveness, which compromises the general\nutility and responsiveness of LLMs. To mitigate these issues, we introduce a\nmulti-agents-based defense framework, Guide for Defense (G4D), which leverages\naccurate external information to provide an unbiased summary of user intentions\nand analytically grounded safety response guidance. Extensive experiments on\npopular jailbreak attacks and benign datasets show that our G4D can enhance\nLLM's robustness against jailbreak attacks on general and domain-specific\nscenarios without compromising the model's general functionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the extensive deployment of Large Language Models (LLMs), ensuring their\nsafety has become increasingly critical. However, existing defense methods\noften struggle with two key issues: (i) inadequate defense capabilities,\nparticularly in domain-specific scenarios like chemistry, where a lack of\nspecialized knowledge can lead to the generation of harmful responses to\nmalicious queries. (ii) over-defensiveness, which compromises the general\nutility and responsiveness of LLMs. To mitigate these issues, we introduce a\nmulti-agents-based defense framework, Guide for Defense (G4D), which leverages\naccurate external information to provide an unbiased summary of user intentions\nand analytically grounded safety response guidance. Extensive experiments on\npopular jailbreak attacks and benign datasets show that our G4D can enhance\nLLM's robustness against jailbreak attacks on general and domain-specific\nscenarios without compromising the model's general functionality."
                },
                "authors": [
                    {
                        "name": "He Cao"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Zijing Liu"
                    },
                    {
                        "name": "Bing Feng"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Yu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yu Li"
                },
                "author": "Yu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01119v2",
                "updated": "2024-10-23T14:37:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    37,
                    50,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-02T09:00:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    9,
                    0,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer"
                },
                "summary": "Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt tuning is an efficient solution for training large language models\n(LLMs). However, current soft-prompt-based methods often sacrifice multi-task\nmodularity, requiring the training process to be fully or partially repeated\nfor each newly added task. While recent work on task vectors applied arithmetic\noperations on full model weights to achieve the desired multi-task performance,\na similar approach for soft-prompts is still missing. To this end, we introduce\nTask Prompt Vectors, created by element-wise difference between weights of\ntuned soft-prompts and their random initialization. Experimental results on 12\nNLU datasets show that task prompt vectors can be used in low-resource settings\nto effectively initialize prompt tuning on similar tasks. In addition, we show\nthat task prompt vectors are independent of the random initialization of prompt\ntuning on 2 different language model architectures. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, we\nprovide a competitive alternative to state-of-the-art baselines by arithmetic\naddition of task prompt vectors from multiple tasks."
                },
                "authors": [
                    {
                        "name": "Robert Belanec"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Ivan Srba"
                    },
                    {
                        "name": "Maria Bielikova"
                    }
                ],
                "author_detail": {
                    "name": "Maria Bielikova"
                },
                "author": "Maria Bielikova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.00279v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.00279v3",
                "updated": "2024-10-23T14:33:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    33,
                    44,
                    2,
                    297,
                    0
                ],
                "published": "2023-07-01T09:18:24Z",
                "published_parsed": [
                    2023,
                    7,
                    1,
                    9,
                    18,
                    24,
                    5,
                    182,
                    0
                ],
                "title": "Let Me Teach You: Pedagogical Foundations of Feedback for Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Me Teach You: Pedagogical Foundations of Feedback for Language\n  Models"
                },
                "summary": "Natural Language Feedback (NLF) is an increasingly popular mechanism for\naligning Large Language Models (LLMs) to human preferences. Despite the\ndiversity of the information it can convey, NLF methods are often hand-designed\nand arbitrary, with little systematic grounding. At the same time, research in\nlearning sciences has long established several effective feedback models. In\nthis opinion piece, we compile ideas from pedagogy to introduce FELT, a\nfeedback framework for LLMs that outlines various characteristics of the\nfeedback space, and a feedback content taxonomy based on these variables,\nproviding a general mapping of the feedback space. In addition to streamlining\nNLF designs, FELT also brings out new, unexplored directions for research in\nNLF. We make our taxonomy available to the community, providing guides and\nexamples for mapping our categorizations to future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Feedback (NLF) is an increasingly popular mechanism for\naligning Large Language Models (LLMs) to human preferences. Despite the\ndiversity of the information it can convey, NLF methods are often hand-designed\nand arbitrary, with little systematic grounding. At the same time, research in\nlearning sciences has long established several effective feedback models. In\nthis opinion piece, we compile ideas from pedagogy to introduce FELT, a\nfeedback framework for LLMs that outlines various characteristics of the\nfeedback space, and a feedback content taxonomy based on these variables,\nproviding a general mapping of the feedback space. In addition to streamlining\nNLF designs, FELT also brings out new, unexplored directions for research in\nNLF. We make our taxonomy available to the community, providing guides and\nexamples for mapping our categorizations to future research."
                },
                "authors": [
                    {
                        "name": "Beatriz Borges"
                    },
                    {
                        "name": "Niket Tandon"
                    },
                    {
                        "name": "Tanja Kser"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "arxiv_comment": "EMNLP 2024; 9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.00279v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.00279v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14703v2",
                "updated": "2024-10-23T14:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    1,
                    14,
                    2,
                    297,
                    0
                ],
                "published": "2024-06-20T19:50:56Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    19,
                    50,
                    56,
                    3,
                    172,
                    0
                ],
                "title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction."
                },
                "authors": [
                    {
                        "name": "Seungbeen Lee"
                    },
                    {
                        "name": "Seungwon Lim"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Giyeong Oh"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Minju Kim"
                    },
                    {
                        "name": "Beong-woo Kwak"
                    },
                    {
                        "name": "Yeonsoo Lee"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "Preprint; Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07799v2",
                "updated": "2024-10-23T14:00:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    0,
                    40,
                    2,
                    297,
                    0
                ],
                "published": "2024-07-10T16:16:02Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    16,
                    16,
                    2,
                    2,
                    192,
                    0
                ],
                "title": "Attribute or Abstain: Large Language Models as Long Document Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute or Abstain: Large Language Models as Long Document Assistants"
                },
                "summary": "LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiments with different approaches to attribution on 5 LLMs\nof different sizes.\n  We find that citation, i.e. response generation and evidence extraction in\none step, performs best for large and fine-tuned models, while additional\nretrieval can help for small, prompted models. We investigate whether the \"Lost\nin the Middle'' phenomenon exists for attribution, but do not find this. We\nalso find that evidence quality can predict response quality on datasets with\nsimple responses, but not so for complex responses, as models struggle with\nproviding evidence for complex claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can help humans working with long documents, but are known to\nhallucinate. Attribution can increase trust in LLM responses: The LLM provides\nevidence that supports its response, which enhances verifiability. Existing\napproaches to attribution have only been evaluated in RAG settings, where the\ninitial retrieval confounds LLM performance. This is crucially different from\nthe long document setting, where retrieval is not needed, but could help. Thus,\na long document specific evaluation of attribution is missing. To fill this\ngap, we present LAB, a benchmark of 6 diverse long document tasks with\nattribution, and experiments with different approaches to attribution on 5 LLMs\nof different sizes.\n  We find that citation, i.e. response generation and evidence extraction in\none step, performs best for large and fine-tuned models, while additional\nretrieval can help for small, prompted models. We investigate whether the \"Lost\nin the Middle'' phenomenon exists for attribution, but do not find this. We\nalso find that evidence quality can predict response quality on datasets with\nsimple responses, but not so for complex responses, as models struggle with\nproviding evidence for complex claims."
                },
                "authors": [
                    {
                        "name": "Jan Buchmann"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Accepted at EMNLP 2024. Code and data:\n  https://github.com/UKPLab/arxiv2024-attribute-or-abstain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12819v2",
                "updated": "2024-10-23T14:00:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    0,
                    36,
                    2,
                    297,
                    0
                ],
                "published": "2024-07-01T10:33:46Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    10,
                    33,
                    46,
                    0,
                    183,
                    0
                ],
                "title": "I've Got 99 Problems But FLOPS Ain't One",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I've Got 99 Problems But FLOPS Ain't One"
                },
                "summary": "Hyperscalers dominate the landscape of large network deployments, yet they\nrarely share data or insights about the challenges they face. In light of this\nsupremacy, what problems can we find to solve in this space? We take an\nunconventional approach to find relevant research directions, starting from\npublic plans to build a $100 billion datacenter for machine learning\napplications. Leveraging the language models scaling laws, we discover what\nworkloads such a datacenter might carry and explore the challenges one may\nencounter in doing so, with a focus on networking research. We conclude that\nbuilding the datacenter and training such models is technically possible, but\nthis requires novel wide-area transports for inter-DC communication, a\nmultipath transport and novel datacenter topologies for intra-datacenter\ncommunication, high speed scale-up networks and transports, outlining a rich\nresearch agenda for the networking community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperscalers dominate the landscape of large network deployments, yet they\nrarely share data or insights about the challenges they face. In light of this\nsupremacy, what problems can we find to solve in this space? We take an\nunconventional approach to find relevant research directions, starting from\npublic plans to build a $100 billion datacenter for machine learning\napplications. Leveraging the language models scaling laws, we discover what\nworkloads such a datacenter might carry and explore the challenges one may\nencounter in doing so, with a focus on networking research. We conclude that\nbuilding the datacenter and training such models is technically possible, but\nthis requires novel wide-area transports for inter-DC communication, a\nmultipath transport and novel datacenter topologies for intra-datacenter\ncommunication, high speed scale-up networks and transports, outlining a rich\nresearch agenda for the networking community."
                },
                "authors": [
                    {
                        "name": "Alexandru M. Gherghescu"
                    },
                    {
                        "name": "Vlad-Andrei Bdoiu"
                    },
                    {
                        "name": "Alexandru Agache"
                    },
                    {
                        "name": "Mihai-Valentin Dumitru"
                    },
                    {
                        "name": "Iuliu Vasilescu"
                    },
                    {
                        "name": "Radu Mantu"
                    },
                    {
                        "name": "Costin Raiciu"
                    }
                ],
                "author_detail": {
                    "name": "Costin Raiciu"
                },
                "author": "Costin Raiciu",
                "arxiv_doi": "10.1145/3696348.3696893",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696893",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.12819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17885v1",
                "updated": "2024-10-23T13:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    58,
                    39,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:58:39Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    58,
                    39,
                    2,
                    297,
                    0
                ],
                "title": "R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric\n  Reasoning in Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-CoT: Reverse Chain-of-Thought Problem Generation for Geometric\n  Reasoning in Large Multimodal Models"
                },
                "summary": "Existing Large Multimodal Models (LMMs) struggle with mathematical geometric\nreasoning due to a lack of high-quality image-text paired data. Current\ngeometric data generation approaches, which apply preset templates to generate\ngeometric data or use Large Language Models (LLMs) to rephrase questions and\nanswers (Q&A), unavoidably limit data accuracy and diversity. To synthesize\nhigher-quality data, we propose a two-stage Reverse Chain-of-Thought (R-CoT)\ngeometry problem generation pipeline. First, we introduce GeoChain to produce\nhigh-fidelity geometric images and corresponding descriptions highlighting\nrelations among geometric elements. We then design a Reverse A&Q method that\nreasons step-by-step based on the descriptions and generates questions in\nreverse from the reasoning results. Experiments demonstrate that the proposed\nmethod brings significant and consistent improvements on multiple LMM\nbaselines, achieving new performance records in the 2B, 7B, and 8B settings.\nNotably, R-CoT-8B significantly outperforms previous state-of-the-art\nopen-source mathematical models by 16.6% on MathVista and 9.2% on GeoQA, while\nalso surpassing the closed-source model GPT-4o by an average of 13% across both\ndatasets. The code is available at https://github.com/dle666/R-CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Large Multimodal Models (LMMs) struggle with mathematical geometric\nreasoning due to a lack of high-quality image-text paired data. Current\ngeometric data generation approaches, which apply preset templates to generate\ngeometric data or use Large Language Models (LLMs) to rephrase questions and\nanswers (Q&A), unavoidably limit data accuracy and diversity. To synthesize\nhigher-quality data, we propose a two-stage Reverse Chain-of-Thought (R-CoT)\ngeometry problem generation pipeline. First, we introduce GeoChain to produce\nhigh-fidelity geometric images and corresponding descriptions highlighting\nrelations among geometric elements. We then design a Reverse A&Q method that\nreasons step-by-step based on the descriptions and generates questions in\nreverse from the reasoning results. Experiments demonstrate that the proposed\nmethod brings significant and consistent improvements on multiple LMM\nbaselines, achieving new performance records in the 2B, 7B, and 8B settings.\nNotably, R-CoT-8B significantly outperforms previous state-of-the-art\nopen-source mathematical models by 16.6% on MathVista and 9.2% on GeoQA, while\nalso surpassing the closed-source model GPT-4o by an average of 13% across both\ndatasets. The code is available at https://github.com/dle666/R-CoT."
                },
                "authors": [
                    {
                        "name": "Linger Deng"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Dongliang Luo"
                    },
                    {
                        "name": "Liang Wu"
                    },
                    {
                        "name": "Chengquan Zhang"
                    },
                    {
                        "name": "Pengyuan Lyu"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Gang Zhang"
                    },
                    {
                        "name": "Errui Ding"
                    },
                    {
                        "name": "Yingying Zhu"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17881v1",
                "updated": "2024-10-23T13:53:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    53,
                    26,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:53:26Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    53,
                    26,
                    2,
                    297,
                    0
                ],
                "title": "AdaRankGrad: Adaptive Gradient-Rank and Moments for Memory-Efficient\n  LLMs Training and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaRankGrad: Adaptive Gradient-Rank and Moments for Memory-Efficient\n  LLMs Training and Fine-Tuning"
                },
                "summary": "Training and fine-tuning large language models (LLMs) come with challenges\nrelated to memory and computational requirements due to the increasing size of\nthe model weights and the optimizer states. Various techniques have been\ndeveloped to tackle these challenges, such as low-rank adaptation (LoRA), which\ninvolves introducing a parallel trainable low-rank matrix to the fixed\npre-trained weights at each layer. However, these methods often fall short\ncompared to the full-rank weight training approach, as they restrict the\nparameter search to a low-rank subspace. This limitation can disrupt training\ndynamics and require a full-rank warm start to mitigate the impact. In this\npaper, we introduce a new method inspired by a phenomenon we formally prove: as\ntraining progresses, the rank of the estimated layer gradients gradually\ndecreases, and asymptotically approaches rank one. Leveraging this, our\napproach involves adaptively reducing the rank of the gradients during Adam\noptimization steps, using an efficient online-updating low-rank projections\nrule. We further present a randomized SVD scheme for efficiently finding the\nprojection matrix. Our technique enables full-parameter fine-tuning with\nadaptive low-rank gradient updates, significantly reducing overall memory\nrequirements during training compared to state-of-the-art methods while\nimproving model performance in both pretraining and fine-tuning. Finally, we\nprovide a convergence analysis of our method and demonstrate its merits for\ntraining and fine-tuning language and biological foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and fine-tuning large language models (LLMs) come with challenges\nrelated to memory and computational requirements due to the increasing size of\nthe model weights and the optimizer states. Various techniques have been\ndeveloped to tackle these challenges, such as low-rank adaptation (LoRA), which\ninvolves introducing a parallel trainable low-rank matrix to the fixed\npre-trained weights at each layer. However, these methods often fall short\ncompared to the full-rank weight training approach, as they restrict the\nparameter search to a low-rank subspace. This limitation can disrupt training\ndynamics and require a full-rank warm start to mitigate the impact. In this\npaper, we introduce a new method inspired by a phenomenon we formally prove: as\ntraining progresses, the rank of the estimated layer gradients gradually\ndecreases, and asymptotically approaches rank one. Leveraging this, our\napproach involves adaptively reducing the rank of the gradients during Adam\noptimization steps, using an efficient online-updating low-rank projections\nrule. We further present a randomized SVD scheme for efficiently finding the\nprojection matrix. Our technique enables full-parameter fine-tuning with\nadaptive low-rank gradient updates, significantly reducing overall memory\nrequirements during training compared to state-of-the-art methods while\nimproving model performance in both pretraining and fine-tuning. Finally, we\nprovide a convergence analysis of our method and demonstrate its merits for\ntraining and fine-tuning language and biological foundation models."
                },
                "authors": [
                    {
                        "name": "Yehonathan Refael"
                    },
                    {
                        "name": "Jonathan Svirsky"
                    },
                    {
                        "name": "Boris Shustin"
                    },
                    {
                        "name": "Wasim Huleihel"
                    },
                    {
                        "name": "Ofir Lindenbaum"
                    }
                ],
                "author_detail": {
                    "name": "Ofir Lindenbaum"
                },
                "author": "Ofir Lindenbaum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17031v2",
                "updated": "2024-10-23T13:52:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    52,
                    51,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-22T13:57:55Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    13,
                    57,
                    55,
                    1,
                    296,
                    0
                ],
                "title": "GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks"
                },
                "summary": "The increasing demand for spatiotemporal data and modeling tasks in\ngeosciences has made geospatial code generation technology a critical factor in\nenhancing productivity. Although large language models (LLMs) have demonstrated\npotential in code generation tasks, they often encounter issues such as refusal\nto code or hallucination in geospatial code generation due to a lack of\ndomain-specific knowledge and code corpora. To address these challenges, this\npaper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along\nwith the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and\nLoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first\nLLM focused on geospatial code generation, fine-tuned from Code Llama-7B.\nFurthermore, we establish a comprehensive geospatial code evaluation framework,\nincorporating option matching, expert validation, and prompt engineering\nscoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the\nGeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms\nother models in multiple-choice accuracy by 9.1% to 32.1%, in code\nsummarization ability by 1.7% to 25.4%, and in code generation capability by\n1.2% to 25.1%. This paper provides a solution and empirical validation for\nenhancing LLMs' performance in geospatial code generation, extends the\nboundaries of domain-specific model applications, and offers valuable insights\ninto unlocking their potential in geospatial code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for spatiotemporal data and modeling tasks in\ngeosciences has made geospatial code generation technology a critical factor in\nenhancing productivity. Although large language models (LLMs) have demonstrated\npotential in code generation tasks, they often encounter issues such as refusal\nto code or hallucination in geospatial code generation due to a lack of\ndomain-specific knowledge and code corpora. To address these challenges, this\npaper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along\nwith the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and\nLoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first\nLLM focused on geospatial code generation, fine-tuned from Code Llama-7B.\nFurthermore, we establish a comprehensive geospatial code evaluation framework,\nincorporating option matching, expert validation, and prompt engineering\nscoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the\nGeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms\nother models in multiple-choice accuracy by 9.1% to 32.1%, in code\nsummarization ability by 1.7% to 25.4%, and in code generation capability by\n1.2% to 25.1%. This paper provides a solution and empirical validation for\nenhancing LLMs' performance in geospatial code generation, extends the\nboundaries of domain-specific model applications, and offers valuable insights\ninto unlocking their potential in geospatial code generation."
                },
                "authors": [
                    {
                        "name": "Shuyang Hou"
                    },
                    {
                        "name": "Zhangxiao Shen"
                    },
                    {
                        "name": "Anqi Zhao"
                    },
                    {
                        "name": "Jianyuan Liang"
                    },
                    {
                        "name": "Zhipeng Gui"
                    },
                    {
                        "name": "Xuefeng Guan"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Huayi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Huayi Wu"
                },
                "author": "Huayi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17875v1",
                "updated": "2024-10-23T13:47:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    47,
                    5,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:47:05Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    47,
                    5,
                    2,
                    297,
                    0
                ],
                "title": "Understanding Layer Significance in LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Layer Significance in LLM Alignment"
                },
                "summary": "Aligning large language models (LLMs) through fine-tuning is essential for\ntailoring them to specific applications. Therefore, understanding what LLMs\nlearn during the alignment process is crucial. Recent studies suggest that\nalignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To delve deeper into LLM alignment, we propose to\nidentify which layers within LLMs are most critical to the alignment process,\nthereby uncovering how alignment influences model behavior at a granular level.\nWe propose a novel approach to identify the important layers for LLM alignment\n(ILA). It involves learning a binary mask for each incremental weight matrix in\nthe LoRA algorithm, indicating the significance of each layer. ILA consistently\nidentifies important layers across various alignment datasets, with nearly 90%\noverlap even with substantial dataset differences, highlighting fundamental\npatterns in LLM alignment. Experimental results indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) through fine-tuning is essential for\ntailoring them to specific applications. Therefore, understanding what LLMs\nlearn during the alignment process is crucial. Recent studies suggest that\nalignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To delve deeper into LLM alignment, we propose to\nidentify which layers within LLMs are most critical to the alignment process,\nthereby uncovering how alignment influences model behavior at a granular level.\nWe propose a novel approach to identify the important layers for LLM alignment\n(ILA). It involves learning a binary mask for each incremental weight matrix in\nthe LoRA algorithm, indicating the significance of each layer. ILA consistently\nidentifies important layers across various alignment datasets, with nearly 90%\noverlap even with substantial dataset differences, highlighting fundamental\npatterns in LLM alignment. Experimental results indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss."
                },
                "authors": [
                    {
                        "name": "Guangyuan Shi"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Xiaoyu Dong"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Xuanyu Zhang"
                    },
                    {
                        "name": "Yujie Feng"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15055v2",
                "updated": "2024-10-23T13:20:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    20,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-23T02:15:47Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    2,
                    15,
                    47,
                    4,
                    54,
                    0
                ],
                "title": "Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions"
                },
                "summary": "Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding."
                },
                "authors": [
                    {
                        "name": "Clement Neo"
                    },
                    {
                        "name": "Shay B. Cohen"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17840v1",
                "updated": "2024-10-23T13:05:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    5,
                    46,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T13:05:46Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    5,
                    46,
                    2,
                    297,
                    0
                ],
                "title": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for\n  LLMs"
                },
                "summary": "Serving systems for Large Language Models (LLMs) improve throughput by\nprocessing several requests concurrently. However, multiplexing hardware\nresources between concurrent requests involves non-trivial scheduling\ndecisions. Practical serving systems typically implement these decisions at two\nlevels: First, a load balancer routes requests to different servers which each\nhold a replica of the LLM. Then, on each server, an engine-level scheduler\ndecides when to run a request, or when to queue or preempt it. Improved\nscheduling policies may benefit a wide range of LLM deployments and can often\nbe implemented as \"drop-in replacements\" to a system's current policy. In this\nwork, we survey scheduling techniques from the literature and from practical\nserving systems. We find that schedulers from the literature often achieve good\nperformance but introduce significant complexity. In contrast, schedulers in\npractical deployments often leave easy performance gains on the table but are\neasy to implement, deploy and configure. This finding motivates us to introduce\ntwo new scheduling techniques, which are both easy to implement, and outperform\ncurrent techniques on production workload traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) improve throughput by\nprocessing several requests concurrently. However, multiplexing hardware\nresources between concurrent requests involves non-trivial scheduling\ndecisions. Practical serving systems typically implement these decisions at two\nlevels: First, a load balancer routes requests to different servers which each\nhold a replica of the LLM. Then, on each server, an engine-level scheduler\ndecides when to run a request, or when to queue or preempt it. Improved\nscheduling policies may benefit a wide range of LLM deployments and can often\nbe implemented as \"drop-in replacements\" to a system's current policy. In this\nwork, we survey scheduling techniques from the literature and from practical\nserving systems. We find that schedulers from the literature often achieve good\nperformance but introduce significant complexity. In contrast, schedulers in\npractical deployments often leave easy performance gains on the table but are\neasy to implement, deploy and configure. This finding motivates us to introduce\ntwo new scheduling techniques, which are both easy to implement, and outperform\ncurrent techniques on production workload traces."
                },
                "authors": [
                    {
                        "name": "Ferdi Kossmann"
                    },
                    {
                        "name": "Bruce Fontaine"
                    },
                    {
                        "name": "Daya Khudia"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "Samuel Madden"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Madden"
                },
                "author": "Samuel Madden",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15956v2",
                "updated": "2024-10-23T13:00:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    13,
                    0,
                    27,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-21T12:34:17Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    12,
                    34,
                    17,
                    0,
                    295,
                    0
                ],
                "title": "Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Have an English Accent? Evaluating and\n  Improving the Naturalness of Multilingual LLMs"
                },
                "summary": "Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Large Language Models (LLMs) are predominantly designed with English\nas the primary language, and even the few that are multilingual tend to exhibit\nstrong English-centric biases. Much like speakers who might produce awkward\nexpressions when learning a second language, LLMs often generate unnatural\noutputs in non-English languages, reflecting English-centric patterns in both\nvocabulary and grammar. Despite the importance of this issue, the naturalness\nof multilingual LLM outputs has received limited attention. In this paper, we\naddress this gap by introducing novel automatic corpus-level metrics to assess\nthe lexical and syntactic naturalness of LLM outputs in a multilingual context.\nUsing our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark\nin French and Chinese, revealing a tendency towards English-influenced\npatterns. To mitigate this issue, we also propose a simple and effective\nalignment method to improve the naturalness of an LLM in a target language and\ndomain, achieving consistent improvements in naturalness without compromising\nthe performance on general-purpose benchmarks. Our work highlights the\nimportance of developing multilingual metrics, resources and methods for the\nnew wave of multilingual LLMs."
                },
                "authors": [
                    {
                        "name": "Yanzhu Guo"
                    },
                    {
                        "name": "Simone Conia"
                    },
                    {
                        "name": "Zelin Zhou"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Saloni Potdar"
                    },
                    {
                        "name": "Henry Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Henry Xiao"
                },
                "author": "Henry Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12376v2",
                "updated": "2024-10-23T12:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    58,
                    14,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-16T08:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    48,
                    27,
                    2,
                    290,
                    0
                ],
                "title": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated\n  Shapefile Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated\n  Shapefile Processing"
                },
                "summary": "Vector data is one of the two core data structures in geographic information\nscience (GIS), essential for accurately storing and representing geospatial\ninformation. Shapefile, the most widely used vector data format, has become the\nindustry standard supported by all major geographic information systems.\nHowever, processing this data typically requires specialized GIS knowledge and\nskills, creating a barrier for researchers from other fields and impeding\ninterdisciplinary research in spatial data analysis. Moreover, while large\nlanguage models (LLMs) have made significant advancements in natural language\nprocessing and task automation, they still face challenges in handling the\ncomplex spatial and topological relationships inherent in GIS vector data. To\naddress these challenges, we propose ShapefileGPT, an innovative framework\npowered by LLMs, specifically designed to automate Shapefile tasks.\nShapefileGPT utilizes a multi-agent architecture, in which the planner agent is\nresponsible for task decomposition and supervision, while the worker agent\nexecutes the tasks. We developed a specialized function library for handling\nShapefiles and provided comprehensive API documentation, enabling the worker\nagent to operate Shapefiles efficiently through function calling. For\nevaluation, we developed a benchmark dataset based on authoritative textbooks,\nencompassing tasks in categories such as geometric operations and spatial\nqueries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the\nGPT series models. In comparison to traditional LLMs, ShapefileGPT effectively\nhandles complex vector data analysis tasks, overcoming the limitations of\ntraditional LLMs in spatial analysis. This breakthrough opens new pathways for\nadvancing automation and intelligence in the GIS field, with significant\npotential in interdisciplinary data analysis and application contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector data is one of the two core data structures in geographic information\nscience (GIS), essential for accurately storing and representing geospatial\ninformation. Shapefile, the most widely used vector data format, has become the\nindustry standard supported by all major geographic information systems.\nHowever, processing this data typically requires specialized GIS knowledge and\nskills, creating a barrier for researchers from other fields and impeding\ninterdisciplinary research in spatial data analysis. Moreover, while large\nlanguage models (LLMs) have made significant advancements in natural language\nprocessing and task automation, they still face challenges in handling the\ncomplex spatial and topological relationships inherent in GIS vector data. To\naddress these challenges, we propose ShapefileGPT, an innovative framework\npowered by LLMs, specifically designed to automate Shapefile tasks.\nShapefileGPT utilizes a multi-agent architecture, in which the planner agent is\nresponsible for task decomposition and supervision, while the worker agent\nexecutes the tasks. We developed a specialized function library for handling\nShapefiles and provided comprehensive API documentation, enabling the worker\nagent to operate Shapefiles efficiently through function calling. For\nevaluation, we developed a benchmark dataset based on authoritative textbooks,\nencompassing tasks in categories such as geometric operations and spatial\nqueries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the\nGPT series models. In comparison to traditional LLMs, ShapefileGPT effectively\nhandles complex vector data analysis tasks, overcoming the limitations of\ntraditional LLMs in spatial analysis. This breakthrough opens new pathways for\nadvancing automation and intelligence in the GIS field, with significant\npotential in interdisciplinary data analysis and application contexts."
                },
                "authors": [
                    {
                        "name": "Qingming Lin"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Huaxia Li"
                    },
                    {
                        "name": "Sensen Wu"
                    },
                    {
                        "name": "Yadong Li"
                    },
                    {
                        "name": "Kai Fang"
                    },
                    {
                        "name": "Hailin Feng"
                    },
                    {
                        "name": "Zhenhong Du"
                    },
                    {
                        "name": "Liuchang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Liuchang Xu"
                },
                "author": "Liuchang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10568v2",
                "updated": "2024-10-23T12:37:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    37,
                    10,
                    2,
                    297,
                    0
                ],
                "published": "2024-09-14T04:17:24Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    4,
                    17,
                    24,
                    5,
                    258,
                    0
                ],
                "title": "On the limits of agency in agent-based models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the limits of agency in agent-based models"
                },
                "summary": "Agent-based modeling (ABM) seeks to understand the behavior of complex\nsystems by simulating a collection of agents that act and interact within an\nenvironment. Their practical utility requires capturing realistic environment\ndynamics and adaptive agent behavior while efficiently simulating million-size\npopulations. Recent advancements in large language models (LLMs) present an\nopportunity to enhance ABMs by using LLMs as agents with further potential to\ncapture adaptive behavior. However, the computational infeasibility of using\nLLMs for large populations has hindered their widespread adoption. In this\npaper, we introduce AgentTorch -- a framework that scales ABMs to millions of\nagents while capturing high-resolution agent behavior using LLMs. We benchmark\nthe utility of LLMs as ABM agents, exploring the trade-off between simulation\nscale and individual agency. Using the COVID-19 pandemic as a case study, we\ndemonstrate how AgentTorch can simulate 8.4 million agents representing New\nYork City, capturing the impact of isolation and employment behavior on health\nand economic outcomes. We compare the performance of different agent\narchitectures based on heuristic and LLM agents in predicting disease waves and\nunemployment rates. Furthermore, we showcase AgentTorch's capabilities for\nretrospective, counterfactual, and prospective analyses, highlighting how\nadaptive agent behavior can help overcome the limitations of historical data in\npolicy design. AgentTorch is an open-source project actively being used for\npolicy-making and scientific discovery around the world. The framework is\navailable here: github.com/AgentTorch/AgentTorch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-based modeling (ABM) seeks to understand the behavior of complex\nsystems by simulating a collection of agents that act and interact within an\nenvironment. Their practical utility requires capturing realistic environment\ndynamics and adaptive agent behavior while efficiently simulating million-size\npopulations. Recent advancements in large language models (LLMs) present an\nopportunity to enhance ABMs by using LLMs as agents with further potential to\ncapture adaptive behavior. However, the computational infeasibility of using\nLLMs for large populations has hindered their widespread adoption. In this\npaper, we introduce AgentTorch -- a framework that scales ABMs to millions of\nagents while capturing high-resolution agent behavior using LLMs. We benchmark\nthe utility of LLMs as ABM agents, exploring the trade-off between simulation\nscale and individual agency. Using the COVID-19 pandemic as a case study, we\ndemonstrate how AgentTorch can simulate 8.4 million agents representing New\nYork City, capturing the impact of isolation and employment behavior on health\nand economic outcomes. We compare the performance of different agent\narchitectures based on heuristic and LLM agents in predicting disease waves and\nunemployment rates. Furthermore, we showcase AgentTorch's capabilities for\nretrospective, counterfactual, and prospective analyses, highlighting how\nadaptive agent behavior can help overcome the limitations of historical data in\npolicy design. AgentTorch is an open-source project actively being used for\npolicy-making and scientific discovery around the world. The framework is\navailable here: github.com/AgentTorch/AgentTorch."
                },
                "authors": [
                    {
                        "name": "Ayush Chopra"
                    },
                    {
                        "name": "Shashank Kumar"
                    },
                    {
                        "name": "Nurullah Giray-Kuru"
                    },
                    {
                        "name": "Ramesh Raskar"
                    },
                    {
                        "name": "Arnau Quera-Bofarull"
                    }
                ],
                "author_detail": {
                    "name": "Arnau Quera-Bofarull"
                },
                "author": "Arnau Quera-Bofarull",
                "arxiv_comment": "19 pages, 5 appendices, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17820v1",
                "updated": "2024-10-23T12:26:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    26,
                    10,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T12:26:10Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    26,
                    10,
                    2,
                    297,
                    0
                ],
                "title": "Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination"
                },
                "summary": "Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. While using even a smaller model as the discriminator, scaling\nthe generator leads to notable improvements in ToT performance, whereas scaling\nthe discriminator with a fixed generator yields only marginal gains. Our\nresults show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. While using even a smaller model as the discriminator, scaling\nthe generator leads to notable improvements in ToT performance, whereas scaling\nthe discriminator with a fixed generator yields only marginal gains. Our\nresults show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT."
                },
                "authors": [
                    {
                        "name": "Qiqi Chen"
                    },
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "Code: github.com/mainlp/tot-eval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17810v1",
                "updated": "2024-10-23T12:12:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    12,
                    56,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T12:12:56Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    12,
                    56,
                    2,
                    297,
                    0
                ],
                "title": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive\n  Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive\n  Contrastive Learning"
                },
                "summary": "Recent advancements in image-text matching have been notable, yet prevailing\nmodels predominantly cater to broad queries and struggle with accommodating\nfine-grained query intention. In this paper, we work towards the\n\\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM),\na task that the text and image involve specific entity-related information. The\nchallenge of this task mainly lies in the larger semantic gap in entity\nassociation modeling, comparing with the general image-text matching problem.To\nnarrow the huge semantic gap between the entity-centric text and the images, we\ntake the fundamental CLIP as the backbone and devise a multimodal attentive\ncontrastive learning framework to tam CLIP to adapt EITM problem, developing a\nmodel named EntityCLIP. The key of our multimodal attentive contrastive\nlearning is to generate interpretive explanation text using Large Language\nModels (LLMs) as the bridge clues. In specific, we proceed by extracting\nexplanatory text from off-the-shelf LLMs. This explanation text, coupled with\nthe image and text, is then input into our specially crafted Multimodal\nAttentive Experts (MMAE) module, which effectively integrates explanation texts\nto narrow the gap of the entity-related text and image in a shared semantic\nspace. Building on the enriched features derived from MMAE, we further design\nan effective Gated Integrative Image-text Matching (GI-ITM) strategy. The\nGI-ITM employs an adaptive gating mechanism to aggregate MMAE's features,\nsubsequently applying image-text matching constraints to steer the alignment\nbetween the text and the image. Extensive experiments are conducted on three\nsocial media news benchmarks including N24News, VisualNews, and GoodNews, the\nresults shows that our method surpasses the competition methods with a clear\nmargin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in image-text matching have been notable, yet prevailing\nmodels predominantly cater to broad queries and struggle with accommodating\nfine-grained query intention. In this paper, we work towards the\n\\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM),\na task that the text and image involve specific entity-related information. The\nchallenge of this task mainly lies in the larger semantic gap in entity\nassociation modeling, comparing with the general image-text matching problem.To\nnarrow the huge semantic gap between the entity-centric text and the images, we\ntake the fundamental CLIP as the backbone and devise a multimodal attentive\ncontrastive learning framework to tam CLIP to adapt EITM problem, developing a\nmodel named EntityCLIP. The key of our multimodal attentive contrastive\nlearning is to generate interpretive explanation text using Large Language\nModels (LLMs) as the bridge clues. In specific, we proceed by extracting\nexplanatory text from off-the-shelf LLMs. This explanation text, coupled with\nthe image and text, is then input into our specially crafted Multimodal\nAttentive Experts (MMAE) module, which effectively integrates explanation texts\nto narrow the gap of the entity-related text and image in a shared semantic\nspace. Building on the enriched features derived from MMAE, we further design\nan effective Gated Integrative Image-text Matching (GI-ITM) strategy. The\nGI-ITM employs an adaptive gating mechanism to aggregate MMAE's features,\nsubsequently applying image-text matching constraints to steer the alignment\nbetween the text and the image. Extensive experiments are conducted on three\nsocial media news benchmarks including N24News, VisualNews, and GoodNews, the\nresults shows that our method surpasses the competition methods with a clear\nmargin."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wang"
                    },
                    {
                        "name": "Yaxiong Wang"
                    },
                    {
                        "name": "Lianwei Wu"
                    },
                    {
                        "name": "Lechao Cheng"
                    },
                    {
                        "name": "Zhun Zhong"
                    },
                    {
                        "name": "Meng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Wang"
                },
                "author": "Meng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17809v1",
                "updated": "2024-10-23T12:11:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    11,
                    26,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T12:11:26Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    11,
                    26,
                    2,
                    297,
                    0
                ],
                "title": "An Intelligent Agentic System for Complex Image Restoration Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Intelligent Agentic System for Complex Image Restoration Problems"
                },
                "summary": "Real-world image restoration (IR) is inherently complex and often requires\ncombining multiple specialized models to address diverse degradations. Inspired\nby human problem-solving, we propose AgenticIR, an agentic system that mimics\nthe human approach to image processing by following five key stages:\nPerception, Scheduling, Execution, Reflection, and Rescheduling. AgenticIR\nleverages large language models (LLMs) and vision-language models (VLMs) that\ninteract via text generation to dynamically operate a toolbox of IR models. We\nfine-tune VLMs for image quality analysis and employ LLMs for reasoning,\nguiding the system step by step. To compensate for LLMs' lack of specific IR\nknowledge and experience, we introduce a self-exploration method, allowing the\nLLM to observe and summarize restoration results into referenceable documents.\nExperiments demonstrate AgenticIR's potential in handling complex IR tasks,\nrepresenting a promising path toward achieving general intelligence in visual\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image restoration (IR) is inherently complex and often requires\ncombining multiple specialized models to address diverse degradations. Inspired\nby human problem-solving, we propose AgenticIR, an agentic system that mimics\nthe human approach to image processing by following five key stages:\nPerception, Scheduling, Execution, Reflection, and Rescheduling. AgenticIR\nleverages large language models (LLMs) and vision-language models (VLMs) that\ninteract via text generation to dynamically operate a toolbox of IR models. We\nfine-tune VLMs for image quality analysis and employ LLMs for reasoning,\nguiding the system step by step. To compensate for LLMs' lack of specific IR\nknowledge and experience, we introduce a self-exploration method, allowing the\nLLM to observe and summarize restoration results into referenceable documents.\nExperiments demonstrate AgenticIR's potential in handling complex IR tasks,\nrepresenting a promising path toward achieving general intelligence in visual\nprocessing."
                },
                "authors": [
                    {
                        "name": "Kaiwen Zhu"
                    },
                    {
                        "name": "Jinjin Gu"
                    },
                    {
                        "name": "Zhiyuan You"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Chao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Chao Dong"
                },
                "author": "Chao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17799v1",
                "updated": "2024-10-23T11:58:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    58,
                    58,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:58:58Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    58,
                    58,
                    2,
                    297,
                    0
                ],
                "title": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation"
                },
                "summary": "Full-duplex spoken dialogue systems significantly advance over traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex communication\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text-based large language model (LLM) backbone into a speech-text\ndialogue LLM, capable of generating text and speech in real time, without\nmodifying the architecture of the backbone LLM. The training process comprises\nthree stages: modality alignment, half-duplex dialogue learning, and\nfull-duplex dialogue learning. Throughout all training stages, we standardize\nthe data using a flattening operation, which allows us to unify the training\nmethods and the model architecture across different modalities and tasks. Our\napproach offers a straightforward modeling technique and a promising research\ndirection for developing efficient and natural end-to-end full-duplex spoken\ndialogue systems. Audio samples of dialogues generated by OmniFlatten can be\nfound at this web site (https://omniflatten.github.io/).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-duplex spoken dialogue systems significantly advance over traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex communication\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text-based large language model (LLM) backbone into a speech-text\ndialogue LLM, capable of generating text and speech in real time, without\nmodifying the architecture of the backbone LLM. The training process comprises\nthree stages: modality alignment, half-duplex dialogue learning, and\nfull-duplex dialogue learning. Throughout all training stages, we standardize\nthe data using a flattening operation, which allows us to unify the training\nmethods and the model architecture across different modalities and tasks. Our\napproach offers a straightforward modeling technique and a promising research\ndirection for developing efficient and natural end-to-end full-duplex spoken\ndialogue systems. Audio samples of dialogues generated by OmniFlatten can be\nfound at this web site (https://omniflatten.github.io/)."
                },
                "authors": [
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Luyao Cheng"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Siqi Zheng"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Chaohong Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chaohong Tan"
                },
                "author": "Chaohong Tan",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17153v2",
                "updated": "2024-10-23T11:56:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    56,
                    26,
                    2,
                    297,
                    0
                ],
                "published": "2024-04-26T04:55:35Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    4,
                    55,
                    35,
                    4,
                    117,
                    0
                ],
                "title": "A Unified Debugging Approach via LLM-Based Multi-Agent Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Debugging Approach via LLM-Based Multi-Agent Synergy"
                },
                "summary": "Software debugging is a time-consuming endeavor involving a series of steps,\nsuch as fault localization and patch generation, each requiring thorough\nanalysis and a deep understanding of the underlying logic. While large language\nmodels (LLMs) demonstrate promising potential in coding tasks, their\nperformance in debugging remains limited. Current LLM-based methods often focus\non isolated steps and struggle with complex bugs. In this paper, we propose the\nfirst end-to-end framework, FixAgent, for unified debugging through multi-agent\nsynergy. It mimics the entire cognitive processes of developers, with each\nagent specialized as a particular component of this process rather than\nmirroring the actions of an independent expert as in previous multi-agent\nsystems. Agents are coordinated through a three-level design, following a\ncognitive model of debugging, allowing adaptive handling of bugs with varying\ncomplexities. Experiments on extensive benchmarks demonstrate that FixAgent\nsignificantly outperforms state-of-the-art repair methods, fixing 1.25$\\times$\nto 2.56$\\times$ bugs on the repo-level benchmark, Defects4J. This performance\nis achieved without requiring ground-truth root-cause code statements, unlike\nthe baselines. Our source code is available on\nhttps://github.com/AcceptePapier/UniDebugger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software debugging is a time-consuming endeavor involving a series of steps,\nsuch as fault localization and patch generation, each requiring thorough\nanalysis and a deep understanding of the underlying logic. While large language\nmodels (LLMs) demonstrate promising potential in coding tasks, their\nperformance in debugging remains limited. Current LLM-based methods often focus\non isolated steps and struggle with complex bugs. In this paper, we propose the\nfirst end-to-end framework, FixAgent, for unified debugging through multi-agent\nsynergy. It mimics the entire cognitive processes of developers, with each\nagent specialized as a particular component of this process rather than\nmirroring the actions of an independent expert as in previous multi-agent\nsystems. Agents are coordinated through a three-level design, following a\ncognitive model of debugging, allowing adaptive handling of bugs with varying\ncomplexities. Experiments on extensive benchmarks demonstrate that FixAgent\nsignificantly outperforms state-of-the-art repair methods, fixing 1.25$\\times$\nto 2.56$\\times$ bugs on the repo-level benchmark, Defects4J. This performance\nis achieved without requiring ground-truth root-cause code statements, unlike\nthe baselines. Our source code is available on\nhttps://github.com/AcceptePapier/UniDebugger."
                },
                "authors": [
                    {
                        "name": "Cheryl Lee"
                    },
                    {
                        "name": "Chunqiu Steven Xia"
                    },
                    {
                        "name": "Longji Yang"
                    },
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Zhouruixin Zhu"
                    },
                    {
                        "name": "Lingming Zhang"
                    },
                    {
                        "name": "Michael R. Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael R. Lyu"
                },
                "author": "Michael R. Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17787v1",
                "updated": "2024-10-23T11:37:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    37,
                    20,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:37:20Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    37,
                    20,
                    2,
                    297,
                    0
                ],
                "title": "Large Language Models Engineer Too Many Simple Features For Tabular Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Engineer Too Many Simple Features For Tabular Data"
                },
                "summary": "Tabular machine learning problems often require time-consuming and\nlabor-intensive feature engineering. Recent efforts have focused on using large\nlanguage models (LLMs) to capitalize on their potential domain knowledge. At\nthe same time, researchers have observed ethically concerning negative biases\nin other LLM-related use cases, such as text generation. These developments\nmotivated us to investigate whether LLMs exhibit a bias that negatively impacts\nthe performance of feature engineering. While not ethically concerning, such a\nbias could hinder practitioners from fully utilizing LLMs for automated data\nscience. Therefore, we propose a method to detect potential biases by detecting\nanomalies in the frequency of operators (e.g., adding two features) suggested\nby LLMs when engineering new features. Our experiments evaluate the bias of\nfour LLMs, two big frontier and two small open-source models, across 27 tabular\ndatasets. Our results indicate that LLMs are biased toward simple operators,\nsuch as addition, and can fail to utilize more complex operators, such as\ngrouping followed by aggregations. Furthermore, the bias can negatively impact\nthe predictive performance when using LLM-generated features. Our results call\nfor mitigating bias when using LLMs for feature engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular machine learning problems often require time-consuming and\nlabor-intensive feature engineering. Recent efforts have focused on using large\nlanguage models (LLMs) to capitalize on their potential domain knowledge. At\nthe same time, researchers have observed ethically concerning negative biases\nin other LLM-related use cases, such as text generation. These developments\nmotivated us to investigate whether LLMs exhibit a bias that negatively impacts\nthe performance of feature engineering. While not ethically concerning, such a\nbias could hinder practitioners from fully utilizing LLMs for automated data\nscience. Therefore, we propose a method to detect potential biases by detecting\nanomalies in the frequency of operators (e.g., adding two features) suggested\nby LLMs when engineering new features. Our experiments evaluate the bias of\nfour LLMs, two big frontier and two small open-source models, across 27 tabular\ndatasets. Our results indicate that LLMs are biased toward simple operators,\nsuch as addition, and can fail to utilize more complex operators, such as\ngrouping followed by aggregations. Furthermore, the bias can negatively impact\nthe predictive performance when using LLM-generated features. Our results call\nfor mitigating bias when using LLMs for feature engineering."
                },
                "authors": [
                    {
                        "name": "Jaris Kken"
                    },
                    {
                        "name": "Lennart Purucker"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05804v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05804v5",
                "updated": "2024-10-24T07:07:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    7,
                    43,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-09T14:42:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    14,
                    42,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning"
                },
                "summary": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey."
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05804v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05804v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17781v1",
                "updated": "2024-10-23T11:31:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    31,
                    52,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:31:52Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    31,
                    52,
                    2,
                    297,
                    0
                ],
                "title": "Evaluating Explanations Through LLMs: Beyond Traditional User Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Explanations Through LLMs: Beyond Traditional User Studies"
                },
                "summary": "As AI becomes fundamental in sectors like healthcare, explainable AI (XAI)\ntools are essential for trust and transparency. However, traditional user\nstudies used to evaluate these tools are often costly, time consuming, and\ndifficult to scale. In this paper, we explore the use of Large Language Models\n(LLMs) to replicate human participants to help streamline XAI evaluation. We\nreproduce a user study comparing counterfactual and causal explanations,\nreplicating human participants with seven LLMs under various settings. Our\nresults show that (i) LLMs can replicate most conclusions from the original\nstudy, (ii) different LLMs yield varying levels of alignment in the results,\nand (iii) experimental factors such as LLM memory and output variability affect\nalignment with human responses. These initial findings suggest that LLMs could\nprovide a scalable and cost-effective way to simplify qualitative XAI\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI becomes fundamental in sectors like healthcare, explainable AI (XAI)\ntools are essential for trust and transparency. However, traditional user\nstudies used to evaluate these tools are often costly, time consuming, and\ndifficult to scale. In this paper, we explore the use of Large Language Models\n(LLMs) to replicate human participants to help streamline XAI evaluation. We\nreproduce a user study comparing counterfactual and causal explanations,\nreplicating human participants with seven LLMs under various settings. Our\nresults show that (i) LLMs can replicate most conclusions from the original\nstudy, (ii) different LLMs yield varying levels of alignment in the results,\nand (iii) experimental factors such as LLM memory and output variability affect\nalignment with human responses. These initial findings suggest that LLMs could\nprovide a scalable and cost-effective way to simplify qualitative XAI\nevaluation."
                },
                "authors": [
                    {
                        "name": "Francesco Bombassei De Bona"
                    },
                    {
                        "name": "Gabriele Dominici"
                    },
                    {
                        "name": "Tim Miller"
                    },
                    {
                        "name": "Marc Langheinrich"
                    },
                    {
                        "name": "Martin Gjoreski"
                    }
                ],
                "author_detail": {
                    "name": "Martin Gjoreski"
                },
                "author": "Martin Gjoreski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17779v1",
                "updated": "2024-10-23T11:31:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    31,
                    6,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:31:06Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    31,
                    6,
                    2,
                    297,
                    0
                ],
                "title": "ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language\n  Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language\n  Tuning"
                },
                "summary": "Recent advancements in multimodal fusion have witnessed the remarkable\nsuccess of vision-language (VL) models, which excel in various multimodal\napplications such as image captioning and visual question answering. However,\nbuilding VL models requires substantial hardware resources, where efficiency is\nrestricted by two key factors: the extended input sequence of the language\nmodel with vision features demands more computational operations, and a large\nnumber of additional learnable parameters increase memory complexity. These\nchallenges significantly restrict the broader applicability of such models. To\nbridge this gap, we propose ADEM-VL, an efficient vision-language method that\ntunes VL models based on pretrained large language models (LLMs) by adopting a\nparameter-free cross-attention mechanism for similarity measurements in\nmultimodal fusion. This approach only requires embedding vision features into\nthe language space, significantly reducing the number of trainable parameters\nand accelerating both training and inference speeds. To enhance representation\nlearning in fusion module, we introduce an efficient multiscale feature\ngeneration scheme that requires only a single forward pass through the vision\nencoder. Moreover, we propose an adaptive fusion scheme that dynamically\ndiscards less relevant visual information for each text token based on its\nattention score. This ensures that the fusion process prioritizes the most\npertinent visual features. With experiments on various tasks including visual\nquestion answering, image captioning, and instruction-following, we demonstrate\nthat our framework outperforms existing approaches. Specifically, our method\nsurpasses existing methods by an average accuracy of 0.77% on ScienceQA\ndataset, with reduced training and inference latency, demonstrating the\nsuperiority of our framework. The code is available at\nhttps://github.com/Hao840/ADEM-VL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal fusion have witnessed the remarkable\nsuccess of vision-language (VL) models, which excel in various multimodal\napplications such as image captioning and visual question answering. However,\nbuilding VL models requires substantial hardware resources, where efficiency is\nrestricted by two key factors: the extended input sequence of the language\nmodel with vision features demands more computational operations, and a large\nnumber of additional learnable parameters increase memory complexity. These\nchallenges significantly restrict the broader applicability of such models. To\nbridge this gap, we propose ADEM-VL, an efficient vision-language method that\ntunes VL models based on pretrained large language models (LLMs) by adopting a\nparameter-free cross-attention mechanism for similarity measurements in\nmultimodal fusion. This approach only requires embedding vision features into\nthe language space, significantly reducing the number of trainable parameters\nand accelerating both training and inference speeds. To enhance representation\nlearning in fusion module, we introduce an efficient multiscale feature\ngeneration scheme that requires only a single forward pass through the vision\nencoder. Moreover, we propose an adaptive fusion scheme that dynamically\ndiscards less relevant visual information for each text token based on its\nattention score. This ensures that the fusion process prioritizes the most\npertinent visual features. With experiments on various tasks including visual\nquestion answering, image captioning, and instruction-following, we demonstrate\nthat our framework outperforms existing approaches. Specifically, our method\nsurpasses existing methods by an average accuracy of 0.77% on ScienceQA\ndataset, with reduced training and inference latency, demonstrating the\nsuperiority of our framework. The code is available at\nhttps://github.com/Hao840/ADEM-VL."
                },
                "authors": [
                    {
                        "name": "Zhiwei Hao"
                    },
                    {
                        "name": "Jianyuan Guo"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17770v1",
                "updated": "2024-10-23T11:19:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    19,
                    8,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T11:19:08Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    19,
                    8,
                    2,
                    297,
                    0
                ],
                "title": "Locating Information in Large Language Models via Random Matrix Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locating Information in Large Language Models via Random Matrix Theory"
                },
                "summary": "As large language models (LLMs) become central to AI applications, gaining a\ndeeper understanding of their inner workings is increasingly important. In this\nwork, we analyze the weight matrices of pretrained transformer models --\nspecifically BERT and Llama -- using random matrix theory (RMT) as a\nzero-information hypothesis. While randomly initialized weights perfectly agree\nwith RMT predictions, deviations emerge after training, allowing us to locate\nlearned structures within the models. We identify layer-type specific behaviors\nthat are consistent across all blocks and architectures considered. By\npinpointing regions that deviate from RMT predictions, we highlight areas of\nfeature learning and confirm this through comparisons with the activation\ncovariance matrices of the corresponding layers. Our method provides a\ndiagnostic tool for identifying relevant regions in transformer weights using\nonly the trained matrices. Additionally, we address the ongoing debate\nregarding the significance of small singular values in the context of\nfine-tuning and alignment in LLMs. Our findings reveal that, after fine-tuning,\nsmall singular values play a crucial role in the models' capabilities,\nsuggesting that removing them in an already aligned transformer can be\ndetrimental, as it may compromise model alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become central to AI applications, gaining a\ndeeper understanding of their inner workings is increasingly important. In this\nwork, we analyze the weight matrices of pretrained transformer models --\nspecifically BERT and Llama -- using random matrix theory (RMT) as a\nzero-information hypothesis. While randomly initialized weights perfectly agree\nwith RMT predictions, deviations emerge after training, allowing us to locate\nlearned structures within the models. We identify layer-type specific behaviors\nthat are consistent across all blocks and architectures considered. By\npinpointing regions that deviate from RMT predictions, we highlight areas of\nfeature learning and confirm this through comparisons with the activation\ncovariance matrices of the corresponding layers. Our method provides a\ndiagnostic tool for identifying relevant regions in transformer weights using\nonly the trained matrices. Additionally, we address the ongoing debate\nregarding the significance of small singular values in the context of\nfine-tuning and alignment in LLMs. Our findings reveal that, after fine-tuning,\nsmall singular values play a crucial role in the models' capabilities,\nsuggesting that removing them in an already aligned transformer can be\ndetrimental, as it may compromise model alignment."
                },
                "authors": [
                    {
                        "name": "Max Staats"
                    },
                    {
                        "name": "Matthias Thamm"
                    },
                    {
                        "name": "Bernd Rosenow"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Rosenow"
                },
                "author": "Bernd Rosenow",
                "arxiv_comment": "17 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16144v2",
                "updated": "2024-10-23T11:17:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    17,
                    42,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-21T16:14:57Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    14,
                    57,
                    0,
                    295,
                    0
                ],
                "title": "1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on\n  CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on\n  CPUs"
                },
                "summary": "Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and\nBitNet b1.58, present a promising approach to enhancing the efficiency of LLMs\nin terms of speed and energy consumption. These developments also enable local\nLLM deployment across a broad range of devices. In this work, we introduce\nbitnet.cpp, a tailored software stack designed to unlock the full potential of\n1-bit LLMs. Specifically, we develop a set of kernels to support fast and\nlossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments\ndemonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x\nto 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model\nsizes. The code is available at https://github.com/microsoft/BitNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and\nBitNet b1.58, present a promising approach to enhancing the efficiency of LLMs\nin terms of speed and energy consumption. These developments also enable local\nLLM deployment across a broad range of devices. In this work, we introduce\nbitnet.cpp, a tailored software stack designed to unlock the full potential of\n1-bit LLMs. Specifically, we develop a set of kernels to support fast and\nlossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments\ndemonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x\nto 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model\nsizes. The code is available at https://github.com/microsoft/BitNet."
                },
                "authors": [
                    {
                        "name": "Jinheng Wang"
                    },
                    {
                        "name": "Hansong Zhou"
                    },
                    {
                        "name": "Ting Song"
                    },
                    {
                        "name": "Shaoguang Mao"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13247v2",
                "updated": "2024-10-23T11:09:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    9,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T06:14:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    6,
                    14,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Collaborative AI in Sentiment Analysis: System Architecture, Data\n  Prediction and Deployment Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative AI in Sentiment Analysis: System Architecture, Data\n  Prediction and Deployment Strategies"
                },
                "summary": "The advancement of large language model (LLM) based artificial intelligence\ntechnologies has been a game-changer, particularly in sentiment analysis. This\nprogress has enabled a shift from highly specialized research environments to\npractical, widespread applications within the industry. However, integrating\ndiverse AI models for processing complex multimodal data and the associated\nhigh costs of feature extraction presents significant challenges. Motivated by\nthe marketing oriented software development +needs, our study introduces a\ncollaborative AI framework designed to efficiently distribute and resolve tasks\nacross various AI systems to address these issues. Initially, we elucidate the\nkey solutions derived from our development process, highlighting the role of\ngenerative AI models like \\emph{chatgpt}, \\emph{google gemini} in simplifying\nintricate sentiment analysis tasks into manageable, phased objectives.\nFurthermore, we present a detailed case study utilizing our collaborative AI\nsystem in edge and cloud, showcasing its effectiveness in analyzing sentiments\nacross diverse online media channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language model (LLM) based artificial intelligence\ntechnologies has been a game-changer, particularly in sentiment analysis. This\nprogress has enabled a shift from highly specialized research environments to\npractical, widespread applications within the industry. However, integrating\ndiverse AI models for processing complex multimodal data and the associated\nhigh costs of feature extraction presents significant challenges. Motivated by\nthe marketing oriented software development +needs, our study introduces a\ncollaborative AI framework designed to efficiently distribute and resolve tasks\nacross various AI systems to address these issues. Initially, we elucidate the\nkey solutions derived from our development process, highlighting the role of\ngenerative AI models like \\emph{chatgpt}, \\emph{google gemini} in simplifying\nintricate sentiment analysis tasks into manageable, phased objectives.\nFurthermore, we present a detailed case study utilizing our collaborative AI\nsystem in edge and cloud, showcasing its effectiveness in analyzing sentiments\nacross diverse online media channels."
                },
                "authors": [
                    {
                        "name": "Chaofeng Zhang"
                    },
                    {
                        "name": "Jia Hou"
                    },
                    {
                        "name": "Xueting Tan"
                    },
                    {
                        "name": "Gaolei Li"
                    },
                    {
                        "name": "Caijuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Caijuan Chen"
                },
                "author": "Caijuan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17736v1",
                "updated": "2024-10-23T10:11:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    11,
                    40,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T10:11:40Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    11,
                    40,
                    2,
                    297,
                    0
                ],
                "title": "MojoBench: Language Modeling and Benchmarks for Mojo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MojoBench: Language Modeling and Benchmarks for Mojo"
                },
                "summary": "The recently introduced Mojo programming language (PL) by Modular, has\nreceived significant attention in the scientific community due to its claimed\nsignificant speed boost over Python. Despite advancements in code Large\nLanguage Models (LLMs) across various PLs, Mojo remains unexplored in this\ncontext. To address this gap, we introduce MojoBench, the first framework for\nMojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset\ndesigned for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM\npretrained and finetuned for Mojo code generation, which supports instructions\nin 5 natural languages (NLs). Our results show that Mojo-Coder achieves a\n30-35% performance improvement over leading models like GPT-4o and\nClaude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with\nunderrepresented and unseen PLs, offering potential strategies for enhancing\nmodel adaptability. MojoBench contributes to our understanding of LLM\ncapabilities and limitations in emerging programming paradigms fostering more\nrobust code generation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently introduced Mojo programming language (PL) by Modular, has\nreceived significant attention in the scientific community due to its claimed\nsignificant speed boost over Python. Despite advancements in code Large\nLanguage Models (LLMs) across various PLs, Mojo remains unexplored in this\ncontext. To address this gap, we introduce MojoBench, the first framework for\nMojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset\ndesigned for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM\npretrained and finetuned for Mojo code generation, which supports instructions\nin 5 natural languages (NLs). Our results show that Mojo-Coder achieves a\n30-35% performance improvement over leading models like GPT-4o and\nClaude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with\nunderrepresented and unseen PLs, offering potential strategies for enhancing\nmodel adaptability. MojoBench contributes to our understanding of LLM\ncapabilities and limitations in emerging programming paradigms fostering more\nrobust code generation systems."
                },
                "authors": [
                    {
                        "name": "Nishat Raihan"
                    },
                    {
                        "name": "Joanna C. S. Santos"
                    },
                    {
                        "name": "Marcos Zampieri"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Zampieri"
                },
                "author": "Marcos Zampieri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14622v2",
                "updated": "2024-10-23T10:06:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    6,
                    10,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-22T15:10:45Z",
                "published_parsed": [
                    2024,
                    2,
                    22,
                    15,
                    10,
                    45,
                    3,
                    53,
                    0
                ],
                "title": "From Keywords to Structured Summaries: Streamlining Scholarly\n  Information Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Keywords to Structured Summaries: Streamlining Scholarly\n  Information Access"
                },
                "summary": "This paper highlights the growing importance of information retrieval (IR)\nengines in the scientific community, addressing the inefficiency of traditional\nkeyword-based search engines due to the rising volume of publications. The\nproposed solution involves structured records, underpinning advanced\ninformation technology (IT) tools, including visualization dashboards, to\nrevolutionize how researchers access and filter articles, replacing the\ntraditional text-heavy approach. This vision is exemplified through a proof of\nconcept centered on the \"reproductive number estimate of infectious diseases\"\nresearch theme, using a fine-tuned large language model (LLM) to automate the\ncreation of structured records to populate a backend database that now goes\nbeyond keywords. The result is a next-generation information access system as\nan IR method accessible at https://orkg.org/usecases/r0-estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper highlights the growing importance of information retrieval (IR)\nengines in the scientific community, addressing the inefficiency of traditional\nkeyword-based search engines due to the rising volume of publications. The\nproposed solution involves structured records, underpinning advanced\ninformation technology (IT) tools, including visualization dashboards, to\nrevolutionize how researchers access and filter articles, replacing the\ntraditional text-heavy approach. This vision is exemplified through a proof of\nconcept centered on the \"reproductive number estimate of infectious diseases\"\nresearch theme, using a fine-tuned large language model (LLM) to automate the\ncreation of structured records to populate a backend database that now goes\nbeyond keywords. The result is a next-generation information access system as\nan IR method accessible at https://orkg.org/usecases/r0-estimates."
                },
                "authors": [
                    {
                        "name": "Mahsa Shamsabadi"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    }
                ],
                "author_detail": {
                    "name": "Jennifer D'Souza"
                },
                "author": "Jennifer D'Souza",
                "arxiv_comment": "8 pages, 3 figures | Accepted for publication as a poster paper at\n  the International Semantic Web Conference (ISWC 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17731v1",
                "updated": "2024-10-23T10:06:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    6,
                    2,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T10:06:02Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    6,
                    2,
                    2,
                    297,
                    0
                ],
                "title": "Time-to-Lie: Identifying Industrial Control System Honeypots Using the\n  Internet Control Message Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-to-Lie: Identifying Industrial Control System Honeypots Using the\n  Internet Control Message Protocol"
                },
                "summary": "The convergence of information and operational technology networks has\ncreated previously unforeseen security issues. To address these issues, both\nresearchers and practitioners have integrated threat intelligence methods into\nthe security operations of converged networks, with some of the most valuable\ntools being honeypots that imitate industrial control systems (ICS). However,\nthe development and deployment of such honeypots is a process rich with\npitfalls, which can lead to undiagnosed weaknesses in the threat intelligence\nbeing gathered. This paper presents a side-channel method of covertly\nidentifying ICS honeypots using the time-to-live (TTL) values of target\ndevices. We show that many ICS honeypots can be readily identified, via minimal\ninteractions, using only basic networking tools. In a study of over 8,000\ndevices presenting as ICS systems, we detail how our method compares to an\nexisting honeypot detection approach, and outline what our methodology reveals\nabout the current population of live ICS honeypots. In demonstrating our\nmethod, this study aims to raise awareness of the viability of the TTL\nheuristic and the prevalence of its misconfiguration despite its presence in\nliterature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The convergence of information and operational technology networks has\ncreated previously unforeseen security issues. To address these issues, both\nresearchers and practitioners have integrated threat intelligence methods into\nthe security operations of converged networks, with some of the most valuable\ntools being honeypots that imitate industrial control systems (ICS). However,\nthe development and deployment of such honeypots is a process rich with\npitfalls, which can lead to undiagnosed weaknesses in the threat intelligence\nbeing gathered. This paper presents a side-channel method of covertly\nidentifying ICS honeypots using the time-to-live (TTL) values of target\ndevices. We show that many ICS honeypots can be readily identified, via minimal\ninteractions, using only basic networking tools. In a study of over 8,000\ndevices presenting as ICS systems, we detail how our method compares to an\nexisting honeypot detection approach, and outline what our methodology reveals\nabout the current population of live ICS honeypots. In demonstrating our\nmethod, this study aims to raise awareness of the viability of the TTL\nheuristic and the prevalence of its misconfiguration despite its presence in\nliterature."
                },
                "authors": [
                    {
                        "name": "Jacob Williams"
                    },
                    {
                        "name": "Matthew Edwards"
                    },
                    {
                        "name": "Joseph Gardiner"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Gardiner"
                },
                "author": "Joseph Gardiner",
                "arxiv_comment": "11 pages, 2 listings, 5 tables, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17728v1",
                "updated": "2024-10-23T10:00:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    0,
                    23,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T10:00:23Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    0,
                    23,
                    2,
                    297,
                    0
                ],
                "title": "Dialectal and Low Resource Machine Translation for Aromanian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialectal and Low Resource Machine Translation for Aromanian"
                },
                "summary": "We present a neural machine translation system that can translate between\nRomanian, English, and Aromanian (an endangered Eastern Romance language); the\nfirst of its kind. BLEU scores range from 17 to 32 depending on the direction\nand genre of the text. Alongside, we release the biggest known\nAromanian-Romanian bilingual corpus, consisting of 79k cleaned sentence pairs.\nAdditional tools such as an agnostic sentence embedder (used for both text\nmining and automatic evaluation) and a diacritics converter are also presented.\nWe publicly release our findings and models. Finally, we describe the\ndeployment of our quantized model at https://arotranslate.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a neural machine translation system that can translate between\nRomanian, English, and Aromanian (an endangered Eastern Romance language); the\nfirst of its kind. BLEU scores range from 17 to 32 depending on the direction\nand genre of the text. Alongside, we release the biggest known\nAromanian-Romanian bilingual corpus, consisting of 79k cleaned sentence pairs.\nAdditional tools such as an agnostic sentence embedder (used for both text\nmining and automatic evaluation) and a diacritics converter are also presented.\nWe publicly release our findings and models. Finally, we describe the\ndeployment of our quantized model at https://arotranslate.com."
                },
                "authors": [
                    {
                        "name": "Alexandru-Iulius Jerpelea"
                    },
                    {
                        "name": "Alina-tefania Rdoi"
                    },
                    {
                        "name": "Sergiu Nisioi"
                    }
                ],
                "author_detail": {
                    "name": "Sergiu Nisioi"
                },
                "author": "Sergiu Nisioi",
                "arxiv_comment": "16 pages, 3 figures, 6 tables, submitted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04249v2",
                "updated": "2024-10-23T09:46:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    46,
                    43,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-05T18:11:14Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    11,
                    14,
                    5,
                    279,
                    0
                ],
                "title": "DiffSpec: Differential Testing with LLMs using Natural Language\n  Specifications and Code Artifacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffSpec: Differential Testing with LLMs using Natural Language\n  Specifications and Code Artifacts"
                },
                "summary": "Differential testing can be an effective way to find bugs in software systems\nwith multiple implementations that conform to the same specification, like\ncompilers, network protocol parsers, and language runtimes. Specifications for\nsuch systems are often standardized in natural language documents, like\nInstruction Set Architecture (ISA) specifications, Wasm specifications or IETF\nRFC's. Large Language Models (LLMs) have demonstrated potential in both\ngenerating tests and handling large volumes of natural language text, making\nthem well-suited for utilizing artifacts like specification documents, bug\nreports, and code implementations. In this work, we leverage natural language\nand code artifacts to guide LLMs to generate targeted, meaningful tests that\nhighlight meaningful behavioral differences between implementations, including\nthose corresponding to bugs. We introduce DiffSpec, a framework for generating\ndifferential tests with LLMs using prompt chaining. We demonstrate the efficacy\nof DiffSpec on two different systems, namely, eBPF runtimes and Wasm\nvalidators. Using DiffSpec, we generated 359 differentiating tests, uncovering\nat least four distinct and confirmed bugs in eBPF, including a kernel memory\nleak, inconsistent behavior in jump instructions, and undefined behavior when\nusing the stack pointer. We also found 279 differentiating tests in Wasm\nvalidators, that point to at least 2 confirmed and fixed bugs in Wizard Engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential testing can be an effective way to find bugs in software systems\nwith multiple implementations that conform to the same specification, like\ncompilers, network protocol parsers, and language runtimes. Specifications for\nsuch systems are often standardized in natural language documents, like\nInstruction Set Architecture (ISA) specifications, Wasm specifications or IETF\nRFC's. Large Language Models (LLMs) have demonstrated potential in both\ngenerating tests and handling large volumes of natural language text, making\nthem well-suited for utilizing artifacts like specification documents, bug\nreports, and code implementations. In this work, we leverage natural language\nand code artifacts to guide LLMs to generate targeted, meaningful tests that\nhighlight meaningful behavioral differences between implementations, including\nthose corresponding to bugs. We introduce DiffSpec, a framework for generating\ndifferential tests with LLMs using prompt chaining. We demonstrate the efficacy\nof DiffSpec on two different systems, namely, eBPF runtimes and Wasm\nvalidators. Using DiffSpec, we generated 359 differentiating tests, uncovering\nat least four distinct and confirmed bugs in eBPF, including a kernel memory\nleak, inconsistent behavior in jump instructions, and undefined behavior when\nusing the stack pointer. We also found 279 differentiating tests in Wasm\nvalidators, that point to at least 2 confirmed and fixed bugs in Wizard Engine."
                },
                "authors": [
                    {
                        "name": "Nikitha Rao"
                    },
                    {
                        "name": "Elizabeth Gilbert"
                    },
                    {
                        "name": "Tahina Ramananandro"
                    },
                    {
                        "name": "Nikhil Swamy"
                    },
                    {
                        "name": "Claire Le Goues"
                    },
                    {
                        "name": "Sarah Fakhoury"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Fakhoury"
                },
                "author": "Sarah Fakhoury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14282v3",
                "updated": "2024-10-23T09:42:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    42,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-06-20T13:07:38Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    13,
                    7,
                    38,
                    3,
                    172,
                    0
                ],
                "title": "Learning to Plan for Retrieval-Augmented Large Language Models from\n  Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Plan for Retrieval-Augmented Large Language Models from\n  Knowledge Graphs"
                },
                "summary": "Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the performance of large language models (LLMs) in complex\nquestion-answering (QA) scenarios has always been a research focal point.\nRecent studies have attempted to enhance LLMs' performance by combining\nstep-wise planning with external retrieval. While effective for advanced models\nlike GPT-3.5, smaller LLMs face challenges in decomposing complex questions,\nnecessitating supervised fine-tuning. Previous work has relied on manual\nannotation and knowledge distillation from teacher LLMs, which are\ntime-consuming and not accurate enough. In this paper, we introduce a novel\nframework for enhancing LLMs' planning capabilities by using planning data\nderived from knowledge graphs (KGs). LLMs fine-tuned with this data have\nimproved planning capabilities, better equipping them to handle complex QA\ntasks that involve retrieval. Evaluations on multiple datasets, including our\nnewly proposed benchmark, highlight the effectiveness of our framework and the\nbenefits of KG-derived planning data."
                },
                "authors": [
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Mingyang Chen"
                    },
                    {
                        "name": "Binbin Hu"
                    },
                    {
                        "name": "Dan Yang"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Yue Shen"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    },
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "EMNLP2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14693v2",
                "updated": "2024-10-23T09:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    42,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-04-23T02:50:38Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    2,
                    50,
                    38,
                    1,
                    114,
                    0
                ],
                "title": "DIP-Watermark: A Double Identity Protection Method Based on Robust\n  Adversarial Watermark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIP-Watermark: A Double Identity Protection Method Based on Robust\n  Adversarial Watermark"
                },
                "summary": "The wide deployment of Face Recognition (FR) systems poses privacy risks. One\ncountermeasure is adversarial attack, deceiving unauthorized malicious FR, but\nit also disrupts regular identity verification of trusted authorizers,\nexacerbating the potential threat of identity impersonation. To address this,\nwe propose the first double identity protection scheme based on traceable\nadversarial watermarking, termed DIP-Watermark. DIP-Watermark employs a\none-time watermark embedding to deceive unauthorized FR models and allows\nauthorizers to perform identity verification by extracting the watermark.\nSpecifically, we propose an information-guided adversarial attack against FR\nmodels. The encoder embeds an identity-specific watermark into the deep feature\nspace of the carrier, guiding recognizable features of the image to deviate\nfrom the source identity. We further adopt a collaborative meta-optimization\nstrategy compatible with sub-tasks, which regularizes the joint optimization\ndirection of the encoder and decoder. This strategy enhances the representation\nof universal carrier features, mitigating multi-objective optimization\nconflicts in watermarking. Experiments confirm that DIP-Watermark achieves\nsignificant attack success rates and traceability accuracy on state-of-the-art\nFR models, exhibiting remarkable robustness that outperforms the existing\nprivacy protection methods using adversarial attacks and deep watermarking, or\nsimple combinations of the two. Our work potentially opens up new insights into\nproactive protection for FR privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Face Recognition (FR) systems poses privacy risks. One\ncountermeasure is adversarial attack, deceiving unauthorized malicious FR, but\nit also disrupts regular identity verification of trusted authorizers,\nexacerbating the potential threat of identity impersonation. To address this,\nwe propose the first double identity protection scheme based on traceable\nadversarial watermarking, termed DIP-Watermark. DIP-Watermark employs a\none-time watermark embedding to deceive unauthorized FR models and allows\nauthorizers to perform identity verification by extracting the watermark.\nSpecifically, we propose an information-guided adversarial attack against FR\nmodels. The encoder embeds an identity-specific watermark into the deep feature\nspace of the carrier, guiding recognizable features of the image to deviate\nfrom the source identity. We further adopt a collaborative meta-optimization\nstrategy compatible with sub-tasks, which regularizes the joint optimization\ndirection of the encoder and decoder. This strategy enhances the representation\nof universal carrier features, mitigating multi-objective optimization\nconflicts in watermarking. Experiments confirm that DIP-Watermark achieves\nsignificant attack success rates and traceability accuracy on state-of-the-art\nFR models, exhibiting remarkable robustness that outperforms the existing\nprivacy protection methods using adversarial attacks and deep watermarking, or\nsimple combinations of the two. Our work potentially opens up new insights into\nproactive protection for FR privacy."
                },
                "authors": [
                    {
                        "name": "Yunming Zhang"
                    },
                    {
                        "name": "Dengpan Ye"
                    },
                    {
                        "name": "Caiyun Xie"
                    },
                    {
                        "name": "Sipeng Shen"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Jiacheng Deng"
                    },
                    {
                        "name": "Long Tang"
                    }
                ],
                "author_detail": {
                    "name": "Long Tang"
                },
                "author": "Long Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17714v1",
                "updated": "2024-10-23T09:40:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    40,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T09:40:15Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    40,
                    15,
                    2,
                    297,
                    0
                ],
                "title": "CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient\n  Semantic Steering in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogSteer: Cognition-Inspired Selective Layer Intervention for Efficient\n  Semantic Steering in Large Language Models"
                },
                "summary": "Despite their impressive capabilities, large language models (LLMs) often\nlack interpretability and can generate toxic content. While using LLMs as\nfoundation models and applying semantic steering methods are widely practiced,\nwe believe that efficient methods should be based on a thorough understanding\nof LLM behavior. To this end, we propose using eye movement measures to\ninterpret LLM behavior across layers. We find that LLMs exhibit patterns\nsimilar to human gaze across layers and different layers function differently.\nInspired by these findings, we introduce a heuristic steering layer selection\nand apply it to layer intervention methods via fine-tuning and inference. Using\nlanguage toxification and detoxification as test beds, we demonstrate that our\nproposed CogSteer methods achieve better results in terms of toxicity scores\nwhile efficiently saving 97% of the computational resources and 60% of the\ntraining time. Our model-agnostic approach can be adopted into various LLMs,\ncontributing to their interpretability and promoting trustworthiness for safe\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, large language models (LLMs) often\nlack interpretability and can generate toxic content. While using LLMs as\nfoundation models and applying semantic steering methods are widely practiced,\nwe believe that efficient methods should be based on a thorough understanding\nof LLM behavior. To this end, we propose using eye movement measures to\ninterpret LLM behavior across layers. We find that LLMs exhibit patterns\nsimilar to human gaze across layers and different layers function differently.\nInspired by these findings, we introduce a heuristic steering layer selection\nand apply it to layer intervention methods via fine-tuning and inference. Using\nlanguage toxification and detoxification as test beds, we demonstrate that our\nproposed CogSteer methods achieve better results in terms of toxicity scores\nwhile efficiently saving 97% of the computational resources and 60% of the\ntraining time. Our model-agnostic approach can be adopted into various LLMs,\ncontributing to their interpretability and promoting trustworthiness for safe\ndeployment."
                },
                "authors": [
                    {
                        "name": "Xintong Wang"
                    },
                    {
                        "name": "Jingheng Pan"
                    },
                    {
                        "name": "Longqin Jiang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Xingshan Li"
                    },
                    {
                        "name": "Chris Biemann"
                    }
                ],
                "author_detail": {
                    "name": "Chris Biemann"
                },
                "author": "Chris Biemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17711v1",
                "updated": "2024-10-23T09:36:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    36,
                    21,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T09:36:21Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    36,
                    21,
                    2,
                    297,
                    0
                ],
                "title": "Beware of Calibration Data for Pruning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beware of Calibration Data for Pruning Large Language Models"
                },
                "summary": "As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters.\nPrevious research has primarily focused on designing advanced pruning methods,\nwhile different calibration data's impact on pruning performance still lacks\nsystematical exploration. We fill this blank and surprisingly observe that the\neffects of calibration data even value more than designing advanced pruning\nstrategies, especially for high sparsity. Our preliminary exploration also\ndiscloses that using calibration data similar to the training data can yield\nbetter performance. As pre-training data is usually inaccessible for advanced\nLLMs, we further provide a self-generating calibration data synthesis strategy\nto construct feasible calibration data. We conduct experiments on the recent\nstrong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that\nthe proposed method outperforms commonly used calibration data and can\neffectively enhance strong pruning methods (e.g., Wanda, OWL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are widely applied across various fields,\nmodel compression has become increasingly crucial for reducing costs and\nimproving inference efficiency. Post-training pruning is a promising method\nthat does not require resource-intensive iterative training and only needs a\nsmall amount of calibration data to assess the importance of parameters.\nPrevious research has primarily focused on designing advanced pruning methods,\nwhile different calibration data's impact on pruning performance still lacks\nsystematical exploration. We fill this blank and surprisingly observe that the\neffects of calibration data even value more than designing advanced pruning\nstrategies, especially for high sparsity. Our preliminary exploration also\ndiscloses that using calibration data similar to the training data can yield\nbetter performance. As pre-training data is usually inaccessible for advanced\nLLMs, we further provide a self-generating calibration data synthesis strategy\nto construct feasible calibration data. We conduct experiments on the recent\nstrong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that\nthe proposed method outperforms commonly used calibration data and can\neffectively enhance strong pruning methods (e.g., Wanda, OWL)."
                },
                "authors": [
                    {
                        "name": "Yixin Ji"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17694v1",
                "updated": "2024-10-23T09:14:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    14,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T09:14:57Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    9,
                    14,
                    57,
                    2,
                    297,
                    0
                ],
                "title": "An Adaptive Framework for Generating Systematic Explanatory Answer in\n  Online Q&A Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Adaptive Framework for Generating Systematic Explanatory Answer in\n  Online Q&A Platforms"
                },
                "summary": "Question Answering (QA) systems face challenges in handling complex questions\nthat require multi-domain knowledge synthesis. The naive RAG models, although\neffective in information retrieval, struggle with complex questions that\nrequire comprehensive and in-depth answers. The pioneering task is defined as\nexplanatory answer generation, which entails handling identified challenges\nsuch as the requirement for comprehensive information and logical coherence\nwithin the generated context. To address these issues, we refer to systematic\nthinking theory and propose SynthRAG, an innovative framework designed to\nenhance QA performance. SynthRAG improves on conventional models by employing\nadaptive outlines for dynamic content structuring, generating systematic\ninformation to ensure detailed coverage, and producing customized answers\ntailored to specific user inquiries. This structured approach guarantees\nlogical coherence and thorough integration of information, yielding responses\nthat are both insightful and methodically organized. Empirical evaluations\nunderscore SynthRAG's effectiveness, demonstrating its superiority in handling\ncomplex questions, overcoming the limitations of naive RAG models, and\nsignificantly improving answer quality and depth. Furthermore, an online\ndeployment on the Zhihu platform revealed that SynthRAG's answers achieved\nnotable user engagement, with each response averaging 5.73 upvotes and\nsurpassing the performance of 79.8% of human contributors, highlighting the\npractical relevance and impact of the proposed framework. Our code is available\nat https://github.com/czy1999/SynthRAG .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering (QA) systems face challenges in handling complex questions\nthat require multi-domain knowledge synthesis. The naive RAG models, although\neffective in information retrieval, struggle with complex questions that\nrequire comprehensive and in-depth answers. The pioneering task is defined as\nexplanatory answer generation, which entails handling identified challenges\nsuch as the requirement for comprehensive information and logical coherence\nwithin the generated context. To address these issues, we refer to systematic\nthinking theory and propose SynthRAG, an innovative framework designed to\nenhance QA performance. SynthRAG improves on conventional models by employing\nadaptive outlines for dynamic content structuring, generating systematic\ninformation to ensure detailed coverage, and producing customized answers\ntailored to specific user inquiries. This structured approach guarantees\nlogical coherence and thorough integration of information, yielding responses\nthat are both insightful and methodically organized. Empirical evaluations\nunderscore SynthRAG's effectiveness, demonstrating its superiority in handling\ncomplex questions, overcoming the limitations of naive RAG models, and\nsignificantly improving answer quality and depth. Furthermore, an online\ndeployment on the Zhihu platform revealed that SynthRAG's answers achieved\nnotable user engagement, with each response averaging 5.73 upvotes and\nsurpassing the performance of 79.8% of human contributors, highlighting the\npractical relevance and impact of the proposed framework. Our code is available\nat https://github.com/czy1999/SynthRAG ."
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Jinzhi Liao"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Xiang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Zhao"
                },
                "author": "Xiang Zhao",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15993v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15993v4",
                "updated": "2024-10-23T08:33:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    33,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-04-24T17:10:35Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    17,
                    10,
                    35,
                    2,
                    115,
                    0
                ],
                "title": "Uncertainty Estimation and Quantification for LLMs: A Simple Supervised\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Estimation and Quantification for LLMs: A Simple Supervised\n  Approach"
                },
                "summary": "In this paper, we study the problem of uncertainty estimation and calibration\nfor LLMs. We begin by formulating the uncertainty estimation problem, a\nrelevant yet underexplored area in existing literature. We then propose a\nsupervised approach that leverages labeled datasets to estimate the uncertainty\nin LLMs' responses. Based on the formulation, we illustrate the difference\nbetween the uncertainty estimation for LLMs and that for standard ML models and\nexplain why the hidden neurons of the LLMs may contain uncertainty information.\nOur designed approach demonstrates the benefits of utilizing hidden activations\nto enhance uncertainty estimation across various tasks and shows robust\ntransferability in out-of-distribution settings. We distinguish the uncertainty\nestimation task from the uncertainty calibration task and show that better\nuncertainty estimation leads to better calibration performance. Furthermore,\nour method is easy to implement and adaptable to different levels of model\naccessibility including black box, grey box, and white box.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the problem of uncertainty estimation and calibration\nfor LLMs. We begin by formulating the uncertainty estimation problem, a\nrelevant yet underexplored area in existing literature. We then propose a\nsupervised approach that leverages labeled datasets to estimate the uncertainty\nin LLMs' responses. Based on the formulation, we illustrate the difference\nbetween the uncertainty estimation for LLMs and that for standard ML models and\nexplain why the hidden neurons of the LLMs may contain uncertainty information.\nOur designed approach demonstrates the benefits of utilizing hidden activations\nto enhance uncertainty estimation across various tasks and shows robust\ntransferability in out-of-distribution settings. We distinguish the uncertainty\nestimation task from the uncertainty calibration task and show that better\nuncertainty estimation leads to better calibration performance. Furthermore,\nour method is easy to implement and adaptable to different levels of model\naccessibility including black box, grey box, and white box."
                },
                "authors": [
                    {
                        "name": "Linyu Liu"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Xiaocheng Li"
                    },
                    {
                        "name": "Guanting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanting Chen"
                },
                "author": "Guanting Chen",
                "arxiv_comment": "29 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15993v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15993v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16237v2",
                "updated": "2024-10-23T08:31:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    31,
                    20,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-21T17:41:42Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    41,
                    42,
                    0,
                    295,
                    0
                ],
                "title": "IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in\n  Communicative Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in\n  Communicative Multi-Agent Systems"
                },
                "summary": "As large language model (LLM) agents increasingly integrate into our\ninfrastructure, their robust coordination and message synchronization become\nvital. The Byzantine Generals Problem (BGP) is a critical model for\nconstructing resilient multi-agent systems (MAS) under adversarial attacks. It\ndescribes a scenario where malicious agents with unknown identities exist in\nthe system-situations that, in our context, could result from LLM agents'\nhallucinations or external attacks. In BGP, the objective of the entire system\nis to reach a consensus on the action to be taken. Traditional BGP requires\nglobal consensus among all agents; however, in practical scenarios, global\nconsensus is not always necessary and can even be inefficient. Therefore, there\nis a pressing need to explore a refined version of BGP that aligns with the\nlocal coordination patterns observed in MAS. We refer to this refined version\nas Imperfect BGP (IBGP) in our research, aiming to address this discrepancy. To\ntackle this issue, we propose a framework that leverages consensus protocols\nwithin general MAS settings, providing provable resilience against\ncommunication attacks and adaptability to changing environments, as validated\nby empirical results. Additionally, we present a case study in a sensor network\nenvironment to illustrate the practical application of our protocol.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language model (LLM) agents increasingly integrate into our\ninfrastructure, their robust coordination and message synchronization become\nvital. The Byzantine Generals Problem (BGP) is a critical model for\nconstructing resilient multi-agent systems (MAS) under adversarial attacks. It\ndescribes a scenario where malicious agents with unknown identities exist in\nthe system-situations that, in our context, could result from LLM agents'\nhallucinations or external attacks. In BGP, the objective of the entire system\nis to reach a consensus on the action to be taken. Traditional BGP requires\nglobal consensus among all agents; however, in practical scenarios, global\nconsensus is not always necessary and can even be inefficient. Therefore, there\nis a pressing need to explore a refined version of BGP that aligns with the\nlocal coordination patterns observed in MAS. We refer to this refined version\nas Imperfect BGP (IBGP) in our research, aiming to address this discrepancy. To\ntackle this issue, we propose a framework that leverages consensus protocols\nwithin general MAS settings, providing provable resilience against\ncommunication attacks and adaptability to changing environments, as validated\nby empirical results. Additionally, we present a case study in a sensor network\nenvironment to illustrate the practical application of our protocol."
                },
                "authors": [
                    {
                        "name": "Yihuan Mao"
                    },
                    {
                        "name": "Yipeng Kang"
                    },
                    {
                        "name": "Peilun Li"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Chongjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chongjie Zhang"
                },
                "author": "Chongjie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10216v2",
                "updated": "2024-10-23T08:22:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    22,
                    44,
                    2,
                    297,
                    0
                ],
                "published": "2024-06-14T17:49:59Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    17,
                    49,
                    59,
                    4,
                    166,
                    0
                ],
                "title": "Regularizing Hidden States Enables Learning Generalizable Reward Model\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularizing Hidden States Enables Learning Generalizable Reward Model\n  for LLMs"
                },
                "summary": "Reward models trained on human preference data have been proven to\neffectively align Large Language Models (LLMs) with human intent within the\nframework of reinforcement learning from human feedback (RLHF). However,\ncurrent reward models have limited generalization capabilities to unseen\nprompts and responses, which can lead to an unexpected phenomenon known as\nreward over-optimization, resulting in a decline in actual performance due to\nexcessive optimization of rewards. While previous research has advocated for\nconstraining policy optimization, our study introduces a novel approach to\nenhance the reward model's generalization ability against distribution shifts\nby regularizing the hidden states. Specifically, we retain the base model's\nlanguage model head and incorporate a suite of text-generation losses to\npreserve the hidden states' text-generation capabilities, while concurrently\nlearning a reward head behind the same hidden states. Our experimental results\ndemonstrate that the introduced regularization technique markedly improves the\naccuracy of learned reward models across a variety of out-of-distribution (OOD)\ntasks and effectively alleviates the over-optimization issue in RLHF, offering\na more reliable and robust preference learning paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models trained on human preference data have been proven to\neffectively align Large Language Models (LLMs) with human intent within the\nframework of reinforcement learning from human feedback (RLHF). However,\ncurrent reward models have limited generalization capabilities to unseen\nprompts and responses, which can lead to an unexpected phenomenon known as\nreward over-optimization, resulting in a decline in actual performance due to\nexcessive optimization of rewards. While previous research has advocated for\nconstraining policy optimization, our study introduces a novel approach to\nenhance the reward model's generalization ability against distribution shifts\nby regularizing the hidden states. Specifically, we retain the base model's\nlanguage model head and incorporate a suite of text-generation losses to\npreserve the hidden states' text-generation capabilities, while concurrently\nlearning a reward head behind the same hidden states. Our experimental results\ndemonstrate that the introduced regularization technique markedly improves the\naccuracy of learned reward models across a variety of out-of-distribution (OOD)\ntasks and effectively alleviates the over-optimization issue in RLHF, offering\na more reliable and robust preference learning paradigm."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruomeng Ding"
                    },
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17657v1",
                "updated": "2024-10-23T08:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    19,
                    18,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T08:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    19,
                    18,
                    2,
                    297,
                    0
                ],
                "title": "ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents"
                },
                "summary": "Large Language Models (LLMs) have shown promising potential in the medical\ndomain, assisting with tasks like clinical note generation and patient\ncommunication. However, current LLMs are limited to text-based communication,\nhindering their ability to interact with diverse forms of information in\nclinical environments. Despite clinical agents succeeding in diverse signal\ninteraction, they are oriented to a single clinical scenario and hence fail for\nbroader applications. To evaluate clinical agents holistically, we propose\nClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting\nof 18 tasks across five key realistic clinical dimensions. Building on this, we\nintroduce ReflecTool, a novel framework that excels at utilizing\ndomain-specific tools within two stages. The first optimization stage\nprogressively enlarges a long-term memory by saving successful solving\nprocesses and tool-wise experience of agents in a tiny pre-defined training\nset. In the following inference stage, ReflecTool can search for supportive\nsuccessful demonstrations from already built long-term memory to guide the tool\nselection strategy, and a verifier improves the tool usage according to the\ntool-wise experience with two verification methods--iterative refinement and\ncandidate selection. Extensive experiments on ClinicalAgent Benchmark\ndemonstrate that ReflecTool surpasses the pure LLMs with more than 10 points\nand the well-established agent-based methods with 3 points, highlighting its\nadaptability and effectiveness in solving complex clinical tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising potential in the medical\ndomain, assisting with tasks like clinical note generation and patient\ncommunication. However, current LLMs are limited to text-based communication,\nhindering their ability to interact with diverse forms of information in\nclinical environments. Despite clinical agents succeeding in diverse signal\ninteraction, they are oriented to a single clinical scenario and hence fail for\nbroader applications. To evaluate clinical agents holistically, we propose\nClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting\nof 18 tasks across five key realistic clinical dimensions. Building on this, we\nintroduce ReflecTool, a novel framework that excels at utilizing\ndomain-specific tools within two stages. The first optimization stage\nprogressively enlarges a long-term memory by saving successful solving\nprocesses and tool-wise experience of agents in a tiny pre-defined training\nset. In the following inference stage, ReflecTool can search for supportive\nsuccessful demonstrations from already built long-term memory to guide the tool\nselection strategy, and a verifier improves the tool usage according to the\ntool-wise experience with two verification methods--iterative refinement and\ncandidate selection. Extensive experiments on ClinicalAgent Benchmark\ndemonstrate that ReflecTool surpasses the pure LLMs with more than 10 points\nand the well-established agent-based methods with 3 points, highlighting its\nadaptability and effectiveness in solving complex clinical tasks."
                },
                "authors": [
                    {
                        "name": "Yusheng Liao"
                    },
                    {
                        "name": "Shuyang Jiang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17656v1",
                "updated": "2024-10-23T08:18:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    18,
                    38,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T08:18:38Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    18,
                    38,
                    2,
                    297,
                    0
                ],
                "title": "AutoRNet: Automatically Optimizing Heuristics for Robust Network Design\n  via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRNet: Automatically Optimizing Heuristics for Robust Network Design\n  via Large Language Models"
                },
                "summary": "Achieving robust networks is a challenging problem due to its NP-hard nature\nand complex solution space. Current methods, from handcrafted feature\nextraction to deep learning, have made progress but remain rigid, requiring\nmanual design and large labeled datasets. To address these issues, we propose\nAutoRNet, a framework that integrates large language models (LLMs) with\nevolutionary algorithms to generate heuristics for robust network design. We\ndesign network optimization strategies to provide domain-specific prompts for\nLLMs, utilizing domain knowledge to generate advanced heuristics. Additionally,\nwe introduce an adaptive fitness function to balance convergence and diversity\nwhile maintaining degree distributions. AutoRNet is evaluated on sparse and\ndense scale-free networks, outperforming current methods by reducing the need\nfor manual design and large datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving robust networks is a challenging problem due to its NP-hard nature\nand complex solution space. Current methods, from handcrafted feature\nextraction to deep learning, have made progress but remain rigid, requiring\nmanual design and large labeled datasets. To address these issues, we propose\nAutoRNet, a framework that integrates large language models (LLMs) with\nevolutionary algorithms to generate heuristics for robust network design. We\ndesign network optimization strategies to provide domain-specific prompts for\nLLMs, utilizing domain knowledge to generate advanced heuristics. Additionally,\nwe introduce an adaptive fitness function to balance convergence and diversity\nwhile maintaining degree distributions. AutoRNet is evaluated on sparse and\ndense scale-free networks, outperforming current methods by reducing the need\nfor manual design and large datasets."
                },
                "authors": [
                    {
                        "name": "He Yu"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17646v1",
                "updated": "2024-10-23T08:03:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    3,
                    42,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T08:03:42Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    8,
                    3,
                    42,
                    2,
                    297,
                    0
                ],
                "title": "Accelerating soft-constrained MPC for linear systems through online\n  constraint removal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating soft-constrained MPC for linear systems through online\n  constraint removal"
                },
                "summary": "Optimization-based controllers, such as Model Predictive Control (MPC), have\nattracted significant research interest due to their intuitive concept,\nconstraint handling capabilities, and natural application to multi-input\nmulti-output systems. However, the computational complexity of solving a\nreceding horizon problem at each time step remains a challenge for the\ndeployment of MPC. This is particularly the case for systems constrained by\nmany inequalities. Recently, we introduced the concept of constraint-adaptive\nMPC (ca-MPC) to address this challenge for linear systems with hard\nconstraints. In ca-MPC, at each time step, a subset of the constraints is\nremoved from the optimization problem, thereby accelerating the optimization\nprocedure, while resulting in identical closed-loop behavior. The present paper\nextends this framework to soft-constrained MPC by detecting and removing\nconstraints based on sub-optimal predicted input sequences, which is rather\neasy for soft-constrained MPC due to the receding horizon principle and the\ninclusion of slack variables. We will translate these new ideas explicitly to\nan offset-free output tracking problem. The effectiveness of these ideas is\ndemonstrated on a two-dimensional thermal transport model, showing a three\norder of magnitude improvement in online computational time of the MPC scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization-based controllers, such as Model Predictive Control (MPC), have\nattracted significant research interest due to their intuitive concept,\nconstraint handling capabilities, and natural application to multi-input\nmulti-output systems. However, the computational complexity of solving a\nreceding horizon problem at each time step remains a challenge for the\ndeployment of MPC. This is particularly the case for systems constrained by\nmany inequalities. Recently, we introduced the concept of constraint-adaptive\nMPC (ca-MPC) to address this challenge for linear systems with hard\nconstraints. In ca-MPC, at each time step, a subset of the constraints is\nremoved from the optimization problem, thereby accelerating the optimization\nprocedure, while resulting in identical closed-loop behavior. The present paper\nextends this framework to soft-constrained MPC by detecting and removing\nconstraints based on sub-optimal predicted input sequences, which is rather\neasy for soft-constrained MPC due to the receding horizon principle and the\ninclusion of slack variables. We will translate these new ideas explicitly to\nan offset-free output tracking problem. The effectiveness of these ideas is\ndemonstrated on a two-dimensional thermal transport model, showing a three\norder of magnitude improvement in online computational time of the MPC scheme."
                },
                "authors": [
                    {
                        "name": "S. A. N. Nouwens"
                    },
                    {
                        "name": "M. M. Paulides"
                    },
                    {
                        "name": "W. P. M. H. Heemels"
                    }
                ],
                "author_detail": {
                    "name": "W. P. M. H. Heemels"
                },
                "author": "W. P. M. H. Heemels",
                "arxiv_doi": "10.1109/CDC49753.2023.10383769",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CDC49753.2023.10383769",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.17646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 5 figures, CDC 2023 conference",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17632v1",
                "updated": "2024-10-23T07:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    48,
                    51,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    48,
                    51,
                    2,
                    297,
                    0
                ],
                "title": "LMLPA: Language Model Linguistic Personality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMLPA: Language Model Linguistic Personality Assessment"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing."
                },
                "authors": [
                    {
                        "name": "Jingyao Zheng"
                    },
                    {
                        "name": "Xian Wang"
                    },
                    {
                        "name": "Simo Hosio"
                    },
                    {
                        "name": "Xiaoxian Xu"
                    },
                    {
                        "name": "Lik-Hang Lee"
                    }
                ],
                "author_detail": {
                    "name": "Lik-Hang Lee"
                },
                "author": "Lik-Hang Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16169v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16169v3",
                "updated": "2024-10-23T07:32:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    32,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2023-11-16T13:17:20Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    13,
                    17,
                    20,
                    3,
                    320,
                    0
                ],
                "title": "Understanding the Effectiveness of Large Language Models in Detecting\n  Security Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Effectiveness of Large Language Models in Detecting\n  Security Vulnerabilities"
                },
                "summary": "While automated vulnerability detection techniques have made promising\nprogress in detecting security vulnerabilities, their scalability and\napplicability remain challenging. The remarkable performance of Large Language\nModels (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted\nrecent works to explore if LLMs can be used to detect vulnerabilities. In this\npaper, we perform a more comprehensive study by concurrently examining a higher\nnumber of datasets, languages and LLMs, and qualitatively evaluating\nperformance across prompts and vulnerability classes while addressing the\nshortcomings of existing tools. Concretely, we evaluate the effectiveness of 16\npre-trained LLMs on 5,000 code samples from five diverse security datasets.\nThese balanced datasets encompass both synthetic and real-world projects in\nJava and C/C++ and cover 25 distinct vulnerability classes.\n  Overall, LLMs across all scales and families show modest effectiveness in\ndetecting vulnerabilities, obtaining an average accuracy of 62.8% and F1 score\nof 0.71 across datasets. They are significantly better at detecting\nvulnerabilities only requiring intra-procedural analysis, such as OS Command\nInjection and NULL Pointer Dereference. Moreover, they report higher accuracies\non these vulnerabilities than popular static analysis tools, such as CodeQL.\n  We find that advanced prompting strategies that involve step-by-step analysis\nsignificantly improve performance of LLMs on real-world datasets in terms of F1\nscore (by upto 0.18 on average). Interestingly, we observe that LLMs show\npromising abilities at performing parts of the analysis correctly, such as\nidentifying vulnerability-related specifications and leveraging natural\nlanguage information to understand code behavior (e.g., to check if code is\nsanitized). We expect our insights to guide future work on LLM-augmented\nvulnerability detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While automated vulnerability detection techniques have made promising\nprogress in detecting security vulnerabilities, their scalability and\napplicability remain challenging. The remarkable performance of Large Language\nModels (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted\nrecent works to explore if LLMs can be used to detect vulnerabilities. In this\npaper, we perform a more comprehensive study by concurrently examining a higher\nnumber of datasets, languages and LLMs, and qualitatively evaluating\nperformance across prompts and vulnerability classes while addressing the\nshortcomings of existing tools. Concretely, we evaluate the effectiveness of 16\npre-trained LLMs on 5,000 code samples from five diverse security datasets.\nThese balanced datasets encompass both synthetic and real-world projects in\nJava and C/C++ and cover 25 distinct vulnerability classes.\n  Overall, LLMs across all scales and families show modest effectiveness in\ndetecting vulnerabilities, obtaining an average accuracy of 62.8% and F1 score\nof 0.71 across datasets. They are significantly better at detecting\nvulnerabilities only requiring intra-procedural analysis, such as OS Command\nInjection and NULL Pointer Dereference. Moreover, they report higher accuracies\non these vulnerabilities than popular static analysis tools, such as CodeQL.\n  We find that advanced prompting strategies that involve step-by-step analysis\nsignificantly improve performance of LLMs on real-world datasets in terms of F1\nscore (by upto 0.18 on average). Interestingly, we observe that LLMs show\npromising abilities at performing parts of the analysis correctly, such as\nidentifying vulnerability-related specifications and leveraging natural\nlanguage information to understand code behavior (e.g., to check if code is\nsanitized). We expect our insights to guide future work on LLM-augmented\nvulnerability detection systems."
                },
                "authors": [
                    {
                        "name": "Avishree Khare"
                    },
                    {
                        "name": "Saikat Dutta"
                    },
                    {
                        "name": "Ziyang Li"
                    },
                    {
                        "name": "Alaia Solko-Breslin"
                    },
                    {
                        "name": "Rajeev Alur"
                    },
                    {
                        "name": "Mayur Naik"
                    }
                ],
                "author_detail": {
                    "name": "Mayur Naik"
                },
                "author": "Mayur Naik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16169v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16169v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17621v1",
                "updated": "2024-10-23T07:22:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    22,
                    33,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:22:33Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    22,
                    33,
                    2,
                    297,
                    0
                ],
                "title": "Process Supervision-Guided Policy Optimization for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Supervision-Guided Policy Optimization for Code Generation"
                },
                "summary": "Reinforcement Learning (RL) with unit test feedback has enhanced large\nlanguage models (LLMs) code generation, but relies on sparse rewards provided\nonly after complete code evaluation, limiting learning efficiency and\nincremental improvements. When generated code fails all unit tests, no learning\nsignal is received, hindering progress on complex tasks. To address this, we\npropose a Process Reward Model (PRM) that delivers dense, line-level feedback\non code correctness during generation, mimicking human code refinement and\nproviding immediate guidance. We explore various strategies for training PRMs\nand integrating them into the RL framework, finding that using PRMs both as\ndense rewards and for value function initialization significantly boosts\nperformance. Our approach increases our in-house LLM's pass rate from 28.2% to\n29.8% on LiveCodeBench and from 31.8% to 35.8% on our internal benchmark. Our\nexperimental results highlight the effectiveness of PRMs in enhancing RL-driven\ncode generation, especially for long-horizon scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) with unit test feedback has enhanced large\nlanguage models (LLMs) code generation, but relies on sparse rewards provided\nonly after complete code evaluation, limiting learning efficiency and\nincremental improvements. When generated code fails all unit tests, no learning\nsignal is received, hindering progress on complex tasks. To address this, we\npropose a Process Reward Model (PRM) that delivers dense, line-level feedback\non code correctness during generation, mimicking human code refinement and\nproviding immediate guidance. We explore various strategies for training PRMs\nand integrating them into the RL framework, finding that using PRMs both as\ndense rewards and for value function initialization significantly boosts\nperformance. Our approach increases our in-house LLM's pass rate from 28.2% to\n29.8% on LiveCodeBench and from 31.8% to 35.8% on our internal benchmark. Our\nexperimental results highlight the effectiveness of PRMs in enhancing RL-driven\ncode generation, especially for long-horizon scenarios."
                },
                "authors": [
                    {
                        "name": "Ning Dai"
                    },
                    {
                        "name": "Zheng Wu"
                    },
                    {
                        "name": "Renjie Zheng"
                    },
                    {
                        "name": "Ziyun Wei"
                    },
                    {
                        "name": "Wenlei Shi"
                    },
                    {
                        "name": "Xing Jin"
                    },
                    {
                        "name": "Guanlin Liu"
                    },
                    {
                        "name": "Chen Dun"
                    },
                    {
                        "name": "Liang Huang"
                    },
                    {
                        "name": "Lin Yan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yan"
                },
                "author": "Lin Yan",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03622v3",
                "updated": "2024-10-23T07:20:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    20,
                    26,
                    2,
                    297,
                    0
                ],
                "published": "2024-04-04T17:45:08Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    17,
                    45,
                    8,
                    3,
                    95,
                    0
                ],
                "title": "Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning\n  in Large Language Models"
                },
                "summary": "Large language models (LLMs) have exhibited impressive performance in\nlanguage comprehension and various reasoning tasks. However, their abilities in\nspatial reasoning, a crucial aspect of human cognition, remain relatively\nunexplored. Human possess a remarkable ability to create mental images of\nunseen objects and actions through a process known as the Mind's Eye, enabling\nthe imagination of the unseen world. Inspired by this cognitive capacity, we\npropose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial\nreasoning of LLMs by visualizing their reasoning traces, thereby guiding\nsubsequent reasoning steps. We employed VoT for multi-hop spatial reasoning\ntasks, including natural language navigation, visual navigation, and visual\ntiling in 2D grid worlds. Experimental results demonstrated that VoT\nsignificantly enhances the spatial reasoning abilities of LLMs. Notably, VoT\noutperformed existing multimodal large language models (MLLMs) in these tasks.\nWhile VoT works surprisingly well on LLMs, the ability to generate mental\nimages to facilitate spatial reasoning resembles the mind's eye process,\nsuggesting its potential viability in MLLMs. Please find the dataset and codes\nat https://microsoft.github.io/visualization-of-thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive performance in\nlanguage comprehension and various reasoning tasks. However, their abilities in\nspatial reasoning, a crucial aspect of human cognition, remain relatively\nunexplored. Human possess a remarkable ability to create mental images of\nunseen objects and actions through a process known as the Mind's Eye, enabling\nthe imagination of the unseen world. Inspired by this cognitive capacity, we\npropose Visualization-of-Thought (VoT) prompting. VoT aims to elicit spatial\nreasoning of LLMs by visualizing their reasoning traces, thereby guiding\nsubsequent reasoning steps. We employed VoT for multi-hop spatial reasoning\ntasks, including natural language navigation, visual navigation, and visual\ntiling in 2D grid worlds. Experimental results demonstrated that VoT\nsignificantly enhances the spatial reasoning abilities of LLMs. Notably, VoT\noutperformed existing multimodal large language models (MLLMs) in these tasks.\nWhile VoT works surprisingly well on LLMs, the ability to generate mental\nimages to facilitate spatial reasoning resembles the mind's eye process,\nsuggesting its potential viability in MLLMs. Please find the dataset and codes\nat https://microsoft.github.io/visualization-of-thought"
                },
                "authors": [
                    {
                        "name": "Wenshan Wu"
                    },
                    {
                        "name": "Shaoguang Mao"
                    },
                    {
                        "name": "Yadong Zhang"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Lei Cui"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17619v1",
                "updated": "2024-10-23T07:17:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    17,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:17:31Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    17,
                    31,
                    2,
                    297,
                    0
                ],
                "title": "From PDFs to Structured Data: Utilizing LLM Analysis in Sports Database\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From PDFs to Structured Data: Utilizing LLM Analysis in Sports Database\n  Management"
                },
                "summary": "This study investigates the effectiveness of Large Language Models (LLMs) in\nprocessing semi-structured data from PDF documents into structured formats,\nspecifically examining their application in updating the Finnish Sports Clubs\nDatabase. Through action research methodology, we developed and evaluated an\nAI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus\nmodels to process data from 72 sports federation membership reports. The system\nachieved a 90% success rate in automated processing, successfully handling 65\nof 72 files without errors and converting over 7,900 rows of data. While the\ninitial development time was comparable to traditional manual processing (three\nmonths), the implemented system shows potential for reducing future processing\ntime by approximately 90%. Key challenges included handling multilingual\ncontent, processing multi-page datasets, and managing extraneous information.\nThe findings suggest that while LLMs demonstrate significant potential for\nautomating semi-structured data processing tasks, optimal results are achieved\nthrough a hybrid approach combining AI automation with selective human\noversight. This research contributes to the growing body of literature on\npractical LLM applications in organizational data management and provides\ninsights into the transformation of traditional data processing workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the effectiveness of Large Language Models (LLMs) in\nprocessing semi-structured data from PDF documents into structured formats,\nspecifically examining their application in updating the Finnish Sports Clubs\nDatabase. Through action research methodology, we developed and evaluated an\nAI-assisted approach utilizing OpenAI's GPT-4 and Anthropic's Claude 3 Opus\nmodels to process data from 72 sports federation membership reports. The system\nachieved a 90% success rate in automated processing, successfully handling 65\nof 72 files without errors and converting over 7,900 rows of data. While the\ninitial development time was comparable to traditional manual processing (three\nmonths), the implemented system shows potential for reducing future processing\ntime by approximately 90%. Key challenges included handling multilingual\ncontent, processing multi-page datasets, and managing extraneous information.\nThe findings suggest that while LLMs demonstrate significant potential for\nautomating semi-structured data processing tasks, optimal results are achieved\nthrough a hybrid approach combining AI automation with selective human\noversight. This research contributes to the growing body of literature on\npractical LLM applications in organizational data management and provides\ninsights into the transformation of traditional data processing workflows."
                },
                "authors": [
                    {
                        "name": "Juhani Merilehto"
                    }
                ],
                "author_detail": {
                    "name": "Juhani Merilehto"
                },
                "author": "Juhani Merilehto",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17195v2",
                "updated": "2024-10-23T07:02:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    2,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-22T17:13:38Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    13,
                    38,
                    1,
                    296,
                    0
                ],
                "title": "Non-myopic Generation of Language Model for Reasoning and Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-myopic Generation of Language Model for Reasoning and Planning"
                },
                "summary": "Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have demonstrated remarkable abilities in reasoning and\nplanning by breaking down complex problems into sequential steps. Despite their\nsuccess in various domains like mathematical problem-solving and coding, LLMs\nface challenges in ensuring reliable and optimal planning due to their inherent\nmyopic nature of autoregressive decoding. This paper revisits LLM reasoning\nfrom an optimal-control perspective, proposing a novel method,\nPredictive-Decoding, that leverages Model Predictive Control to enhance\nplanning accuracy. By re-weighting LLM distributions based on foresight\ntrajectories, Predictive-Decoding aims to mitigate early errors and promote\nnon-myopic planning. Our experiments show significant improvements in a wide\nrange of tasks for math, coding, and agents. Furthermore, Predictive-Decoding\ndemonstrates computational efficiency, outperforming search baselines with\nreduced computational resources. This study provides insights into optimizing\nLLM planning capabilities."
                },
                "authors": [
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Junlei Zhang"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17602v1",
                "updated": "2024-10-23T06:56:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    56,
                    53,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T06:56:53Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    56,
                    53,
                    2,
                    297,
                    0
                ],
                "title": "Integrating Large Language Models for UAV Control in Simulated\n  Environments: A Modular Interaction Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models for UAV Control in Simulated\n  Environments: A Modular Interaction Approach"
                },
                "summary": "The intersection of LLMs (Large Language Models) and UAV (Unoccupied Aerial\nVehicles) technology represents a promising field of research with the\npotential to enhance UAV capabilities significantly. This study explores the\napplication of LLMs in UAV control, focusing on the opportunities for\nintegrating advanced natural language processing into autonomous aerial\nsystems. By enabling UAVs to interpret and respond to natural language\ncommands, LLMs simplify the UAV control and usage, making them accessible to a\nbroader user base and facilitating more intuitive human-machine interactions.\nThe paper discusses several key areas where LLMs can impact UAV technology,\nincluding autonomous decision-making, dynamic mission planning, enhanced\nsituational awareness, and improved safety protocols. Through a comprehensive\nreview of current developments and potential future directions, this study aims\nto highlight how LLMs can transform UAV operations, making them more adaptable,\nresponsive, and efficient in complex environments. A template development\nframework for integrating LLMs in UAV control is also described. Proof of\nConcept results that integrate existing LLM models and popular robotic\nsimulation platforms are demonstrated. The findings suggest that while there\nare substantial technical and ethical challenges to address, integrating LLMs\ninto UAV control holds promising implications for advancing autonomous aerial\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The intersection of LLMs (Large Language Models) and UAV (Unoccupied Aerial\nVehicles) technology represents a promising field of research with the\npotential to enhance UAV capabilities significantly. This study explores the\napplication of LLMs in UAV control, focusing on the opportunities for\nintegrating advanced natural language processing into autonomous aerial\nsystems. By enabling UAVs to interpret and respond to natural language\ncommands, LLMs simplify the UAV control and usage, making them accessible to a\nbroader user base and facilitating more intuitive human-machine interactions.\nThe paper discusses several key areas where LLMs can impact UAV technology,\nincluding autonomous decision-making, dynamic mission planning, enhanced\nsituational awareness, and improved safety protocols. Through a comprehensive\nreview of current developments and potential future directions, this study aims\nto highlight how LLMs can transform UAV operations, making them more adaptable,\nresponsive, and efficient in complex environments. A template development\nframework for integrating LLMs in UAV control is also described. Proof of\nConcept results that integrate existing LLM models and popular robotic\nsimulation platforms are demonstrated. The findings suggest that while there\nare substantial technical and ethical challenges to address, integrating LLMs\ninto UAV control holds promising implications for advancing autonomous aerial\nsystems."
                },
                "authors": [
                    {
                        "name": "Abhishek Phadke"
                    },
                    {
                        "name": "Alihan Hadimlioglu"
                    },
                    {
                        "name": "Tianxing Chu"
                    },
                    {
                        "name": "Chandra N Sekharan"
                    }
                ],
                "author_detail": {
                    "name": "Chandra N Sekharan"
                },
                "author": "Chandra N Sekharan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17600v1",
                "updated": "2024-10-23T06:54:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    54,
                    3,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T06:54:03Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    54,
                    3,
                    2,
                    297,
                    0
                ],
                "title": "Graphusion: A RAG Framework for Knowledge Graph Construction with a\n  Global Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphusion: A RAG Framework for Knowledge Graph Construction with a\n  Global Perspective"
                },
                "summary": "Knowledge Graphs (KGs) are crucial in the field of artificial intelligence\nand are widely used in downstream tasks, such as question-answering (QA). The\nconstruction of KGs typically requires significant effort from domain experts.\nLarge Language Models (LLMs) have recently been used for Knowledge Graph\nConstruction (KGC). However, most existing approaches focus on a local\nperspective, extracting knowledge triplets from individual sentences or\ndocuments, missing a fusion process to combine the knowledge in a global KG.\nThis work introduces Graphusion, a zero-shot KGC framework from free text. It\ncontains three steps: in Step 1, we extract a list of seed entities using topic\nmodeling to guide the final KG includes the most relevant entities; in Step 2,\nwe conduct candidate triplet extraction using LLMs; in Step 3, we design the\nnovel fusion module that provides a global view of the extracted knowledge,\nincorporating entity merging, conflict resolution, and novel triplet discovery.\nResults show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for\nentity extraction and relation recognition, respectively. Moreover, we showcase\nhow Graphusion could be applied to the Natural Language Processing (NLP) domain\nand validate it in an educational scenario. Specifically, we introduce TutorQA,\na new expert-verified benchmark for QA, comprising six tasks and a total of\n1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant\nimprovement on the benchmark, for example, a 9.2% accuracy improvement on\nsub-graph completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs (KGs) are crucial in the field of artificial intelligence\nand are widely used in downstream tasks, such as question-answering (QA). The\nconstruction of KGs typically requires significant effort from domain experts.\nLarge Language Models (LLMs) have recently been used for Knowledge Graph\nConstruction (KGC). However, most existing approaches focus on a local\nperspective, extracting knowledge triplets from individual sentences or\ndocuments, missing a fusion process to combine the knowledge in a global KG.\nThis work introduces Graphusion, a zero-shot KGC framework from free text. It\ncontains three steps: in Step 1, we extract a list of seed entities using topic\nmodeling to guide the final KG includes the most relevant entities; in Step 2,\nwe conduct candidate triplet extraction using LLMs; in Step 3, we design the\nnovel fusion module that provides a global view of the extracted knowledge,\nincorporating entity merging, conflict resolution, and novel triplet discovery.\nResults show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for\nentity extraction and relation recognition, respectively. Moreover, we showcase\nhow Graphusion could be applied to the Natural Language Processing (NLP) domain\nand validate it in an educational scenario. Specifically, we introduce TutorQA,\na new expert-verified benchmark for QA, comprising six tasks and a total of\n1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant\nimprovement on the benchmark, for example, a 9.2% accuracy improvement on\nsub-graph completion."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Boming Yang"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Sixun Ouyang"
                    },
                    {
                        "name": "Moritz Blum"
                    },
                    {
                        "name": "Tianwei She"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Freddy Lecue"
                    },
                    {
                        "name": "Jinghui Lu"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2407.10794",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17599v1",
                "updated": "2024-10-23T06:52:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    52,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T06:52:09Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    52,
                    9,
                    2,
                    297,
                    0
                ],
                "title": "Cross-model Control: Improving Multiple Large Language Models in\n  One-time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-model Control: Improving Multiple Large Language Models in\n  One-time Training"
                },
                "summary": "The number of large language models (LLMs) with varying parameter scales and\nvocabularies is increasing. While they deliver powerful performance, they also\nface a set of common optimization needs to meet specific requirements or\nstandards, such as instruction following or avoiding the output of sensitive\ninformation from the real world. However, how to reuse the fine-tuning outcomes\nof one model to other models to reduce training costs remains a challenge. To\nbridge this gap, we introduce Cross-model Control (CMC), a method that improves\nmultiple LLMs in one-time training with a portable tiny language model.\nSpecifically, we have observed that the logit shift before and after\nfine-tuning is remarkably similar across different models. Based on this\ninsight, we incorporate a tiny language model with a minimal number of\nparameters. By training alongside a frozen template LLM, the tiny model gains\nthe capability to alter the logits output by the LLMs. To make this tiny\nlanguage model applicable to models with different vocabularies, we propose a\nnovel token mapping strategy named PM-MinED. We have conducted extensive\nexperiments on instruction tuning and unlearning tasks, demonstrating the\neffectiveness of CMC. Our code is available at https://github.com/wujwyi/CMC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The number of large language models (LLMs) with varying parameter scales and\nvocabularies is increasing. While they deliver powerful performance, they also\nface a set of common optimization needs to meet specific requirements or\nstandards, such as instruction following or avoiding the output of sensitive\ninformation from the real world. However, how to reuse the fine-tuning outcomes\nof one model to other models to reduce training costs remains a challenge. To\nbridge this gap, we introduce Cross-model Control (CMC), a method that improves\nmultiple LLMs in one-time training with a portable tiny language model.\nSpecifically, we have observed that the logit shift before and after\nfine-tuning is remarkably similar across different models. Based on this\ninsight, we incorporate a tiny language model with a minimal number of\nparameters. By training alongside a frozen template LLM, the tiny model gains\nthe capability to alter the logits output by the LLMs. To make this tiny\nlanguage model applicable to models with different vocabularies, we propose a\nnovel token mapping strategy named PM-MinED. We have conducted extensive\nexperiments on instruction tuning and unlearning tasks, demonstrating the\neffectiveness of CMC. Our code is available at https://github.com/wujwyi/CMC."
                },
                "authors": [
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Ming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Ming Gao"
                },
                "author": "Ming Gao",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06813v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06813v4",
                "updated": "2024-10-23T06:39:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    39,
                    57,
                    2,
                    297,
                    0
                ],
                "published": "2024-07-09T12:37:54Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    12,
                    37,
                    54,
                    1,
                    191,
                    0
                ],
                "title": "Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Richelieu: Self-Evolving LLM-Based Agents for AI Diplomacy"
                },
                "summary": "Diplomacy is one of the most sophisticated activities in human society,\ninvolving complex interactions among multiple parties that require skills in\nsocial reasoning, negotiation, and long-term strategic planning. Previous AI\nagents have demonstrated their ability to handle multi-step games and large\naction spaces in multi-agent tasks. However, diplomacy involves a staggering\nmagnitude of decision spaces, especially considering the negotiation stage\nrequired. While recent agents based on large language models (LLMs) have shown\npotential in various applications, they still struggle with extended planning\nperiods in complex multi-agent settings. Leveraging recent technologies for\nLLM-based agents, we aim to explore AI's potential to create a human-like agent\ncapable of executing comprehensive multi-agent missions by integrating three\nfundamental capabilities: 1) strategic planning with memory and reflection; 2)\ngoal-oriented negotiation with social reasoning; and 3) augmenting memory\nthrough self-play games for self-evolution without human in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diplomacy is one of the most sophisticated activities in human society,\ninvolving complex interactions among multiple parties that require skills in\nsocial reasoning, negotiation, and long-term strategic planning. Previous AI\nagents have demonstrated their ability to handle multi-step games and large\naction spaces in multi-agent tasks. However, diplomacy involves a staggering\nmagnitude of decision spaces, especially considering the negotiation stage\nrequired. While recent agents based on large language models (LLMs) have shown\npotential in various applications, they still struggle with extended planning\nperiods in complex multi-agent settings. Leveraging recent technologies for\nLLM-based agents, we aim to explore AI's potential to create a human-like agent\ncapable of executing comprehensive multi-agent missions by integrating three\nfundamental capabilities: 1) strategic planning with memory and reflection; 2)\ngoal-oriented negotiation with social reasoning; and 3) augmenting memory\nthrough self-play games for self-evolution without human in the loop."
                },
                "authors": [
                    {
                        "name": "Zhenyu Guan"
                    },
                    {
                        "name": "Xiangyu Kong"
                    },
                    {
                        "name": "Fangwei Zhong"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "arxiv_journal_ref": "NuerIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06813v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06813v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00386v2",
                "updated": "2024-10-23T06:15:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    15,
                    38,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-01T07:15:03Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    7,
                    15,
                    3,
                    3,
                    32,
                    0
                ],
                "title": "AssertLLM: Generating and Evaluating Hardware Verification Assertions\n  from Design Specifications via Multi-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AssertLLM: Generating and Evaluating Hardware Verification Assertions\n  from Design Specifications via Multi-LLMs"
                },
                "summary": "Assertion-based verification (ABV) is a critical method to ensure logic\ndesigns comply with their architectural specifications. ABV requires\nassertions, which are generally converted from specifications through human\ninterpretation by verification engineers. Existing methods for generating\nassertions from specification documents are limited to sentences extracted by\nengineers, discouraging their practical applications. In this work, we present\nAssertLLM, an automatic assertion generation framework that processes complete\nspecification documents. AssertLLM can generate assertions from both natural\nlanguage and waveform diagrams in specification files. It first converts\nunstructured specification sentences and waveforms into structured descriptions\nusing natural language templates. Then, a customized Large Language Model (LLM)\ngenerates the final assertions based on these descriptions. Our evaluation\ndemonstrates that AssertLLM can generate more accurate and higher-quality\nassertions compared to GPT-4o and GPT-3.5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assertion-based verification (ABV) is a critical method to ensure logic\ndesigns comply with their architectural specifications. ABV requires\nassertions, which are generally converted from specifications through human\ninterpretation by verification engineers. Existing methods for generating\nassertions from specification documents are limited to sentences extracted by\nengineers, discouraging their practical applications. In this work, we present\nAssertLLM, an automatic assertion generation framework that processes complete\nspecification documents. AssertLLM can generate assertions from both natural\nlanguage and waveform diagrams in specification files. It first converts\nunstructured specification sentences and waveforms into structured descriptions\nusing natural language templates. Then, a customized Large Language Model (LLM)\ngenerates the final assertions based on these descriptions. Our evaluation\ndemonstrates that AssertLLM can generate more accurate and higher-quality\nassertions compared to GPT-4o and GPT-3.5."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Yan"
                    },
                    {
                        "name": "Wenji Fang"
                    },
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Zhiyao Xie"
                    },
                    {
                        "name": "Hongce Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongce Zhang"
                },
                "author": "Hongce Zhang",
                "arxiv_comment": "Accepted by ASPDAC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17578v1",
                "updated": "2024-10-23T06:04:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    4,
                    55,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T06:04:55Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    4,
                    55,
                    2,
                    297,
                    0
                ],
                "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and\n  Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and\n  Reward Models"
                },
                "summary": "Large language models (LLMs) are commonly used as evaluators in tasks (e.g.,\nreward modeling, LLM-as-a-judge), where they act as proxies for human\npreferences or judgments. This leads to the need for meta-evaluation:\nevaluating the credibility of LLMs as evaluators. However, existing benchmarks\nprimarily focus on English, offering limited insight into LLMs' effectiveness\nas evaluators in non-English contexts. To address this, we introduce MM-Eval, a\nmultilingual meta-evaluation benchmark that covers 18 languages across six\ncategories. MM-Eval evaluates various dimensions, including language-specific\nchallenges like linguistics and language hallucinations. Evaluation results\nshow that both proprietary and open-source language models have considerable\nroom for improvement. Further analysis reveals a tendency for these models to\nassign middle-ground scores to low-resource languages. We publicly release our\nbenchmark and code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are commonly used as evaluators in tasks (e.g.,\nreward modeling, LLM-as-a-judge), where they act as proxies for human\npreferences or judgments. This leads to the need for meta-evaluation:\nevaluating the credibility of LLMs as evaluators. However, existing benchmarks\nprimarily focus on English, offering limited insight into LLMs' effectiveness\nas evaluators in non-English contexts. To address this, we introduce MM-Eval, a\nmultilingual meta-evaluation benchmark that covers 18 languages across six\ncategories. MM-Eval evaluates various dimensions, including language-specific\nchallenges like linguistics and language hallucinations. Evaluation results\nshow that both proprietary and open-source language models have considerable\nroom for improvement. Further analysis reveals a tendency for these models to\nassign middle-ground scores to low-resource languages. We publicly release our\nbenchmark and code."
                },
                "authors": [
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Dongkeun Yoon"
                    },
                    {
                        "name": "Juyoung Suk"
                    },
                    {
                        "name": "Javier Aula-Blasco"
                    },
                    {
                        "name": "Mano Aslan"
                    },
                    {
                        "name": "Vu Trong Kim"
                    },
                    {
                        "name": "Shayekh Bin Islam"
                    },
                    {
                        "name": "Jaume Prats-Cristi"
                    },
                    {
                        "name": "Luca Tormo-Bauelos"
                    },
                    {
                        "name": "Seungone Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seungone Kim"
                },
                "author": "Seungone Kim",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10648v2",
                "updated": "2024-10-23T05:24:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    24,
                    23,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-14T15:59:16Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    15,
                    59,
                    16,
                    0,
                    288,
                    0
                ],
                "title": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Baseline for Predicting Events with Auto-Regressive Tabular\n  Transformers"
                },
                "summary": "Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world applications of tabular data involve using historic events to\npredict properties of new ones, for example whether a credit card transaction\nis fraudulent or what rating a customer will assign a product on a retail\nplatform. Existing approaches to event prediction include costly, brittle, and\napplication-dependent techniques such as time-aware positional embeddings,\nlearned row and field encodings, and oversampling methods for addressing class\nimbalance. Moreover, these approaches often assume specific use-cases, for\nexample that we know the labels of all historic events or that we only predict\na pre-specified label and not the data's features themselves. In this work, we\npropose a simple but flexible baseline using standard autoregressive LLM-style\ntransformers with elementary positional embeddings and a causal language\nmodeling objective. Our baseline outperforms existing approaches across popular\ndatasets and can be employed for various use-cases. We demonstrate that the\nsame model can predict labels, impute missing values, or model event sequences."
                },
                "authors": [
                    {
                        "name": "Alex Stein"
                    },
                    {
                        "name": "Samuel Sharpe"
                    },
                    {
                        "name": "Doron Bergman"
                    },
                    {
                        "name": "Senthil Kumar"
                    },
                    {
                        "name": "C. Bayan Bruss"
                    },
                    {
                        "name": "John Dickerson"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Micah Goldblum"
                    }
                ],
                "author_detail": {
                    "name": "Micah Goldblum"
                },
                "author": "Micah Goldblum",
                "arxiv_comment": "10 pages, 6 pages of references+appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17566v1",
                "updated": "2024-10-23T05:19:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    19,
                    51,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T05:19:51Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    19,
                    51,
                    2,
                    297,
                    0
                ],
                "title": "Differentially Private Learning Needs Better Model Initialization and\n  Self-Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Learning Needs Better Model Initialization and\n  Self-Distillation"
                },
                "summary": "Differentially private SGD (DPSGD) enables privacy-preserving training of\nlanguage models, but often reduces utility, diversity, and linguistic quality.\nWe introduce DPRefine, a three-phase method that initializes a model using data\nsynthesis from a small pre-trained LM with rigorous filtering, applies DP\nfinetuning on private data, and performs self-distillation to refine outputs.\nThis approach significantly outperforms vanilla DPSGD, with AlpacaEval\npreferring DPRefine's generations in 78.4% of cases across all datasets. Our\nanalysis reveals that DPRefine reduces linguistic errors in generated text by\n84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD.\nIt also reduces inconsistencies of non-private models, such as hallucinated\ndetails and misattributed quotes. We find that small models like GPT-2 can be\neffective for initialization and distillation, highlighting their potential in\nenabling scalable and efficient deployment of privacy-preserving language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially private SGD (DPSGD) enables privacy-preserving training of\nlanguage models, but often reduces utility, diversity, and linguistic quality.\nWe introduce DPRefine, a three-phase method that initializes a model using data\nsynthesis from a small pre-trained LM with rigorous filtering, applies DP\nfinetuning on private data, and performs self-distillation to refine outputs.\nThis approach significantly outperforms vanilla DPSGD, with AlpacaEval\npreferring DPRefine's generations in 78.4% of cases across all datasets. Our\nanalysis reveals that DPRefine reduces linguistic errors in generated text by\n84.0%, mitigating grammar and spelling errors, commonly associated with DPSGD.\nIt also reduces inconsistencies of non-private models, such as hallucinated\ndetails and misattributed quotes. We find that small models like GPT-2 can be\neffective for initialization and distillation, highlighting their potential in\nenabling scalable and efficient deployment of privacy-preserving language."
                },
                "authors": [
                    {
                        "name": "Ivoline C. Ngong"
                    },
                    {
                        "name": "Joseph P. Near"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    }
                ],
                "author_detail": {
                    "name": "Niloofar Mireshghallah"
                },
                "author": "Niloofar Mireshghallah",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17558v1",
                "updated": "2024-10-23T04:55:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    4,
                    55,
                    8,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T04:55:08Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    4,
                    55,
                    8,
                    2,
                    297,
                    0
                ],
                "title": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLR-Bench: Evaluating Large Language Models in College-level Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated their remarkable performance\nacross various language understanding tasks. While emerging benchmarks have\nbeen proposed to evaluate LLMs in various domains such as mathematics and\ncomputer science, they merely measure the accuracy in terms of the final\nprediction on multi-choice questions. However, it remains insufficient to\nverify the essential understanding of LLMs given a chosen choice. To fill this\ngap, we present CLR-Bench to comprehensively evaluate the LLMs in complex\ncollege-level reasoning. Specifically, (i) we prioritize 16 challenging college\ndisciplines in computer science and artificial intelligence. The dataset\ncontains 5 types of questions, while each question is associated with detailed\nexplanations from experts. (ii) To quantify a fair evaluation of LLMs'\nreasoning ability, we formalize the criteria with two novel metrics.\nQ$\\rightarrow$A is utilized to measure the performance of direct answer\nprediction, and Q$\\rightarrow$AR effectively considers the joint ability to\nanswer the question and provide rationale simultaneously. Extensive experiments\nare conducted with 40 LLMs over 1,018 discipline-specific questions. The\nresults demonstrate the key insights that LLMs, even the best closed-source\nLLM, i.e., GPT-4 turbo, tend to `guess' the college-level answers. It shows a\ndramatic decrease in accuracy from 63.31% Q$\\rightarrow$A to 39.00%\nQ$\\rightarrow$AR, indicating an unsatisfactory reasoning ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated their remarkable performance\nacross various language understanding tasks. While emerging benchmarks have\nbeen proposed to evaluate LLMs in various domains such as mathematics and\ncomputer science, they merely measure the accuracy in terms of the final\nprediction on multi-choice questions. However, it remains insufficient to\nverify the essential understanding of LLMs given a chosen choice. To fill this\ngap, we present CLR-Bench to comprehensively evaluate the LLMs in complex\ncollege-level reasoning. Specifically, (i) we prioritize 16 challenging college\ndisciplines in computer science and artificial intelligence. The dataset\ncontains 5 types of questions, while each question is associated with detailed\nexplanations from experts. (ii) To quantify a fair evaluation of LLMs'\nreasoning ability, we formalize the criteria with two novel metrics.\nQ$\\rightarrow$A is utilized to measure the performance of direct answer\nprediction, and Q$\\rightarrow$AR effectively considers the joint ability to\nanswer the question and provide rationale simultaneously. Extensive experiments\nare conducted with 40 LLMs over 1,018 discipline-specific questions. The\nresults demonstrate the key insights that LLMs, even the best closed-source\nLLM, i.e., GPT-4 turbo, tend to `guess' the college-level answers. It shows a\ndramatic decrease in accuracy from 63.31% Q$\\rightarrow$A to 39.00%\nQ$\\rightarrow$AR, indicating an unsatisfactory reasoning ability."
                },
                "authors": [
                    {
                        "name": "Junnan Dong"
                    },
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "arxiv_comment": "18 pages, 6 figures, dataset and evaluation framework will be\n  opensourced",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17552v2",
                "updated": "2024-10-24T02:35:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    2,
                    35,
                    9,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-23T04:34:49Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    4,
                    34,
                    49,
                    2,
                    297,
                    0
                ],
                "title": "ESpeW: Robust Copyright Protection for LLM-based EaaS via\n  Embedding-Specific Watermark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESpeW: Robust Copyright Protection for LLM-based EaaS via\n  Embedding-Specific Watermark"
                },
                "summary": "Embeddings as a Service (EaaS) is emerging as a crucial role in AI\napplications. Unfortunately, EaaS is vulnerable to model extraction attacks,\nhighlighting the urgent need for copyright protection. Although some\npreliminary works propose applying embedding watermarks to protect EaaS, recent\nresearch reveals that these watermarks can be easily removed. Hence, it is\ncrucial to inject robust watermarks resistant to watermark removal attacks.\nExisting watermarking methods typically inject a target embedding into\nembeddings through linear interpolation when the text contains triggers.\nHowever, this mechanism results in each watermarked embedding having the same\ncomponent, which makes the watermark easy to identify and eliminate. Motivated\nby this, in this paper, we propose a novel embedding-specific watermarking\n(ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach\ninvolves injecting unique, yet readily identifiable watermarks into each\nembedding. Watermarks inserted by ESpeW are designed to maintain a significant\ndistance from one another and to avoid sharing common components, thus making\nit significantly more challenging to remove the watermarks. Extensive\nexperiments on four popular datasets demonstrate that ESpeW can even watermark\nsuccessfully against a highly aggressive removal strategy without sacrificing\nthe quality of embeddings. Code is available at\nhttps://github.com/liudan193/ESpeW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embeddings as a Service (EaaS) is emerging as a crucial role in AI\napplications. Unfortunately, EaaS is vulnerable to model extraction attacks,\nhighlighting the urgent need for copyright protection. Although some\npreliminary works propose applying embedding watermarks to protect EaaS, recent\nresearch reveals that these watermarks can be easily removed. Hence, it is\ncrucial to inject robust watermarks resistant to watermark removal attacks.\nExisting watermarking methods typically inject a target embedding into\nembeddings through linear interpolation when the text contains triggers.\nHowever, this mechanism results in each watermarked embedding having the same\ncomponent, which makes the watermark easy to identify and eliminate. Motivated\nby this, in this paper, we propose a novel embedding-specific watermarking\n(ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach\ninvolves injecting unique, yet readily identifiable watermarks into each\nembedding. Watermarks inserted by ESpeW are designed to maintain a significant\ndistance from one another and to avoid sharing common components, thus making\nit significantly more challenging to remove the watermarks. Extensive\nexperiments on four popular datasets demonstrate that ESpeW can even watermark\nsuccessfully against a highly aggressive removal strategy without sacrificing\nthe quality of embeddings. Code is available at\nhttps://github.com/liudan193/ESpeW."
                },
                "authors": [
                    {
                        "name": "Zongqi Wang"
                    },
                    {
                        "name": "Baoyuan Wu"
                    },
                    {
                        "name": "Jingyuan Deng"
                    },
                    {
                        "name": "Yujiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yujiu Yang"
                },
                "author": "Yujiu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16638v2",
                "updated": "2024-10-23T03:41:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    41,
                    49,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-22T02:27:57Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    2,
                    27,
                    57,
                    1,
                    296,
                    0
                ],
                "title": "LLMScan: Causal Scan for LLM Misbehavior Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMScan: Causal Scan for LLM Misbehavior Detection"
                },
                "summary": "Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Large Language Models (LLMs) across various fields,\ntheir potential to generate untruthful, biased and harmful responses poses\nsignificant risks, particularly in critical applications. This highlights the\nurgent need for systematic methods to detect and prevent such misbehavior.\nWhile existing approaches target specific issues such as harmful responses,\nthis work introduces LLMScan, an innovative LLM monitoring technique based on\ncausality analysis, offering a comprehensive solution. LLMScan systematically\nmonitors the inner workings of an LLM through the lens of causal inference,\noperating on the premise that the LLM's `brain' behaves differently when\nmisbehaving. By analyzing the causal contributions of the LLM's input tokens\nand transformer layers, LLMScan effectively detects misbehavior. Extensive\nexperiments across various tasks and models reveal clear distinctions in the\ncausal distributions between normal behavior and misbehavior, enabling the\ndevelopment of accurate, lightweight detectors for a variety of misbehavior\ndetection tasks."
                },
                "authors": [
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Kai Kiat Goh"
                    },
                    {
                        "name": "Peixin Zhang"
                    },
                    {
                        "name": "Jun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jun Sun"
                },
                "author": "Jun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17541v1",
                "updated": "2024-10-23T03:41:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    41,
                    1,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T03:41:01Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    41,
                    1,
                    2,
                    297,
                    0
                ],
                "title": "Improving Connectivity of RIS-Assisted UAV Networks using RIS\n  Partitioning and Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Connectivity of RIS-Assisted UAV Networks using RIS\n  Partitioning and Deployment"
                },
                "summary": "Reconfigurable intelligent surface (RIS) is pivotal for beyond 5G networks in\nregards to the surge demand for reliable communication in unmanned aerial\nvehicle (UAV) networks. This paper presents an innovative approach to maximize\nconnectivity of UAV networks using RIS deployment and virtual partitioning,\nwherein an RIS is deployed to assist in the communications between an\nuser-equipment (UE) and blocked UAVs. Closed-form (CF) expressions for\nsignal-to-noise ratio (SNR) of the two-UAV setup are derived and validated.\nThen, an optimization problem is formulated to maximize network connectivity by\noptimizing the 3D deployment of the RIS and its partitioning subject to\npredefined quality-of-service (QoS) constraints. To tackle this problem, we\npropose a method of virtually partitioning the RIS given a fixed 3D location,\nsuch that the partition phase shifts are configured to create cascaded channels\nbetween the UE and the blocked two UAVs. Then, simulated-annealing (SA) method\nis used to find the 3D location of the RIS. Simulation results demonstrate that\nthe proposed joint RIS deployment and partitioning framework can significantly\nimprove network connectivity compared to benchmarks, including RIS-free and RIS\nwith a single narrow-beam link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) is pivotal for beyond 5G networks in\nregards to the surge demand for reliable communication in unmanned aerial\nvehicle (UAV) networks. This paper presents an innovative approach to maximize\nconnectivity of UAV networks using RIS deployment and virtual partitioning,\nwherein an RIS is deployed to assist in the communications between an\nuser-equipment (UE) and blocked UAVs. Closed-form (CF) expressions for\nsignal-to-noise ratio (SNR) of the two-UAV setup are derived and validated.\nThen, an optimization problem is formulated to maximize network connectivity by\noptimizing the 3D deployment of the RIS and its partitioning subject to\npredefined quality-of-service (QoS) constraints. To tackle this problem, we\npropose a method of virtually partitioning the RIS given a fixed 3D location,\nsuch that the partition phase shifts are configured to create cascaded channels\nbetween the UE and the blocked two UAVs. Then, simulated-annealing (SA) method\nis used to find the 3D location of the RIS. Simulation results demonstrate that\nthe proposed joint RIS deployment and partitioning framework can significantly\nimprove network connectivity compared to benchmarks, including RIS-free and RIS\nwith a single narrow-beam link."
                },
                "authors": [
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Shahrokh Valaee"
                    }
                ],
                "author_detail": {
                    "name": "Shahrokh Valaee"
                },
                "author": "Shahrokh Valaee",
                "arxiv_comment": "6 pages, 5 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05741v2",
                "updated": "2024-10-23T03:39:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    39,
                    0,
                    2,
                    297,
                    0
                ],
                "published": "2024-02-08T15:19:50Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    15,
                    19,
                    50,
                    3,
                    39,
                    0
                ],
                "title": "Real-World Robot Applications of Foundation Models: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-World Robot Applications of Foundation Models: A Review"
                },
                "summary": "Recent developments in foundation models, like Large Language Models (LLMs)\nand Vision-Language Models (VLMs), trained on extensive data, facilitate\nflexible application across different tasks and modalities. Their impact spans\nvarious fields, including healthcare, education, and robotics. This paper\nprovides an overview of the practical application of foundation models in\nreal-world robotics, with a primary emphasis on the replacement of specific\ncomponents within existing robot systems. The summary encompasses the\nperspective of input-output relationships in foundation models, as well as\ntheir role in perception, motion planning, and control within the field of\nrobotics. This paper concludes with a discussion of future challenges and\nimplications for practical robot applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in foundation models, like Large Language Models (LLMs)\nand Vision-Language Models (VLMs), trained on extensive data, facilitate\nflexible application across different tasks and modalities. Their impact spans\nvarious fields, including healthcare, education, and robotics. This paper\nprovides an overview of the practical application of foundation models in\nreal-world robotics, with a primary emphasis on the replacement of specific\ncomponents within existing robot systems. The summary encompasses the\nperspective of input-output relationships in foundation models, as well as\ntheir role in perception, motion planning, and control within the field of\nrobotics. This paper concludes with a discussion of future challenges and\nimplications for practical robot applications."
                },
                "authors": [
                    {
                        "name": "Kento Kawaharazuka"
                    },
                    {
                        "name": "Tatsuya Matsushima"
                    },
                    {
                        "name": "Andrew Gambardella"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Chris Paxton"
                    },
                    {
                        "name": "Andy Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Andy Zeng"
                },
                "author": "Andy Zeng",
                "arxiv_doi": "10.1080/01691864.2024.2408593",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/01691864.2024.2408593",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.05741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17533v1",
                "updated": "2024-10-23T03:25:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    25,
                    55,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T03:25:55Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    25,
                    55,
                    2,
                    297,
                    0
                ],
                "title": "FedGMark: Certifiably Robust Watermarking for Federated Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedGMark: Certifiably Robust Watermarking for Federated Graph Learning"
                },
                "summary": "Federated graph learning (FedGL) is an emerging learning paradigm to\ncollaboratively train graph data from various clients. However, during the\ndevelopment and deployment of FedGL models, they are susceptible to illegal\ncopying and model theft. Backdoor-based watermarking is a well-known method for\nmitigating these attacks, as it offers ownership verification to the model\nowner. We take the first step to protect the ownership of FedGL models via\nbackdoor-based watermarking. Existing techniques have challenges in achieving\nthe goal: 1) they either cannot be directly applied or yield unsatisfactory\nperformance; 2) they are vulnerable to watermark removal attacks; and 3) they\nlack of formal guarantees. To address all the challenges, we propose FedGMark,\nthe first certified robust backdoor-based watermarking for FedGL. FedGMark\nleverages the unique graph structure and client information in FedGL to learn\ncustomized and diverse watermarks. It also designs a novel GL architecture that\nfacilitates defending against both the empirical and theoretically worst-case\nwatermark removal attacks. Extensive experiments validate the promising\nempirical and provable watermarking performance of FedGMark. Source code is\navailable at: https://github.com/Yuxin104/FedGMark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated graph learning (FedGL) is an emerging learning paradigm to\ncollaboratively train graph data from various clients. However, during the\ndevelopment and deployment of FedGL models, they are susceptible to illegal\ncopying and model theft. Backdoor-based watermarking is a well-known method for\nmitigating these attacks, as it offers ownership verification to the model\nowner. We take the first step to protect the ownership of FedGL models via\nbackdoor-based watermarking. Existing techniques have challenges in achieving\nthe goal: 1) they either cannot be directly applied or yield unsatisfactory\nperformance; 2) they are vulnerable to watermark removal attacks; and 3) they\nlack of formal guarantees. To address all the challenges, we propose FedGMark,\nthe first certified robust backdoor-based watermarking for FedGL. FedGMark\nleverages the unique graph structure and client information in FedGL to learn\ncustomized and diverse watermarks. It also designs a novel GL architecture that\nfacilitates defending against both the empirical and theoretically worst-case\nwatermark removal attacks. Extensive experiments validate the promising\nempirical and provable watermarking performance of FedGMark. Source code is\navailable at: https://github.com/Yuxin104/FedGMark."
                },
                "authors": [
                    {
                        "name": "Yuxin Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Yuan Hong"
                    },
                    {
                        "name": "Binghui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Binghui Wang"
                },
                "arxiv_affiliation": "Department of Computer Science, Illinois Institute of Technology",
                "author": "Binghui Wang",
                "arxiv_comment": "This paper is accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17532v1",
                "updated": "2024-10-23T03:19:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    19,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T03:19:15Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    19,
                    15,
                    2,
                    297,
                    0
                ],
                "title": "Responsible Multilingual Large Language Models: A Survey of Development,\n  Applications, and Societal Impact",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsible Multilingual Large Language Models: A Survey of Development,\n  Applications, and Societal Impact"
                },
                "summary": "Multilingual Large Language Models (MLLMs) represent a pivotal advancement in\ndemocratizing artificial intelligence across linguistic boundaries. While\ntheoretical foundations are well-established, practical implementation\nguidelines remain scattered. This work bridges this gap by providing a\ncomprehensive end-to-end framework for developing and deploying MLLMs in\nproduction environments. We make three distinctive contributions: First, we\npresent an actionable pipeline from data pre-processing through deployment,\nintegrating insights from academic research and industrial applications.\nSecond, using Llama2 as a case study, we provide detailed optimization\nstrategies for enhancing multilingual capabilities, including curriculum\nlearning approaches for balancing high-resource and low-resource languages,\ntokenization strategies, and effective sampling methods. Third, we offer an\ninterdisciplinary analysis that considers technical, linguistic, and cultural\nperspectives in MLLM development. Our findings reveal critical challenges in\nsupporting linguistic diversity, with 88.38% of world languages categorized as\nlow-resource, affecting over a billion speakers. We examine practical solutions\nthrough real-world applications in customer service, search engines, and\nmachine translation. By synthesizing theoretical frameworks with\nproduction-ready implementation strategies, this survey provides essential\nguidance for practitioners and researchers working to develop more inclusive\nand effective multilingual AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (MLLMs) represent a pivotal advancement in\ndemocratizing artificial intelligence across linguistic boundaries. While\ntheoretical foundations are well-established, practical implementation\nguidelines remain scattered. This work bridges this gap by providing a\ncomprehensive end-to-end framework for developing and deploying MLLMs in\nproduction environments. We make three distinctive contributions: First, we\npresent an actionable pipeline from data pre-processing through deployment,\nintegrating insights from academic research and industrial applications.\nSecond, using Llama2 as a case study, we provide detailed optimization\nstrategies for enhancing multilingual capabilities, including curriculum\nlearning approaches for balancing high-resource and low-resource languages,\ntokenization strategies, and effective sampling methods. Third, we offer an\ninterdisciplinary analysis that considers technical, linguistic, and cultural\nperspectives in MLLM development. Our findings reveal critical challenges in\nsupporting linguistic diversity, with 88.38% of world languages categorized as\nlow-resource, affecting over a billion speakers. We examine practical solutions\nthrough real-world applications in customer service, search engines, and\nmachine translation. By synthesizing theoretical frameworks with\nproduction-ready implementation strategies, this survey provides essential\nguidance for practitioners and researchers working to develop more inclusive\nand effective multilingual AI systems."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Bin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Fu"
                },
                "author": "Bin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17529v1",
                "updated": "2024-10-23T03:14:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    14,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T03:14:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    14,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Navigate Complex Physical Worlds via Geometrically Constrained LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigate Complex Physical Worlds via Geometrically Constrained LLM"
                },
                "summary": "This study investigates the potential of Large Language Models (LLMs) for\nreconstructing and constructing the physical world solely based on textual\nknowledge. It explores the impact of model performance on spatial understanding\nabilities. To enhance the comprehension of geometric and spatial relationships\nin the complex physical world, the study introduces a set of geometric\nconventions and develops a workflow based on multi-layer graphs and multi-agent\nsystem frameworks. It examines how LLMs achieve multi-step and multi-objective\ngeometric inference in a spatial environment using multi-layer graphs under\nunified geometric conventions. Additionally, the study employs a genetic\nalgorithm, inspired by large-scale model knowledge, to solve geometric\nconstraint problems. In summary, this work innovatively explores the\nfeasibility of using text-based LLMs as physical world builders and designs a\nworkflow to enhance their capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the potential of Large Language Models (LLMs) for\nreconstructing and constructing the physical world solely based on textual\nknowledge. It explores the impact of model performance on spatial understanding\nabilities. To enhance the comprehension of geometric and spatial relationships\nin the complex physical world, the study introduces a set of geometric\nconventions and develops a workflow based on multi-layer graphs and multi-agent\nsystem frameworks. It examines how LLMs achieve multi-step and multi-objective\ngeometric inference in a spatial environment using multi-layer graphs under\nunified geometric conventions. Additionally, the study employs a genetic\nalgorithm, inspired by large-scale model knowledge, to solve geometric\nconstraint problems. In summary, this work innovatively explores the\nfeasibility of using text-based LLMs as physical world builders and designs a\nworkflow to enhance their capabilities."
                },
                "authors": [
                    {
                        "name": "Yongqiang Huang"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14515v2",
                "updated": "2024-10-23T03:09:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    9,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-06-20T17:26:01Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    26,
                    1,
                    3,
                    172,
                    0
                ],
                "title": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video\n  Understanding"
                },
                "summary": "The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large vision-language models (LVLMs) has spurred research into\ntheir applications in multi-modal contexts, particularly in video\nunderstanding. Traditional VideoQA benchmarks, despite providing quantitative\nmetrics, often fail to encompass the full spectrum of video content and\ninadequately assess models' temporal comprehension. To address these\nlimitations, we introduce MMBench-Video, a quantitative benchmark designed to\nrigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video\nincorporates lengthy videos from YouTube and employs free-form questions,\nmirroring practical use cases. The benchmark is meticulously crafted to probe\nthe models' temporal reasoning skills, with all questions human-annotated\naccording to a carefully constructed ability taxonomy. We employ GPT-4 for\nautomated assessment, demonstrating superior accuracy and robustness over\nearlier LLM-based evaluations. Utilizing MMBench-Video, we have conducted\ncomprehensive evaluations that include both proprietary and open-source LVLMs\nfor images and videos. MMBench-Video stands as a valuable resource for the\nresearch community, facilitating improved evaluation of LVLMs and catalyzing\nprogress in the field of video understanding. The evalutation code of\nMMBench-Video will be integrated into VLMEvalKit:\nhttps://github.com/open-compass/VLMEvalKit."
                },
                "authors": [
                    {
                        "name": "Xinyu Fang"
                    },
                    {
                        "name": "Kangrui Mao"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yining Li"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Accepted in NeurIPS 2024 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14687v2",
                "updated": "2024-10-23T03:05:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    5,
                    37,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-03T14:17:43Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    17,
                    43,
                    3,
                    277,
                    0
                ],
                "title": "BrainTransformers: SNN-LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BrainTransformers: SNN-LLM"
                },
                "summary": "This study introduces BrainTransformers, an innovative Large Language Model\n(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions\ninclude: (1) designing SNN-compatible Transformer components such as SNNMatmul,\nSNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU\nactivation function; and (3) developing a Synapsis module to simulate synaptic\nplasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,\ndemonstrates competitive performance across various benchmarks, including MMLU\n(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering\nimproved energy efficiency and biological plausibility. The model employs a\nthree-stage training approach, including SNN-specific neuronal synaptic\nplasticity training. This research opens new avenues for brain-like AI systems\nin natural language processing and neuromorphic computing. Future work will\nfocus on hardware optimization, developing specialized SNN fine-tuning tools,\nand exploring practical applications in energy-efficient computing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces BrainTransformers, an innovative Large Language Model\n(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions\ninclude: (1) designing SNN-compatible Transformer components such as SNNMatmul,\nSNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU\nactivation function; and (3) developing a Synapsis module to simulate synaptic\nplasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,\ndemonstrates competitive performance across various benchmarks, including MMLU\n(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering\nimproved energy efficiency and biological plausibility. The model employs a\nthree-stage training approach, including SNN-specific neuronal synaptic\nplasticity training. This research opens new avenues for brain-like AI systems\nin natural language processing and neuromorphic computing. Future work will\nfocus on hardware optimization, developing specialized SNN fine-tuning tools,\nand exploring practical applications in energy-efficient computing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zhengzheng Tang"
                    },
                    {
                        "name": "Eva Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Eva Zhu"
                },
                "author": "Eva Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17525v1",
                "updated": "2024-10-23T03:04:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    4,
                    24,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T03:04:24Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    3,
                    4,
                    24,
                    2,
                    297,
                    0
                ],
                "title": "Physics-driven AI for Channel Estimation in Cellular Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-driven AI for Channel Estimation in Cellular Network"
                },
                "summary": "In cellular mobile networks, wireless channel quality (CQ) is a crucial\nfactor in determining communication performance and user's network experience.\nAccurately predicting CQ based on real environmental characteristics, specific\nbase station configurations and user trajectories can help network operators\noptimize base station deployment, improving coverage and capacity. The Received\nSignal Reference Power (RSRP) and Signal-to-Interference-plus-Noise Ratio\n(SINR) of user equipment (UE) are key indicators of CQ in wireless\ncommunication. However, existing researches have limitations in terms of\ngeneration accuracy. Regression methods such as statistical inference and\nrandom forests fail to effectively capture the unique characteristics of\nwireless environments; theoretical derivations relying on specific\ncommunication protocols lack generalization capability; data-driven machine\nlearning (ML) methods like Long Short-Term Memory (LSTM) Network often suffer\nfrom a lack of interpretability. To overcome these limitations, we propose\nphysics-informed diffusion models, which accurately generate RSRP and SINR at\nUE based on the wireless environment, base station configurations, and user\ntrajectories. The model adopts a modular and end-to-end design, employing a\nteacher-student framework to achieve knowledge distillation. This method\nintegrates expert knowledge into the training of diffusion models, enhancing\nboth the interpretability and accuracy, while also facilitating faster\nconvergence of the model parameters. Furthermore, it allows for self-adaptation\nin various scenarios through few-shot learning. This approach provides valuable\nguidance for optimizing base station deployment, predicting user network\nexperience, and building real-world simulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cellular mobile networks, wireless channel quality (CQ) is a crucial\nfactor in determining communication performance and user's network experience.\nAccurately predicting CQ based on real environmental characteristics, specific\nbase station configurations and user trajectories can help network operators\noptimize base station deployment, improving coverage and capacity. The Received\nSignal Reference Power (RSRP) and Signal-to-Interference-plus-Noise Ratio\n(SINR) of user equipment (UE) are key indicators of CQ in wireless\ncommunication. However, existing researches have limitations in terms of\ngeneration accuracy. Regression methods such as statistical inference and\nrandom forests fail to effectively capture the unique characteristics of\nwireless environments; theoretical derivations relying on specific\ncommunication protocols lack generalization capability; data-driven machine\nlearning (ML) methods like Long Short-Term Memory (LSTM) Network often suffer\nfrom a lack of interpretability. To overcome these limitations, we propose\nphysics-informed diffusion models, which accurately generate RSRP and SINR at\nUE based on the wireless environment, base station configurations, and user\ntrajectories. The model adopts a modular and end-to-end design, employing a\nteacher-student framework to achieve knowledge distillation. This method\nintegrates expert knowledge into the training of diffusion models, enhancing\nboth the interpretability and accuracy, while also facilitating faster\nconvergence of the model parameters. Furthermore, it allows for self-adaptation\nin various scenarios through few-shot learning. This approach provides valuable\nguidance for optimizing base station deployment, predicting user network\nexperience, and building real-world simulators."
                },
                "authors": [
                    {
                        "name": "Xiaoqian Qi"
                    },
                    {
                        "name": "Haoye Chai"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16534v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16534v2",
                "updated": "2024-10-23T02:55:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    55,
                    14,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-21T21:48:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    48,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "No more hard prompts: SoftSRV prompting for synthetic data generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No more hard prompts: SoftSRV prompting for synthetic data generation"
                },
                "summary": "We present a novel soft prompt based framework, SoftSRV, that leverages a\nfrozen pre-trained large language model (LLM) to generate targeted synthetic\ntext sequences. Given a sample from the target distribution, our proposed\nframework uses data-driven loss minimization to train a parameterized\n\"contextual\" soft prompt. This soft prompt is then used to steer the frozen LLM\nto generate synthetic sequences that are similar to the target distribution. We\nargue that SoftSRV provides a practical improvement over common hard-prompting\napproaches that rely on human-curated prompt-templates, which can be\nidiosyncratic, labor-intensive to craft, and may need to be specialized per\ndomain. We empirically evaluate SoftSRV and hard-prompting baselines by\ngenerating synthetic data to fine-tune a small Gemma model on three different\ndomains (coding, math, reasoning). To stress the generality of SoftSRV, we\nperform these evaluations without any particular specialization of the\nframework to each domain. We find that SoftSRV significantly improves upon\nhard-prompting baselines, generating data with superior fine-tuning performance\nand that better matches the target distribution according to the MAUVE\nsimilarity metric.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel soft prompt based framework, SoftSRV, that leverages a\nfrozen pre-trained large language model (LLM) to generate targeted synthetic\ntext sequences. Given a sample from the target distribution, our proposed\nframework uses data-driven loss minimization to train a parameterized\n\"contextual\" soft prompt. This soft prompt is then used to steer the frozen LLM\nto generate synthetic sequences that are similar to the target distribution. We\nargue that SoftSRV provides a practical improvement over common hard-prompting\napproaches that rely on human-curated prompt-templates, which can be\nidiosyncratic, labor-intensive to craft, and may need to be specialized per\ndomain. We empirically evaluate SoftSRV and hard-prompting baselines by\ngenerating synthetic data to fine-tune a small Gemma model on three different\ndomains (coding, math, reasoning). To stress the generality of SoftSRV, we\nperform these evaluations without any particular specialization of the\nframework to each domain. We find that SoftSRV significantly improves upon\nhard-prompting baselines, generating data with superior fine-tuning performance\nand that better matches the target distribution according to the MAUVE\nsimilarity metric."
                },
                "authors": [
                    {
                        "name": "Giulia DeSalvo"
                    },
                    {
                        "name": "Jean-Fracois Kagy"
                    },
                    {
                        "name": "Lazaros Karydas"
                    },
                    {
                        "name": "Afshin Rostamizadeh"
                    },
                    {
                        "name": "Sanjiv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiv Kumar"
                },
                "author": "Sanjiv Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16534v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17520v1",
                "updated": "2024-10-23T02:51:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    51,
                    43,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T02:51:43Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    51,
                    43,
                    2,
                    297,
                    0
                ],
                "title": "MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile\n  Device Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile\n  Device Control"
                },
                "summary": "Autonomous agents powered by large language models (LLMs) show promising\npotential in assistive tasks across various domains, including mobile device\ncontrol. As these agents interact directly with personal information and device\nsettings, ensuring their safe and reliable behavior is crucial to prevent\nundesirable outcomes. However, no benchmark exists for standardized evaluation\nof the safety of mobile device-control agents. In this work, we introduce\nMobileSafetyBench, a benchmark designed to evaluate the safety of\ndevice-control agents within a realistic mobile environment based on Android\nemulators. We develop a diverse set of tasks involving interactions with\nvarious mobile applications, including messaging and banking applications. To\nclearly evaluate safety apart from general capabilities, we design separate\ntasks measuring safety and tasks evaluating helpfulness. The safety tasks\nchallenge agents with managing potential risks prevalent in daily life and\ninclude tests to evaluate robustness against indirect prompt injections. Our\nexperiments demonstrate that while baseline agents, based on state-of-the-art\nLLMs, perform well in executing helpful tasks, they show poor performance in\nsafety tasks. To mitigate these safety concerns, we propose a prompting method\nthat encourages agents to prioritize safety considerations. While this method\nshows promise in promoting safer behaviors, there is still considerable room\nfor improvement to fully earn user trust. This highlights the urgent need for\ncontinued research to develop more robust safety mechanisms in mobile\nenvironments. We open-source our benchmark at:\nhttps://mobilesafetybench.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents powered by large language models (LLMs) show promising\npotential in assistive tasks across various domains, including mobile device\ncontrol. As these agents interact directly with personal information and device\nsettings, ensuring their safe and reliable behavior is crucial to prevent\nundesirable outcomes. However, no benchmark exists for standardized evaluation\nof the safety of mobile device-control agents. In this work, we introduce\nMobileSafetyBench, a benchmark designed to evaluate the safety of\ndevice-control agents within a realistic mobile environment based on Android\nemulators. We develop a diverse set of tasks involving interactions with\nvarious mobile applications, including messaging and banking applications. To\nclearly evaluate safety apart from general capabilities, we design separate\ntasks measuring safety and tasks evaluating helpfulness. The safety tasks\nchallenge agents with managing potential risks prevalent in daily life and\ninclude tests to evaluate robustness against indirect prompt injections. Our\nexperiments demonstrate that while baseline agents, based on state-of-the-art\nLLMs, perform well in executing helpful tasks, they show poor performance in\nsafety tasks. To mitigate these safety concerns, we propose a prompting method\nthat encourages agents to prioritize safety considerations. While this method\nshows promise in promoting safer behaviors, there is still considerable room\nfor improvement to fully earn user trust. This highlights the urgent need for\ncontinued research to develop more robust safety mechanisms in mobile\nenvironments. We open-source our benchmark at:\nhttps://mobilesafetybench.github.io/."
                },
                "authors": [
                    {
                        "name": "Juyong Lee"
                    },
                    {
                        "name": "Dongyoon Hahm"
                    },
                    {
                        "name": "June Suk Choi"
                    },
                    {
                        "name": "W. Bradley Knox"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17519v1",
                "updated": "2024-10-23T02:51:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    51,
                    33,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T02:51:33Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    51,
                    33,
                    2,
                    297,
                    0
                ],
                "title": "Large Language Models Still Exhibit Bias in Long Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Still Exhibit Bias in Long Text"
                },
                "summary": "Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing fairness benchmarks for large language models (LLMs) primarily focus\non simple tasks, such as multiple-choice questions, overlooking biases that may\narise in more complex scenarios like long-text generation. To address this gap,\nwe introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates\nbiases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10\ndemographic axes, including gender and race, resulting in 11,948 samples. By\nassessing both model responses and the reasoning behind them, LTF-TEST uncovers\nsubtle biases that are difficult to detect in simple responses. In our\nevaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two\nkey patterns of bias. First, these models frequently favor certain demographic\ngroups in their responses. Second, they show excessive sensitivity toward\ntraditionally disadvantaged groups, often providing overly protective responses\nwhile neglecting others. To mitigate these biases, we propose FT-REGARD, a\nfinetuning approach that pairs biased prompts with neutral responses. FT-REGARD\nreduces gender bias by 34.6% and improves performance by 1.4 percentage points\non the BBQ benchmark, offering a promising approach to addressing biases in\nlong-text generation tasks."
                },
                "authors": [
                    {
                        "name": "Wonje Jeung"
                    },
                    {
                        "name": "Dongjae Jeon"
                    },
                    {
                        "name": "Ashkan Yousefpour"
                    },
                    {
                        "name": "Jonghyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jonghyun Choi"
                },
                "author": "Jonghyun Choi",
                "arxiv_comment": "22 page, 38 figures, Neurips (SoLaR Workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20279v2",
                "updated": "2024-10-23T02:38:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    2,
                    38,
                    44,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-30T17:33:10Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    17,
                    33,
                    10,
                    3,
                    151,
                    0
                ],
                "title": "CV-VAE: A Compatible Video VAE for Latent Generative Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CV-VAE: A Compatible Video VAE for Latent Generative Video Models"
                },
                "summary": "Spatio-temporal compression of videos, utilizing networks such as Variational\nAutoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other\nvideo generative models. For instance, many LLM-like video models learn the\ndistribution of discrete tokens derived from 3D VAEs within the VQVAE\nframework, while most diffusion-based video models capture the distribution of\ncontinuous latent extracted by 2D VAEs without quantization. The temporal\ncompression is simply realized by uniform frame sampling which results in\nunsmooth motion between consecutive frames. Currently, there lacks of a\ncommonly used continuous video (3D) VAE for latent diffusion-based video models\nin the research community. Moreover, since current diffusion-based approaches\nare often implemented using pre-trained text-to-image (T2I) models, directly\ntraining a video VAE without considering the compatibility with existing T2I\nmodels will result in a latent space gap between them, which will take huge\ncomputational resources for training to bridge the gap even with the T2I models\nas initialization. To address this issue, we propose a method for training a\nvideo VAE of latent video models, namely CV-VAE, whose latent space is\ncompatible with that of a given image VAE, e.g., image VAE of Stable Diffusion\n(SD). The compatibility is achieved by the proposed novel latent space\nregularization, which involves formulating a regularization loss using the\nimage VAE. Benefiting from the latent space compatibility, video models can be\ntrained seamlessly from pre-trained T2I or video models in a truly\nspatio-temporally compressed latent space, rather than simply sampling video\nframes at equal intervals. With our CV-VAE, existing video models can generate\nfour times more frames with minimal finetuning. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed video VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal compression of videos, utilizing networks such as Variational\nAutoencoders (VAE), plays a crucial role in OpenAI's SORA and numerous other\nvideo generative models. For instance, many LLM-like video models learn the\ndistribution of discrete tokens derived from 3D VAEs within the VQVAE\nframework, while most diffusion-based video models capture the distribution of\ncontinuous latent extracted by 2D VAEs without quantization. The temporal\ncompression is simply realized by uniform frame sampling which results in\nunsmooth motion between consecutive frames. Currently, there lacks of a\ncommonly used continuous video (3D) VAE for latent diffusion-based video models\nin the research community. Moreover, since current diffusion-based approaches\nare often implemented using pre-trained text-to-image (T2I) models, directly\ntraining a video VAE without considering the compatibility with existing T2I\nmodels will result in a latent space gap between them, which will take huge\ncomputational resources for training to bridge the gap even with the T2I models\nas initialization. To address this issue, we propose a method for training a\nvideo VAE of latent video models, namely CV-VAE, whose latent space is\ncompatible with that of a given image VAE, e.g., image VAE of Stable Diffusion\n(SD). The compatibility is achieved by the proposed novel latent space\nregularization, which involves formulating a regularization loss using the\nimage VAE. Benefiting from the latent space compatibility, video models can be\ntrained seamlessly from pre-trained T2I or video models in a truly\nspatio-temporally compressed latent space, rather than simply sampling video\nframes at equal intervals. With our CV-VAE, existing video models can generate\nfour times more frames with minimal finetuning. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed video VAE."
                },
                "authors": [
                    {
                        "name": "Sijie Zhao"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Shaoshu Yang"
                    },
                    {
                        "name": "Muyao Niu"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project Page: https://ailab-cvc.github.io/cvvae/index.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]